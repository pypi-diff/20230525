# Comparing `tmp/onnxruntime_azure-1.14.0-cp39-cp39-win_amd64.whl.zip` & `tmp/onnxruntime_azure-1.15.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,212 +1,228 @@
-Zip file size: 6762003 bytes, number of entries: 210
--rw-rw-rw-  2.0 fat     1094 b- defN 23-Feb-09 21:43 onnxruntime/LICENSE
--rw-rw-rw-  2.0 fat     2490 b- defN 23-Feb-09 21:43 onnxruntime/Privacy.md
--rw-rw-rw-  2.0 fat   305548 b- defN 23-Feb-09 21:43 onnxruntime/ThirdPartyNotices.txt
--rw-rw-rw-  2.0 fat     2670 b- defN 23-Feb-09 21:43 onnxruntime/__init__.py
--rw-rw-rw-  2.0 fat      320 b- defN 23-Feb-09 21:43 onnxruntime/backend/__init__.py
--rw-rw-rw-  2.0 fat     8145 b- defN 23-Feb-09 21:43 onnxruntime/backend/backend.py
--rw-rw-rw-  2.0 fat     1816 b- defN 23-Feb-09 21:43 onnxruntime/backend/backend_rep.py
--rw-rw-rw-  2.0 fat      251 b- defN 23-Feb-09 21:43 onnxruntime/capi/__init__.py
--rw-rw-rw-  2.0 fat      864 b- defN 23-Feb-09 21:49 onnxruntime/capi/_ld_preload.py
--rw-rw-rw-  2.0 fat     1544 b- defN 23-Feb-09 21:43 onnxruntime/capi/_pybind_state.py
--rw-rw-rw-  2.0 fat     4064 b- defN 23-Feb-09 21:43 onnxruntime/capi/onnxruntime_collect_build_info.py
--rw-rw-rw-  2.0 fat    38915 b- defN 23-Feb-09 21:43 onnxruntime/capi/onnxruntime_inference_collection.py
--rw-rw-rw-  2.0 fat    21912 b- defN 23-Feb-09 21:48 onnxruntime/capi/onnxruntime_providers_shared.dll
--rw-rw-rw-  2.0 fat 19383200 b- defN 23-Feb-09 21:48 onnxruntime/capi/onnxruntime_pybind11_state.pyd
--rw-rw-rw-  2.0 fat     6336 b- defN 23-Feb-09 21:43 onnxruntime/capi/onnxruntime_validation.py
--rw-rw-rw-  2.0 fat       33 b- defN 23-Feb-09 21:43 onnxruntime/capi/version_info.py
--rw-rw-rw-  2.0 fat      326 b- defN 23-Feb-09 21:43 onnxruntime/capi/training/__init__.py
--rw-rw-rw-  2.0 fat      480 b- defN 23-Feb-09 21:43 onnxruntime/datasets/__init__.py
--rw-rw-rw-  2.0 fat      670 b- defN 23-Feb-09 21:43 onnxruntime/datasets/logreg_iris.onnx
--rw-rw-rw-  2.0 fat      130 b- defN 23-Feb-09 21:43 onnxruntime/datasets/mul_1.onnx
--rw-rw-rw-  2.0 fat      103 b- defN 23-Feb-09 21:43 onnxruntime/datasets/sigmoid.onnx
--rw-rw-rw-  2.0 fat      441 b- defN 23-Feb-09 21:43 onnxruntime/quantization/__init__.py
--rw-rw-rw-  2.0 fat    38106 b- defN 23-Feb-09 21:43 onnxruntime/quantization/calibrate.py
--rw-rw-rw-  2.0 fat    18572 b- defN 23-Feb-09 21:43 onnxruntime/quantization/onnx_model.py
--rw-rw-rw-  2.0 fat    46368 b- defN 23-Feb-09 21:43 onnxruntime/quantization/onnx_quantizer.py
--rw-rw-rw-  2.0 fat     5045 b- defN 23-Feb-09 21:43 onnxruntime/quantization/preprocess.py
--rw-rw-rw-  2.0 fat    15308 b- defN 23-Feb-09 21:43 onnxruntime/quantization/qdq_loss_debug.py
--rw-rw-rw-  2.0 fat    18436 b- defN 23-Feb-09 21:43 onnxruntime/quantization/qdq_quantizer.py
--rw-rw-rw-  2.0 fat    19882 b- defN 23-Feb-09 21:43 onnxruntime/quantization/quant_utils.py
--rw-rw-rw-  2.0 fat    27812 b- defN 23-Feb-09 21:43 onnxruntime/quantization/quantize.py
--rw-rw-rw-  2.0 fat     3498 b- defN 23-Feb-09 21:43 onnxruntime/quantization/registry.py
--rw-rw-rw-  2.0 fat     6154 b- defN 23-Feb-09 21:43 onnxruntime/quantization/shape_inference.py
--rw-rw-rw-  2.0 fat     2076 b- defN 23-Feb-09 21:43 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
--rw-rw-rw-  2.0 fat     2477 b- defN 23-Feb-09 21:43 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-09 21:43 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
--rw-rw-rw-  2.0 fat       85 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/__init__.py
--rw-rw-rw-  2.0 fat     4495 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/activation.py
--rw-rw-rw-  2.0 fat      589 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/argmax.py
--rw-rw-rw-  2.0 fat     2625 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/attention.py
--rw-rw-rw-  2.0 fat     1118 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/base_operator.py
--rw-rw-rw-  2.0 fat     2536 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/binary_op.py
--rw-rw-rw-  2.0 fat     2096 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/concat.py
--rw-rw-rw-  2.0 fat     9713 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/conv.py
--rw-rw-rw-  2.0 fat     3350 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/direct_q8.py
--rw-rw-rw-  2.0 fat     4046 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/embed_layernorm.py
--rw-rw-rw-  2.0 fat     2166 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/gather.py
--rw-rw-rw-  2.0 fat     2451 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/gavgpool.py
--rw-rw-rw-  2.0 fat     5829 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/gemm.py
--rw-rw-rw-  2.0 fat     4786 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/lstm.py
--rw-rw-rw-  2.0 fat     7796 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/matmul.py
--rw-rw-rw-  2.0 fat      961 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/maxpool.py
--rw-rw-rw-  2.0 fat     4297 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/pad.py
--rw-rw-rw-  2.0 fat     2291 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/pooling.py
--rw-rw-rw-  2.0 fat      840 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/qdq_base_operator.py
--rw-rw-rw-  2.0 fat      962 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/resize.py
--rw-rw-rw-  2.0 fat     3165 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/softmax.py
--rw-rw-rw-  2.0 fat     2250 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/split.py
--rw-rw-rw-  2.0 fat     3133 b- defN 23-Feb-09 21:43 onnxruntime/quantization/operators/where.py
--rw-rw-rw-  2.0 fat      522 b- defN 23-Feb-09 21:43 onnxruntime/tools/__init__.py
--rw-rw-rw-  2.0 fat     2868 b- defN 23-Feb-09 21:43 onnxruntime/tools/check_onnx_model_mobile_usability.py
--rw-rw-rw-  2.0 fat    16132 b- defN 23-Feb-09 21:43 onnxruntime/tools/convert_onnx_models_to_ort.py
--rw-rw-rw-  2.0 fat     1569 b- defN 23-Feb-09 21:43 onnxruntime/tools/file_utils.py
--rw-rw-rw-  2.0 fat      286 b- defN 23-Feb-09 21:43 onnxruntime/tools/logger.py
--rw-rw-rw-  2.0 fat     2608 b- defN 23-Feb-09 21:43 onnxruntime/tools/make_dynamic_shape_fixed.py
--rw-rw-rw-  2.0 fat    14368 b- defN 23-Feb-09 21:43 onnxruntime/tools/onnx_model_utils.py
--rw-rw-rw-  2.0 fat     3361 b- defN 23-Feb-09 21:43 onnxruntime/tools/onnx_randomizer.py
--rw-rw-rw-  2.0 fat     5667 b- defN 23-Feb-09 21:43 onnxruntime/tools/onnxruntime_test.py
--rw-rw-rw-  2.0 fat     1949 b- defN 23-Feb-09 21:43 onnxruntime/tools/optimize_onnx_model.py
--rw-rw-rw-  2.0 fat     4077 b- defN 23-Feb-09 21:43 onnxruntime/tools/pytorch_export_contrib_ops.py
--rw-rw-rw-  2.0 fat     5970 b- defN 23-Feb-09 21:43 onnxruntime/tools/pytorch_export_helpers.py
--rw-rw-rw-  2.0 fat    10160 b- defN 23-Feb-09 21:43 onnxruntime/tools/reduced_build_config_parser.py
--rw-rw-rw-  2.0 fat   116516 b- defN 23-Feb-09 21:43 onnxruntime/tools/symbolic_shape_infer.py
--rw-rw-rw-  2.0 fat     1182 b- defN 23-Feb-09 21:43 onnxruntime/tools/update_onnx_opset.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-09 21:43 onnxruntime/tools/mobile_helpers/__init__.py
--rw-rw-rw-  2.0 fat    12697 b- defN 23-Feb-09 21:43 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
--rw-rw-rw-  2.0 fat      982 b- defN 23-Feb-09 21:43 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
--rw-rw-rw-  2.0 fat     3069 b- defN 23-Feb-09 21:43 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
--rw-rw-rw-  2.0 fat     2179 b- defN 23-Feb-09 21:43 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
--rw-rw-rw-  2.0 fat    25781 b- defN 23-Feb-09 21:43 onnxruntime/tools/mobile_helpers/usability_checker.py
--rw-rw-rw-  2.0 fat     1235 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/__init__.py
--rw-rw-rw-  2.0 fat    27375 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
--rw-rw-rw-  2.0 fat     4472 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_model_processor.py
--rw-rw-rw-  2.0 fat     4172 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/types.py
--rw-rw-rw-  2.0 fat     2614 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
--rw-rw-rw-  2.0 fat      149 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
--rw-rw-rw-  2.0 fat     1611 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
--rw-rw-rw-  2.0 fat     9310 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
--rw-rw-rw-  2.0 fat      348 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
--rw-rw-rw-  2.0 fat     3754 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
--rw-rw-rw-  2.0 fat     1924 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
--rw-rw-rw-  2.0 fat     2939 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
--rw-rw-rw-  2.0 fat     2112 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
--rw-rw-rw-  2.0 fat     1792 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
--rw-rw-rw-  2.0 fat     1988 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
--rw-rw-rw-  2.0 fat      176 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
--rw-rw-rw-  2.0 fat     1076 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
--rw-rw-rw-  2.0 fat     9000 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
--rw-rw-rw-  2.0 fat     2477 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
--rw-rw-rw-  2.0 fat     2532 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
--rw-rw-rw-  2.0 fat     2244 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
--rw-rw-rw-  2.0 fat     1728 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
--rw-rw-rw-  2.0 fat     6145 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
--rw-rw-rw-  2.0 fat     8635 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
--rw-rw-rw-  2.0 fat     3339 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
--rw-rw-rw-  2.0 fat      153 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
--rw-rw-rw-  2.0 fat     4785 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
--rw-rw-rw-  2.0 fat     2664 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
--rw-rw-rw-  2.0 fat     1621 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
--rw-rw-rw-  2.0 fat     3194 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
--rw-rw-rw-  2.0 fat     2954 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
--rw-rw-rw-  2.0 fat     2253 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
--rw-rw-rw-  2.0 fat     1437 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
--rw-rw-rw-  2.0 fat     1889 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
--rw-rw-rw-  2.0 fat     3133 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
--rw-rw-rw-  2.0 fat     1673 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
--rw-rw-rw-  2.0 fat     5144 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
--rw-rw-rw-  2.0 fat      408 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
--rw-rw-rw-  2.0 fat     1828 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
--rw-rw-rw-  2.0 fat     2039 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
--rw-rw-rw-  2.0 fat      200 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
--rw-rw-rw-  2.0 fat     2118 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
--rw-rw-rw-  2.0 fat      259 b- defN 23-Feb-09 21:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Feb-09 21:43 onnxruntime/tools/qdq_helpers/__init__.py
--rw-rw-rw-  2.0 fat     1171 b- defN 23-Feb-09 21:43 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
--rw-rw-rw-  2.0 fat     4925 b- defN 23-Feb-09 21:43 onnxruntime/tools/qdq_helpers/qdq_model_utils.py
--rw-rw-rw-  2.0 fat      512 b- defN 23-Feb-09 21:43 onnxruntime/transformers/__init__.py
--rw-rw-rw-  2.0 fat     1442 b- defN 23-Feb-09 21:43 onnxruntime/transformers/affinity_helper.py
--rw-rw-rw-  2.0 fat    31838 b- defN 23-Feb-09 21:43 onnxruntime/transformers/benchmark.py
--rw-rw-rw-  2.0 fat    20592 b- defN 23-Feb-09 21:43 onnxruntime/transformers/benchmark_helper.py
--rw-rw-rw-  2.0 fat    18322 b- defN 23-Feb-09 21:43 onnxruntime/transformers/bert_perf_test.py
--rw-rw-rw-  2.0 fat    19056 b- defN 23-Feb-09 21:43 onnxruntime/transformers/bert_test_data.py
--rw-rw-rw-  2.0 fat     7695 b- defN 23-Feb-09 21:43 onnxruntime/transformers/compare_bert_results.py
--rw-rw-rw-  2.0 fat    92884 b- defN 23-Feb-09 21:43 onnxruntime/transformers/convert_generation.py
--rw-rw-rw-  2.0 fat     6706 b- defN 23-Feb-09 21:43 onnxruntime/transformers/convert_tf_models_to_pytorch.py
--rw-rw-rw-  2.0 fat    18498 b- defN 23-Feb-09 21:43 onnxruntime/transformers/float16.py
--rw-rw-rw-  2.0 fat    26690 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_attention.py
--rw-rw-rw-  2.0 fat    16227 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_attention_unet.py
--rw-rw-rw-  2.0 fat     2832 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_base.py
--rw-rw-rw-  2.0 fat     2300 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_biasgelu.py
--rw-rw-rw-  2.0 fat     4503 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_biassplitgelu.py
--rw-rw-rw-  2.0 fat    34537 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_embedlayer.py
--rw-rw-rw-  2.0 fat    13324 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_fastgelu.py
--rw-rw-rw-  2.0 fat    10180 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_gelu.py
--rw-rw-rw-  2.0 fat     1062 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_gelu_approximation.py
--rw-rw-rw-  2.0 fat     4262 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_gemmfastgelu.py
--rw-rw-rw-  2.0 fat    22256 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_gpt_attention.py
--rw-rw-rw-  2.0 fat    13845 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_gpt_attention_megatron.py
--rw-rw-rw-  2.0 fat    10981 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_gpt_attention_no_past.py
--rw-rw-rw-  2.0 fat     8412 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_group_norm.py
--rw-rw-rw-  2.0 fat    12199 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_layernorm.py
--rw-rw-rw-  2.0 fat     3592 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_nhwc_conv.py
--rw-rw-rw-  2.0 fat     8946 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_options.py
--rw-rw-rw-  2.0 fat    17162 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_qordered_attention.py
--rw-rw-rw-  2.0 fat     4394 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_qordered_gelu.py
--rw-rw-rw-  2.0 fat     4916 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_qordered_layernorm.py
--rw-rw-rw-  2.0 fat     8567 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_qordered_matmul.py
--rw-rw-rw-  2.0 fat     6463 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_reshape.py
--rw-rw-rw-  2.0 fat     3844 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_shape.py
--rw-rw-rw-  2.0 fat     7878 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_skiplayernorm.py
--rw-rw-rw-  2.0 fat     3019 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_transpose.py
--rw-rw-rw-  2.0 fat    11883 b- defN 23-Feb-09 21:43 onnxruntime/transformers/fusion_utils.py
--rw-rw-rw-  2.0 fat     8774 b- defN 23-Feb-09 21:43 onnxruntime/transformers/huggingface_models.py
--rw-rw-rw-  2.0 fat     7731 b- defN 23-Feb-09 21:43 onnxruntime/transformers/io_binding_helper.py
--rw-rw-rw-  2.0 fat     7294 b- defN 23-Feb-09 21:43 onnxruntime/transformers/machine_info.py
--rw-rw-rw-  2.0 fat    24077 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_exporter.py
--rw-rw-rw-  2.0 fat    45624 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model.py
--rw-rw-rw-  2.0 fat    11903 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_bart.py
--rw-rw-rw-  2.0 fat    20614 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_bert.py
--rw-rw-rw-  2.0 fat    19511 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_bert_keras.py
--rw-rw-rw-  2.0 fat    25536 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_bert_tf.py
--rw-rw-rw-  2.0 fat     3780 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_gpt2.py
--rw-rw-rw-  2.0 fat     8633 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_tnlr.py
--rw-rw-rw-  2.0 fat     6382 b- defN 23-Feb-09 21:43 onnxruntime/transformers/onnx_model_unet.py
--rw-rw-rw-  2.0 fat    17781 b- defN 23-Feb-09 21:43 onnxruntime/transformers/optimizer.py
--rw-rw-rw-  2.0 fat    25015 b- defN 23-Feb-09 21:43 onnxruntime/transformers/profiler.py
--rw-rw-rw-  2.0 fat     2811 b- defN 23-Feb-09 21:43 onnxruntime/transformers/quantize_helper.py
--rw-rw-rw-  2.0 fat     4576 b- defN 23-Feb-09 21:43 onnxruntime/transformers/shape_infer_helper.py
--rw-rw-rw-  2.0 fat    15557 b- defN 23-Feb-09 21:43 onnxruntime/transformers/shape_optimizer.py
--rw-rw-rw-  2.0 fat     2493 b- defN 23-Feb-09 21:43 onnxruntime/transformers/torch_onnx_export_helper.py
--rw-rw-rw-  2.0 fat     4279 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/bart/export.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/bert/__init__.py
--rw-rw-rw-  2.0 fat    10798 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/bert/eval_squad.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/__init__.py
--rw-rw-rw-  2.0 fat    15999 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
--rw-rw-rw-  2.0 fat    20705 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    41748 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/gpt2_helper.py
--rw-rw-rw-  2.0 fat    18248 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/gpt2_parity.py
--rw-rw-rw-  2.0 fat    20109 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/gpt2_tester.py
--rw-rw-rw-  2.0 fat     5876 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/gpt2/parity_check_helper.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/longformer/__init__.py
--rw-rw-rw-  2.0 fat    30347 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/longformer/benchmark_longformer.py
--rw-rw-rw-  2.0 fat    15299 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/longformer/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     9205 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/longformer/generate_test_data.py
--rw-rw-rw-  2.0 fat     3180 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/longformer/longformer_helper.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/stable_diffusion/__init__.py
--rw-rw-rw-  2.0 fat     8431 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/stable_diffusion/benchmark.py
--rw-rw-rw-  2.0 fat     8062 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
--rw-rw-rw-  2.0 fat      252 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/__init__.py
--rw-rw-rw-  2.0 fat     8794 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     3122 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/past_helper.py
--rw-rw-rw-  2.0 fat    17415 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/t5_decoder.py
--rw-rw-rw-  2.0 fat     6407 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/t5_encoder.py
--rw-rw-rw-  2.0 fat    12326 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
--rw-rw-rw-  2.0 fat    10669 b- defN 23-Feb-09 21:43 onnxruntime/transformers/models/t5/t5_helper.py
--rw-rw-rw-  2.0 fat     3957 b- defN 23-Feb-09 21:49 onnxruntime_azure-1.14.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Feb-09 21:49 onnxruntime_azure-1.14.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       78 b- defN 23-Feb-09 21:49 onnxruntime_azure-1.14.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 23-Feb-09 21:49 onnxruntime_azure-1.14.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    22592 b- defN 23-Feb-09 21:49 onnxruntime_azure-1.14.0.dist-info/RECORD
-210 files, 21474533 bytes uncompressed, 6724689 bytes compressed:  68.7%
+Zip file size: 6997881 bytes, number of entries: 226
+-rw-rw-rw-  2.0 fat     1094 b- defN 23-May-24 18:43 onnxruntime/LICENSE
+-rw-rw-rw-  2.0 fat     2490 b- defN 23-May-24 18:43 onnxruntime/Privacy.md
+-rw-rw-rw-  2.0 fat   318315 b- defN 23-May-24 18:43 onnxruntime/ThirdPartyNotices.txt
+-rw-rw-rw-  2.0 fat     4246 b- defN 23-May-24 18:43 onnxruntime/__init__.py
+-rw-rw-rw-  2.0 fat      334 b- defN 23-May-24 18:43 onnxruntime/backend/__init__.py
+-rw-rw-rw-  2.0 fat     8141 b- defN 23-May-24 18:43 onnxruntime/backend/backend.py
+-rw-rw-rw-  2.0 fat     1821 b- defN 23-May-24 18:43 onnxruntime/backend/backend_rep.py
+-rw-rw-rw-  2.0 fat      251 b- defN 23-May-24 18:43 onnxruntime/capi/__init__.py
+-rw-rw-rw-  2.0 fat      413 b- defN 23-May-24 18:43 onnxruntime/capi/_ld_preload.py
+-rw-rw-rw-  2.0 fat     1544 b- defN 23-May-24 18:43 onnxruntime/capi/_pybind_state.py
+-rw-rw-rw-  2.0 fat     4068 b- defN 23-May-24 18:43 onnxruntime/capi/onnxruntime_collect_build_info.py
+-rw-rw-rw-  2.0 fat    39683 b- defN 23-May-24 18:43 onnxruntime/capi/onnxruntime_inference_collection.py
+-rw-rw-rw-  2.0 fat    21936 b- defN 23-May-24 18:48 onnxruntime/capi/onnxruntime_providers_shared.dll
+-rw-rw-rw-  2.0 fat 19976120 b- defN 23-May-24 18:48 onnxruntime/capi/onnxruntime_pybind11_state.pyd
+-rw-rw-rw-  2.0 fat     6382 b- defN 23-May-24 18:43 onnxruntime/capi/onnxruntime_validation.py
+-rw-rw-rw-  2.0 fat       33 b- defN 23-May-24 18:43 onnxruntime/capi/version_info.py
+-rw-rw-rw-  2.0 fat      326 b- defN 23-May-24 18:43 onnxruntime/capi/training/__init__.py
+-rw-rw-rw-  2.0 fat      471 b- defN 23-May-24 18:43 onnxruntime/datasets/__init__.py
+-rw-rw-rw-  2.0 fat      670 b- defN 23-May-24 18:43 onnxruntime/datasets/logreg_iris.onnx
+-rw-rw-rw-  2.0 fat      130 b- defN 23-May-24 18:43 onnxruntime/datasets/mul_1.onnx
+-rw-rw-rw-  2.0 fat      103 b- defN 23-May-24 18:43 onnxruntime/datasets/sigmoid.onnx
+-rw-rw-rw-  2.0 fat      686 b- defN 23-May-24 18:43 onnxruntime/quantization/__init__.py
+-rw-rw-rw-  2.0 fat    38011 b- defN 23-May-24 18:43 onnxruntime/quantization/calibrate.py
+-rw-rw-rw-  2.0 fat    18686 b- defN 23-May-24 18:43 onnxruntime/quantization/onnx_model.py
+-rw-rw-rw-  2.0 fat    46664 b- defN 23-May-24 18:43 onnxruntime/quantization/onnx_quantizer.py
+-rw-rw-rw-  2.0 fat     5045 b- defN 23-May-24 18:43 onnxruntime/quantization/preprocess.py
+-rw-rw-rw-  2.0 fat    15307 b- defN 23-May-24 18:43 onnxruntime/quantization/qdq_loss_debug.py
+-rw-rw-rw-  2.0 fat    19061 b- defN 23-May-24 18:43 onnxruntime/quantization/qdq_quantizer.py
+-rw-rw-rw-  2.0 fat    20102 b- defN 23-May-24 18:43 onnxruntime/quantization/quant_utils.py
+-rw-rw-rw-  2.0 fat    28359 b- defN 23-May-24 18:43 onnxruntime/quantization/quantize.py
+-rw-rw-rw-  2.0 fat     3671 b- defN 23-May-24 18:43 onnxruntime/quantization/registry.py
+-rw-rw-rw-  2.0 fat     6149 b- defN 23-May-24 18:43 onnxruntime/quantization/shape_inference.py
+-rw-rw-rw-  2.0 fat     2250 b- defN 23-May-24 18:43 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+-rw-rw-rw-  2.0 fat     2665 b- defN 23-May-24 18:43 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-24 18:43 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+-rw-rw-rw-  2.0 fat       85 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/__init__.py
+-rw-rw-rw-  2.0 fat     4463 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/activation.py
+-rw-rw-rw-  2.0 fat      589 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/argmax.py
+-rw-rw-rw-  2.0 fat     2637 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/attention.py
+-rw-rw-rw-  2.0 fat     1118 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/base_operator.py
+-rw-rw-rw-  2.0 fat     2544 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/binary_op.py
+-rw-rw-rw-  2.0 fat     2149 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/concat.py
+-rw-rw-rw-  2.0 fat     9695 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/conv.py
+-rw-rw-rw-  2.0 fat     3350 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/direct_q8.py
+-rw-rw-rw-  2.0 fat     4058 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/embed_layernorm.py
+-rw-rw-rw-  2.0 fat     2166 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/gather.py
+-rw-rw-rw-  2.0 fat     2445 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/gavgpool.py
+-rw-rw-rw-  2.0 fat     5905 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/gemm.py
+-rw-rw-rw-  2.0 fat     1114 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/instnorm.py
+-rw-rw-rw-  2.0 fat     5050 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/lstm.py
+-rw-rw-rw-  2.0 fat     7762 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/matmul.py
+-rw-rw-rw-  2.0 fat      961 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/maxpool.py
+-rw-rw-rw-  2.0 fat     4277 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/pad.py
+-rw-rw-rw-  2.0 fat     2285 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/pooling.py
+-rw-rw-rw-  2.0 fat      823 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/qdq_base_operator.py
+-rw-rw-rw-  2.0 fat      962 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/resize.py
+-rw-rw-rw-  2.0 fat     3386 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/softmax.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/split.py
+-rw-rw-rw-  2.0 fat     3127 b- defN 23-May-24 18:43 onnxruntime/quantization/operators/where.py
+-rw-rw-rw-  2.0 fat      528 b- defN 23-May-24 18:43 onnxruntime/tools/__init__.py
+-rw-rw-rw-  2.0 fat     2871 b- defN 23-May-24 18:43 onnxruntime/tools/check_onnx_model_mobile_usability.py
+-rw-rw-rw-  2.0 fat    16064 b- defN 23-May-24 18:43 onnxruntime/tools/convert_onnx_models_to_ort.py
+-rw-rw-rw-  2.0 fat     1569 b- defN 23-May-24 18:43 onnxruntime/tools/file_utils.py
+-rw-rw-rw-  2.0 fat      286 b- defN 23-May-24 18:43 onnxruntime/tools/logger.py
+-rw-rw-rw-  2.0 fat     2608 b- defN 23-May-24 18:43 onnxruntime/tools/make_dynamic_shape_fixed.py
+-rw-rw-rw-  2.0 fat     6380 b- defN 23-May-24 18:43 onnxruntime/tools/offline_tuning.py
+-rw-rw-rw-  2.0 fat    14355 b- defN 23-May-24 18:43 onnxruntime/tools/onnx_model_utils.py
+-rw-rw-rw-  2.0 fat     3361 b- defN 23-May-24 18:43 onnxruntime/tools/onnx_randomizer.py
+-rw-rw-rw-  2.0 fat     5657 b- defN 23-May-24 18:43 onnxruntime/tools/onnxruntime_test.py
+-rw-rw-rw-  2.0 fat     1949 b- defN 23-May-24 18:43 onnxruntime/tools/optimize_onnx_model.py
+-rw-rw-rw-  2.0 fat     4091 b- defN 23-May-24 18:43 onnxruntime/tools/pytorch_export_contrib_ops.py
+-rw-rw-rw-  2.0 fat     5971 b- defN 23-May-24 18:43 onnxruntime/tools/pytorch_export_helpers.py
+-rw-rw-rw-  2.0 fat    10137 b- defN 23-May-24 18:43 onnxruntime/tools/reduced_build_config_parser.py
+-rw-rw-rw-  2.0 fat   129481 b- defN 23-May-24 18:43 onnxruntime/tools/symbolic_shape_infer.py
+-rw-rw-rw-  2.0 fat     1182 b- defN 23-May-24 18:43 onnxruntime/tools/update_onnx_opset.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-24 18:43 onnxruntime/tools/mobile_helpers/__init__.py
+-rw-rw-rw-  2.0 fat    12691 b- defN 23-May-24 18:43 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
+-rw-rw-rw-  2.0 fat     1319 b- defN 23-May-24 18:43 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
+-rw-rw-rw-  2.0 fat     3069 b- defN 23-May-24 18:43 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
+-rw-rw-rw-  2.0 fat     2226 b- defN 23-May-24 18:43 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
+-rw-rw-rw-  2.0 fat    25768 b- defN 23-May-24 18:43 onnxruntime/tools/mobile_helpers/usability_checker.py
+-rw-rw-rw-  2.0 fat     1378 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/__init__.py
+-rw-rw-rw-  2.0 fat    27366 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+-rw-rw-rw-  2.0 fat     4484 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_model_processor.py
+-rw-rw-rw-  2.0 fat     4154 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/types.py
+-rw-rw-rw-  2.0 fat     2604 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+-rw-rw-rw-  2.0 fat      149 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
+-rw-rw-rw-  2.0 fat     1611 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
+-rw-rw-rw-  2.0 fat     9310 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
+-rw-rw-rw-  2.0 fat      348 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
+-rw-rw-rw-  2.0 fat     3754 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
+-rw-rw-rw-  2.0 fat     1924 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
+-rw-rw-rw-  2.0 fat     2939 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
+-rw-rw-rw-  2.0 fat     2112 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
+-rw-rw-rw-  2.0 fat     1792 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
+-rw-rw-rw-  2.0 fat     1988 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
+-rw-rw-rw-  2.0 fat      176 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
+-rw-rw-rw-  2.0 fat     1076 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
+-rw-rw-rw-  2.0 fat     9000 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
+-rw-rw-rw-  2.0 fat     2477 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
+-rw-rw-rw-  2.0 fat     2532 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
+-rw-rw-rw-  2.0 fat     1728 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
+-rw-rw-rw-  2.0 fat     6145 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
+-rw-rw-rw-  2.0 fat     8635 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
+-rw-rw-rw-  2.0 fat     3339 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
+-rw-rw-rw-  2.0 fat      153 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
+-rw-rw-rw-  2.0 fat     4785 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
+-rw-rw-rw-  2.0 fat     2664 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     1621 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
+-rw-rw-rw-  2.0 fat     3194 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
+-rw-rw-rw-  2.0 fat     2954 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
+-rw-rw-rw-  2.0 fat     2253 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
+-rw-rw-rw-  2.0 fat     1437 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
+-rw-rw-rw-  2.0 fat     1889 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
+-rw-rw-rw-  2.0 fat     3133 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
+-rw-rw-rw-  2.0 fat     1673 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
+-rw-rw-rw-  2.0 fat     5144 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
+-rw-rw-rw-  2.0 fat      408 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
+-rw-rw-rw-  2.0 fat     1828 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
+-rw-rw-rw-  2.0 fat     2039 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
+-rw-rw-rw-  2.0 fat      200 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
+-rw-rw-rw-  2.0 fat     2118 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
+-rw-rw-rw-  2.0 fat      253 b- defN 23-May-24 18:43 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-24 18:43 onnxruntime/tools/qdq_helpers/__init__.py
+-rw-rw-rw-  2.0 fat     1279 b- defN 23-May-24 18:43 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
+-rw-rw-rw-  2.0 fat      552 b- defN 23-May-24 18:43 onnxruntime/transformers/__init__.py
+-rw-rw-rw-  2.0 fat     1442 b- defN 23-May-24 18:43 onnxruntime/transformers/affinity_helper.py
+-rw-rw-rw-  2.0 fat    33334 b- defN 23-May-24 18:43 onnxruntime/transformers/benchmark.py
+-rw-rw-rw-  2.0 fat    20969 b- defN 23-May-24 18:43 onnxruntime/transformers/benchmark_helper.py
+-rw-rw-rw-  2.0 fat    19728 b- defN 23-May-24 18:43 onnxruntime/transformers/bert_perf_test.py
+-rw-rw-rw-  2.0 fat    19039 b- defN 23-May-24 18:43 onnxruntime/transformers/bert_test_data.py
+-rw-rw-rw-  2.0 fat     7639 b- defN 23-May-24 18:43 onnxruntime/transformers/compare_bert_results.py
+-rw-rw-rw-  2.0 fat      740 b- defN 23-May-24 18:43 onnxruntime/transformers/constants.py
+-rw-rw-rw-  2.0 fat   108650 b- defN 23-May-24 18:43 onnxruntime/transformers/convert_generation.py
+-rw-rw-rw-  2.0 fat     6705 b- defN 23-May-24 18:43 onnxruntime/transformers/convert_tf_models_to_pytorch.py
+-rw-rw-rw-  2.0 fat    10057 b- defN 23-May-24 18:43 onnxruntime/transformers/convert_to_packing_mode.py
+-rw-rw-rw-  2.0 fat    22806 b- defN 23-May-24 18:43 onnxruntime/transformers/float16.py
+-rw-rw-rw-  2.0 fat    50487 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_attention.py
+-rw-rw-rw-  2.0 fat    18666 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_attention_unet.py
+-rw-rw-rw-  2.0 fat    12600 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_attention_vae.py
+-rw-rw-rw-  2.0 fat    18807 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_bart_attention.py
+-rw-rw-rw-  2.0 fat     2820 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_base.py
+-rw-rw-rw-  2.0 fat     2066 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_bias_add.py
+-rw-rw-rw-  2.0 fat     2300 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_biasgelu.py
+-rw-rw-rw-  2.0 fat     4516 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_biassplitgelu.py
+-rw-rw-rw-  2.0 fat    35646 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_embedlayer.py
+-rw-rw-rw-  2.0 fat    13324 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_fastgelu.py
+-rw-rw-rw-  2.0 fat    10180 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     1076 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_gelu_approximation.py
+-rw-rw-rw-  2.0 fat     4262 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_gemmfastgelu.py
+-rw-rw-rw-  2.0 fat    22327 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_gpt_attention.py
+-rw-rw-rw-  2.0 fat    13887 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_gpt_attention_megatron.py
+-rw-rw-rw-  2.0 fat    11023 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_gpt_attention_no_past.py
+-rw-rw-rw-  2.0 fat     7693 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_group_norm.py
+-rw-rw-rw-  2.0 fat    12194 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat     3592 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_nhwc_conv.py
+-rw-rw-rw-  2.0 fat    10607 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_options.py
+-rw-rw-rw-  2.0 fat    17163 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_qordered_attention.py
+-rw-rw-rw-  2.0 fat     4393 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_qordered_gelu.py
+-rw-rw-rw-  2.0 fat     4915 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_qordered_layernorm.py
+-rw-rw-rw-  2.0 fat     8566 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_qordered_matmul.py
+-rw-rw-rw-  2.0 fat     6463 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_reshape.py
+-rw-rw-rw-  2.0 fat     3845 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_shape.py
+-rw-rw-rw-  2.0 fat     8269 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_skiplayernorm.py
+-rw-rw-rw-  2.0 fat     6980 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_transpose.py
+-rw-rw-rw-  2.0 fat    12229 b- defN 23-May-24 18:43 onnxruntime/transformers/fusion_utils.py
+-rw-rw-rw-  2.0 fat     9130 b- defN 23-May-24 18:43 onnxruntime/transformers/huggingface_models.py
+-rw-rw-rw-  2.0 fat     7726 b- defN 23-May-24 18:43 onnxruntime/transformers/io_binding_helper.py
+-rw-rw-rw-  2.0 fat     7336 b- defN 23-May-24 18:43 onnxruntime/transformers/machine_info.py
+-rw-rw-rw-  2.0 fat    25364 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_exporter.py
+-rw-rw-rw-  2.0 fat    49231 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model.py
+-rw-rw-rw-  2.0 fat     5397 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_bart.py
+-rw-rw-rw-  2.0 fat    20938 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_bert.py
+-rw-rw-rw-  2.0 fat    19132 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_bert_keras.py
+-rw-rw-rw-  2.0 fat    25561 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_bert_tf.py
+-rw-rw-rw-  2.0 fat     1067 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_clip.py
+-rw-rw-rw-  2.0 fat     3747 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_gpt2.py
+-rw-rw-rw-  2.0 fat    31346 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_t5.py
+-rw-rw-rw-  2.0 fat     8682 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_tnlr.py
+-rw-rw-rw-  2.0 fat     7100 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_unet.py
+-rw-rw-rw-  2.0 fat     1515 b- defN 23-May-24 18:43 onnxruntime/transformers/onnx_model_vae.py
+-rw-rw-rw-  2.0 fat    19209 b- defN 23-May-24 18:43 onnxruntime/transformers/optimizer.py
+-rw-rw-rw-  2.0 fat    25009 b- defN 23-May-24 18:43 onnxruntime/transformers/profiler.py
+-rw-rw-rw-  2.0 fat     2825 b- defN 23-May-24 18:43 onnxruntime/transformers/quantize_helper.py
+-rw-rw-rw-  2.0 fat     4590 b- defN 23-May-24 18:43 onnxruntime/transformers/shape_infer_helper.py
+-rw-rw-rw-  2.0 fat    15555 b- defN 23-May-24 18:43 onnxruntime/transformers/shape_optimizer.py
+-rw-rw-rw-  2.0 fat     2507 b- defN 23-May-24 18:43 onnxruntime/transformers/torch_onnx_export_helper.py
+-rw-rw-rw-  2.0 fat     4285 b- defN 23-May-24 18:43 onnxruntime/transformers/models/bart/export.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-24 18:43 onnxruntime/transformers/models/bert/__init__.py
+-rw-rw-rw-  2.0 fat    10783 b- defN 23-May-24 18:43 onnxruntime/transformers/models/bert/eval_squad.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/__init__.py
+-rw-rw-rw-  2.0 fat    16036 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
+-rw-rw-rw-  2.0 fat    20751 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    41547 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/gpt2_helper.py
+-rw-rw-rw-  2.0 fat    18322 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/gpt2_parity.py
+-rw-rw-rw-  2.0 fat    20178 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/gpt2_tester.py
+-rw-rw-rw-  2.0 fat     5906 b- defN 23-May-24 18:43 onnxruntime/transformers/models/gpt2/parity_check_helper.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-24 18:43 onnxruntime/transformers/models/longformer/__init__.py
+-rw-rw-rw-  2.0 fat    30370 b- defN 23-May-24 18:43 onnxruntime/transformers/models/longformer/benchmark_longformer.py
+-rw-rw-rw-  2.0 fat    15342 b- defN 23-May-24 18:43 onnxruntime/transformers/models/longformer/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     9225 b- defN 23-May-24 18:43 onnxruntime/transformers/models/longformer/generate_test_data.py
+-rw-rw-rw-  2.0 fat     3180 b- defN 23-May-24 18:43 onnxruntime/transformers/models/longformer/longformer_helper.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-24 18:43 onnxruntime/transformers/models/stable_diffusion/__init__.py
+-rw-rw-rw-  2.0 fat    21893 b- defN 23-May-24 18:43 onnxruntime/transformers/models/stable_diffusion/benchmark.py
+-rw-rw-rw-  2.0 fat    11932 b- defN 23-May-24 18:43 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
+-rw-rw-rw-  2.0 fat      252 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/__init__.py
+-rw-rw-rw-  2.0 fat     8832 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     6987 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/past_helper.py
+-rw-rw-rw-  2.0 fat    17207 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/t5_decoder.py
+-rw-rw-rw-  2.0 fat     6407 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/t5_encoder.py
+-rw-rw-rw-  2.0 fat    12324 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    11027 b- defN 23-May-24 18:43 onnxruntime/transformers/models/t5/t5_helper.py
+-rw-rw-rw-  2.0 fat      321 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/__init__.py
+-rw-rw-rw-  2.0 fat    11670 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     4441 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/whisper_chain.py
+-rw-rw-rw-  2.0 fat    16818 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/whisper_decoder.py
+-rw-rw-rw-  2.0 fat     5931 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/whisper_encoder.py
+-rw-rw-rw-  2.0 fat    13107 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    10529 b- defN 23-May-24 18:43 onnxruntime/transformers/models/whisper/whisper_helper.py
+-rw-rw-rw-  2.0 fat     4056 b- defN 23-May-24 18:48 onnxruntime_azure-1.15.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-24 18:48 onnxruntime_azure-1.15.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       78 b- defN 23-May-24 18:48 onnxruntime_azure-1.15.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 23-May-24 18:48 onnxruntime_azure-1.15.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    24310 b- defN 23-May-24 18:48 onnxruntime_azure-1.15.0.dist-info/RECORD
+226 files, 22322966 bytes uncompressed, 6957759 bytes compressed:  68.8%
```

## zipnote {}

```diff
@@ -138,14 +138,17 @@
 
 Filename: onnxruntime/quantization/operators/gavgpool.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/gemm.py
 Comment: 
 
+Filename: onnxruntime/quantization/operators/instnorm.py
+Comment: 
+
 Filename: onnxruntime/quantization/operators/lstm.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/matmul.py
 Comment: 
 
 Filename: onnxruntime/quantization/operators/maxpool.py
@@ -186,14 +189,17 @@
 
 Filename: onnxruntime/tools/logger.py
 Comment: 
 
 Filename: onnxruntime/tools/make_dynamic_shape_fixed.py
 Comment: 
 
+Filename: onnxruntime/tools/offline_tuning.py
+Comment: 
+
 Filename: onnxruntime/tools/onnx_model_utils.py
 Comment: 
 
 Filename: onnxruntime/tools/onnx_randomizer.py
 Comment: 
 
 Filename: onnxruntime/tools/onnxruntime_test.py
@@ -369,17 +375,14 @@
 
 Filename: onnxruntime/tools/qdq_helpers/__init__.py
 Comment: 
 
 Filename: onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
 Comment: 
 
-Filename: onnxruntime/tools/qdq_helpers/qdq_model_utils.py
-Comment: 
-
 Filename: onnxruntime/transformers/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/affinity_helper.py
 Comment: 
 
 Filename: onnxruntime/transformers/benchmark.py
@@ -393,32 +396,47 @@
 
 Filename: onnxruntime/transformers/bert_test_data.py
 Comment: 
 
 Filename: onnxruntime/transformers/compare_bert_results.py
 Comment: 
 
+Filename: onnxruntime/transformers/constants.py
+Comment: 
+
 Filename: onnxruntime/transformers/convert_generation.py
 Comment: 
 
 Filename: onnxruntime/transformers/convert_tf_models_to_pytorch.py
 Comment: 
 
+Filename: onnxruntime/transformers/convert_to_packing_mode.py
+Comment: 
+
 Filename: onnxruntime/transformers/float16.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_attention.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_attention_unet.py
 Comment: 
 
+Filename: onnxruntime/transformers/fusion_attention_vae.py
+Comment: 
+
+Filename: onnxruntime/transformers/fusion_bart_attention.py
+Comment: 
+
 Filename: onnxruntime/transformers/fusion_base.py
 Comment: 
 
+Filename: onnxruntime/transformers/fusion_bias_add.py
+Comment: 
+
 Filename: onnxruntime/transformers/fusion_biasgelu.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_biassplitgelu.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_embedlayer.py
@@ -507,23 +525,32 @@
 
 Filename: onnxruntime/transformers/onnx_model_bert_keras.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_bert_tf.py
 Comment: 
 
+Filename: onnxruntime/transformers/onnx_model_clip.py
+Comment: 
+
 Filename: onnxruntime/transformers/onnx_model_gpt2.py
 Comment: 
 
+Filename: onnxruntime/transformers/onnx_model_t5.py
+Comment: 
+
 Filename: onnxruntime/transformers/onnx_model_tnlr.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_unet.py
 Comment: 
 
+Filename: onnxruntime/transformers/onnx_model_vae.py
+Comment: 
+
 Filename: onnxruntime/transformers/optimizer.py
 Comment: 
 
 Filename: onnxruntime/transformers/profiler.py
 Comment: 
 
 Filename: onnxruntime/transformers/quantize_helper.py
@@ -609,23 +636,44 @@
 
 Filename: onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/t5/t5_helper.py
 Comment: 
 
-Filename: onnxruntime_azure-1.14.0.dist-info/METADATA
+Filename: onnxruntime/transformers/models/whisper/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_chain.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_decoder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_encoder.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/whisper/whisper_helper.py
+Comment: 
+
+Filename: onnxruntime_azure-1.15.0.dist-info/METADATA
 Comment: 
 
-Filename: onnxruntime_azure-1.14.0.dist-info/WHEEL
+Filename: onnxruntime_azure-1.15.0.dist-info/WHEEL
 Comment: 
 
-Filename: onnxruntime_azure-1.14.0.dist-info/entry_points.txt
+Filename: onnxruntime_azure-1.15.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: onnxruntime_azure-1.14.0.dist-info/top_level.txt
+Filename: onnxruntime_azure-1.15.0.dist-info/top_level.txt
 Comment: 
 
-Filename: onnxruntime_azure-1.14.0.dist-info/RECORD
+Filename: onnxruntime_azure-1.15.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## onnxruntime/ThirdPartyNotices.txt

```diff
@@ -5418,43 +5418,43 @@
 
    END OF TERMS AND CONDITIONS
 
 _____
 
 Tencent/rapidjson, https://github.com/Tencent/rapidjson
 
-Tencent is pleased to support the open source community by making RapidJSON available. 
- 
+Tencent is pleased to support the open source community by making RapidJSON available.
+
 Copyright (C) 2015 THL A29 Limited, a Tencent company, and Milo Yip.  All rights reserved.
 
 If you have downloaded a copy of the RapidJSON binary from Tencent, please note that the RapidJSON binary is licensed under the MIT License.
 If you have downloaded a copy of the RapidJSON source code from Tencent, please note that RapidJSON source code is licensed under the MIT License, except for the third-party components listed below which are subject to different license terms.  Your integration of RapidJSON into your own projects may require compliance with the MIT License, as well as the other licenses applicable to the third-party components included within RapidJSON. To avoid the problematic JSON license in your own projects, it's sufficient to exclude the bin/jsonchecker/ directory, as it's the only code under the JSON license.
 A copy of the MIT License is included in this file.
 
 Other dependencies and licenses:
 
 Open Source Software Licensed Under the BSD License:
 --------------------------------------------------------------------
 
-The msinttypes r29 
-Copyright (c) 2006-2013 Alexander Chemeris 
+The msinttypes r29
+Copyright (c) 2006-2013 Alexander Chemeris
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
 
-* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 
+* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
 * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
 * Neither the name of  copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
 
 THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 Open Source Software Licensed Under the JSON License:
 --------------------------------------------------------------------
 
-json.org 
+json.org
 Copyright (c) 2002 JSON.org
 All Rights Reserved.
 
 JSON_checker
 Copyright (c) 2002 JSON.org
 All Rights Reserved.
 
@@ -5780,7 +5780,245 @@
 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
+
+_____
+
+TensorFlow.js
+
+https://github.com/tensorflow/tfjs
+
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+——
+
+curl/curl
+
+https://github.com/curl
+
+COPYRIGHT AND PERMISSION NOTICE
+
+Copyright (C) Daniel Stenberg, <daniel@haxx.se>, and many
+contributors, see the THANKS file.
+
+All rights reserved.
+
+Permission to use, copy, modify, and distribute this software for any purpose
+with or without fee is hereby granted, provided that the above copyright
+notice and this permission notice appear in all copies.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS. IN
+NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
+DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE
+OR OTHER DEALINGS IN THE SOFTWARE.
+
+Except as contained in this notice, the name of a copyright holder shall not
+be used in advertising or otherwise to promote the sale, use or other dealings
+in this Software without prior written authorization of the copyright holder.
```

## onnxruntime/__init__.py

```diff
@@ -3,75 +3,73 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_
 or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 """
-__version__ = "1.14.0"
+__version__ = "1.15.0"
 __author__ = "Microsoft"
 
 # we need to do device version validation (for example to check Cuda version for an onnxruntime-training package).
 # in order to know whether the onnxruntime package is for training it needs
 # to do import onnxruntime.training.ortmodule first.
 # onnxruntime.capi._pybind_state is required before import onnxruntime.training.ortmodule.
 # however, import onnxruntime.capi._pybind_state will already raise an exception if a required Cuda version
 # is not found.
 # here we need to save the exception and continue with Cuda version validation in order to post
 # meaningful messages to the user.
 # the saved exception is raised after device version validation.
 try:
-    from onnxruntime.capi._pybind_state import (
-        ExecutionMode,
-        ExecutionOrder,
-        GraphOptimizationLevel,
-        ModelMetadata,
-        NodeArg,
-        OrtAllocatorType,
-        OrtArenaCfg,
-        OrtMemoryInfo,
-        OrtMemType,
-        OrtSparseFormat,
-        RunOptions,
-        SessionIOBinding,
-        SessionOptions,
-        create_and_register_allocator,
-        disable_telemetry_events,
-        enable_telemetry_events,
-        get_all_providers,
-        get_available_providers,
-        get_device,
-        set_default_logger_severity,
-        set_default_logger_verbosity,
-        set_seed,
-    )
+    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401
+    from onnxruntime.capi._pybind_state import ExecutionOrder  # noqa: F401
+    from onnxruntime.capi._pybind_state import GraphOptimizationLevel  # noqa: F401
+    from onnxruntime.capi._pybind_state import ModelMetadata  # noqa: F401
+    from onnxruntime.capi._pybind_state import NodeArg  # noqa: F401
+    from onnxruntime.capi._pybind_state import OrtAllocatorType  # noqa: F401
+    from onnxruntime.capi._pybind_state import OrtArenaCfg  # noqa: F401
+    from onnxruntime.capi._pybind_state import OrtMemoryInfo  # noqa: F401
+    from onnxruntime.capi._pybind_state import OrtMemType  # noqa: F401
+    from onnxruntime.capi._pybind_state import OrtSparseFormat  # noqa: F401
+    from onnxruntime.capi._pybind_state import RunOptions  # noqa: F401
+    from onnxruntime.capi._pybind_state import SessionIOBinding  # noqa: F401
+    from onnxruntime.capi._pybind_state import SessionOptions  # noqa: F401
+    from onnxruntime.capi._pybind_state import create_and_register_allocator  # noqa: F401
+    from onnxruntime.capi._pybind_state import disable_telemetry_events  # noqa: F401
+    from onnxruntime.capi._pybind_state import enable_telemetry_events  # noqa: F401
+    from onnxruntime.capi._pybind_state import get_all_providers  # noqa: F401
+    from onnxruntime.capi._pybind_state import get_available_providers  # noqa: F401
+    from onnxruntime.capi._pybind_state import get_build_info  # noqa: F401
+    from onnxruntime.capi._pybind_state import get_device  # noqa: F401
+    from onnxruntime.capi._pybind_state import get_version_string  # noqa: F401
+    from onnxruntime.capi._pybind_state import set_default_logger_severity  # noqa: F401
+    from onnxruntime.capi._pybind_state import set_default_logger_verbosity  # noqa: F401
+    from onnxruntime.capi._pybind_state import set_seed  # noqa: F401
 
     import_capi_exception = None
 except Exception as e:
     import_capi_exception = e
 
 from onnxruntime.capi import onnxruntime_validation
 
 if import_capi_exception:
     raise import_capi_exception
 
-from onnxruntime.capi.onnxruntime_inference_collection import (
-    InferenceSession,
-    IOBinding,
-    OrtDevice,
-    OrtValue,
-    SparseTensor,
-)
+from onnxruntime.capi.onnxruntime_inference_collection import InferenceSession  # noqa: F401
+from onnxruntime.capi.onnxruntime_inference_collection import IOBinding  # noqa: F401
+from onnxruntime.capi.onnxruntime_inference_collection import OrtDevice  # noqa: F401
+from onnxruntime.capi.onnxruntime_inference_collection import OrtValue  # noqa: F401
+from onnxruntime.capi.onnxruntime_inference_collection import SparseTensor  # noqa: F401
 from onnxruntime.capi.training import *  # noqa: F403
 
 # TODO: thiagofc: Temporary experimental namespace for new PyTorch front-end
-try:
-    from . import experimental
+try:  # noqa: SIM105
+    from . import experimental  # noqa: F401
 except ImportError:
     pass
 
-from onnxruntime.capi.onnxruntime_validation import cuda_version, package_name, version
+from onnxruntime.capi.onnxruntime_validation import cuda_version, package_name, version  # noqa: F401
 
 if version:
     __version__ = version
 
 onnxruntime_validation.check_distro_info()
```

## onnxruntime/backend/__init__.py

```diff
@@ -1,6 +1,6 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-from .backend import is_compatible, prepare, run, supports_device
+from .backend import is_compatible, prepare, run, supports_device  # noqa: F401
```

## onnxruntime/backend/backend.py

```diff
@@ -5,15 +5,15 @@
 """
 Implements ONNX's backend API.
 """
 import os
 import unittest
 
 import packaging.version
-from onnx import ModelProto, helper, version
+from onnx import ModelProto, helper, version  # noqa: F401
 from onnx.backend.base import Backend
 from onnx.checker import check_model
 
 from onnxruntime import InferenceSession, SessionOptions, get_available_providers, get_device
 from onnxruntime.backend.backend_rep import OnnxRuntimeBackendRep
 
 
@@ -23,17 +23,17 @@
     `ONNX's backend API <https://github.com/onnx/onnx/blob/main/docs/ImplementingAnOnnxBackend.md>`_
     with *ONNX Runtime*.
     The backend is mostly used when you need to switch between
     multiple runtimes with the same API.
     `Importing models from ONNX to Caffe2 <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb>`_
     shows how to use *caffe2* as a backend for a converted model.
     Note: This is not the official Python API.
-    """  # noqa: E501
+    """
 
-    allowReleasedOpsetsOnly = bool(os.getenv("ALLOW_RELEASED_ONNX_OPSET_ONLY", "1") == "1")
+    allowReleasedOpsetsOnly = bool(os.getenv("ALLOW_RELEASED_ONNX_OPSET_ONLY", "1") == "1")  # noqa: N815
 
     @classmethod
     def is_compatible(cls, model, device=None, **kwargs):
         """
         Return whether the model is compatible with the backend.
 
         :param model: unused
@@ -55,30 +55,30 @@
         :return: boolean and error message if opset is not supported.
         """
         if cls.allowReleasedOpsetsOnly:
             for opset in model.opset_import:
                 domain = opset.domain if opset.domain else "ai.onnx"
                 try:
                     key = (domain, opset.version)
-                    if not (key in helper.OP_SET_ID_VERSION_MAP):
+                    if key not in helper.OP_SET_ID_VERSION_MAP:
                         error_message = (
                             "Skipping this test as only released onnx opsets are supported."
                             "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
-                            " Got Domain '{0}' version '{1}'.".format(domain, opset.version)
+                            " Got Domain '{}' version '{}'.".format(domain, opset.version)
                         )
                         return False, error_message
                 except AttributeError:
                     # for some CI pipelines accessing helper.OP_SET_ID_VERSION_MAP
                     # is generating attribute error. TODO investigate the pipelines to
                     # fix this error. Falling back to a simple version check when this error is encountered
                     if (domain == "ai.onnx" and opset.version > 12) or (domain == "ai.ommx.ml" and opset.version > 2):
                         error_message = (
                             "Skipping this test as only released onnx opsets are supported."
                             "To run this test set env variable ALLOW_RELEASED_ONNX_OPSET_ONLY to 0."
-                            " Got Domain '{0}' version '{1}'.".format(domain, opset.version)
+                            " Got Domain '{}' version '{}'.".format(domain, opset.version)
                         )
                         return False, error_message
         return True, ""
 
     @classmethod
     def supports_device(cls, device):
         """
@@ -117,15 +117,15 @@
             providers = [x for x in get_available_providers() if (x not in excluded_providers)]
 
             inf = InferenceSession(model, sess_options=options, providers=providers)
             # backend API is primarily used for ONNX test/validation. As such, we should disable session.run() fallback
             # which may hide test failures.
             inf.disable_fallback()
             if device is not None and not cls.supports_device(device):
-                raise RuntimeError("Incompatible device expected '{0}', got '{1}'".format(device, get_device()))
+                raise RuntimeError(f"Incompatible device expected '{device}', got '{get_device()}'")
             return cls.prepare(inf, device, **kwargs)
         else:
             # type: ModelProto
             # check_model serializes the model anyways, so serialize the model once here
             # and reuse it below in the cls.prepare call to avoid an additional serialization
             # only works with onnx >= 1.10.0 hence the version check
             onnx_version = packaging.version.parse(version.version) or packaging.version.Version("0")
```

## onnxruntime/backend/backend_rep.py

```diff
@@ -1,15 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 Implements ONNX's backend API.
 """
-from typing import Any, Tuple
+from typing import Any, Tuple  # noqa: F401
 
 from onnx.backend.base import BackendRep
 
 from onnxruntime import RunOptions
 
 
 class OnnxRuntimeBackendRep(BackendRep):
@@ -44,10 +44,10 @@
                 return outs
             else:
                 output_names = [o.name for o in self._session.get_outputs()]
                 return [outs[name] for name in output_names]
         else:
             inp = self._session.get_inputs()
             if len(inp) != 1:
-                raise RuntimeError("Model expect {0} inputs".format(len(inp)))
+                raise RuntimeError(f"Model expect {len(inp)} inputs")
             inps = {inp[0].name: inputs}
             return self._session.run(None, inps, options)
```

## onnxruntime/capi/_ld_preload.py

```diff
@@ -1,19 +1,7 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 # This file can be modified by setup.py when building a manylinux2010 wheel
 # When modified, it will preload some libraries needed for the python C extension
-import os
-from ctypes import CDLL, RTLD_GLOBAL, util
-def LoadLib(lib_name):
-    lib_path = util.find_library(lib_name)
-    if lib_path: _ = CDLL(lib_path, mode=RTLD_GLOBAL)
-    else: _ = CDLL(lib_name, mode=RTLD_GLOBAL)
-for lib_name in ["RE2", "ZLIB1"]:
-    try:
-        LoadLib(lib_name)
-    except OSError:
-        print("Could not load ort azure-ep dependency: " + lib_name)
-        os.environ["ORT_" + lib_name + "_UNAVAILABLE"] = "1"
```

## onnxruntime/capi/onnxruntime_collect_build_info.py

```diff
@@ -31,15 +31,15 @@
             cudart = ctypes.CDLL(cudart_lib_filename)
             cudart.cudaRuntimeGetVersion.restype = int
             cudart.cudaRuntimeGetVersion.argtypes = [ctypes.POINTER(ctypes.c_int)]
             version = ctypes.c_int()
             status = cudart.cudaRuntimeGetVersion(ctypes.byref(version))
             if status != 0:
                 return None
-        except:  # noqa
+        except Exception:
             return None
 
         return version.value
 
     # use set to avoid duplications
     cudart_found_versions = {get_cudart_version(cudart_version) for cudart_version in cudart_possible_versions}
 
@@ -89,15 +89,15 @@
         # in cudnn.h cudnn version are calculated as:
         # #define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
         try:
             cudnn = ctypes.CDLL(cudnn_lib_filename)
             # cudnn_ver = cudnn.cudnnGetVersion()
             cuda_ver = cudnn.cudnnGetCudartVersion()
             return cuda_ver
-        except:  # noqa
+        except Exception:
             return None
 
     # use set to avoid duplications
     cuda_found_versions = {get_cudnn_supported_cuda_version(cudnn_version) for cudnn_version in cudnn_possible_versions}
 
     # convert to list and remove None
     return [ver for ver in cuda_found_versions if ver]
```

## onnxruntime/capi/onnxruntime_inference_collection.py

```diff
@@ -1,31 +1,42 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+from __future__ import annotations
+
 import collections
 import collections.abc
 import os
+import typing
 import warnings
+from typing import Any, Sequence
 
 from onnxruntime.capi import _pybind_state as C
 
+if typing.TYPE_CHECKING:
+    import onnxruntime
+
 
-def get_ort_device_type(device_type, device_index):
+def get_ort_device_type(device_type: str, device_index) -> C.OrtDevice:
     if device_type == "cuda":
         return C.OrtDevice.cuda()
     elif device_type == "cpu":
         return C.OrtDevice.cpu()
     elif device_type == "ort":
         return C.get_ort_device(device_index).device_type()
     else:
         raise Exception("Unsupported device type: " + device_type)
 
 
-def check_and_normalize_provider_args(providers, provider_options, available_provider_names):
+def check_and_normalize_provider_args(
+    providers: Sequence[str, tuple[str, dict[Any, Any]]] | None,
+    provider_options: Sequence[dict[Any, Any]] | None,
+    available_provider_names: Sequence[str],
+):
     """
     Validates the 'providers' and 'provider_options' arguments and returns a
         normalized version.
 
     :param providers: Optional sequence of providers in order of decreasing
         precedence. Values can either be provider names or tuples of
         (provider name, options dict).
@@ -53,15 +64,15 @@
         if name not in available_provider_names:
             warnings.warn(
                 "Specified provider '{}' is not in available provider names."
                 "Available providers: '{}'".format(name, ", ".join(available_provider_names))
             )
 
         if name in provider_name_to_options:
-            warnings.warn("Duplicate provider '{}' encountered, ignoring.".format(name))
+            warnings.warn(f"Duplicate provider '{name}' encountered, ignoring.")
             return
 
         normalized_options = {str(key): str(value) for key, value in options.items()}
         provider_name_to_options[name] = normalized_options
 
     if not isinstance(providers, collections.abc.Sequence):
         raise ValueError("'providers' should be a sequence.")
@@ -101,15 +112,14 @@
 
 class Session:
     """
     This is the main class used to run a model.
     """
 
     def __init__(self):
-
         # self._sess is managed by the derived class and relies on bindings from C.InferenceSession
         self._sess = None
         self._enable_fallback = True
 
     def get_session_options(self):
         "Return the session options. See :class:`onnxruntime.SessionOptions`."
         return self._sess_options
@@ -171,47 +181,53 @@
         Enable session.Run() fallback mechanism. If session.Run() fails due to an internal Execution Provider failure,
         reset the Execution Providers enabled for this session.
         If GPU is enabled, fall back to CUDAExecutionProvider.
         otherwise fall back to CPUExecutionProvider.
         """
         self._enable_fallback = True
 
+    def _validate_input(self, feed_input_names):
+        # import pdb; pdb.set_trace()
+        missing_input_names = []
+        for input in self._inputs_meta:
+            if input.name not in feed_input_names and not input.type.startswith("optional"):
+                missing_input_names.append(input.name)
+        if missing_input_names:
+            raise ValueError(
+                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
+            )
+
     def run(self, output_names, input_feed, run_options=None):
         """
         Compute the predictions.
 
         :param output_names: name of the outputs
         :param input_feed: dictionary ``{ input_name: input_value }``
         :param run_options: See :class:`onnxruntime.RunOptions`.
         :return: list of results, every result is either a numpy array,
             a sparse tensor, a list or a dictionary.
 
         ::
 
             sess.run([output_name], {input_name: x})
         """
-        num_required_inputs = len(self._inputs_meta)
-        num_inputs = len(input_feed)
-        # the graph may have optional inputs used to override initializers. allow for that.
-        if num_inputs < num_required_inputs:
-            raise ValueError("Model requires {} inputs. Input Feed contains {}".format(num_required_inputs, num_inputs))
+        self._validate_input(list(input_feed.keys()))
         if not output_names:
             output_names = [output.name for output in self._outputs_meta]
         try:
             return self._sess.run(output_names, input_feed, run_options)
         except C.EPFail as err:
             if self._enable_fallback:
-                print("EP Error: {} using {}".format(str(err), self._providers))
-                print("Falling back to {} and retrying.".format(self._fallback_providers))
+                print(f"EP Error: {str(err)} using {self._providers}")
+                print(f"Falling back to {self._fallback_providers} and retrying.")
                 self.set_providers(self._fallback_providers)
                 # Fallback only once.
                 self.disable_fallback()
                 return self._sess.run(output_names, input_feed, run_options)
-            else:
-                raise
+            raise
 
     def run_with_ort_values(self, output_names, input_dict_ort_values, run_options=None):
         """
         Compute the predictions.
 
         :param output_names: name of the outputs
         :param input_dict_ort_values: dictionary ``{ input_name: input_ort_value }``
@@ -231,33 +247,28 @@
                 input_dict[n] = v._get_c_value()
             result = sess.run_with_ort_values(input_dict, output_names, run_options)
             if not isinstance(result, C.OrtValueVector):
                 raise TypeError("run_with_ort_values() must return a instance of type 'OrtValueVector'.")
             ort_values = [OrtValue(v) for v in result]
             return ort_values
 
-        num_required_inputs = len(self._inputs_meta)
-        num_inputs = len(input_dict_ort_values)
-        # the graph may have optional inputs used to override initializers. allow for that.
-        if num_inputs < num_required_inputs:
-            raise ValueError("Model requires {} inputs. Input Feed contains {}".format(num_required_inputs, num_inputs))
+        self._validate_input(list(input_dict_ort_values.keys()))
         if not output_names:
             output_names = [output.name for output in self._outputs_meta]
         try:
             return invoke(self._sess, output_names, input_dict_ort_values, run_options)
         except C.EPFail as err:
             if self._enable_fallback:
-                print("EP Error: {} using {}".format(str(err), self._providers))
-                print("Falling back to {} and retrying.".format(self._fallback_providers))
+                print(f"EP Error: {str(err)} using {self._providers}")
+                print(f"Falling back to {self._fallback_providers} and retrying.")
                 self.set_providers(self._fallback_providers)
                 # Fallback only once.
                 self.disable_fallback()
                 return invoke(self._sess, output_names, input_dict_ort_values, run_options)
-            else:
-                raise
+            raise
 
     def end_profiling(self):
         """
         End profiling and return results in a file.
 
         The results are stored in a filename if the option
         :meth:`onnxruntime.SessionOptions.enable_profiling`.
@@ -282,14 +293,20 @@
         Compute the predictions.
 
         :param iobinding: the iobinding object that has graph inputs/outputs bind.
         :param run_options: See :class:`onnxruntime.RunOptions`.
         """
         self._sess.run_with_iobinding(iobinding._iobinding, run_options)
 
+    def get_tuning_results(self):
+        return self._sess.get_tuning_results()
+
+    def set_tuning_results(self, results, *, error_on_invalid=False):
+        return self._sess.set_tuning_results(results, error_on_invalid)
+
     def run_with_ortvaluevector(self, run_options, feed_names, feeds, fetch_names, fetches, fetch_devices):
         """
         Compute the predictions similar to other run_*() methods but with minimal C++/Python conversion overhead.
 
         :param run_options: See :class:`onnxruntime.RunOptions`.
         :param feed_names: list of input names.
         :param feeds: list of input OrtValue.
@@ -301,18 +318,25 @@
 
 
 class InferenceSession(Session):
     """
     This is the main class used to run a model.
     """
 
-    def __init__(self, path_or_bytes, sess_options=None, providers=None, provider_options=None, **kwargs):
+    def __init__(
+        self,
+        path_or_bytes: str | bytes | os.PathLike,
+        sess_options: Sequence[onnxruntime.SessionOptions] | None = None,
+        providers: Sequence[str, tuple[str, dict[Any, Any]]] | None = None,
+        provider_options: Sequence[dict[Any, Any]] | None = None,
+        **kwargs,
+    ) -> None:
         """
-        :param path_or_bytes: filename or serialized ONNX or ORT format model in a byte string
-        :param sess_options: session options
+        :param path_or_bytes: Filename or serialized ONNX or ORT format model in a byte string.
+        :param sess_options: Session options.
         :param providers: Optional sequence of providers in order of decreasing
             precedence. Values can either be provider names or tuples of
             (provider name, options dict). If not provided, then all available
             providers are used with the default precedence.
         :param provider_options: Optional sequence of options dicts corresponding
             to the providers listed in 'providers'.
 
@@ -332,45 +356,48 @@
         are given in 'providers', 'provider_options' should not be used.
 
         The list of providers is ordered by precedence. For example
         `['CUDAExecutionProvider', 'CPUExecutionProvider']`
         means execute a node using `CUDAExecutionProvider`
         if capable, otherwise execute using `CPUExecutionProvider`.
         """
+        super().__init__()
 
-        Session.__init__(self)
-
-        if isinstance(path_or_bytes, str):
-            self._model_path = path_or_bytes
+        if isinstance(path_or_bytes, (str, os.PathLike)):
+            self._model_path = os.fspath(path_or_bytes)
             self._model_bytes = None
         elif isinstance(path_or_bytes, bytes):
             self._model_path = None
             self._model_bytes = path_or_bytes  # TODO: This is bad as we're holding the memory indefinitely
         else:
-            raise TypeError("Unable to load from type '{0}'".format(type(path_or_bytes)))
+            raise TypeError(f"Unable to load from type '{type(path_or_bytes)}'")
 
         self._sess_options = sess_options
         self._sess_options_initial = sess_options
         self._enable_fallback = True
         self._read_config_from_model = os.environ.get("ORT_LOAD_CONFIG_FROM_MODEL") == "1"
 
         # internal parameters that we don't expect to be used in general so aren't documented
         disabled_optimizers = kwargs["disabled_optimizers"] if "disabled_optimizers" in kwargs else None
 
         try:
             self._create_inference_session(providers, provider_options, disabled_optimizers)
-        except ValueError:
+        except (ValueError, RuntimeError) as e:
             if self._enable_fallback:
-                print("EP Error using {}".format(providers))
-                print("Falling back to {} and retrying.".format(self._fallback_providers))
-                self._create_inference_session(self._fallback_providers, None)
-                # Fallback only once.
-                self.disable_fallback()
-            else:
-                raise
+                try:
+                    print(f"EP Error {e} when using {providers}")
+                    print(f"Falling back to {self._fallback_providers} and retrying.")
+                    self._create_inference_session(self._fallback_providers, None)
+                    # Fallback only once.
+                    self.disable_fallback()
+                    return
+                except Exception as fallback_error:
+                    raise fallback_error from e
+            # Fallback is disabled. Raise the original error.
+            raise e
 
     def _create_inference_session(self, providers, provider_options, disabled_optimizers=None):
         available_providers = C.get_available_providers()
 
         # Tensorrt can fall back to CUDA. All others fall back to CPU.
         if "TensorrtExecutionProvider" in available_providers:
             self._fallback_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
@@ -379,21 +406,21 @@
         else:
             self._fallback_providers = ["CPUExecutionProvider"]
 
         # validate providers and provider_options before other initialization
         providers, provider_options = check_and_normalize_provider_args(
             providers, provider_options, available_providers
         )
-        if providers == [] and len(available_providers) > 1:
+        if not providers and len(available_providers) > 1:
             self.disable_fallback()
             raise ValueError(
-                "This ORT build has {} enabled. ".format(available_providers)
-                + "Since ORT 1.9, you are required to explicitly set "
-                + "the providers parameter when instantiating InferenceSession. For example, "
-                "onnxruntime.InferenceSession(..., providers={}, ...)".format(available_providers)
+                f"This ORT build has {available_providers} enabled. "
+                "Since ORT 1.9, you are required to explicitly set "
+                "the providers parameter when instantiating InferenceSession. For example, "
+                f"onnxruntime.InferenceSession(..., providers={available_providers}, ...)"
             )
 
         session_options = self._sess_options if self._sess_options else C.get_default_session_options()
         if self._model_path:
             sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
         else:
             sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
@@ -437,15 +464,15 @@
 
 
 class IOBinding:
     """
     This class provides API to bind input/output to a specified device, e.g. GPU.
     """
 
-    def __init__(self, session):
+    def __init__(self, session: Session):
         self._iobinding = C.SessionIOBinding(session._sess)
         self._numpy_obj_references = {}
 
     def bind_cpu_input(self, name, arr_on_cpu):
         """
         bind an input to array on CPU
         :param name: input name
@@ -582,15 +609,15 @@
             self._ortvalue = ortvalue
             # Hold a ref count to the numpy object if the OrtValue is backed directly
             # by its data buffer so that it isn't destroyed when the OrtValue is in use
             self._numpy_obj = numpy_obj
         else:
             # An end user won't hit this error
             raise ValueError(
-                "`Provided ortvalue` needs to be of type " + "`onnxruntime.capi.onnxruntime_pybind11_state.OrtValue`"
+                "`Provided ortvalue` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.OrtValue`"
             )
 
     def _get_c_value(self):
         return self._ortvalue
 
     @staticmethod
     def ortvalue_from_numpy(numpy_obj, device_type="cpu", device_id=0):
@@ -740,15 +767,15 @@
         """
         Internal constructor
         """
         if isinstance(c_ort_device, C.OrtDevice):
             self._ort_device = c_ort_device
         else:
             raise ValueError(
-                "`Provided object` needs to be of type " + "`onnxruntime.capi.onnxruntime_pybind11_state.OrtDevice`"
+                "`Provided object` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.OrtDevice`"
             )
 
     def _get_c_device(self):
         """
         Internal accessor to underlying object
         """
         return self._ort_device
@@ -783,15 +810,15 @@
         Internal constructor
         """
         if isinstance(sparse_tensor, C.SparseTensor):
             self._tensor = sparse_tensor
         else:
             # An end user won't hit this error
             raise ValueError(
-                "`Provided object` needs to be of type " + "`onnxruntime.capi.onnxruntime_pybind11_state.SparseTensor`"
+                "`Provided object` needs to be of type `onnxruntime.capi.onnxruntime_pybind11_state.SparseTensor`"
             )
 
     def _get_c_tensor(self):
         return self._tensor
 
     @staticmethod
     def sparse_coo_from_numpy(dense_shape, values, coo_indices, ort_device):
```

## onnxruntime/capi/onnxruntime_providers_shared.dll

### objdump

```diff
@@ -4,15 +4,15 @@
 start address 0x0000000180001370
 
 Characteristics 0x2022
 	executable
 	large address aware
 	DLL
 
-Time/Date		Thu Feb  9 21:30:05 2023
+Time/Date		Wed May 24 18:25:26 2023
 Magic			020b	(PE32+)
 MajorLinkerVersion	14
 MinorLinkerVersion	29
 SizeOfCode		0000000000001000
 SizeOfInitializedData	0000000000002000
 SizeOfUninitializedData	0000000000000000
 AddressOfEntryPoint	0000000000001370
@@ -25,15 +25,15 @@
 MajorImageVersion	0
 MinorImageVersion	0
 MajorSubsystemVersion	6
 MinorSubsystemVersion	0
 Win32Version		00000000
 SizeOfImage		00007000
 SizeOfHeaders		00000400
-CheckSum		0000f119
+CheckSum		0000712e
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00004160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 					GUARD_CF
 SizeOfStackReserve	0000000000100000
@@ -44,15 +44,15 @@
 NumberOfRvaAndSizes	00000010
 
 The Data Directory
 Entry 0 0000000000002830 00000080 Export Directory [.edata (or where ever we found it)]
 Entry 1 00000000000028b0 00000050 Import Directory [parts of .idata]
 Entry 2 0000000000005000 000005c8 Resource Directory [.rsrc]
 Entry 3 0000000000004000 000001b0 Exception Directory [.pdata]
-Entry 4 0000000000002e00 00002798 Security Directory
+Entry 4 0000000000002e00 000027b0 Security Directory
 Entry 5 0000000000006000 00000028 Base Relocation Directory [.reloc]
 Entry 6 00000000000021a0 00000070 Debug Directory
 Entry 7 0000000000000000 00000000 Description Directory
 Entry 8 0000000000000000 00000000 Special Directory
 Entry 9 0000000000000000 00000000 Thread Storage Directory [.tls]
 Entry a 0000000000002210 00000138 Load Configuration Directory
 Entry b 0000000000000000 00000000 Bound Import Directory
@@ -87,29 +87,29 @@
 	2a48	   55  _initterm_e
 	2a3c	   54  _initterm
 
  000028d8	00002900 00000000 00000000 00002c6a 00002000
 
 	DLL Name: KERNEL32.dll
 	vma:  Hint/Ord Member-Name Bound-To
-	2bf4	  559  GetCurrentThreadId
-	2b32	 1272  RtlVirtualUnwind
-	2b18	 1265  RtlLookupFunctionEntry
-	2b04	 1257  RtlCaptureContext
-	2b62	 1431  SetUnhandledExceptionFilter
-	2c56	  919  IsDebuggerPresent
-	2c40	  897  InitializeSListHead
-	2c24	  300  DisableThreadLibraryCalls
-	2c0a	  769  GetSystemTimeAsFileTime
-	2b46	 1496  UnhandledExceptionFilter
-	2bde	  555  GetCurrentProcessId
-	2bc4	 1124  QueryPerformanceCounter
-	2ba8	  926  IsProcessorFeaturePresent
-	2b94	 1462  TerminateProcess
-	2b80	  554  GetCurrentProcess
+	2bf4	  567  GetCurrentThreadId
+	2b32	 1284  RtlVirtualUnwind
+	2b18	 1277  RtlLookupFunctionEntry
+	2b04	 1269  RtlCaptureContext
+	2b62	 1444  SetUnhandledExceptionFilter
+	2c56	  928  IsDebuggerPresent
+	2c40	  906  InitializeSListHead
+	2c24	  308  DisableThreadLibraryCalls
+	2c0a	  778  GetSystemTimeAsFileTime
+	2b46	 1510  UnhandledExceptionFilter
+	2bde	  563  GetCurrentProcessId
+	2bc4	 1136  QueryPerformanceCounter
+	2ba8	  936  IsProcessorFeaturePresent
+	2b94	 1476  TerminateProcess
+	2b80	  562  GetCurrentProcess
 
  000028ec	00000000 00000000 00000000 00000000 00000000
 
 There is an export table in .rdata at 0x180002830
 
 The Export Tables (interpreted .rdata section contents)
 
@@ -363,36 +363,36 @@
 	  260: 69 7a 65 5f 6e 61 72 72 6f 77 5f 65 6e 76 69 72
 	  270: 6f 6e 6d 65 6e 74 00 00 34 00 5f 69 6e 69 74 69
 	  280: 61 6c 69 7a 65 5f 6f 6e 65 78 69 74 5f 74 61 62
 	  290: 6c 65 00 00 22 00 5f 65 78 65 63 75 74 65 5f 6f
 	  2a0: 6e 65 78 69 74 5f 74 61 62 6c 65 00 16 00 5f 63
 	  2b0: 65 78 69 74 00 00 61 70 69 2d 6d 73 2d 77 69 6e
 	  2c0: 2d 63 72 74 2d 72 75 6e 74 69 6d 65 2d 6c 31 2d
-	  2d0: 31 2d 30 2e 64 6c 6c 00 e9 04 52 74 6c 43 61 70
-	  2e0: 74 75 72 65 43 6f 6e 74 65 78 74 00 f1 04 52 74
+	  2d0: 31 2d 30 2e 64 6c 6c 00 f5 04 52 74 6c 43 61 70
+	  2e0: 74 75 72 65 43 6f 6e 74 65 78 74 00 fd 04 52 74
 	  2f0: 6c 4c 6f 6f 6b 75 70 46 75 6e 63 74 69 6f 6e 45
-	  300: 6e 74 72 79 00 00 f8 04 52 74 6c 56 69 72 74 75
-	  310: 61 6c 55 6e 77 69 6e 64 00 00 d8 05 55 6e 68 61
+	  300: 6e 74 72 79 00 00 04 05 52 74 6c 56 69 72 74 75
+	  310: 61 6c 55 6e 77 69 6e 64 00 00 e6 05 55 6e 68 61
 	  320: 6e 64 6c 65 64 45 78 63 65 70 74 69 6f 6e 46 69
-	  330: 6c 74 65 72 00 00 97 05 53 65 74 55 6e 68 61 6e
+	  330: 6c 74 65 72 00 00 a4 05 53 65 74 55 6e 68 61 6e
 	  340: 64 6c 65 64 45 78 63 65 70 74 69 6f 6e 46 69 6c
-	  350: 74 65 72 00 2a 02 47 65 74 43 75 72 72 65 6e 74
-	  360: 50 72 6f 63 65 73 73 00 b6 05 54 65 72 6d 69 6e
-	  370: 61 74 65 50 72 6f 63 65 73 73 00 00 9e 03 49 73
+	  350: 74 65 72 00 32 02 47 65 74 43 75 72 72 65 6e 74
+	  360: 50 72 6f 63 65 73 73 00 c4 05 54 65 72 6d 69 6e
+	  370: 61 74 65 50 72 6f 63 65 73 73 00 00 a8 03 49 73
 	  380: 50 72 6f 63 65 73 73 6f 72 46 65 61 74 75 72 65
-	  390: 50 72 65 73 65 6e 74 00 64 04 51 75 65 72 79 50
+	  390: 50 72 65 73 65 6e 74 00 70 04 51 75 65 72 79 50
 	  3a0: 65 72 66 6f 72 6d 61 6e 63 65 43 6f 75 6e 74 65
-	  3b0: 72 00 2b 02 47 65 74 43 75 72 72 65 6e 74 50 72
-	  3c0: 6f 63 65 73 73 49 64 00 2f 02 47 65 74 43 75 72
-	  3d0: 72 65 6e 74 54 68 72 65 61 64 49 64 00 00 01 03
+	  3b0: 72 00 33 02 47 65 74 43 75 72 72 65 6e 74 50 72
+	  3c0: 6f 63 65 73 73 49 64 00 37 02 47 65 74 43 75 72
+	  3d0: 72 65 6e 74 54 68 72 65 61 64 49 64 00 00 0a 03
 	  3e0: 47 65 74 53 79 73 74 65 6d 54 69 6d 65 41 73 46
-	  3f0: 69 6c 65 54 69 6d 65 00 2c 01 44 69 73 61 62 6c
+	  3f0: 69 6c 65 54 69 6d 65 00 34 01 44 69 73 61 62 6c
 	  400: 65 54 68 72 65 61 64 4c 69 62 72 61 72 79 43 61
-	  410: 6c 6c 73 00 81 03 49 6e 69 74 69 61 6c 69 7a 65
-	  420: 53 4c 69 73 74 48 65 61 64 00 97 03 49 73 44 65
+	  410: 6c 6c 73 00 8a 03 49 6e 69 74 69 61 6c 69 7a 65
+	  420: 53 4c 69 73 74 48 65 61 64 00 a0 03 49 73 44 65
 	  430: 62 75 67 67 65 72 50 72 65 73 65 6e 74 00 4b 45
 	  440: 52 4e 45 4c 33 32 2e 64 6c 6c 00 00
  00000001800026e8 (rva: 000026e8): 0000000180001d96 - 0000000180001dad
 	Version: 1, Flags: none
 	Nbr codes: 2, Prologue size: 0x06, Frame offset: 0x0, Frame reg: none
 	  pc+0x06: alloc small area: rsp = rsp - 0x20
 	  pc+0x02: push rbp
@@ -429,15 +429,15 @@
 	reloc   14 offset  338 [2338] DIR64
 	reloc   15 offset  340 [2340] DIR64
 
 There is a debug directory in .rdata at 0x1800021a0
 
 Type                Size     Rva      Offset
   2        CodeView 00000066 000023a0 000017a0
-(format RSDS signature 85754e6a7adb4b8bb5c65339eaf7a2fe age 1 pdb D:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
+(format RSDS signature 0155f5974a4d40bf9f546b2781ab1960 age 1 pdb C:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
  12         Feature 00000014 00002408 00001808
  13         CoffGrp 00000268 0000241c 0000181c
  20         Unknown 00000004 00002684 00001a84
 
 The .rsrc Resource Directory section:
 000  Type Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 2
 010   Entry: ID: 0x000010, Value: 0x80000020
@@ -1681,54 +1681,49 @@
    18000219a:	(bad)
    18000219b:	(bad)
    18000219c:	(bad)
    18000219d:	(bad)
    18000219e:	(bad)
    18000219f:	incl   (%rax)
    1800021a1:	add    %al,(%rax)
-   1800021a3:	add    %bl,%ch
-   1800021a5:	gs in  $0x63,%eax
-   1800021a8:	add    %al,(%rax)
-   1800021aa:	add    %al,(%rax)
-   1800021ac:	add    (%rax),%al
-   1800021ae:	add    %al,(%rax)
-   1800021b0:	data16 add %al,(%rax)
-   1800021b3:	add    %ah,-0x5fffffdd(%rax)
-   1800021b9:	(bad)
-   1800021ba:	add    %al,(%rax)
-   1800021bc:	add    %al,(%rax)
-   1800021be:	add    %al,(%rax)
-   1800021c0:	frstor -0x1b(%rbp)
-   1800021c3:	movsxd (%rax),%eax
+   1800021a3:	add    %dl,0x646e56(%rsi)
+   1800021a9:	add    %al,(%rax)
+   1800021ab:	add    %al,(%rdx)
+   1800021ad:	add    %al,(%rax)
+   1800021af:	add    %ah,0x0(%rsi)
+   1800021b2:	add    %al,(%rax)
+   1800021b4:	movabs 0x17a0000023,%al
+   1800021bd:	add    %al,(%rax)
+   1800021bf:	add    %dl,0x646e56(%rsi)
    1800021c5:	add    %al,(%rax)
    1800021c7:	add    %cl,(%rax,%rax,1)
    1800021ca:	add    %al,(%rax)
    1800021cc:	adc    $0x0,%al
    1800021ce:	add    %al,(%rax)
    1800021d0:	or     %ah,(%rax,%rax,1)
    1800021d3:	add    %cl,(%rax)
    1800021d5:	sbb    %al,(%rax)
    1800021d7:	add    %al,(%rax)
    1800021d9:	add    %al,(%rax)
-   1800021db:	add    %bl,%ch
-   1800021dd:	gs in  $0x63,%eax
-   1800021e0:	add    %al,(%rax)
-   1800021e2:	add    %al,(%rax)
-   1800021e4:	or     $0x68000000,%eax
+   1800021db:	add    %dl,0x646e56(%rsi)
+   1800021e1:	add    %al,(%rax)
+   1800021e3:	add    %cl,0x68000000(%rip)        # 0x1e80021e9
    1800021e9:	add    (%rax),%al
    1800021eb:	add    %bl,(%rsp)
    1800021ee:	add    %al,(%rax)
    1800021f0:	sbb    $0x18,%al
    1800021f2:	add    %al,(%rax)
    1800021f4:	add    %al,(%rax)
    1800021f6:	add    %al,(%rax)
-   1800021f8:	frstor -0x1b(%rbp)
-   1800021fb:	movsxd (%rax),%eax
-   1800021fd:	add    %al,(%rax)
-   1800021ff:	add    %dl,(%rax,%rax,1)
+   1800021f8:	xchg   %eax,%esi
+   1800021f9:	push   %rsi
+   1800021fa:	outsb  %ds:(%rsi),(%dx)
+   1800021fb:	add    %al,%fs:(%rax)
+   1800021fe:	add    %al,(%rax)
+   180002200:	adc    $0x0,%al
    180002202:	add    %al,(%rax)
    180002204:	add    $0x0,%al
    180002206:	add    %al,(%rax)
    180002208:	test   %ah,(%rsi)
    18000220a:	add    %al,(%rax)
    18000220c:	test   %bl,(%rdx)
    18000220e:	add    %al,(%rax)
@@ -1780,22 +1775,22 @@
    180002395:	add    %al,(%rax)
    180002397:	add    %al,(%rax)
    180002399:	adc    %al,(%rax)
    18000239b:	add    %ah,(%rax)
    18000239d:	add    %al,(%rax)
    18000239f:	add    %dl,0x53(%rdx)
    1800023a2:	rex.R push %rbx
-   1800023a4:	push   $0x4e
-   1800023a6:	jne    0x18000232d
-   1800023a8:	fstpt  -0x75(%rdx)
-   1800023ab:	rex.WXB mov $0xc6,%r13b
-   1800023ae:	push   %rbx
-   1800023af:	cmp    %ebp,%edx
-   1800023b1:	mull   0x1fe(%rdx)
-   1800023b7:	add    %al,0x5c(%rdx,%rdi,1)
+   1800023a4:	xchg   %eax,%edi
+   1800023a5:	cmc
+   1800023a6:	push   %rbp
+   1800023a7:	add    %ecx,0x4a(%rbp)
+   1800023aa:	mov    $0x6b549f40,%edi
+   1800023af:	(bad)
+   1800023b0:	subl   $0x3a430000,0x16019(%rbx)
+   1800023ba:	pop    %rsp
    1800023bb:	(bad)
    1800023bc:	pop    %rsp
    1800023bd:	pop    %rdi
    1800023be:	ja     0x18000242f
    1800023c0:	jb     0x18000242d
    1800023c2:	pop    %rsp
    1800023c3:	xor    %ebx,0x5c(%rdx,%riz,2)
@@ -2552,146 +2547,145 @@
    180002ae1:	add    %ah,0x70(%rcx)
    180002ae4:	imul   $0x632d6e69,0x772d736d(%rip),%ebp        # 0x1f72d9e5b
    180002aee:	jb     0x180002b64
    180002af0:	sub    $0x746e7572,%eax
    180002af5:	imul   $0x2d316c2d,0x65(%rbp),%ebp
    180002afc:	xor    %ebp,0x6c642e30(%rip)        # 0x1ec645932
    180002b02:	insb   (%dx),%es:(%rdi)
-   180002b03:	add    %ch,%cl
+   180002b03:	add    %dh,%ch
    180002b05:	add    $0x52,%al
    180002b07:	je     0x180002b75
    180002b09:	rex.XB (bad)
    180002b0b:	jo     0x180002b81
    180002b0d:	jne    0x180002b81
    180002b0f:	rex.XB outsl %gs:(%rsi),(%dx)
    180002b12:	outsb  %ds:(%rsi),(%dx)
    180002b13:	je     0x180002b7a
    180002b15:	js     0x180002b8b
-   180002b17:	add    %dh,%cl
+   180002b17:	add    %bh,%ch
    180002b19:	add    $0x52,%al
    180002b1b:	je     0x180002b89
    180002b1d:	rex.WR outsl %ds:(%rsi),(%dx)
    180002b1f:	outsl  %ds:(%rsi),(%dx)
    180002b20:	imul   $0x46,0x70(%rbp),%esi
    180002b24:	jne    0x180002b94
    180002b26:	movsxd 0x6f(%rcx,%rbp,2),%esi
    180002b2a:	outsb  %ds:(%rsi),(%dx)
    180002b2b:	rex.RB outsb %ds:(%rsi),(%dx)
    180002b2d:	je     0x180002ba1
    180002b2f:	jns    0x180002b31
-   180002b31:	add    %bh,%al
-   180002b33:	add    $0x52,%al
-   180002b35:	je     0x180002ba3
-   180002b37:	push   %rsi
+   180002b31:	add    %al,0x566c7452(,%rax,1)
    180002b38:	imul   $0x556c6175,0x74(%rdx),%esi
    180002b3f:	outsb  %ds:(%rsi),(%dx)
    180002b40:	ja     0x180002bab
    180002b42:	outsb  %ds:(%rsi),(%dx)
    180002b43:	add    %al,%fs:(%rax)
-   180002b46:	fadds  0x61686e55(%rip)        # 0x1e16899a1
-   180002b4c:	outsb  %ds:(%rsi),(%dx)
-   180002b4d:	fs insb (%dx),%es:(%rdi)
+   180002b46:	out    %al,$0x5
+   180002b48:	push   %rbp
+   180002b49:	outsb  %ds:(%rsi),(%dx)
+   180002b4a:	push   $0x6c646e61
    180002b4f:	gs fs rex.RB js 0x180002bb7
    180002b54:	gs jo  0x180002bcb
    180002b57:	imul   $0x746c6946,0x6e(%rdi),%ebp
    180002b5e:	gs jb  0x180002b61
-   180002b61:	add    %dl,0x74655305(%rdi)
-   180002b67:	push   %rbp
+   180002b61:	add    %ah,0x55746553(%rbp,%rax,1)
    180002b68:	outsb  %ds:(%rsi),(%dx)
    180002b69:	push   $0x6c646e61
    180002b6e:	gs fs rex.RB js 0x180002bd6
    180002b73:	gs jo  0x180002bea
    180002b76:	imul   $0x746c6946,0x6e(%rdi),%ebp
    180002b7d:	gs jb  0x180002b80
-   180002b80:	sub    (%rdx),%al
+   180002b80:	xor    (%rdx),%al
    180002b82:	rex.RXB
    180002b83:	gs je  0x180002bc9
    180002b86:	jne    0x180002bfa
    180002b88:	jb     0x180002bef
    180002b8a:	outsb  %ds:(%rsi),(%dx)
    180002b8b:	je     0x180002bdd
    180002b8d:	jb     0x180002bfe
    180002b8f:	movsxd 0x73(%rbp),%esp
    180002b92:	jae    0x180002b94
-   180002b94:	mov    $0x5,%dh
-   180002b96:	push   %rsp
-   180002b97:	gs jb  0x180002c07
+   180002b94:	(bad)
+   180002b95:	add    $0x6d726554,%eax
    180002b9a:	imul   $0x72506574,0x61(%rsi),%ebp
    180002ba1:	outsl  %ds:(%rsi),(%dx)
    180002ba2:	movsxd 0x73(%rbp),%esp
    180002ba5:	jae    0x180002ba7
-   180002ba7:	add    %bl,0x50734903(%rsi)
+   180002ba7:	add    %ch,0x50734903(%rax)
    180002bad:	jb     0x180002c1e
    180002baf:	movsxd 0x73(%rbp),%esp
    180002bb2:	jae    0x180002c23
    180002bb4:	jb     0x180002bfc
    180002bb6:	gs (bad)
    180002bb8:	je     0x180002c2f
    180002bba:	jb     0x180002c21
    180002bbc:	push   %rax
    180002bbd:	jb     0x180002c24
    180002bbf:	jae    0x180002c26
    180002bc1:	outsb  %ds:(%rsi),(%dx)
    180002bc2:	je     0x180002bc4
-   180002bc4:	fs add $0x51,%al
+   180002bc4:	jo     0x180002bca
+   180002bc6:	push   %rcx
    180002bc7:	jne    0x180002c2e
    180002bc9:	jb     0x180002c44
    180002bcb:	push   %rax
    180002bcc:	gs jb  0x180002c35
    180002bcf:	outsl  %ds:(%rsi),(%dx)
    180002bd0:	jb     0x180002c3f
    180002bd2:	(bad)
    180002bd3:	outsb  %ds:(%rsi),(%dx)
    180002bd4:	movsxd 0x43(%rbp),%esp
    180002bd7:	outsl  %ds:(%rsi),(%dx)
    180002bd8:	jne    0x180002c48
    180002bda:	je     0x180002c41
    180002bdc:	jb     0x180002bde
-   180002bde:	sub    (%rdx),%eax
+   180002bde:	xor    (%rdx),%eax
    180002be0:	rex.RXB
    180002be1:	gs je  0x180002c27
    180002be4:	jne    0x180002c58
    180002be6:	jb     0x180002c4d
    180002be8:	outsb  %ds:(%rsi),(%dx)
    180002be9:	je     0x180002c3b
    180002beb:	jb     0x180002c5c
    180002bed:	movsxd 0x73(%rbp),%esp
    180002bf0:	jae    0x180002c3b
-   180002bf2:	add    %ch,%fs:(%rdi)
+   180002bf2:	add    %dh,%fs:(%rdi)
    180002bf5:	add    0x65(%rdi),%al
    180002bf8:	je     0x180002c3d
    180002bfa:	jne    0x180002c6e
    180002bfc:	jb     0x180002c63
    180002bfe:	outsb  %ds:(%rsi),(%dx)
    180002bff:	je     0x180002c55
    180002c01:	push   $0x64616572
    180002c06:	rex.WB
    180002c07:	add    %al,%fs:(%rax)
-   180002c0a:	add    %eax,(%rbx)
+   180002c0a:	or     (%rbx),%al
    180002c0c:	rex.RXB
    180002c0d:	gs je  0x180002c63
    180002c10:	jns    0x180002c85
    180002c12:	je     0x180002c79
    180002c14:	insl   (%dx),%es:(%rdi)
    180002c15:	push   %rsp
    180002c16:	imul   $0x69467341,0x65(%rbp),%ebp
    180002c1d:	insb   (%dx),%es:(%rdi)
    180002c1e:	gs push %rsp
-   180002c20:	imul   $0x44012c00,0x65(%rbp),%ebp
+   180002c20:	imul   $0x44013400,0x65(%rbp),%ebp
    180002c27:	imul   $0x54656c62,0x61(%rbx),%esi
    180002c2e:	push   $0x64616572
    180002c33:	imul   $0x43797261,0x72(%rdx),%r12
    180002c3b:	(bad)
    180002c3c:	insb   (%dx),%es:(%rdi)
    180002c3d:	insb   (%dx),%es:(%rdi)
    180002c3e:	jae    0x180002c40
-   180002c40:	addl   $0x74696e49,(%rbx)
-   180002c46:	imul   $0x53657a69,0x6c(%rcx),%esp
+   180002c40:	mov    (%rbx),%al
+   180002c42:	rex.WB outsb %ds:(%rsi),(%dx)
+   180002c44:	imul   $0x657a696c,0x61(%rcx,%rbp,2),%esi
+   180002c4c:	push   %rbx
    180002c4d:	imul   $0x64616548,0x74(%rbx),%r14
-   180002c55:	add    %dl,0x44734903(%rdi)
+   180002c55:	add    %ah,0x44734903(%rax)
    180002c5b:	(bad)
    180002c61:	jb     0x180002cb3
    180002c63:	jb     0x180002cca
    180002c65:	jae    0x180002ccc
    180002c67:	outsb  %ds:(%rsi),(%dx)
    180002c68:	je     0x180002c6a
    180002c6a:	rex.WXB
@@ -2968,22 +2962,22 @@
    1800050ba:	pop    %rdi
    1800050bb:	add    %cl,0x0(%rcx)
    1800050be:	rex.WRX add %r8b,0x0(%rsi)
    1800050c2:	rex.WRXB add %r8b,(%r8)
    1800050c5:	add    %al,(%rax)
    1800050c7:	add    %bh,0xfeef04(%rbp)
    1800050cd:	add    %al,(%rcx)
-   1800050cf:	add    %cl,(%rsi)
+   1800050cf:	add    %cl,(%rdi)
    1800050d1:	add    %al,(%rcx)
-   1800050d3:	add    %dl,%cl
-   1800050d5:	add    %dl,(%rdi)
-   1800050d7:	add    %cl,(%rsi)
+   1800050d3:	add    %cl,(%rdx,%rax,1)
+   1800050d6:	(bad)
+   1800050d7:	add    %cl,(%rdi)
    1800050d9:	add    %al,(%rcx)
-   1800050db:	add    %dl,%cl
-   1800050dd:	add    %dl,(%rdi)
+   1800050db:	add    %cl,(%rdx,%rax,1)
+   1800050de:	(bad)
    1800050df:	add    %bh,(%rdi)
    1800050e1:	add    %al,(%rax)
    1800050e3:	add    %al,(%rax)
    1800050e5:	add    %al,(%rax)
    1800050e7:	add    %al,(%rax,%rax,1)
    1800050ea:	add    %al,(%rax)
    1800050ec:	add    (%rax),%al
@@ -3081,31 +3075,27 @@
    1800051da:	jb     0x1800051dc
    1800051dc:	jae    0x1800051de
    1800051de:	imul   $0x6e006f,(%rax),%eax
    1800051e4:	add    %al,(%rax)
    1800051e6:	add    %al,(%rax)
    1800051e8:	xor    %eax,(%rax)
    1800051ea:	cs add %dh,(%rcx)
-   1800051ed:	add    %dh,(%rax,%rax,1)
-   1800051f0:	cs add %dh,(%rdx)
+   1800051ed:	add    %dh,0x32002e00(%rip)        # 0x1b2007ff3
    1800051f3:	add    %dh,(%rax)
    1800051f5:	add    %dh,(%rdx)
    1800051f7:	add    %dh,(%rbx)
    1800051f9:	add    %dh,(%rax)
-   1800051fb:	add    %dh,(%rdx)
-   1800051fd:	add    %dh,(%rax)
-   1800051ff:	add    %bh,(%rcx)
+   1800051fb:	add    %dh,0x34003200(%rip)        # 0x1b4008401
    180005201:	add    %ch,(%rsi)
-   180005203:	add    %dh,(%rax,%rax,1)
-   180005206:	cs add %dh,(%rsi)
-   180005209:	add    %ah,0x0(%rbx)
-   18000520c:	movsxd (%rax),%eax
-   18000520e:	(bad)
-   18000520f:	add    %ah,0x0(%rbp)
-   180005212:	add    %ah,%fs:0x0(%rax,%rax,1)
+   180005203:	add    %dh,0x64002e00(%rip)        # 0x1e4008009
+   180005209:	add    %ah,0x61(%rax,%rax,1)
+   18000520d:	add    %ah,0x0(%rcx)
+   180005210:	add    %ah,%gs:0x0(%rbp)
+   180005214:	(bad)
+   180005215:	add    %al,(%rax)
    180005217:	add    %bh,(%rdx)
    180005219:	add    %cl,0x49000100(%rip)        # 0x1c900531f
    18000521f:	add    %ch,0x0(%rsi)
    180005222:	je     0x180005224
    180005224:	add    %dh,%gs:0x0(%rdx)
    180005228:	outsb  %ds:(%rsi),(%dx)
    180005229:	add    %ah,0x0(%rcx)
@@ -3268,31 +3258,27 @@
    1800053c1:	add    %ah,0x0(%rbp)
    1800053c4:	jb     0x1800053c6
    1800053c6:	jae    0x1800053c8
    1800053c8:	imul   $0x6e006f,(%rax),%eax
    1800053ce:	add    %al,(%rax)
    1800053d0:	xor    %eax,(%rax)
    1800053d2:	cs add %dh,(%rcx)
-   1800053d5:	add    %dh,(%rax,%rax,1)
-   1800053d8:	cs add %dh,(%rdx)
+   1800053d5:	add    %dh,0x32002e00(%rip)        # 0x1b20081db
    1800053db:	add    %dh,(%rax)
    1800053dd:	add    %dh,(%rdx)
    1800053df:	add    %dh,(%rbx)
    1800053e1:	add    %dh,(%rax)
-   1800053e3:	add    %dh,(%rdx)
-   1800053e5:	add    %dh,(%rax)
-   1800053e7:	add    %bh,(%rcx)
+   1800053e3:	add    %dh,0x34003200(%rip)        # 0x1b40085e9
    1800053e9:	add    %ch,(%rsi)
-   1800053eb:	add    %dh,(%rax,%rax,1)
-   1800053ee:	cs add %dh,(%rsi)
-   1800053f1:	add    %ah,0x0(%rbx)
-   1800053f4:	movsxd (%rax),%eax
-   1800053f6:	(bad)
-   1800053f7:	add    %ah,0x0(%rbp)
-   1800053fa:	add    %ah,%fs:0x0(%rax,%rax,1)
+   1800053eb:	add    %dh,0x64002e00(%rip)        # 0x1e40081f1
+   1800053f1:	add    %ah,0x61(%rax,%rax,1)
+   1800053f5:	add    %ah,0x0(%rcx)
+   1800053f8:	add    %ah,%gs:0x0(%rbp)
+   1800053fc:	(bad)
+   1800053fd:	add    %al,(%rax)
    1800053ff:	add    %al,0x0(%rax,%rax,1)
    180005403:	add    %al,(%rcx)
    180005405:	add    %dl,0x0(%rsi)
    180005408:	(bad)
    180005409:	add    %dh,0x0(%rdx)
    18000540c:	rex.RX add %r13b,0x0(%rcx)
    180005410:	insb   (%dx),%es:(%rdi)
```

## onnxruntime/capi/onnxruntime_validation.py

```diff
@@ -11,16 +11,16 @@
 
 
 def check_distro_info():
     __my_distro__ = ""
     __my_distro_ver__ = ""
     __my_system__ = platform.system().lower()
 
-    __OS_RELEASE_FILE__ = "/etc/os-release"
-    __LSB_RELEASE_FILE__ = "/etc/lsb-release"
+    __OS_RELEASE_FILE__ = "/etc/os-release"  # noqa: N806
+    __LSB_RELEASE_FILE__ = "/etc/lsb-release"  # noqa: N806
 
     if __my_system__ == "windows":
         __my_distro__ = __my_system__
         __my_distro_ver__ = platform.release().lower()
 
         if __my_distro_ver__ != "10":
             warnings.warn(
@@ -63,15 +63,15 @@
 
 
 def validate_build_package_info():
     import_ortmodule_exception = None
 
     has_ortmodule = False
     try:
-        from onnxruntime.training.ortmodule import ORTModule  # noqa
+        from onnxruntime.training.ortmodule import ORTModule  # noqa: F401
 
         has_ortmodule = True
     except ImportError:
         # ORTModule not present
         has_ortmodule = False
     except Exception as e:
         # this may happen if Cuda is not installed, we want to raise it after
@@ -96,25 +96,25 @@
 
     if has_ortmodule:
         try:
             # collect onnxruntime package name, version, and cuda version
             from .build_and_package_info import __version__ as version
             from .build_and_package_info import package_name
 
-            try:
+            try:  # noqa: SIM105
                 from .build_and_package_info import cuda_version
-            except:  # noqa
+            except Exception:
                 pass
 
             if cuda_version:
                 # collect cuda library build info. the library info may not be available
                 # when the build environment has none or multiple libraries installed
                 try:
                     from .build_and_package_info import cudart_version
-                except:  # noqa
+                except Exception:
                     warnings.warn("WARNING: failed to get cudart_version from onnxruntime build info.")
                     cudart_version = None
 
                 def print_build_package_info():
                     warnings.warn("onnxruntime training package info: package_name: %s" % package_name)
                     warnings.warn("onnxruntime training package info: __version__: %s" % version)
                     warnings.warn("onnxruntime training package info: cuda_version: %s" % cuda_version)
@@ -128,15 +128,15 @@
                     print_build_package_info()
                     warnings.warn("WARNING: failed to find cudart version that matches onnxruntime build info")
                     warnings.warn("WARNING: found cudart versions: %s" % local_cudart_versions)
             else:
                 # TODO: rcom
                 pass
 
-        except Exception as e:  # noqa
+        except Exception as e:
             warnings.warn("WARNING: failed to collect onnxruntime version and build info")
             print(e)
 
     if import_ortmodule_exception:
         raise import_ortmodule_exception
 
     return has_ortmodule, package_name, version, cuda_version
```

## onnxruntime/datasets/__init__.py

```diff
@@ -9,9 +9,9 @@
 def get_example(name):
     """
     Retrieves the absolute file name of an example.
     """
     this = os.path.abspath(os.path.dirname(__file__))
     full = os.path.join(this, name)
     if not os.path.exists(full):
-        raise FileNotFoundError("Unable to find example '{0}'".format(name))
+        raise FileNotFoundError(f"Unable to find example '{name}'")
     return full
```

## onnxruntime/quantization/__init__.py

```diff
@@ -1,12 +1,16 @@
-from .calibrate import CalibraterBase, CalibrationDataReader, CalibrationMethod, MinMaxCalibrater, create_calibrator
-from .qdq_quantizer import QDQQuantizer
-from .quant_utils import QuantFormat, QuantType, write_calibration_table
-from .quantize import (
-    DynamicQuantConfig,
-    QuantizationMode,
-    StaticQuantConfig,
-    quantize,
-    quantize_dynamic,
-    quantize_static,
+from .calibrate import (  # noqa: F401
+    CalibraterBase,
+    CalibrationDataReader,
+    CalibrationMethod,
+    MinMaxCalibrater,
+    create_calibrator,
 )
-from .shape_inference import quant_pre_process
+from .qdq_quantizer import QDQQuantizer  # noqa: F401
+from .quant_utils import QuantFormat, QuantType, write_calibration_table  # noqa: F401
+from .quantize import DynamicQuantConfig  # noqa: F401
+from .quantize import QuantizationMode  # noqa: F401
+from .quantize import StaticQuantConfig  # noqa: F401
+from .quantize import quantize  # noqa: F401
+from .quantize import quantize_dynamic  # noqa: F401
+from .quantize import quantize_static  # noqa: F401
+from .shape_inference import quant_pre_process  # noqa: F401
```

## onnxruntime/quantization/calibrate.py

```diff
@@ -1,9 +1,8 @@
 #!/usr/bin/env python
-# coding: utf-8
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft, Intel Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import abc
 import itertools
@@ -77,15 +76,15 @@
         self.symmetric = symmetric
         self.use_external_data_format = use_external_data_format
 
         self.augment_model = None
         self.infer_session = None
         self.execution_providers = ["CPUExecutionProvider"]
 
-    def set_execution_providers(self, execution_providers=["CPUExecutionProvider"]):
+    def set_execution_providers(self, execution_providers=["CPUExecutionProvider"]):  # noqa: B006
         """
         reset the execution providers to execute the collect_data. It triggers to re-creating inference session.
         """
         self.execution_providers = execution_providers
         self.create_inference_session()
 
     def create_inference_session(self):
@@ -106,23 +105,23 @@
         returns:
             tensors (set): set of tensor name.
             value_infos (dict): tensor name to value info.
         """
         value_infos = {vi.name: vi for vi in model.graph.value_info}
         value_infos.update({ot.name: ot for ot in model.graph.output})
         value_infos.update({it.name: it for it in model.graph.input})
-        initializer = set(init.name for init in model.graph.initializer)
+        initializer = {init.name for init in model.graph.initializer}
 
         tensors_to_calibrate = set()
-        tensor_type_to_calibrate = set([TensorProto.FLOAT, TensorProto.FLOAT16])
+        tensor_type_to_calibrate = {TensorProto.FLOAT, TensorProto.FLOAT16}
 
         for node in model.graph.node:
             if not self.op_types_to_calibrate or node.op_type in self.op_types_to_calibrate:
                 for tensor_name in itertools.chain(node.input, node.output):
-                    if tensor_name in value_infos.keys():
+                    if tensor_name in value_infos:
                         vi = value_infos[tensor_name]
                         if (
                             vi.type.HasField("tensor_type")
                             and (vi.type.tensor_type.elem_type in tensor_type_to_calibrate)
                             and (tensor_name not in initializer)
                         ):
                             tensors_to_calibrate.add(tensor_name)
@@ -172,25 +171,25 @@
         :param op_types_to_calibrate: operator types to calibrate. By default, calibrate all the float32/float16 tensors.
         :param augmented_model_path: save augmented model to this path.
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param moving_average: compute the moving average of the minimum and maximum values instead of the global minimum and maximum.
         :param averaging_constant: constant smoothing factor to use when computing the moving average.
         """
-        super(MinMaxCalibrater, self).__init__(
+        super().__init__(
             model,
             op_types_to_calibrate=op_types_to_calibrate,
             augmented_model_path=augmented_model_path,
             symmetric=symmetric,
             use_external_data_format=use_external_data_format,
         )
         self.intermediate_outputs = []
         self.calibrate_tensors_range = None
         self.num_model_outputs = len(self.model.graph.output)
-        self.model_original_outputs = set(output.name for output in self.model.graph.output)
+        self.model_original_outputs = {output.name for output in self.model.graph.output}
         self.moving_average = moving_average
         if moving_average and (averaging_constant < 0 or averaging_constant > 1):
             raise ValueError("Invalid averaging constant, which should not be < 0 or > 1.")
         self.averaging_constant = averaging_constant
 
     def augment_graph(self):
         """
@@ -288,17 +287,17 @@
             for k, v in d.items():
                 merged_output_dict.setdefault(k, []).append(v)
         added_output_names = output_names[self.num_model_outputs :]
         calibrate_tensor_names = [
             added_output_names[i].rpartition("_")[0] for i in range(0, len(added_output_names), 2)
         ]  # output names
 
-        merged_added_output_dict = dict(
-            (i, merged_output_dict[i]) for i in merged_output_dict if i not in self.model_original_outputs
-        )
+        merged_added_output_dict = {
+            i: merged_output_dict[i] for i in merged_output_dict if i not in self.model_original_outputs
+        }
 
         pairs = []
         for i in range(0, len(added_output_names), 2):
             min_value = 0
             max_value = 0
             if self.moving_average:
                 min_value_array = np.mean(merged_added_output_dict[added_output_names[i]], axis=0)
@@ -346,25 +345,25 @@
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param method: A string. One of ['entropy', 'percentile'].
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param num_bins: number of bins to create a new histogram for collecting tensor values.
         :param num_quantized_bins: number of quantized bins. Default 128.
         :param percentile: A float number between [0, 100]. Default 99.99.
         """
-        super(HistogramCalibrater, self).__init__(
+        super().__init__(
             model,
             op_types_to_calibrate=op_types_to_calibrate,
             augmented_model_path=augmented_model_path,
             symmetric=symmetric,
             use_external_data_format=use_external_data_format,
         )
         self.intermediate_outputs = []
         self.calibrate_tensors_range = None
         self.num_model_outputs = len(self.model.graph.output)
-        self.model_original_outputs = set(output.name for output in self.model.graph.output)
+        self.model_original_outputs = {output.name for output in self.model.graph.output}
         self.collector = None
         self.method = method
         self.num_bins = num_bins
         self.num_quantized_bins = num_quantized_bins
         self.percentile = percentile
         self.tensors_to_calibrate = None
 
@@ -409,15 +408,15 @@
         ]
 
         merged_dict = {}
         for d in output_dicts_list:
             for k, v in d.items():
                 merged_dict.setdefault(k, []).append(v)
 
-        clean_merged_dict = dict((i, merged_dict[i]) for i in merged_dict if i in self.tensors_to_calibrate)
+        clean_merged_dict = {i: merged_dict[i] for i in merged_dict if i in self.tensors_to_calibrate}
 
         if not self.collector:
             self.collector = HistogramCollector(
                 method=self.method,
                 symmetric=self.symmetric,
                 num_bins=self.num_bins,
                 num_quantized_bins=self.num_quantized_bins,
@@ -456,15 +455,15 @@
         :param augmented_model_path: save augmented model to this path.
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param method: A string. One of ['entropy', 'percentile'].
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param num_bins: number of bins to create a new histogram for collecting tensor values.
         :param num_quantized_bins: number of quantized bins. Default 128.
         """
-        super(EntropyCalibrater, self).__init__(
+        super().__init__(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format,
             method=method,
             symmetric=symmetric,
             num_bins=num_bins,
@@ -490,15 +489,15 @@
         :param augmented_model_path: save augmented model to this path.
         :param use_external_data_format: use external data format to store model which size is >= 2Gb
         :param method: A string. One of ['entropy', 'percentile'].
         :param symmetric: make range of tensor symmetric (central point is 0).
         :param num_quantized_bins: number of quantized bins. Default 128.
         :param percentile: A float number between [0, 100]. Default 99.99.
         """
-        super(PercentileCalibrater, self).__init__(
+        super().__init__(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format,
             method=method,
             symmetric=symmetric,
             num_bins=num_bins,
@@ -564,24 +563,24 @@
             raise ValueError("Only 'entropy' or 'percentile' method are supported")
 
     def collect_absolute_value(self, name_to_arr):
         """
         Collect histogram on absolute value
         """
         for tensor, data_arr in name_to_arr.items():
-            data_arr = np.asarray(data_arr)
-            data_arr = data_arr.flatten()
+            data_arr = np.asarray(data_arr)  # noqa: PLW2901
+            data_arr = data_arr.flatten()  # noqa: PLW2901
             if data_arr.size > 0:
                 min_value = np.min(data_arr)
                 max_value = np.max(data_arr)
             else:
                 min_value = 0
                 max_value = 0
 
-            data_arr = np.absolute(data_arr)  # only consider absolute value
+            data_arr = np.absolute(data_arr)  # only consider absolute value  # noqa: PLW2901
 
             if tensor not in self.histogram_dict:
                 # first time it uses num_bins to compute histogram.
                 hist, hist_edges = np.histogram(data_arr, bins=self.num_bins)
                 self.histogram_dict[tensor] = (hist, hist_edges, min_value, max_value)
             else:
                 old_histogram = self.histogram_dict[tensor]
@@ -601,16 +600,16 @@
                 self.histogram_dict[tensor] = (hist, hist_edges, min(old_min, min_value), max(old_max, max_value))
 
     def collect_value(self, name_to_arr):
         """
         Collect histogram on real value
         """
         for tensor, data_arr in name_to_arr.items():
-            data_arr = np.asarray(data_arr)
-            data_arr = data_arr.flatten()
+            data_arr = np.asarray(data_arr)  # noqa: PLW2901
+            data_arr = data_arr.flatten()  # noqa: PLW2901
 
             if data_arr.size > 0:
                 min_value = np.min(data_arr)
                 max_value = np.max(data_arr)
             else:
                 min_value = 0
                 max_value = 0
@@ -629,15 +628,14 @@
                     hist_edges,
                     min_value,
                     max_value,
                     threshold,
                 )
 
     def merge_histogram(self, old_histogram, data_arr, new_min, new_max, new_threshold):
-
         (old_hist, old_hist_edges, old_min, old_max, old_threshold) = old_histogram
 
         if new_threshold <= old_threshold:
             new_hist, _ = np.histogram(data_arr, len(old_hist), range=(-old_threshold, old_threshold))
             return (
                 new_hist + old_hist,
                 old_hist_edges,
@@ -664,15 +662,15 @@
                 max(old_max, new_max),
                 new_threshold,
             )
 
     def compute_collection_result(self):
         if not self.histogram_dict or len(self.histogram_dict) == 0:
             raise ValueError("Histogram has not been collected. Please run collect() first.")
-        print("Finding optimal threshold for each tensor using {} algorithm ...".format(self.method))
+        print(f"Finding optimal threshold for each tensor using {self.method} algorithm ...")
 
         if self.method == "entropy":
             return self.compute_entropy()
         elif self.method == "percentile":
             return self.compute_percentile()
         else:
             raise ValueError("Only 'entropy' or 'percentile' method are supported")
@@ -682,17 +680,17 @@
             raise ValueError("Invalid percentile. Must be in range 0 <= percentile <= 100.")
 
         histogram_dict = self.histogram_dict
         percentile = self.percentile
 
         thresholds_dict = {}  # per tensor thresholds
 
-        print("Number of tensors : {}".format(len(histogram_dict)))
-        print("Number of histogram bins : {}".format(self.num_bins))
-        print("Percentile : ({},{})".format(100.0 - percentile, percentile))
+        print(f"Number of tensors : {len(histogram_dict)}")
+        print(f"Number of histogram bins : {self.num_bins}")
+        print(f"Percentile : ({100.0 - percentile},{percentile})")
 
         for tensor, histogram in histogram_dict.items():
             hist = histogram[0]
             hist_edges = histogram[1]
             total = hist.sum()
             cdf = np.cumsum(hist / total)
             if self.symmetric:
@@ -724,21 +722,21 @@
 
     def compute_entropy(self):
         histogram_dict = self.histogram_dict
         num_quantized_bins = self.num_quantized_bins
 
         thresholds_dict = {}  # per tensor thresholds
 
-        print("Number of tensors : {}".format(len(histogram_dict)))
+        print(f"Number of tensors : {len(histogram_dict)}")
         print(
             "Number of histogram bins : {} (The number may increase depends on the data it collects)".format(
                 self.num_bins
             )
         )
-        print("Number of quantized bins : {}".format(self.num_quantized_bins))
+        print(f"Number of quantized bins : {self.num_quantized_bins}")
 
         for tensor, histogram in histogram_dict.items():
             optimal_threshold = self.get_entropy_threshold(histogram, num_quantized_bins)
             thresholds_dict[tensor] = optimal_threshold
 
             # Plot histogram for debug only
             if False:
@@ -843,17 +841,16 @@
 
 def create_calibrator(
     model,
     op_types_to_calibrate: Optional[Sequence[str]] = None,
     augmented_model_path="augmented_model.onnx",
     calibrate_method=CalibrationMethod.MinMax,
     use_external_data_format=False,
-    extra_options={},
+    extra_options={},  # noqa: B006
 ):
-
     calibrator = None
     if calibrate_method == CalibrationMethod.MinMax:
         # default settings for min-max algorithm
         symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
         moving_average = False if "moving_average" not in extra_options else extra_options["moving_average"]
         averaging_constant = 0.01 if "averaging_constant" not in extra_options else extra_options["averaging_constant"]
         calibrator = MinMaxCalibrater(
@@ -895,8 +892,8 @@
         )
 
     if calibrator:
         calibrator.augment_graph()
         calibrator.create_inference_session()
         return calibrator
 
-    raise ValueError("Unsupported calibration method {}".format(calibrate_method))
+    raise ValueError(f"Unsupported calibration method {calibrate_method}")
```

## onnxruntime/quantization/onnx_model.py

```diff
@@ -125,15 +125,15 @@
     def get_initializer(self, name):
         for tensor in self.model.graph.initializer:
             if tensor.name == name:
                 return tensor
         return None
 
     def get_initializer_name_set(self):
-        return set(initializer.name for initializer in self.model.graph.initializer)
+        return {initializer.name for initializer in self.model.graph.initializer}
 
     def remove_initializer(self, tensor):
         if tensor in self.model.graph.initializer:
             self.model.graph.initializer.remove(tensor)
             for input in self.model.graph.input:
                 if input.name == tensor.name:
                     self.model.graph.input.remove(input)
@@ -251,69 +251,71 @@
                         for subgraph in attr.graphs:
                             graph_path.append(subgraph)
                             value.extend([ONNXModel.__replace_gemm_with_matmul(graph_path)])
                         kv = {attr.name: value}
                     else:
                         kv = attribute_to_kwarg(attr)
                     kwargs.update(kv)
-                node = onnx_helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
+                node = onnx_helper.make_node(  # noqa: PLW2901
+                    node.op_type, node.input, node.output, name=node.name, **kwargs
+                )
 
             if node.op_type == "Gemm":
                 alpha = 1.0
                 beta = 1.0
-                transA = 0
-                transB = 0
+                transA = 0  # noqa: N806
+                transB = 0  # noqa: N806
                 for attr in node.attribute:
                     if attr.name == "alpha":
                         alpha = onnx_helper.get_attribute_value(attr)
                     elif attr.name == "beta":
                         beta = onnx_helper.get_attribute_value(attr)
                     elif attr.name == "transA":
-                        transA = onnx_helper.get_attribute_value(attr)
+                        transA = onnx_helper.get_attribute_value(attr)  # noqa: N806
                     elif attr.name == "transB":
-                        transB = onnx_helper.get_attribute_value(attr)
+                        transB = onnx_helper.get_attribute_value(attr)  # noqa: N806
                 if alpha == 1.0 and beta == 1.0 and transA == 0:
-                    inputB = node.input[1]
+                    inputB = node.input[1]  # noqa: N806
                     if transB == 1:
-                        B, Bs_graph = ONNXModel.__get_initializer(node.input[1], graph_path)
+                        B, Bs_graph = ONNXModel.__get_initializer(node.input[1], graph_path)  # noqa: N806
                         if B:
                             # assume B is not used by any other node
-                            B_array = onnx_numpy_helper.to_array(B)
-                            B_trans = onnx_numpy_helper.from_array(B_array.T)
+                            B_array = onnx_numpy_helper.to_array(B)  # noqa: N806
+                            B_trans = onnx_numpy_helper.from_array(B_array.T)  # noqa: N806
                             B_trans.name = B.name
                             Bs_graph.initializer.remove(B)
                             for input in Bs_graph.input:
                                 if input.name == inputB:
                                     Bs_graph.input.remove(input)
                                     break
                             Bs_graph.initializer.extend([B_trans])
                         else:
-                            inputB += "_Transposed"
+                            inputB += "_Transposed"  # noqa: N806
                             transpose_node = onnx_helper.make_node(
                                 "Transpose",
                                 inputs=[node.input[1]],
                                 outputs=[inputB],
-                                name=node.name + "_Transpose" if node.name != "" else "",
+                                name=node.name + "_Transpose" if node.name else "",
                             )
                             new_nodes.append(transpose_node)
 
                     matmul_node = onnx_helper.make_node(
                         "MatMul",
                         inputs=[node.input[0], inputB],
                         outputs=[node.output[0] + ("_MatMul" if len(node.input) > 2 else "")],
-                        name=node.name + "_MatMul" if node.name != "" else "",
+                        name=node.name + "_MatMul" if node.name else "",
                     )
                     new_nodes.append(matmul_node)
 
                     if len(node.input) > 2:
                         add_node = onnx_helper.make_node(
                             "Add",
                             inputs=[node.output[0] + "_MatMul", node.input[2]],
                             outputs=node.output,
-                            name=node.name + "_Add" if node.name != "" else "",
+                            name=node.name + "_Add" if node.name else "",
                         )
                         new_nodes.append(add_node)
 
                 # unsupported
                 else:
                     new_nodes.append(node)
 
@@ -389,24 +391,18 @@
                 for graph_input in self.graph().input:
                     if graph_input.name == w.name:
                         self.graph().input.remove(graph_input)
 
         self.remove_initializers(ununsed_weights)
 
     def is_graph_output(self, output_name):
-        for output in self.model.graph.output:
-            if output.name == output_name:
-                return True
-        return False
+        return any(output.name == output_name for output in self.model.graph.output)
 
     def is_graph_input(self, tensor_name: str) -> bool:
-        for input in self.model.graph.input:
-            if input.name == tensor_name:
-                return True
-        return False
+        return any(input.name == tensor_name for input in self.model.graph.input)
 
     # TODO:use OnnxModel.graph_topological_sort(self.model.graph) from transformers.onnx_model
     # Currently it breaks Openvino/Linux training gpu pipeline so hold off for 1.8 release
     def topological_sort(self):
         deps_count = [0] * len(self.nodes())  # dependency count of each node
         deps_to_nodes = {}  # input to node indice
         sorted_nodes = []  # initialize sorted_nodes
@@ -414,14 +410,16 @@
             # CANNOT use len(node.input) directly because input can be optional
             deps_count[node_idx] = sum(1 for _ in node.input if _)
             if deps_count[node_idx] == 0:  # Constant doesn't depend on any inputs
                 sorted_nodes.append(self.nodes()[node_idx])
                 continue
 
             for input_name in node.input:
+                if not input_name:
+                    continue
                 if input_name not in deps_to_nodes:
                     deps_to_nodes[input_name] = [node_idx]
                 else:
                     deps_to_nodes[input_name].append(node_idx)
 
         initializer_names = [init.name for init in self.initializer()]
         graph_input_names = [input.name for input in self.model.graph.input]
```

## onnxruntime/quantization/onnx_quantizer.py

```diff
@@ -45,15 +45,14 @@
         activation_qType,
         tensors_range,
         nodes_to_quantize,
         nodes_to_exclude,
         op_types_to_quantize,
         extra_options=None,
     ):
-
         if not model_has_infer_metadata(model):
             model = save_and_reload_model(model)
         self.value_infos = {vi.name: vi for vi in model.graph.value_info}
         self.value_infos.update({ot.name: ot for ot in model.graph.output})
         self.value_infos.update({it.name: it for it in model.graph.input})
 
         self.model = ONNXModel(model)
@@ -110,16 +109,16 @@
         self.tensor_names.update({ot.name: 1 for ot in model.graph.output})
         self.tensor_names.update({it.name: 1 for it in model.graph.input})
         for node in self.model.model.graph.node:
             self.tensor_names.update({output_name: 1 for output_name in node.output})
 
         self.opset_version = self.check_opset_version()
 
-        if not self.mode in QuantizationMode:
-            raise ValueError("unsupported quantization mode {}".format(self.mode))
+        if self.mode not in QuantizationMode:
+            raise ValueError(f"unsupported quantization mode {self.mode}")
 
         self.quantization_params = self.calculate_quantization_params()
 
         # QuantizeRange tensor name and zero tensor name for scale and zero point calculation.
         # Used when static is False
         self.fixed_qrange_uint8_name = "fixed_quantization_range_uint8"
         self.fixed_qrange_int8_name = "fixed_quantization_range_int8"
@@ -160,15 +159,15 @@
             self.tensors_range,
             self.nodes_to_quantize,
             self.nodes_to_exclude,
             self.op_types_to_quantize,
             self.extra_options,
         )
         sub_quanitzer.parent = self
-        sub_quanitzer.graph_scope = "{}{}/".format(self.graph_scope, graph_key)
+        sub_quanitzer.graph_scope = f"{self.graph_scope}{graph_key}/"
         sub_quanitzer.quantize_model()
         return sub_quanitzer.model.model.graph
 
     def quantize_node_with_sub_graph(self, node):
         """
         Check subgraph, if any, quantize it and replace it.
         return new_nodes added for quantizing subgraph
@@ -176,41 +175,41 @@
         graph_attrs = [
             attr
             for attr in node.attribute
             if attr.type == onnx.AttributeProto.GRAPH or attr.type == onnx.AttributeProto.GRAPHS
         ]
         if len(graph_attrs) == 0:
             return node
-        node_name = node.name if node.name != "" else "{}_node_count_{}".format(node.op_type, len(self.new_nodes))
+        node_name = node.name if node.name else f"{node.op_type}_node_count_{len(self.new_nodes)}"
         kwargs = {}
         for attr in node.attribute:
             if attr.type == onnx.AttributeProto.GRAPH:
-                kv = {attr.name: self.quantize_subgraph(attr.g, "{}:{}".format(node_name, attr.name))}
+                kv = {attr.name: self.quantize_subgraph(attr.g, f"{node_name}:{attr.name}")}
             elif attr.type == onnx.AttributeProto.GRAPHS:
                 value = []
                 for subgraph in attr.graphs:
                     value.extend(
                         [
                             self.quantize_subgraph(
                                 subgraph,
-                                "{}:{}:{}".format(node_name, attr.name, len(value)),
+                                f"{node_name}:{attr.name}:{len(value)}",
                             )
                         ]
                     )
                 kv = {attr.name: value}
             else:
                 kv = attribute_to_kwarg(attr)
             kwargs.update(kv)
         return onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
 
     def check_opset_version(self):
         ai_onnx_domain = [
             opset for opset in self.model.model.opset_import if not opset.domain or opset.domain == "ai.onnx"
         ]
-        if 1 != len(ai_onnx_domain):
+        if len(ai_onnx_domain) != 1:
             raise ValueError("Failed to find proper ai.onnx domain")
         opset_version = ai_onnx_domain[0].version
 
         if opset_version == 10:
             logging.warning(
                 "The original model opset version is {}, which does not support node fusions. Please update the model to opset >= 11 for better performance.".format(
                     opset_version
@@ -227,15 +226,15 @@
             self.model.model.opset_import.remove(ai_onnx_domain[0])
             self.model.model.opset_import.extend([onnx.helper.make_opsetid("", 11)])
             opset_version = 11
 
         self.fuse_dynamic_quant = True
         return opset_version
 
-    def has_QDQ_nodes(self):
+    def has_QDQ_nodes(self):  # noqa: N802
         """
         Detect if model already has QuantizeLinear or DequantizeLinear.
         """
         return any(
             node.op_type == "QuantizeLinear" or node.op_type == "DequantizeLinear" for node in self.model.nodes()
         )
 
@@ -258,15 +257,15 @@
                 "Please check if the model is already quantized."
                 "Note you don't need to quantize a QAT model. OnnxRuntime support to run QAT model directly."
             )
 
         for node in self.model.nodes():
             # quantize subgraphes if have
             if self.enable_subgraph_quantization:
-                node = self.quantize_node_with_sub_graph(node)
+                node = self.quantize_node_with_sub_graph(node)  # noqa: PLW2901
 
             number_of_existing_new_nodes = len(self.new_nodes)
             op_quantizer = CreateOpQuantizer(self, node)
             op_quantizer.quantize()
             for i in range(number_of_existing_new_nodes, len(self.new_nodes)):
                 for output_name in self.new_nodes[i].output:
                     self.generated_value_names.add(output_name)
@@ -304,15 +303,15 @@
             return False
         return self.parent.is_valid_quantize_weight(weight_name)
 
     def is_float_tensor(self, tensor_name):
         if self.is_input_a_initializer(tensor_name):
             return self.is_valid_quantize_weight(tensor_name)
 
-        if tensor_name in self.value_infos.keys():
+        if tensor_name in self.value_infos:
             vi = self.value_infos[tensor_name]
             if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
                 return True
         elif self.enable_subgraph_quantization and self.parent:
             return self.parent.is_float_tensor(tensor_name)
         else:
             logging.warning(
@@ -354,15 +353,15 @@
     def _get_dynamic_input_quantization_params_int8(self, input_name, nodes_list):
         """
         Create nodes for dynamic quantization of input to int8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
-        qType = onnx_proto.TensorProto.INT8
+        qType = onnx_proto.TensorProto.INT8  # noqa: N806
 
         # Reduce min and Reduce max
         input_scale_name = input_name + "_scale"
 
         reduce_min_name = input_name + "_ReduceMin"
         reduce_min_node = onnx.helper.make_node(
             "ReduceMin",
@@ -437,15 +436,15 @@
     def _get_dynamic_input_quantization_params_uint8(self, input_name, nodes_list):
         """
         Create nodes for dynamic quantization of input to uint8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
-        qType = onnx_proto.TensorProto.UINT8
+        qType = onnx_proto.TensorProto.UINT8  # noqa: N806
         # Reduce min and Reduce max
         input_scale_name = input_name + "_scale"
         input_zp_name = input_name + "_zero_point"
 
         reduce_min_name = input_name + "_ReduceMin"
         reduce_min_node = onnx.helper.make_node(
             "ReduceMin",
@@ -532,15 +531,15 @@
         Create initializers and inputs in the graph for zero point and scale of output.
         Zero point and scale values are obtained from self.quantization_params if specified.
             parameter param_name: Name of the quantization parameter.
             return: result, scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
         if use_scale is None or use_zeropoint is None:
             if self.quantization_params is None or param_name not in self.quantization_params:
-                logging.info('Quantization parameters for tensor:"{}" not specified'.format(param_name))
+                logging.info(f'Quantization parameters for tensor:"{param_name}" not specified')
                 return False, "", "", "", ""
 
             params = self.quantization_params[param_name]
             if params is None or len(params) != 2:
                 raise ValueError(
                     "Quantization parameters should contain zero point and scale. "
                     "Specified values for output {}: {}".format(param_name, params)
@@ -622,15 +621,15 @@
                     "QuantizeLinear",
                     [input_name, scale_name, zp_name],
                     [output_name],
                     ql_node_name,
                 )
 
         self.quantized_value_map[input_name] = QuantizedValue(input_name, output_name, scale_name, zp_name, qType)
-        return nodes + [qlinear_node]
+        return [*nodes, qlinear_node]
 
     def set_quant_scale_zp(self, tensor_name, value):
         assert isinstance(value, tuple) and len(value) == 2, "value must be scale(float) and zeropoint"
         assert tensor_name not in self.used_scale_zp_map, f"{tensor_name} has been setted before"
         self.used_scale_zp_map[tensor_name] = value
 
     def find_quant_scale_zp(self, input_name):
@@ -668,15 +667,15 @@
 
         # get scale for input
         if input_name in self.quantized_value_map:
             input_scale_name = self.quantized_value_map[input_name].scale_name
         elif input_name in self.quantization_params:
             _, input_scale_name, _, _, _ = self._get_quantization_params(input_name)
         else:
-            raise ValueError("Expected {} to be in quantized value map for static quantization".format(input_name))
+            raise ValueError(f"Expected {input_name} to be in quantized value map for static quantization")
 
         inputscale_initializer = find_by_name(input_scale_name, self.model.initializer())
         input_scale = tensor_proto_to_array(inputscale_initializer)
 
         # calcuate scale for bias
         bias_scale = input_scale * weight_scale * beta
 
@@ -799,20 +798,29 @@
             # Find if this input is already quantized
             if node_input in self.quantized_value_map:
                 quantized_value = self.quantized_value_map[node_input]
                 scale_names.append(quantized_value.scale_name)
                 zero_point_names.append(quantized_value.zp_name)
                 quantized_input_names.append(quantized_value.q_name)
                 continue
-
+            # adding this for case embed_layernorm.py has optional segment_embedding
+            if not node_input:
+                quantized_input_names.append("")
+                scale_names.append("")
+                zero_point_names.append("")
+                continue
             # Quantize the input
             initializer = find_by_name(node_input, self.model.initializer())
             if initializer is not None:
                 if self.per_channel and op_level_per_channel:
-                    (q_weight_name, zp_name, scale_name,) = self.quantize_weight_per_channel(
+                    (
+                        q_weight_name,
+                        zp_name,
+                        scale_name,
+                    ) = self.quantize_weight_per_channel(
                         initializer.name,
                         self.weight_qType if initializer_use_weight_qType else self.activation_qType,
                         axis,
                         reduce_range,
                     )
                 else:
                     q_weight_name, zp_name, scale_name = self.quantize_initializer(
@@ -863,17 +871,15 @@
                     from_subgraph=True,
                 )
                 quantized_input_names.append(parent_quantized_input_names[0])
                 scale_names.append(parent_scale_names[0])
                 zero_point_names.append(parent_zero_point_names[0])
                 # node should not be add this child level here
             else:
-                raise ValueError(
-                    "Invalid tensor name to quantize: {} @graph scope{}".format(node_input, self.graph_scope)
-                )
+                raise ValueError(f"Invalid tensor name to quantize: {node_input} @graph scope{self.graph_scope}")
 
         return quantized_input_names, zero_point_names, scale_names, nodes
 
     def quantize_initializer(self, weight, qType, reduce_range=False, keep_float_weight=False):
         """
         :param weight: TensorProto initializer
         :param qType: type to quantize to
@@ -1064,14 +1070,14 @@
                 continue
             if len(self.model.input_name_to_nodes()[node.input[0]]) != 1:
                 continue
             if node.input[0] not in self.tensors_range.keys() or node.output[0] not in self.tensors_range.keys():
                 continue
             self.tensors_range[node.input[0]] = self.tensors_range[node.output[0]]
         quantization_params = {}
-        for tensor_name in self.tensors_range.keys():
+        for tensor_name in self.tensors_range:
             rmin, rmax = self.tensors_range[tensor_name]
             qmin, qmax = get_qmin_qmax_for_qType(self.activation_qType, symmetric=self.is_activation_symmetric)
 
             quantization_params[tensor_name] = compute_scale_zp(rmin, rmax, qmin, qmax, self.is_activation_symmetric)
 
         return quantization_params
```

## onnxruntime/quantization/qdq_loss_debug.py

```diff
@@ -6,15 +6,15 @@
 
 """Utilities to run a given ONNX model, while saving input/output tensors of
 eligible operator nodes.
 
 A use case is to debug quantization induced accuracy drop. An AI engineer can
 run the original float32 model and the quantized model with the same inputs,
 then compare the corresponding activations between the two models to find
-where the divergence is. 
+where the divergence is.
 
 Example Usage:
 
 ```python
     class ExampleDataReader(CalibrationDataReader):
         def __init__(self):
             ...
```

## onnxruntime/quantization/qdq_quantizer.py

```diff
@@ -93,16 +93,23 @@
         # We do quantization on Dequantizelinear's input to remove Quantizelinear for weight as an optimization.
         # In some cases, for example QDQ BERT model for TensorRT, QDQ should always appear as a pair.
         # Therefore, we need to disable this optimization and add qdq pair to weight.
         self.add_qdq_pair_to_weight = (
             False if "AddQDQPairToWeight" not in extra_options else extra_options["AddQDQPairToWeight"]
         )
 
+        # Some scenarios do not need the bias quantized. For example, in the case of Quantization Aware Training,
+        # quantizing the bias is not needed. This is because in QAT, all model parameters are expected to be in
+        # floating point format. To that end, we can use the FakeQuant operator for weights and activations that
+        # can always have QDQ pairs (by using AddQDQPairToWeight). But for biases in a quantized model, we can't use
+        # FakeQuant because it only ever appears before a DQ (since it is quantized as int32).
+        self.quantize_bias = True if "QuantizeBias" not in extra_options else extra_options["QuantizeBias"]
+
         # The default behavior is that multiple nodes can share a QDQ pair as their inputs.
-        # In TRT, QDQ pair can’t be shared between nodes, so it will create dedicated QDQ pairs for each node.
+        # In TRT, QDQ pair can`t be shared between nodes, so it will create dedicated QDQ pairs for each node.
         self.dedicated_qdq_pair = (
             False if "DedicatedQDQPair" not in extra_options else extra_options["DedicatedQDQPair"]
         )
         if self.dedicated_qdq_pair:
             self.tensor_to_its_receiving_nodes = {}
 
         # Let user set channel axis for specific op type and it's effective only when per channel quantization is supported and per_channel is True.
@@ -116,15 +123,15 @@
         """
         Check if tensor can be quantized
         """
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight is not None:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
                 return True
-        elif tensor_name in self.value_infos.keys():
+        elif tensor_name in self.value_infos:
             vi = self.value_infos[tensor_name]
             if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type == TensorProto.FLOAT:
                 return True
         else:
             logging.warning(
                 "failed to infer the type of tensor: {}. Skip to quantize it. Please check if it is expected.".format(
                     tensor_name
@@ -175,25 +182,23 @@
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
                 self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
                     tensor_type=QDQQuantTensorType.WEIGHT, axis=axis
                 )
         else:
-            logging.warning(
-                "only support per-channel quantization on weight. Tensor: {} is not quantized.".format(tensor_name)
-            )
+            logging.warning(f"only support per-channel quantization on weight. Tensor: {tensor_name} is not quantized.")
 
     def quantize_bias_tensor(self, bias_name, input_name, weight_name, beta=1.0):
         weight = find_by_name(bias_name, self.model.initializer())
         if weight is not None:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
                 self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
         else:
-            logging.warning("Expected {} to be a weight".format(bias_name))
+            logging.warning(f"Expected {bias_name} to be a weight")
 
     def remove_node(self, node):
         self.nodes_to_remove.append(node)
 
     def remove_nodes(self):
         self.model.remove_nodes(self.nodes_to_remove)
 
@@ -207,27 +212,28 @@
                     for tensor_name in node.input:
                         if tensor_name not in self.tensor_to_its_receiving_nodes:
                             self.tensor_to_its_receiving_nodes[tensor_name] = []
                         self.tensor_to_its_receiving_nodes[tensor_name].append(node)
 
         self._quantize_normal_tensors()
         self._quantize_sharing_param_tensors()
-        self._quantize_bias_tensors()
+        if self.quantize_bias:
+            self._quantize_bias_tensors()
         self.remove_nodes()
         if not self.add_qdq_pair_to_weight:
             self.model.clean_initializers()
 
         self.model.model.producer_name = __producer__
         self.model.model.producer_version = __version__
 
         return self.model.model
 
     def try_replacing_upstream_output(self, upstream_output_name, output_name):
         if (
-            output_name in self.quantization_params.keys()
+            output_name in self.quantization_params
             and len(self.model.input_name_to_nodes()[upstream_output_name]) == 1
             and not self.model.is_graph_output(upstream_output_name)
             and not self.model.is_graph_input(upstream_output_name)
         ):
             self.model.replace_output_of_all_nodes(upstream_output_name, output_name)
             if upstream_output_name in self.tensors_to_quantize:
                 del self.tensors_to_quantize[upstream_output_name]
@@ -357,15 +363,15 @@
                 zp_name,
                 QuantizedValueType.Input,
             )
             self.quantized_value_map[tensor_name] = quantized_value
 
     def _quantize_normal_tensors(self):
         for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
-            if tensor_name in self.quantized_value_map.keys():
+            if tensor_name in self.quantized_value_map:
                 continue
 
             if not tensor_info.is_shared:
                 # Quantize the input
                 initializer = find_by_name(tensor_name, self.model.initializer())
                 if initializer:
                     self._add_qdq_pair_for_initializer(initializer, tensor_info.tensor_type, tensor_info.axis)
@@ -397,15 +403,15 @@
                     initializer = find_by_name(tensor_name, self.model.initializer())
                     if initializer is not None:
                         raise ValueError("Quantization parameter shared mode is not supported for weight yet")
                     self._add_qdq_pair_for_activation(tensor_name, quantized_value.scale_name, quantized_value.zp_name)
 
     def _quantize_bias_tensors(self):
         for bias_name, input_name, weight_name, beta in self.bias_to_quantize:
-            if bias_name in self.quantized_value_map.keys():
+            if bias_name in self.quantized_value_map:
                 continue
             # Quantize the input
             self.quantize_bias_static(bias_name, input_name, weight_name, beta)
             self.model.remove_initializer(find_by_name(bias_name, self.model.initializer()))
             quant_value = self.quantized_value_map[bias_name]
             inputs = [quant_value.q_name, quant_value.scale_name, quant_value.zp_name]
             node_name = add_dequant_suffix(bias_name)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## onnxruntime/quantization/quant_utils.py

```diff
@@ -52,72 +52,70 @@
         return self.name
 
     @staticmethod
     def from_string(mode):
         try:
             return QuantizationMode[mode]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 class QuantizedValueType(Enum):
     Input = 0
     Initializer = 1
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(v):
         try:
             return QuantizedValueType[v]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 class QuantType(Enum):
     QInt8 = 0
     QUInt8 = 1
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(t):
         try:
             return QuantType[t]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 class QuantFormat(Enum):
     QOperator = 0
     QDQ = 1
 
     def __str__(self):
         return self.name
 
     @staticmethod
     def from_string(format):
         try:
             return QuantFormat[format]
         except KeyError:
-            raise ValueError()
+            raise ValueError()  # noqa: B904
 
 
 ONNX_TYPE_TO_NP_TYPE = {
     onnx_proto.TensorProto.INT8: numpy.dtype("int8"),
     onnx_proto.TensorProto.UINT8: numpy.dtype("uint8"),
 }
 
 
 def quantize_nparray(qType, arr, scale, zero_point, low=None, high=None):
-    assert (
-        qType in ONNX_TYPE_TO_NP_TYPE
-    ), "Unexpected data type {} requested. Only INT8 and UINT8 are supported.".format(qType)
+    assert qType in ONNX_TYPE_TO_NP_TYPE, f"Unexpected data type {qType} requested. Only INT8 and UINT8 are supported."
     dtype = ONNX_TYPE_TO_NP_TYPE[qType]
     cliplow = max(0 if dtype == numpy.uint8 else -127, -127 if low is None else low)
     cliphigh = min(255 if dtype == numpy.uint8 else 127, 255 if high is None else high)
     arr_fp32 = numpy.asarray((arr.astype(numpy.float32) / scale).round() + zero_point)
     numpy.clip(arr_fp32, cliplow, cliphigh, out=arr_fp32)
     return arr_fp32.astype(dtype)
 
@@ -200,33 +198,33 @@
         zero_point, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric)
 
     quantized_data = quantize_nparray(qType, numpy.asarray(data), scale, zero_point)
 
     return rmin, rmax, zero_point, scale, quantized_data
 
 
-def get_qmin_qmax_for_qType(qType, reduce_range=False, symmetric=False):
+def get_qmin_qmax_for_qType(qType, reduce_range=False, symmetric=False):  # noqa: N802
     """
     Return qmin and qmax, the minimum and maximum value representable by the given qType
     :parameter qType: onnx.onnx_pb.TensorProto.UINT8 or onnx.onnx_pb.TensorProto.UINT8
     :return: qmin, qmax
     """
     if qType == onnx_proto.TensorProto.UINT8:
         (qmin, qmax) = (0, 127) if reduce_range else (0, 255)
     elif qType == onnx_proto.TensorProto.INT8:
         if symmetric:
             (qmin, qmax) = (-64, 64) if reduce_range else (-127, 127)
         else:
             (qmin, qmax) = (-64, 64) if reduce_range else (-128, 127)
     else:
-        raise ValueError("Unexpected data type {} requested. Only INT8 and UINT8 are supported.".format(qType))
+        raise ValueError(f"Unexpected data type {qType} requested. Only INT8 and UINT8 are supported.")
     return qmin, qmax
 
 
-def get_qrange_for_qType(qType, reduce_range=False, symmetric=False):
+def get_qrange_for_qType(qType, reduce_range=False, symmetric=False):  # noqa: N802
     """
     Helper function to get the quantization range for a type.
         parameter qType: quantization type.
         return: quantization range.
     """
     qmin, qmax = get_qmin_qmax_for_qType(qType, reduce_range, symmetric=symmetric)
     return qmax - qmin
@@ -241,16 +239,16 @@
         self,
         name,
         initializer,
         rmins,
         rmaxs,
         zero_points,
         scales,
-        data=[],
-        quantized_data=[],
+        data=[],  # noqa: B006
+        quantized_data=[],  # noqa: B006
         axis=None,
     ):
         self.name = name
         self.initializer = initializer  # TensorProto initializer in ONNX graph
         self.rmins = rmins  # List of minimum range for each axis
         self.rmaxs = rmaxs  # List of maximum range for each axis
         # 1D tensor of zero points computed for each axis. scalar if axis is empty
@@ -261,15 +259,15 @@
         # Scalar to specify which dimension in the initializer to weight pack.
         self.axis = axis
         # If empty, single zero point and scales computed from a single rmin and rmax
 
 
 class QuantizedValue:
     """
-    Represents a linearly quantized value (input\output\intializer)
+    Represents a linearly quantized value (input\\output\\intializer)
     """
 
     def __init__(
         self,
         name,
         new_quantized_name,
         scale_name,
@@ -299,15 +297,15 @@
 def attribute_to_kwarg(attribute):
     """
     Convert attribute to kwarg format for use with onnx.helper.make_node.
         :parameter attribute: attribute in AttributeProto format.
         :return: attribute in {key: value} format.
     """
     if attribute.type == 0:
-        raise ValueError("attribute {} does not have type specified.".format(attribute.name))
+        raise ValueError(f"attribute {attribute.name} does not have type specified.")
 
     # Based on attribute type definitions from AttributeProto
     # definition in https://github.com/onnx/onnx/blob/main/onnx/onnx.proto
     if attribute.type == 1:
         value = attribute.f
     elif attribute.type == 2:
         value = attribute.i
@@ -324,15 +322,15 @@
     elif attribute.type == 8:
         value = attribute.strings
     elif attribute.type == 9:
         value = attribute.tensors
     elif attribute.type == 10:
         value = attribute.graphs
     else:
-        raise ValueError("attribute {} has unsupported type {}.".format(attribute.name, attribute.type))
+        raise ValueError(f"attribute {attribute.name} has unsupported type {attribute.type}.")
 
     return {attribute.name: value}
 
 
 def find_by_name(item_name, item_list):
     """
     Helper function to find item by name in a list.
@@ -399,15 +397,15 @@
     import json
 
     import flatbuffers
 
     import onnxruntime.quantization.CalTableFlatBuffers.KeyValue as KeyValue
     import onnxruntime.quantization.CalTableFlatBuffers.TrtTable as TrtTable
 
-    logging.info("calibration cache: {}".format(calibration_cache))
+    logging.info(f"calibration cache: {calibration_cache}")
 
     with open("calibration.json", "w") as file:
         file.write(json.dumps(calibration_cache))  # use `json.loads` to do the reverse
 
     # Serialize data using FlatBuffers
     builder = flatbuffers.Builder(1024)
     key_value_list = []
@@ -503,15 +501,18 @@
         parameter model_path: path to the original onnx model
         parameter opt_model_path: path to the optimized onnx model
     :return: optimized onnx model
     """
     sess_option = SessionOptions()
     sess_option.optimized_model_filepath = opt_model_path.as_posix()
     sess_option.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_BASIC
-    _ = InferenceSession(model_path.as_posix(), sess_option, providers=["CPUExecutionProvider"])
+    kwargs = {}
+    # This will rename constant initializer names, disable it to make test pass.
+    kwargs["disabled_optimizers"] = ["ConstantSharing"]
+    _ = InferenceSession(model_path.as_posix(), sess_option, providers=["CPUExecutionProvider"], **kwargs)
 
 
 def add_pre_process_metadata(model):
     """Tag the model that it went through quantization pre-processing"""
     metadata_props = {"onnx.quant.pre_process": "onnxruntime.quant"}
     if model.metadata_props:
         for prop in model.metadata_props:
```

## onnxruntime/quantization/quantize.py

```diff
@@ -7,15 +7,15 @@
 import tempfile
 from pathlib import Path
 
 from .calibrate import CalibrationDataReader, CalibrationMethod, create_calibrator
 from .onnx_quantizer import ONNXQuantizer
 from .qdq_quantizer import QDQQuantizer
 from .quant_utils import QuantFormat, QuantizationMode, QuantType, load_model, model_has_pre_process_metadata
-from .registry import IntegerOpsRegistry, QLinearOpsRegistry
+from .registry import IntegerOpsRegistry, QDQRegistry, QLinearOpsRegistry
 
 
 class QuantConfig:
     def __init__(
         self,
         activation_type=QuantType.QUInt8,
         weight_type=QuantType.QInt8,
@@ -135,14 +135,19 @@
                     CalibMovingAverage = True/False :
                         Default is False. If enabled, the moving average of the minimum and maximum values will be
                         computed when the calibration method selected is MinMax.
                     CalibMovingAverageConstant = float :
                         Default is 0.01. Constant smoothing factor to use when computing the moving average of the
                         minimum and maximum values. Effective only when the calibration method selected is MinMax and
                         when CalibMovingAverage is set to True.
+                    QuantizeBias = True/False :
+                        Default is True which quantizes floating-point biases and it solely inserts
+                        a DeQuantizeLinear node. If False, it remains floating-point bias and does not insert
+                        any quantization nodes associated with biases.
+                        This extra option is only effective when quant_format is QuantFormat.QDQ.
             execution_provider : A enum indicates the Execution Provider such as: CPU, TRT, NNAPI, SNE, etc.
         Raises:
             ValueError: Raise ValueError if execution provider is unknown
         """
 
         super().__init__(
             activation_type=activation_type,
@@ -330,15 +335,17 @@
     extra_options = extra_options or {}
     nodes_to_exclude = nodes_to_exclude or []
     nodes_to_quantize = nodes_to_quantize or []
     op_types_to_quantize = op_types_to_quantize or []
     mode = QuantizationMode.QLinearOps
 
     if not op_types_to_quantize or len(op_types_to_quantize) == 0:
-        op_types_to_quantize = list(QLinearOpsRegistry.keys())
+        q_linear_ops = list(QLinearOpsRegistry.keys())
+        qdq_ops = list(QDQRegistry.keys())
+        op_types_to_quantize = list(set(q_linear_ops + qdq_ops))
 
     model = load_model(Path(model_input), optimize_model)
 
     pre_processed: bool = model_has_pre_process_metadata(model)
     if not pre_processed:
         logging.warning(
             "Please consider pre-processing before quantization. See "
```

## onnxruntime/quantization/registry.py

```diff
@@ -6,14 +6,15 @@
 from .operators.concat import QLinearConcat
 from .operators.conv import ConvInteger, QDQConv, QLinearConv
 from .operators.direct_q8 import Direct8BitOp, QDQDirect8BitOp
 from .operators.embed_layernorm import EmbedLayerNormalizationQuant
 from .operators.gather import GatherQuant, QDQGather
 from .operators.gavgpool import QGlobalAveragePool
 from .operators.gemm import QDQGemm, QLinearGemm
+from .operators.instnorm import QDQInstanceNormalization
 from .operators.lstm import LSTMQuant
 from .operators.matmul import MatMulInteger, QDQMatMul, QLinearMatMul
 from .operators.maxpool import QDQMaxPool, QMaxPool
 from .operators.pad import QPad
 from .operators.pooling import QLinearPool
 from .operators.qdq_base_operator import QDQOperatorBase
 from .operators.resize import QDQResize, QResize
@@ -60,14 +61,15 @@
     "Softmax": QLinearSoftmax,
     "Where": QLinearWhere,
 }
 QLinearOpsRegistry.update(CommonOpsRegistry)
 
 QDQRegistry = {
     "Conv": QDQConv,
+    "ConvTranspose": QDQConv,
     "Gemm": QDQGemm,
     "Clip": QDQRemovableActivation,
     "Relu": QDQRemovableActivation,
     "Reshape": QDQDirect8BitOp,
     "Transpose": QDQDirect8BitOp,
     "Squeeze": QDQDirect8BitOp,
     "Unsqueeze": QDQDirect8BitOp,
@@ -75,27 +77,28 @@
     "MaxPool": QDQMaxPool,
     "AveragePool": QDQDirect8BitOp,
     "MatMul": QDQMatMul,
     "Split": QDQSplit,
     "Gather": QDQGather,
     "Softmax": QDQSoftmax,
     "Where": QDQWhere,
+    "InstanceNormalization": QDQInstanceNormalization,
 }
 
 
-def CreateDefaultOpQuantizer(onnx_quantizer, node):
+def CreateDefaultOpQuantizer(onnx_quantizer, node):  # noqa: N802
     return QuantOperatorBase(onnx_quantizer, node)
 
 
-def CreateOpQuantizer(onnx_quantizer, node):
+def CreateOpQuantizer(onnx_quantizer, node):  # noqa: N802
     registry = IntegerOpsRegistry if onnx_quantizer.mode == QuantizationMode.IntegerOps else QLinearOpsRegistry
-    if node.op_type in registry.keys():
+    if node.op_type in registry:
         op_quantizer = registry[node.op_type](onnx_quantizer, node)
         if op_quantizer.should_quantize():
             return op_quantizer
     return QuantOperatorBase(onnx_quantizer, node)
 
 
-def CreateQDQQuantizer(onnx_quantizer, node):
-    if node.op_type in QDQRegistry.keys():
+def CreateQDQQuantizer(onnx_quantizer, node):  # noqa: N802
+    if node.op_type in QDQRegistry:
         return QDQRegistry[node.op_type](onnx_quantizer, node)
     return QDQOperatorBase(onnx_quantizer, node)
```

## onnxruntime/quantization/shape_inference.py

```diff
@@ -85,15 +85,15 @@
 
             opt_model_path = str(temp_path / "optimized.onnx")
             try:
                 sess_option = onnxruntime.SessionOptions()
                 sess_option.optimized_model_filepath = opt_model_path
                 sess_option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
                 _ = onnxruntime.InferenceSession(input_model_path, sess_option, providers=["CPUExecutionProvider"])
-            except Exception as e:
+            except Exception:
                 logger.error(
                     "ONNX Runtime Model Optimization Failed! Consider rerun with option `--skip_optimization'."
                 )
                 logger.error(traceback.format_exc())
 
             input_model_path = opt_model_path
```

## onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py

```diff
@@ -4,75 +4,75 @@
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 
 np = import_numpy()
 
 
-class KeyValue(object):
+class KeyValue:
     __slots__ = ["_tab"]
 
     @classmethod
-    def GetRootAs(cls, buf, offset=0):
+    def GetRootAs(cls, buf, offset=0):  # noqa: N802
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = KeyValue()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def GetRootAsKeyValue(cls, buf, offset=0):
+    def GetRootAsKeyValue(cls, buf, offset=0):  # noqa: N802
         """This method is deprecated. Please switch to GetRootAs."""
         return cls.GetRootAs(buf, offset)
 
     # KeyValue
-    def Init(self, buf, pos):
+    def Init(self, buf, pos):  # noqa: N802
         self._tab = flatbuffers.table.Table(buf, pos)
 
     # KeyValue
-    def Key(self):
+    def Key(self):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
     # KeyValue
-    def Value(self):
+    def Value(self):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
 
-def Start(builder):
+def Start(builder):  # noqa: N802
     builder.StartObject(2)
 
 
-def KeyValueStart(builder):
+def KeyValueStart(builder):  # noqa: N802
     """This method is deprecated. Please switch to Start."""
     return Start(builder)
 
 
-def AddKey(builder, key):
+def AddKey(builder, key):  # noqa: N802
     builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(key), 0)
 
 
-def KeyValueAddKey(builder, key):
+def KeyValueAddKey(builder, key):  # noqa: N802
     """This method is deprecated. Please switch to AddKey."""
     return AddKey(builder, key)
 
 
-def AddValue(builder, value):
+def AddValue(builder, value):  # noqa: N802
     builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
 
 
-def KeyValueAddValue(builder, value):
+def KeyValueAddValue(builder, value):  # noqa: N802
     """This method is deprecated. Please switch to AddValue."""
     return AddValue(builder, value)
 
 
-def End(builder):
+def End(builder):  # noqa: N802
     return builder.EndObject()
 
 
-def KeyValueEnd(builder):
+def KeyValueEnd(builder):  # noqa: N802
     """This method is deprecated. Please switch to End."""
     return End(builder)
```

## onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py

```diff
@@ -4,87 +4,87 @@
 
 import flatbuffers
 from flatbuffers.compat import import_numpy
 
 np = import_numpy()
 
 
-class TrtTable(object):
+class TrtTable:
     __slots__ = ["_tab"]
 
     @classmethod
-    def GetRootAs(cls, buf, offset=0):
+    def GetRootAs(cls, buf, offset=0):  # noqa: N802
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = TrtTable()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
-    def GetRootAsTrtTable(cls, buf, offset=0):
+    def GetRootAsTrtTable(cls, buf, offset=0):  # noqa: N802
         """This method is deprecated. Please switch to GetRootAs."""
         return cls.GetRootAs(buf, offset)
 
     # TrtTable
-    def Init(self, buf, pos):
+    def Init(self, buf, pos):  # noqa: N802
         self._tab = flatbuffers.table.Table(buf, pos)
 
     # TrtTable
-    def Dict(self, j):
+    def Dict(self, j):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             x = self._tab.Vector(o)
             x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
             x = self._tab.Indirect(x)
             from onnxruntime.quantization.CalTableFlatBuffers.KeyValue import KeyValue
 
             obj = KeyValue()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
     # TrtTable
-    def DictLength(self):
+    def DictLength(self):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             return self._tab.VectorLen(o)
         return 0
 
     # TrtTable
-    def DictIsNone(self):
+    def DictIsNone(self):  # noqa: N802
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         return o == 0
 
 
-def Start(builder):
+def Start(builder):  # noqa: N802
     builder.StartObject(1)
 
 
-def TrtTableStart(builder):
+def TrtTableStart(builder):  # noqa: N802
     """This method is deprecated. Please switch to Start."""
     return Start(builder)
 
 
-def AddDict(builder, dict):
+def AddDict(builder, dict):  # noqa: N802
     builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(dict), 0)
 
 
-def TrtTableAddDict(builder, dict):
+def TrtTableAddDict(builder, dict):  # noqa: N802
     """This method is deprecated. Please switch to AddDict."""
     return AddDict(builder, dict)
 
 
-def StartDictVector(builder, numElems):
+def StartDictVector(builder, numElems):  # noqa: N802
     return builder.StartVector(4, numElems, 4)
 
 
-def TrtTableStartDictVector(builder, numElems):
+def TrtTableStartDictVector(builder, numElems):  # noqa: N802
     """This method is deprecated. Please switch to Start."""
     return StartDictVector(builder, numElems)
 
 
-def End(builder):
+def End(builder):  # noqa: N802
     return builder.EndObject()
 
 
-def TrtTableEnd(builder):
+def TrtTableEnd(builder):  # noqa: N802
     """This method is deprecated. Please switch to End."""
     return End(builder)
```

## onnxruntime/quantization/operators/activation.py

```diff
@@ -1,20 +1,19 @@
 import onnx
-from onnx import onnx_pb as onnx_proto
 
 from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
 
 
 class QLinearActivation(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
-    def QuantizeClipRelu(self):
+    def QuantizeClipRelu(self):  # noqa: N802
         node = self.node
         assert node.op_type == "Relu" or node.op_type == "Clip"
 
         # When mode is QLinearOps, the output quantization params are calculated based on outputs from
         # activation nodes, therefore these nodes can be removed from the graph if they follow a quantized op.
         # If input to this node is not quantized then keep this node
         # If activation is symmetric, not quantize the op and simply return
@@ -55,15 +54,15 @@
             nodes,
         ) = self.quantizer.quantize_activation(node, [0])
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
         qlinear_activation_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
         qlinear_activation_name = ""
-        if node.name != "":
+        if node.name:
             qlinear_activation_name = node.name + "_quant"
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
         qlinear_activation_inputs = [
```

## onnxruntime/quantization/operators/attention.py

```diff
@@ -1,9 +1,9 @@
 import onnx
-from onnx import onnx_pb as onnx_proto
+from onnx import onnx_pb as onnx_proto  # noqa: F401
 
 from ..quant_utils import attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
 
 """
     Quantize Attention
 """
@@ -25,15 +25,15 @@
         node = self.node
         assert node.op_type == "Attention"
 
         # TODO This is a temporary fix to stop exporting QAttention with qkv_hidden_sizes
         # attribute. This needs to be removed once the QAttention for varied q,k,v sizes
         # is implemented
         for attr in node.attribute:
-            if "qkv_hidden_sizes" == attr.name:
+            if attr.name == "qkv_hidden_sizes":
                 return super().quantize()
 
         (
             quantized_input_names,
             zero_point_names,
             scale_names,
             nodes,
@@ -49,15 +49,15 @@
         zero_point_names.extend(zero_point_names_weight)
         scale_names.extend(scale_names_weight)
         nodes.extend(nodes_weight)
 
         if quantized_input_names is None:
             return super().quantize()
 
-        qattention_name = "" if node.name == "" else node.name + "_quant"
+        qattention_name = "" if not node.name else node.name + "_quant"
 
         inputs = []
         inputs.extend(quantized_input_names)
         inputs.extend([node.input[2]])
         inputs.extend(scale_names)
         inputs.extend([node.input[3] if len(node.input) > 3 else ""])
         inputs.extend(zero_point_names)
```

## onnxruntime/quantization/operators/binary_op.py

```diff
@@ -1,9 +1,9 @@
 import onnx
-from onnx import onnx_pb as onnx_proto
+from onnx import onnx_pb as onnx_proto  # noqa: F401
 
 from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
 
 
 class QLinearBinaryOp(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
@@ -25,15 +25,15 @@
             scale_names,
             nodes,
         ) = self.quantizer.quantize_activation(node, [0, 1])
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
         qlinear_binary_math_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
-        qlinear_binary_math_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_binary_math_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
         qlinear_binary_math_inputs = []
```

## onnxruntime/quantization/operators/concat.py

```diff
@@ -1,12 +1,18 @@
 import onnx
 
-from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
+from ..quant_utils import (  # noqa: F401
+    TENSOR_NAME_QUANT_SUFFIX,
+    QuantizedValue,
+    QuantizedValueType,
+    attribute_to_kwarg,
+    ms_domain,
+)
 from .base_operator import QuantOperatorBase
-from .qdq_base_operator import QDQOperatorBase
+from .qdq_base_operator import QDQOperatorBase  # noqa: F401
 
 
 class QLinearConcat(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
@@ -39,15 +45,15 @@
         )
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
-        qnode_name = node.name + "_quant" if node.name != "" else ""
+        qnode_name = node.name + "_quant" if node.name else ""
 
         qlconcat_inputs = [output_scale_name, output_zp_name]
         for i in range(0, len(q_input_names)):
             qlconcat_inputs.extend([q_input_names[i], scale_names[i], zero_point_names[i]])
         qlconcat_node = onnx.helper.make_node(
             "QLinearConcat", qlconcat_inputs, [quantized_output_value.q_name], qnode_name, **kwargs
         )
```

## onnxruntime/quantization/operators/conv.py

```diff
@@ -1,14 +1,13 @@
 import numpy as np
 import onnx
 from onnx import onnx_pb as onnx_proto
 
 from ..quant_utils import (
     TENSOR_NAME_QUANT_SUFFIX,
-    BiasToQuantize,
     QuantizedValue,
     QuantizedValueType,
     attribute_to_kwarg,
     find_by_name,
     get_mul_node,
 )
 from .base_operator import QuantOperatorBase
@@ -30,15 +29,15 @@
             return: the name of output
         """
         node = self.node
         model = self.quantizer.model
         # Add tensors for the shape to be reshaped to
         weight = find_by_name(node.input[1], model.initializer())
         if weight is None:
-            raise ValueError("Expected {} to be an initializer".format(node.input[1]))
+            raise ValueError(f"Expected {node.input[1]} to be an initializer")
 
         # Add reshape for correct broadcase
         output = node.output[0]
         reshape_input_data = node.input[2]  # bias of Conv
         reshape_input_shape = output + "_bias_reshape_shape"
         reshape_output = output + "_bias_reshape_output"
 
@@ -75,15 +74,15 @@
         ) = self.quantizer.quantize_weight(node, [1], reduce_range=self.quantizer.reduce_range)
         quantized_input_names.extend(quantized_input_names_weight)
         zero_point_names.extend(zero_point_names_weight)
         scale_names.extend(scale_names_weight)
         nodes.extend(nodes_weight)
 
         conv_integer_output = node.output[0] + "_output_quantized"
-        conv_integer_name = node.name + "_quant" if node.name != "" else ""
+        conv_integer_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         conv_integer_node = onnx.helper.make_node(
             "ConvInteger", quantized_input_names + zero_point_names, [conv_integer_output], conv_integer_name, **kwargs
         )
@@ -98,15 +97,15 @@
             conv_integer_output + "_cast",
             to=onnx_proto.TensorProto.FLOAT,
         )
         nodes.append(cast_node)
 
         # Add mul operation to multiply scales of two inputs.
         assert len(scale_names) == 2
-        if conv_integer_name != "":
+        if conv_integer_name:
             scales_mul_op = conv_integer_name + "_scales_mul"
         else:
             scales_mul_op = scale_names[0] + "_" + scale_names[1] + "_mul"
 
         scales_mul_node = find_by_name(scales_mul_op, self.quantizer.new_nodes)
         if scales_mul_node is None:
             scales_mul_node = get_mul_node(scale_names, scales_mul_op + ":0", scales_mul_op)
@@ -115,15 +114,15 @@
         scales_mul_op_output = scales_mul_node.output[0]
 
         has_bias = len(node.input) == 3
         scaled_output_name = node.output[0] if not has_bias else node.output[0] + "quant_scaled_output"
 
         # Add mul operation to multiply mul_scales_op result with output of ConvInteger
         # and make the output of this node the same as output of original conv node.
-        output_scale_mul_op = conv_integer_name + "_output_scale_mul" if conv_integer_name != "" else ""
+        output_scale_mul_op = conv_integer_name + "_output_scale_mul" if conv_integer_name else ""
         nodes.append(
             get_mul_node(
                 [cast_op_output, scales_mul_op_output],
                 scaled_output_name,
                 output_scale_mul_op,
             )
         )
@@ -188,15 +187,15 @@
         quantized_bias_name = ""
         bias_present = False
         if len(node.input) == 3:
             quantized_bias_name = self.quantizer.quantize_bias_static(node.input[2], node.input[0], node.input[1])
             bias_present = True
 
         qlinear_conv_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
-        qlinear_conv_name = qlinear_conv_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_conv_name = qlinear_conv_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         qlinear_conv_inputs = []
         # Input 0
         qlinear_conv_inputs.append(quantized_input_names[0])
@@ -234,15 +233,15 @@
 
 class QDQConv(QDQOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
-        assert node.op_type == "Conv"
+        assert node.op_type == "Conv" or node.op_type == "ConvTranspose"
 
         self.quantizer.quantize_activation_tensor(node.input[0])
         if not self.disable_qdq_for_node_output:
             self.quantizer.quantize_activation_tensor(node.output[0])
 
         if self.quantizer.is_per_channel():
             self.quantizer.quantize_weight_tensor_per_channel(node.input[1], 0)
```

## onnxruntime/quantization/operators/embed_layernorm.py

```diff
@@ -1,11 +1,11 @@
 import logging
 
 import onnx
-from onnx import onnx_pb as onnx_proto
+from onnx import onnx_pb as onnx_proto  # noqa: F401
 
 from ..quant_utils import attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
 
 """
 Quantizes the EmbedLayerNorm fused ONNXRuntime Op.
 
@@ -45,15 +45,15 @@
             zero_point_names,
             scale_names,
             nodes,
         ) = self.quantizer.quantize_activation(node, [2, 3, 4, 5, 6])
         if quantized_input_names is None:
             return super().quantize()
 
-        qembed_layer_norm_name = "" if node.name == "" else node.name + "_quant"
+        qembed_layer_norm_name = "" if not node.name else node.name + "_quant"
 
         """
         Quantized Input Tensor List
         [0] input_ids (int32)
         [1] segment_ids (int32)
         [2] word_embedding (uint8)
         [3] position_embedding (uint8)
```

## onnxruntime/quantization/operators/gavgpool.py

```diff
@@ -40,15 +40,15 @@
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
         kwargs["channels_last"] = 0
-        qnode_name = node.name + "_quant" if node.name != "" else ""
+        qnode_name = node.name + "_quant" if node.name else ""
 
         qnode = onnx.helper.make_node(
             "QLinear" + node.op_type,
             [
                 quantized_input_value.q_name,
                 quantized_input_value.scale_name,
                 quantized_input_value.zp_name,
```

## onnxruntime/quantization/operators/gemm.py

```diff
@@ -1,31 +1,25 @@
 import logging
 
-import numpy as np
+import numpy as np  # noqa: F401
 import onnx
 from onnx import onnx_pb as onnx_proto
 
-from ..quant_utils import (
-    TENSOR_NAME_QUANT_SUFFIX,
-    QuantizedValue,
-    QuantizedValueType,
-    attribute_to_kwarg,
-    find_by_name,
-    get_mul_node,
-    ms_domain,
-)
-from .base_operator import QuantOperatorBase
+from ..quant_utils import find_by_name  # noqa: F401
+from ..quant_utils import get_mul_node  # noqa: F401
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
+from .base_operator import QuantOperatorBase  # noqa: F401
 from .matmul import QOpMatMul
 from .qdq_base_operator import QDQOperatorBase
 
 
-def is_B_transposed(gemm_node):
-    transB_attribute = [attr for attr in gemm_node.attribute if attr.name == "transB"]
+def is_B_transposed(gemm_node):  # noqa: N802
+    transB_attribute = [attr for attr in gemm_node.attribute if attr.name == "transB"]  # noqa: N806
     if len(transB_attribute):
-        return 0 < onnx.helper.get_attribute_value(transB_attribute[0])
+        return onnx.helper.get_attribute_value(transB_attribute[0]) > 0
 
     return False
 
 
 def get_beta(gemm_node):
     beta_attribute = [attr for attr in gemm_node.attribute if attr.name == "beta"]
     if len(beta_attribute):
@@ -102,15 +96,15 @@
                 return super().quantize()
 
             quantized_bias_name = self.quantizer.quantize_bias_static(
                 node.input[2], node.input[0], node.input[1], get_beta(self.node)
             )
 
         qgemm_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
-        qgemm_name = qgemm_name = node.name + "_quant" if node.name != "" else ""
+        qgemm_name = node.name + "_quant" if node.name else ""
 
         kwargs = {}
         for attribute in node.attribute:
             if attribute.name != "beta":
                 kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
```

## onnxruntime/quantization/operators/lstm.py

```diff
@@ -1,12 +1,12 @@
 import numpy
 import onnx
 from onnx import onnx_pb as onnx_proto
 
-from ..quant_utils import QuantType, attribute_to_kwarg, ms_domain
+from ..quant_utils import QuantType, attribute_to_kwarg, ms_domain  # noqa: F401
 from .base_operator import QuantOperatorBase
 
 """
     Quantize LSTM
 """
 
 
@@ -26,60 +26,60 @@
         if not self.quantizer.is_valid_quantize_weight(node.input[1]) or not self.quantizer.is_valid_quantize_weight(
             node.input[2]
         ):
             super().quantize()
             return
 
         model = self.quantizer.model
-        W = model.get_initializer(node.input[1])
-        R = model.get_initializer(node.input[2])
+        W = model.get_initializer(node.input[1])  # noqa: N806
+        R = model.get_initializer(node.input[2])  # noqa: N806
 
         if len(W.dims) != 3 or len(R.dims) != 3:
             super().quantize()
             return
 
-        [W_num_dir, W_4_hidden_size, W_input_size] = W.dims
-        [R_num_dir, R_4_hidden_size, R_hidden_size] = R.dims
+        [W_num_dir, W_4_hidden_size, W_input_size] = W.dims  # noqa: N806
+        [R_num_dir, R_4_hidden_size, R_hidden_size] = R.dims  # noqa: N806
 
         if self.quantizer.is_per_channel():
             del W.dims[0]
             del R.dims[0]
             W.dims[0] = W_num_dir * W_4_hidden_size
             R.dims[0] = R_num_dir * R_4_hidden_size
 
         quant_input_weight_tuple = self.quantizer.quantize_weight_per_channel(
             node.input[1], onnx_proto.TensorProto.INT8, 0
         )
         quant_recurrent_weight_tuple = self.quantizer.quantize_weight_per_channel(
             node.input[2], onnx_proto.TensorProto.INT8, 0
         )
 
-        W_quant_weight = model.get_initializer(quant_input_weight_tuple[0])
-        R_quant_weight = model.get_initializer(quant_recurrent_weight_tuple[0])
+        W_quant_weight = model.get_initializer(quant_input_weight_tuple[0])  # noqa: N806
+        R_quant_weight = model.get_initializer(quant_recurrent_weight_tuple[0])  # noqa: N806
 
-        W_quant_array = onnx.numpy_helper.to_array(W_quant_weight)
-        R_quant_array = onnx.numpy_helper.to_array(R_quant_weight)
+        W_quant_array = onnx.numpy_helper.to_array(W_quant_weight)  # noqa: N806
+        R_quant_array = onnx.numpy_helper.to_array(R_quant_weight)  # noqa: N806
 
-        W_quant_array = numpy.reshape(W_quant_array, (W_num_dir, W_4_hidden_size, W_input_size))
-        R_quant_array = numpy.reshape(R_quant_array, (R_num_dir, R_4_hidden_size, R_hidden_size))
+        W_quant_array = numpy.reshape(W_quant_array, (W_num_dir, W_4_hidden_size, W_input_size))  # noqa: N806
+        R_quant_array = numpy.reshape(R_quant_array, (R_num_dir, R_4_hidden_size, R_hidden_size))  # noqa: N806
 
-        W_quant_array = numpy.transpose(W_quant_array, (0, 2, 1))
-        R_quant_array = numpy.transpose(R_quant_array, (0, 2, 1))
+        W_quant_array = numpy.transpose(W_quant_array, (0, 2, 1))  # noqa: N806
+        R_quant_array = numpy.transpose(R_quant_array, (0, 2, 1))  # noqa: N806
 
-        W_quant_tranposed = onnx.numpy_helper.from_array(W_quant_array, quant_input_weight_tuple[0])
-        R_quant_tranposed = onnx.numpy_helper.from_array(R_quant_array, quant_recurrent_weight_tuple[0])
+        W_quant_tranposed = onnx.numpy_helper.from_array(W_quant_array, quant_input_weight_tuple[0])  # noqa: N806
+        R_quant_tranposed = onnx.numpy_helper.from_array(R_quant_array, quant_recurrent_weight_tuple[0])  # noqa: N806
 
         model.remove_initializers([W_quant_weight, R_quant_weight])
         model.add_initializer(W_quant_tranposed)
         model.add_initializer(R_quant_tranposed)
 
-        W_quant_zp = model.get_initializer(quant_input_weight_tuple[1])
-        R_quant_zp = model.get_initializer(quant_recurrent_weight_tuple[1])
-        W_quant_scale = model.get_initializer(quant_input_weight_tuple[2])
-        R_quant_scale = model.get_initializer(quant_recurrent_weight_tuple[2])
+        W_quant_zp = model.get_initializer(quant_input_weight_tuple[1])  # noqa: N806
+        R_quant_zp = model.get_initializer(quant_recurrent_weight_tuple[1])  # noqa: N806
+        W_quant_scale = model.get_initializer(quant_input_weight_tuple[2])  # noqa: N806
+        R_quant_scale = model.get_initializer(quant_recurrent_weight_tuple[2])  # noqa: N806
 
         if self.quantizer.is_per_channel():
             W_quant_zp.dims[:] = [W_num_dir, W_4_hidden_size]
             R_quant_zp.dims[:] = [R_num_dir, R_4_hidden_size]
             W_quant_scale.dims[:] = [W_num_dir, W_4_hidden_size]
             R_quant_scale.dims[:] = [R_num_dir, R_4_hidden_size]
 
@@ -102,14 +102,14 @@
         )
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
-        quant_lstm_name = "" if node.name == "" else node.name + "_quant"
+        quant_lstm_name = "" if not node.name else node.name + "_quant"
         quant_lstm_node = onnx.helper.make_node("DynamicQuantizeLSTM", inputs, node.output, quant_lstm_name, **kwargs)
         self.quantizer.new_nodes.append(quant_lstm_node)
 
         dequantize_node = self.quantizer._dequantize_value(node.input[0])
         if dequantize_node is not None:
             self.quantizer.new_nodes.append(dequantize_node)
```

## onnxruntime/quantization/operators/matmul.py

```diff
@@ -20,15 +20,15 @@
             not self.quantizer.is_float_tensor(self.node.input[0])
         ):
             return False
 
         # do not quantize non-constant B matrices for matmul
         if self.quantizer.q_matmul_const_b_only:
             if not self.quantizer.find_initializer_in_path(self.node.input[1]):
-                print("Ignore MatMul due to non constant B: {}[{}]".format(self.quantizer.graph_scope, self.node.name))
+                print(f"Ignore MatMul due to non constant B: {self.quantizer.graph_scope}[{self.node.name}]")
                 return False
         return True
 
 
 """
     Used when quantize mode is QuantizationMode.IntegerOps.
 """
@@ -57,15 +57,15 @@
         ) = self.quantizer.quantize_weight(node, [1], reduce_range=True, op_level_per_channel=True)
         quantized_input_names.extend(quantized_input_names_weight)
         zero_point_names.extend(zero_point_names_weight)
         scale_names.extend(scale_names_weight)
         nodes.extend(nodes_weight)
 
         matmul_integer_output = node.output[0] + "_output_quantized"
-        matmul_integer_name = node.name + "_quant" if node.name != "" else ""
+        matmul_integer_name = node.name + "_quant" if node.name else ""
         matmul_integer_node = onnx.helper.make_node(
             "MatMulInteger",
             quantized_input_names + zero_point_names,
             [matmul_integer_output],
             matmul_integer_name,
         )
         nodes.append(matmul_integer_node)
@@ -81,29 +81,29 @@
         )
         nodes.append(cast_node)
 
         # Add mul operation to multiply scales of two inputs.
         assert len(scale_names) == 2
         scales_mul_op = (
             matmul_integer_name + "_scales_mul"
-            if matmul_integer_name != ""
+            if matmul_integer_name
             else scale_names[0] + "_" + scale_names[1] + "_mul"
         )
 
         scales_mul_node = find_by_name(scales_mul_op, self.quantizer.new_nodes)
         if scales_mul_node is None:
             scales_mul_node = get_mul_node(scale_names, scales_mul_op + ":0", scales_mul_op)
             nodes.append(scales_mul_node)
 
         scales_mul_op_output = scales_mul_node.output[0]
 
         # Add mul operation to multiply mul_scales_op result with output of MatMulInteger
         # and make the output of this node the same as output of original matmul node.
         output_scale_mul_op = ""
-        if matmul_integer_name != "":
+        if matmul_integer_name:
             output_scale_mul_op = matmul_integer_name + "_output_scale_mul"
         nodes.append(
             get_mul_node(
                 [cast_op_output, scales_mul_op_output],
                 node.output[0],
                 output_scale_mul_op,
             )
@@ -149,15 +149,15 @@
             _,
             _,
         ) = self.quantizer._get_quantization_params(node.output[0])
         if not data_found or quantized_input_names is None:
             return super().quantize()
 
         qlinear_matmul_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
-        qlinear_matmul_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_matmul_name = node.name + "_quant" if node.name else ""
 
         qlinear_matmul_inputs = []
         # Input 0
         qlinear_matmul_inputs.append(quantized_input_names[0])
         qlinear_matmul_inputs.append(scale_names[0])
         qlinear_matmul_inputs.append(zero_point_names[0])
         # Input 1
```

## onnxruntime/quantization/operators/pad.py

```diff
@@ -1,8 +1,7 @@
-import numpy as np
 import onnx
 
 from ..quant_utils import (
     TENSOR_NAME_QUANT_SUFFIX,
     QuantizedValue,
     QuantizedValueType,
     attribute_to_kwarg,
```

## onnxruntime/quantization/operators/pooling.py

```diff
@@ -43,15 +43,15 @@
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         # Create qlinear pool node for given type (AveragePool, etc)
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
-        qlinear_node_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_node_name = node.name + "_quant" if node.name else ""
         qnode = onnx.helper.make_node(
             "QLinear" + node.op_type,
             [
                 quantized_input_names[0],
                 input_scale_names[0],
                 input_zero_point_names[0],
                 output_scale_name,
```

## onnxruntime/quantization/operators/qdq_base_operator.py

```diff
@@ -1,20 +1,18 @@
 import itertools
 
-from ..quant_utils import QuantizedValue, QuantizedValueType, attribute_to_kwarg, quantize_nparray
-from .base_operator import QuantOperatorBase
+from ..quant_utils import QuantizedValue, QuantizedValueType, attribute_to_kwarg, quantize_nparray  # noqa: F401
+from .base_operator import QuantOperatorBase  # noqa: F401
 
 
 class QDQOperatorBase:
     def __init__(self, onnx_quantizer, onnx_node):
         self.quantizer = onnx_quantizer
         self.node = onnx_node
-        self.disable_qdq_for_node_output = (
-            True if onnx_node.op_type in onnx_quantizer.op_types_to_exclude_output_quantization else False
-        )
+        self.disable_qdq_for_node_output = onnx_node.op_type in onnx_quantizer.op_types_to_exclude_output_quantization
 
     def quantize(self):
         node = self.node
 
         if self.disable_qdq_for_node_output:
             tensors_to_quantize = node.input
         else:
```

## onnxruntime/quantization/operators/softmax.py

```diff
@@ -49,15 +49,15 @@
         # Create qlinear softmax node for given type
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
         # make qlinearsoft has the real opset_version, its default SinceVersion would be 1
         kwargs["opset"] = self.quantizer.opset_version
-        qlinear_node_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_node_name = node.name + "_quant" if node.name else ""
         qnode = onnx.helper.make_node(
             "QLinear" + node.op_type,
             [
                 quantized_input_names[0],
                 input_scale_names[0],
                 input_zero_point_names[0],
                 output_scale_name,
@@ -76,11 +76,16 @@
 
 class QDQSoftmax(QDQOperatorBase):
     def quantize(self):
         super().quantize()
         if self.quantizer.activation_qType == onnx.onnx_pb.TensorProto.UINT8:
             out_scale = 1 / 256.0
             out_zero_point = 0
+        elif self.quantizer.is_activation_symmetric:
+            # results are all greater or equal to 0, so we can only use
+            # half of the range
+            out_scale = 1 / 127.0
+            out_zero_point = 0
         else:
             out_scale = 1 / 256.0
             out_zero_point = -128
         self.quantizer.set_quant_scale_zp(self.node.output[0], (out_scale, out_zero_point))
```

## onnxruntime/quantization/operators/split.py

```diff
@@ -17,15 +17,15 @@
             scale_names,
             nodes,
         ) = self.quantizer.quantize_activation(node, [0])
         if quantized_input_names is None:
             return super().quantize()
 
         quantized_node_name = ""
-        if node.name != "":
+        if node.name:
             quantized_node_name = node.name + "_quant"
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
 
         # Output just derive the scale/zero from input
         quantized_output_names = []
```

## onnxruntime/quantization/operators/where.py

```diff
@@ -27,15 +27,15 @@
             zero_point_names,
             scale_names,
             nodes,
         ) = self.quantizer.quantize_activation(node, [1, 2])
         if not data_found or q_input_names is None:
             return super().quantize()
         qlinear_output = node.output[0] + TENSOR_NAME_QUANT_SUFFIX
-        qlinear_output_name = node.name + "_quant" if node.name != "" else ""
+        qlinear_output_name = node.name + "_quant" if node.name else ""
 
         q_output = QuantizedValue(
             node.output[0],
             qlinear_output,
             output_scale_name,
             output_zp_name,
             QuantizedValueType.Input,
```

## onnxruntime/tools/__init__.py

```diff
@@ -3,8 +3,8 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 # appended to the __init__.py in the onnxruntime module's 'tools' folder from /tools/python/util/__init__append.py
 import importlib.util
 
 have_torch = importlib.util.find_spec("torch")
 if have_torch:
-    from .pytorch_export_helpers import infer_input_info  # noqa
+    from .pytorch_export_helpers import infer_input_info  # noqa: F401
```

## onnxruntime/tools/check_onnx_model_mobile_usability.py

```diff
@@ -4,27 +4,27 @@
 import argparse
 import logging
 import pathlib
 
 # need this before the mobile helper imports for some reason
 logging.basicConfig(format="%(levelname)s:  %(message)s")
 
-from .mobile_helpers import check_model_can_use_ort_mobile_pkg, usability_checker  # noqa
+from .mobile_helpers import check_model_can_use_ort_mobile_pkg, usability_checker  # noqa: E402
 
 
 def check_usability():
     parser = argparse.ArgumentParser(
         description="""Analyze an ONNX model to determine how well it will work in mobile scenarios, and whether
         it is likely to be able to use the pre-built ONNX Runtime Mobile Android or iOS package.""",
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
     )
 
     parser.add_argument(
         "--config_path",
-        help="Path to required operators and types configuration used to build " "the pre-built ORT mobile package.",
+        help="Path to required operators and types configuration used to build the pre-built ORT mobile package.",
         required=False,
         type=pathlib.Path,
         default=check_model_can_use_ort_mobile_pkg.get_default_config_path(),
     )
     parser.add_argument(
         "--log_level", choices=["debug", "info", "warning", "error"], default="info", help="Logging level"
     )
```

## onnxruntime/tools/convert_onnx_models_to_ort.py

```diff
@@ -79,15 +79,14 @@
     optimization_style: OptimizationStyle,
     custom_op_library: pathlib.Path,
     create_optimized_onnx_model: bool,
     allow_conversion_failures: bool,
     target_platform: str,
     session_options_config_entries: typing.Dict[str, str],
 ) -> typing.List[pathlib.Path]:
-
     model_dir = model_path_or_dir if model_path_or_dir.is_dir() else model_path_or_dir.parent
     output_dir = output_dir or model_dir
 
     optimization_level = get_optimization_level(optimization_level_str)
 
     def is_model_file_to_convert(file_path: pathlib.Path):
         if not path_match_suffix_ignore_case(file_path, ".onnx"):
@@ -98,15 +97,15 @@
             print(f"Ignoring '{file_path}'")
             return False
         return True
 
     models = files_from_file_or_dir(model_path_or_dir, is_model_file_to_convert)
 
     if len(models) == 0:
-        raise ValueError("No model files were found in '{}'".format(model_path_or_dir))
+        raise ValueError(f"No model files were found in '{model_path_or_dir}'")
 
     providers = ["CPUExecutionProvider"]
 
     # if the optimization level is 'all' we manually exclude the NCHWc transformer. It's not applicable to ARM
     # devices, and creates a device specific model which won't run on all hardware.
     # If someone really really really wants to run it they could manually create an optimized onnx model first,
     # or they could comment out this code.
@@ -114,15 +113,14 @@
     if optimization_level == ort.GraphOptimizationLevel.ORT_ENABLE_ALL and target_platform != "amd64":
         optimizer_filter = ["NchwcTransformer"]
 
     converted_models = []
 
     for model in models:
         try:
-
             relative_model_path = model.relative_to(model_dir)
 
             (output_dir / relative_model_path).parent.mkdir(parents=True, exist_ok=True)
 
             ort_target_path = (output_dir / relative_model_path).with_suffix(
                 _optimization_suffix(optimization_level_str, optimization_style, ".ort")
             )
@@ -138,44 +136,44 @@
                 so = _create_session_options(
                     optimization_level, optimized_target_path, custom_op_library, session_options_config_entries
                 )
                 if optimization_style == OptimizationStyle.Runtime:
                     # Limit the optimizations to those that can run in a model with runtime optimizations.
                     so.add_session_config_entry("optimization.minimal_build_optimizations", "apply")
 
-                print("Saving optimized ONNX model {} to {}".format(model, optimized_target_path))
+                print(f"Saving optimized ONNX model {model} to {optimized_target_path}")
                 _ = ort.InferenceSession(
                     str(model), sess_options=so, providers=providers, disabled_optimizers=optimizer_filter
                 )
 
             # Load ONNX model, optimize, and save to ORT format
             so = _create_session_options(
                 optimization_level, ort_target_path, custom_op_library, session_options_config_entries
             )
             so.add_session_config_entry("session.save_model_format", "ORT")
             if optimization_style == OptimizationStyle.Runtime:
                 so.add_session_config_entry("optimization.minimal_build_optimizations", "save")
 
-            print("Converting optimized ONNX model {} to ORT format model {}".format(model, ort_target_path))
+            print(f"Converting optimized ONNX model {model} to ORT format model {ort_target_path}")
             _ = ort.InferenceSession(
                 str(model), sess_options=so, providers=providers, disabled_optimizers=optimizer_filter
             )
 
             converted_models.append(ort_target_path)
 
             # orig_size = os.path.getsize(onnx_target_path)
             # new_size = os.path.getsize(ort_target_path)
             # print("Serialized {} to {}. Sizes: orig={} new={} diff={} new:old={:.4f}:1.0".format(
             #     onnx_target_path, ort_target_path, orig_size, new_size, new_size - orig_size, new_size / orig_size))
         except Exception as e:
-            print("Error converting {}: {}".format(model, e))
+            print(f"Error converting {model}: {e}")
             if not allow_conversion_failures:
                 raise
 
-    print("Converted {}/{} models successfully.".format(len(converted_models), len(models)))
+    print(f"Converted {len(converted_models)}/{len(models)} models successfully.")
 
     return converted_models
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         os.path.basename(__file__),
@@ -276,18 +274,18 @@
     # setting optimization level is not expected to be needed by typical users, but it can be set with this
     # environment variable
     optimization_level_str = os.getenv("ORT_CONVERT_ONNX_MODELS_TO_ORT_OPTIMIZATION_LEVEL", "all")
     model_path_or_dir = args.model_path_or_dir.resolve()
     custom_op_library = args.custom_op_library.resolve() if args.custom_op_library else None
 
     if not model_path_or_dir.is_dir() and not model_path_or_dir.is_file():
-        raise FileNotFoundError("Model path '{}' is not a file or directory.".format(model_path_or_dir))
+        raise FileNotFoundError(f"Model path '{model_path_or_dir}' is not a file or directory.")
 
     if custom_op_library and not custom_op_library.is_file():
-        raise FileNotFoundError("Unable to find custom operator library '{}'".format(custom_op_library))
+        raise FileNotFoundError(f"Unable to find custom operator library '{custom_op_library}'")
 
     session_options_config_entries = {}
 
     if args.target_platform == "arm":
         session_options_config_entries["session.qdqisint8allowed"] = "1"
     else:
         session_options_config_entries["session.qdqisint8allowed"] = "0"
```

## onnxruntime/tools/onnx_model_utils.py

```diff
@@ -250,16 +250,16 @@
 
     # add entry mapping this node to the producer of this input
     node_to_producers[consumer].add(producer)
     node_to_consumers[producer].add(consumer)
 
 
 def _map_node_dependencies(graph: onnx.GraphProto, node_to_producers: dict, node_to_consumers: dict):
-    graph_inputs = set([i.name for i in graph.input])
-    initializers = set([i.name for i in graph.initializer])
+    graph_inputs = {i.name for i in graph.input}
+    initializers = {i.name for i in graph.initializer}
 
     # map of value name to node that creates it. copy parent values but override if values get shadowed
     producers = {}
 
     implicit_inputs = set()
 
     def is_local_value(value):
@@ -315,15 +315,15 @@
     node_to_consumers = {}  # map of node instance to nodes consuming output values it produces
 
     implicit_inputs = _map_node_dependencies(graph, node_to_producers, node_to_consumers)
 
     # top level graph should have no implicit inputs
     if implicit_inputs:
         raise ValueError(
-            "This appears to be an invalid model with missing inputs of " f'{",".join(sorted(implicit_inputs))}'
+            f'This appears to be an invalid model with missing inputs of {",".join(sorted(implicit_inputs))}'
         )
 
     return node_to_producers, node_to_consumers
 
 
 def is_fixed_size_tensor(value: onnx.ValueInfoProto):
     """
```

## onnxruntime/tools/onnxruntime_test.py

```diff
@@ -25,15 +25,15 @@
     "tensor(int16)": "int16",
     "tensor(uint16)": "uint16",
     "tensor(int64)": "int64",
     "tensor(uint64)": "uint64",
 }
 
 
-def generate_feeds(sess, symbolic_dims={}):
+def generate_feeds(sess, symbolic_dims={}):  # noqa: B006
     feeds = {}
     for input_meta in sess.get_inputs():
         # replace any symbolic dimensions
         shape = []
         for dim in input_meta.shape:
             if not dim:
                 # unknown dim
@@ -52,31 +52,31 @@
         elif input_meta.type in integer_dict:
             feeds[input_meta.name] = np.random.uniform(high=1000, size=tuple(shape)).astype(
                 integer_dict[input_meta.type]
             )
         elif input_meta.type == "tensor(bool)":
             feeds[input_meta.name] = np.random.randint(2, size=tuple(shape)).astype("bool")
         else:
-            print("unsupported input type {} for input {}".format(input_meta.type, input_meta.name))
+            print(f"unsupported input type {input_meta.type} for input {input_meta.name}")
             sys.exit(-1)
     return feeds
 
 
 # simple test program for loading onnx model, feeding all inputs and running the model num_iters times.
 def run_model(
     model_path,
     num_iters=1,
     debug=None,
     profile=None,
-    symbolic_dims={},
+    symbolic_dims={},  # noqa: B006
     feeds=None,
     override_initializers=True,
 ):
     if debug:
-        print("Pausing execution ready for debugger to attach to pid: {}".format(os.getpid()))
+        print(f"Pausing execution ready for debugger to attach to pid: {os.getpid()}")
         print("Press key to continue.")
         sys.stdin.read(1)
 
     sess_options = None
     if profile:
         sess_options = onnxrt.SessionOptions()
         sess_options.enable_profiling = True
@@ -103,35 +103,35 @@
             elif initializer.type in integer_dict:
                 feeds[initializer.name] = np.random.uniform(high=1000, size=tuple(shape)).astype(
                     integer_dict[initializer.type]
                 )
             elif initializer.type == "tensor(bool)":
                 feeds[initializer.name] = np.random.randint(2, size=tuple(shape)).astype("bool")
             else:
-                print("unsupported initializer type {} for initializer {}".format(initializer.type, initializer.name))
+                print(f"unsupported initializer type {initializer.type} for initializer {initializer.name}")
                 sys.exit(-1)
 
     start = timer()
-    for i in range(num_iters):
+    for _i in range(num_iters):
         outputs = sess.run([], feeds)  # fetch all outputs
     end = timer()
 
-    print("model: {}".format(meta.graph_name))
-    print("version: {}".format(meta.version))
-    print("iterations: {}".format(num_iters))
-    print("avg latency: {} ms".format(((end - start) * 1000) / num_iters))
+    print(f"model: {meta.graph_name}")
+    print(f"version: {meta.version}")
+    print(f"iterations: {num_iters}")
+    print(f"avg latency: {((end - start) * 1000) / num_iters} ms")
 
     if profile:
         trace_file = sess.end_profiling()
-        print("trace file written to: {}".format(trace_file))
+        print(f"trace file written to: {trace_file}")
 
     return 0, feeds, num_iters > 0 and outputs
 
 
-if __name__ == "__main__":
+def main():
     parser = argparse.ArgumentParser(description="Simple ONNX Runtime Test Tool.")
     parser.add_argument("model_path", help="model path")
     parser.add_argument(
         "num_iters",
         nargs="?",
         type=int,
         default=1000,
@@ -151,7 +151,11 @@
         "e.g. --symbolic_dims batch=1,seqlen=5. "
         "If not provided, the value of 1 will be used for all symbolic dimensions.",
     )
 
     args = parser.parse_args()
     exit_code, _, _ = run_model(args.model_path, args.num_iters, args.debug, args.profile, args.symbolic_dims)
     sys.exit(exit_code)
+
+
+if __name__ == "__main__":
+    main()
```

## onnxruntime/tools/pytorch_export_contrib_ops.py

```diff
@@ -7,15 +7,15 @@
 """
 import typing
 
 try:
     # TODO(justinchuby): Create a function to alert users when torch is not installed
     import torch
 except ModuleNotFoundError:
-    raise ModuleNotFoundError(
+    raise ModuleNotFoundError(  # noqa: B904
         "This module is only useful in combination with PyTorch. To install PyTorch see https://pytorch.org/."
     )
 
 from torch.onnx import symbolic_helper
 
 _OPSET_VERSION = 1
 _registered_ops: typing.AbstractSet[str] = set()
```

## onnxruntime/tools/pytorch_export_helpers.py

```diff
@@ -4,15 +4,15 @@
 import inspect
 from collections import abc
 
 import torch
 
 
 def _parse_inputs_for_onnx_export(all_input_parameters, inputs, kwargs):
-    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L433  # noqa
+    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L433
 
     def _add_input(name, input):
         """Returns number of expanded inputs that _add_input processed"""
 
         if input is None:
             # Drop all None inputs and return 0.
             return 0
@@ -62,15 +62,15 @@
             input_parameter.kind == inspect.Parameter.POSITIONAL_ONLY
             or input_parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
             or input_parameter.kind == inspect.Parameter.KEYWORD_ONLY
         ):
             # All positional non-*args and non-**kwargs are processed here
             name = input_parameter.name
             inp = None
-            input_idx += var_positional_idx
+            input_idx += var_positional_idx  # noqa: PLW2901
             is_positional = True
             if input_idx < len(inputs) and inputs[input_idx] is not None:
                 inp = inputs[input_idx]
             elif name in kwargs and kwargs[name] is not None:
                 inp = kwargs[name]
                 is_positional = False
             num_expanded_non_none_inputs_local = _add_input(name, inp)
@@ -83,15 +83,15 @@
                     _add_input(name, inp)
 
     return input_names
 
 
 def _flatten_module_input(names, args, kwargs):
     """Flatten args and kwargs in a single tuple of tensors."""
-    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L110  # noqa
+    # extracted from https://github.com/microsoft/onnxruntime/blob/239c6ad3f021ff7cc2e6247eb074bd4208dc11e2/orttraining/orttraining/python/training/ortmodule/_io.py#L110
 
     def is_primitive_type(value):
         return type(value) in {int, bool, float}
 
     def to_tensor(value):
         return torch.tensor(value)
```

## onnxruntime/tools/reduced_build_config_parser.py

```diff
@@ -1,18 +1,18 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 import os
 
 # Check if the flatbuffers module is available. If not we cannot handle type reduction information in the config.
 try:
-    import flatbuffers  # noqa
+    import flatbuffers  # noqa: F401
 
     have_flatbuffers = True
-    from .ort_format_model import GloballyAllowedTypesOpTypeImplFilter, OperatorTypeUsageManager  # noqa
+    from .ort_format_model import GloballyAllowedTypesOpTypeImplFilter, OperatorTypeUsageManager
 except ImportError:
     have_flatbuffers = False
 
 
 def parse_config(config_file: str, enable_type_reduction: bool = False):
     """
     Parse the configuration file and return the required operators dictionary and an
@@ -78,15 +78,15 @@
     :return: required_ops: Dictionary of domain:opset:[ops] for required operators. If None, all operators are
                            required.
              op_type_impl_filter: OpTypeImplFilterInterface instance if type reduction is enabled, the flatbuffers
                                   module is available, and type reduction information is present. None otherwise.
     """
 
     if not os.path.isfile(config_file):
-        raise ValueError("Configuration file {} does not exist".format(config_file))
+        raise ValueError(f"Configuration file {config_file} does not exist")
 
     # only enable type reduction when flatbuffers is available
     enable_type_reduction = enable_type_reduction and have_flatbuffers
 
     required_ops = {}
     no_ops_specified_means_all_ops_are_required = False
     op_type_usage_manager = OperatorTypeUsageManager() if enable_type_reduction else None
@@ -98,30 +98,30 @@
             return True
 
         if line.startswith("!globally_allowed_types;"):  # handle globally allowed types
             if enable_type_reduction:
                 nonlocal globally_allowed_types
                 if globally_allowed_types is not None:
                     raise RuntimeError("Globally allowed types were already specified.")
-                globally_allowed_types = set(segment.strip() for segment in line.split(";")[1].split(","))
+                globally_allowed_types = {segment.strip() for segment in line.split(";")[1].split(",")}
             return True
 
         if line == "!no_ops_specified_means_all_ops_are_required":  # handle all ops required line
             nonlocal no_ops_specified_means_all_ops_are_required
             no_ops_specified_means_all_ops_are_required = True
             return True
 
         return False
 
-    with open(config_file, "r") as config:
+    with open(config_file) as config:
         for line in [orig_line.strip() for orig_line in config.readlines()]:
             if process_non_op_line(line):
                 continue
 
-            domain, opset_str, operators_str = [segment.strip() for segment in line.split(";")]
+            domain, opset_str, operators_str = (segment.strip() for segment in line.split(";"))
             opsets = [int(s) for s in opset_str.split(",")]
 
             # any type reduction information is serialized json that starts/ends with { and }.
             # type info is optional for each operator.
             if "{" in operators_str:
                 has_op_type_reduction_info = True
 
@@ -168,15 +168,15 @@
                     else:
                         # comma or end of line is next
                         end_str = next_comma if next_comma != -1 else end
                         operators.add(operators_str[cur:end_str].strip())
                         cur = end_str + 1
 
             else:
-                operators = set([op.strip() for op in operators_str.split(",")])
+                operators = {op.strip() for op in operators_str.split(",")}
 
             for opset in opsets:
                 if domain not in required_ops:
                     required_ops[domain] = {opset: operators}
                 elif opset not in required_ops[domain]:
                     required_ops[domain][opset] = operators
                 else:
```

## onnxruntime/tools/symbolic_shape_infer.py

```diff
@@ -49,15 +49,15 @@
 
 
 def get_shape_from_value_info(vi):
     cls_type = vi.type.WhichOneof("value")
     if cls_type is None:
         return None
     if is_sequence(vi.type):
-        if "tensor_type" == vi.type.sequence_type.elem_type.WhichOneof("value"):
+        if vi.type.sequence_type.elem_type.WhichOneof("value") == "tensor_type":
             return get_shape_from_type_proto(vi.type.sequence_type.elem_type)
         else:
             return None
     else:
         return get_shape_from_type_proto(vi.type)
 
 
@@ -159,14 +159,15 @@
             "NonZero": self._infer_NonZero,
             "OneHot": self._infer_OneHot,
             "Pad": self._infer_Pad,
             "Range": self._infer_Range,
             "Reciprocal": self._pass_on_shape_and_type,
             "ReduceSum": self._infer_ReduceSum,
             "ReduceProd": self._infer_ReduceProd,
+            "RelativePositionBias": self._infer_RelativePositionBias,
             "Reshape": self._infer_Reshape,
             "Resize": self._infer_Resize,
             "Round": self._pass_on_shape_and_type,
             "Scan": self._infer_Scan,
             "ScatterElements": self._infer_ScatterElements,
             "SequenceAt": self._infer_SequenceAt,
             "SequenceInsert": self._infer_SequenceInsert,
@@ -185,27 +186,32 @@
             "Transpose": self._infer_Transpose,
             "Unsqueeze": self._infer_Unsqueeze,
             "Where": self._infer_symbolic_compute_ops,
             "ZipMap": self._infer_ZipMap,
             "Neg": self._infer_symbolic_compute_ops,
             # contrib ops:
             "Attention": self._infer_Attention,
+            "PackedAttention": self._infer_PackedAttention,
+            "RemovePadding": self._infer_RemovePadding,
+            "RestorePadding": self._infer_RestorePadding,
             "BiasGelu": self._infer_BiasGelu,
             "MultiHeadAttention": self._infer_MultiHeadAttention,
             "EmbedLayerNormalization": self._infer_EmbedLayerNormalization,
             "FastGelu": self._infer_FastGelu,
             "Gelu": self._infer_Gelu,
             "GemmFastGelu": self._infer_GemmFastGelu,
             "LayerNormalization": self._infer_LayerNormalization,
             "LongformerAttention": self._infer_LongformerAttention,
             "PythonOp": self._infer_PythonOp,
+            "SimplifiedLayerNormalization": self._infer_LayerNormalization,
             "SkipLayerNormalization": self._infer_SkipLayerNormalization,
             "SkipSimplifiedLayerNormalization": self._infer_SkipLayerNormalization,
             "GroupNorm": self._infer_GroupNorm,
             "BiasSplitGelu": self._infer_BiasSplitGelu,
+            "BiasAdd": self._infer_BiasAdd,
             "NhwcConv": self._infer_NhwcConv,
         }
         self.aten_op_dispatcher_ = {
             "embedding": self._infer_Gather,
             "bitwise_or": self._infer_aten_bitwise_or,
             "diagonal": self._infer_aten_diagonal,
             "max_pool2d_with_indices": self._infer_aten_pool2d,
@@ -214,17 +220,18 @@
             "multinomial": self._infer_aten_multinomial,
             "unfold": self._infer_aten_unfold,
             "argmax": self._infer_aten_argmax,
             "avg_pool2d": self._infer_aten_pool2d,
             "_adaptive_avg_pool2d": self._infer_aten_pool2d,
             "numpy_T": self._infer_Transpose,
             "native_group_norm": self._infer_aten_group_norm,
-            "upsample_nearest1d": self._infer_aten_upsample_nearest,
-            "upsample_nearest2d": self._infer_aten_upsample_nearest,
-            "upsample_nearest3d": self._infer_aten_upsample_nearest,
+            "upsample_nearest1d": self._infer_aten_upsample,
+            "upsample_nearest2d": self._infer_aten_upsample,
+            "upsample_nearest3d": self._infer_aten_upsample,
+            "upsample_bilinear2d": self._infer_aten_upsample,
         }
         self.run_ = True
         self.suggested_merge_ = {}
         self.symbolic_dims_ = {}
         self.input_symbols_ = {}
         self.auto_merge_ = auto_merge
         self.guess_output_rank_ = guess_output_rank
@@ -289,27 +296,22 @@
                         d.dim_value = int(v)
                     else:
                         d.dim_param = v
 
     def _preprocess(self, in_mp):
         self.out_mp_ = onnx.ModelProto()
         self.out_mp_.CopyFrom(in_mp)
-        self.graph_inputs_ = dict([(i.name, i) for i in list(self.out_mp_.graph.input)])
-        self.initializers_ = dict([(i.name, i) for i in self.out_mp_.graph.initializer])
-        self.known_vi_ = dict([(i.name, i) for i in list(self.out_mp_.graph.input)])
+        self.graph_inputs_ = {i.name: i for i in list(self.out_mp_.graph.input)}
+        self.initializers_ = {i.name: i for i in self.out_mp_.graph.initializer}
+        self.known_vi_ = {i.name: i for i in list(self.out_mp_.graph.input)}
         self.known_vi_.update(
-            dict(
-                [
-                    (
-                        i.name,
-                        helper.make_tensor_value_info(i.name, i.data_type, list(i.dims)),
-                    )
-                    for i in self.out_mp_.graph.initializer
-                ]
-            )
+            {
+                i.name: helper.make_tensor_value_info(i.name, i.data_type, list(i.dims))
+                for i in self.out_mp_.graph.initializer
+            }
         )
 
     def _merge_symbols(self, dims):
         if not all([type(d) == str for d in dims]):
             if self.auto_merge_:
                 unique_dims = list(set(dims))
                 is_int = [is_literal(d) for d in unique_dims]
@@ -323,15 +325,15 @@
                                 unique_dims[int_dim],
                             )
                         )
                     self._check_merged_dims(unique_dims, allow_broadcast=False)
                     return unique_dims[int_dim]
                 else:
                     if self.verbose_ > 0:
-                        logger.debug("dim {} has been mergd with dim {}".format(unique_dims[1:], unique_dims[0]))
+                        logger.debug(f"dim {unique_dims[1:]} has been merged with dim {unique_dims[0]}")
                     return dims[0]
             else:
                 return None
         if all([d == dims[0] for d in dims]):
             return dims[0]
         merged = [self.suggested_merge_[d] if d in self.suggested_merge_ else d for d in dims]
         if all([d == merged[0] for d in merged]):
@@ -359,40 +361,51 @@
                     # warning about unsupported broadcast when not auto merge
                     # note that auto merge has the risk of incorrectly merge symbols while one of them being 1
                     # for example, 'a' = 1, 'b' = 5 at runtime is valid broadcasting, but with auto merge 'a' == 'b'
                     if self.auto_merge_:
                         self._add_suggested_merge([dim1, dim2], apply=True)
                     else:
                         logger.warning("unsupported broadcast between " + str(dim1) + " " + str(dim2))
-            new_shape = [new_dim] + new_shape
+            new_shape = [new_dim, *new_shape]
         return new_shape
 
     def _get_shape(self, node, idx):
         name = node.input[idx]
         if name in self.known_vi_:
             vi = self.known_vi_[name]
             return get_shape_from_value_info(vi)
         else:
             assert name in self.initializers_
             return list(self.initializers_[name].dims)
 
+    def _try_get_shape(self, node, idx):
+        if idx > len(node.input) - 1:
+            return None
+        name = node.input[idx]
+        if name in self.known_vi_:
+            vi = self.known_vi_[name]
+            return get_shape_from_value_info(vi)
+        if name in self.initializers_:
+            return list(self.initializers_[name].dims)
+        return None
+
     def _get_shape_rank(self, node, idx):
         return len(self._get_shape(node, idx))
 
     def _get_sympy_shape(self, node, idx):
         sympy_shape = []
         for d in self._get_shape(node, idx):
             if type(d) == str:
                 sympy_shape.append(
                     self.symbolic_dims_[d]
                     if d in self.symbolic_dims_
                     else sympy.Symbol(d, integer=True, nonnegative=True)
                 )
             else:
-                assert None != d
+                assert None is not d
                 sympy_shape.append(d)
         return sympy_shape
 
     def _get_value(self, node, idx):
         name = node.input[idx]
         assert name in self.sympy_data_ or name in self.initializers_
         return self.sympy_data_[name] if name in self.sympy_data_ else numpy_helper.to_array(self.initializers_[name])
@@ -403,23 +416,23 @@
         name = node.input[idx]
         if name in self.sympy_data_ or name in self.initializers_:
             return self._get_value(node, idx)
         return None
 
     def _update_computed_dims(self, new_sympy_shape):
         for i, new_dim in enumerate(new_sympy_shape):
-            if not is_literal(new_dim) and not type(new_dim) == str:
+            if not is_literal(new_dim) and type(new_dim) != str:
                 str_dim = str(new_dim)
                 if str_dim in self.suggested_merge_:
                     if is_literal(self.suggested_merge_[str_dim]):
                         continue  # no need to create dim for literals
                     new_sympy_shape[i] = self.symbolic_dims_[self.suggested_merge_[str_dim]]
                 else:
                     # add new_dim if it's a computational expression
-                    if not str(new_dim) in self.symbolic_dims_:
+                    if str(new_dim) not in self.symbolic_dims_:
                         self.symbolic_dims_[str(new_dim)] = new_dim
 
     def _onnx_infer_single_node(self, node):
         # skip onnx shape inference for some ops, as they are handled in _infer_*
         skip_infer = node.op_type in [
             "If",
             "Loop",
@@ -430,19 +443,26 @@
             "BiasGelu",
             "EmbedLayerNormalization",
             "FastGelu",
             "Gelu",
             "GemmFastGelu",
             "LayerNormalization",
             "LongformerAttention",
+            "RelativePositionBias",
+            "RemovePadding",
+            "RestorePadding",
+            "SimplifiedLayerNormalization",
             "SkipLayerNormalization",
+            "SkipSimplifiedLayerNormalization",
+            "PackedAttention",
             "PythonOp",
             "MultiHeadAttention",
             "GroupNorm",
             "BiasSplitGelu",
+            "BiasAdd",
             "NhwcConv",
         ]
 
         if not skip_infer:
             # Only pass initializers that satisfy the following condition:
             # (1) Operator need value of some input for shape inference.
             #     For example, Unsqueeze in opset 13 uses the axes input to calculate shape of output.
@@ -467,32 +487,31 @@
 
             self.tmp_mp_.graph.CopyFrom(tmp_graph)
 
             self.tmp_mp_ = shape_inference.infer_shapes(self.tmp_mp_)
 
         for i_o in range(len(node.output)):
             o = node.output[i_o]
-            vi = self.out_mp_.graph.value_info.add()
-            if not skip_infer:
-                vi.CopyFrom(self.tmp_mp_.graph.output[i_o])
-            else:
-                vi.name = o
-            self.known_vi_[o] = vi
+            if o:  # skip optional output
+                vi = self.out_mp_.graph.value_info.add()
+                if not skip_infer:
+                    vi.CopyFrom(self.tmp_mp_.graph.output[i_o])
+                else:
+                    vi.name = o
+                self.known_vi_[o] = vi
 
     def _onnx_infer_subgraph(self, node, subgraph, use_node_input=True, inc_subgraph_id=True):
         if self.verbose_ > 2:
-            logger.debug(
-                "Inferencing subgraph of node {} with output({}...): {}".format(node.name, node.output[0], node.op_type)
-            )
+            logger.debug(f"Inferencing subgraph of node {node.name} with output({node.output[0]}...): {node.op_type}")
         # node inputs are not passed directly to the subgraph
         # it's up to the node dispatcher to prepare subgraph input
         # for example, with Scan/Loop, subgraph input shape would be trimmed from node input shape
         # besides, inputs in subgraph could shadow implicit inputs
-        subgraph_inputs = set([i.name for i in list(subgraph.initializer) + list(subgraph.input)])
-        subgraph_implicit_input = set([name for name in self.known_vi_.keys() if not name in subgraph_inputs])
+        subgraph_inputs = {i.name for i in list(subgraph.initializer) + list(subgraph.input)}
+        subgraph_implicit_input = {name for name in self.known_vi_ if name not in subgraph_inputs}
         tmp_graph = helper.make_graph(
             list(subgraph.node),
             "tmp",
             list(subgraph.input) + [self.known_vi_[i] for i in subgraph_implicit_input],
             [make_named_value_info(i.name) for i in subgraph.output],
         )
         tmp_graph.initializer.extend([i for i in self.out_mp_.graph.initializer if i.name in subgraph_implicit_input])
@@ -505,56 +524,61 @@
             self.guess_output_rank_,
             self.verbose_,
             prefix=self.prefix_ + "_" + str(self.subgraph_id_),
         )
         if inc_subgraph_id:
             self.subgraph_id_ += 1
 
-        all_shapes_inferred = False
         symbolic_shape_inference._preprocess(self.tmp_mp_)
         symbolic_shape_inference.suggested_merge_ = self.suggested_merge_.copy()
         while symbolic_shape_inference.run_:
-            all_shapes_inferred = symbolic_shape_inference._infer_impl(self.sympy_data_.copy())
+            symbolic_shape_inference._infer_impl(self.sympy_data_.copy())
         symbolic_shape_inference._update_output_from_vi()
         if use_node_input:
             # if subgraph uses node input, it needs to update to merged dims
             subgraph.ClearField("input")
             subgraph.input.extend(symbolic_shape_inference.out_mp_.graph.input[: len(node.input)])
         subgraph.ClearField("output")
         subgraph.output.extend(symbolic_shape_inference.out_mp_.graph.output)
         subgraph.ClearField("value_info")
         subgraph.value_info.extend(symbolic_shape_inference.out_mp_.graph.value_info)
         subgraph.ClearField("node")
         subgraph.node.extend(symbolic_shape_inference.out_mp_.graph.node)
         # for new symbolic dims from subgraph output, add to main graph symbolic dims
         subgraph_shapes = [get_shape_from_value_info(o) for o in symbolic_shape_inference.out_mp_.graph.output]
-        subgraph_new_symbolic_dims = set(
-            [d for s in subgraph_shapes if s for d in s if type(d) == str and not d in self.symbolic_dims_]
-        )
+        subgraph_new_symbolic_dims = {
+            d for s in subgraph_shapes if s for d in s if type(d) == str and d not in self.symbolic_dims_
+        }
         new_dims = {}
         for d in subgraph_new_symbolic_dims:
             assert d in symbolic_shape_inference.symbolic_dims_
             new_dims[d] = symbolic_shape_inference.symbolic_dims_[d]
         self.symbolic_dims_.update(new_dims)
         return symbolic_shape_inference
 
-    def _get_int_values(self, node, broadcast=False):
+    def _get_int_or_float_values(self, node, broadcast=False, allow_float_values=False):
+        def int_or_float(value, allow_float_values):
+            # If casting into int has precision loss: keep float output
+            if allow_float_values and value % 1 != 0:
+                return value
+            return int(value)
+
         values = [self._try_get_value(node, i) for i in range(len(node.input))]
         if all([v is not None for v in values]):
             # some shape compute is in floating point, cast to int for sympy
             for i, v in enumerate(values):
                 if type(v) != np.ndarray:
                     continue
                 if len(v.shape) > 1:
                     new_v = None  # ignore value for rank > 1
                 elif len(v.shape) == 0:
-                    new_v = int(v.item())
+                    new_v = int_or_float(v.item(), allow_float_values)
                 else:
                     assert len(v.shape) == 1
-                    new_v = [int(vv) for vv in v]
+                    new_v = [int_or_float(vv, allow_float_values) for vv in v]
                 values[i] = new_v
         values_len = [len(v) if type(v) == list else 0 for v in values]
         max_len = max(values_len)
         if max_len >= 1 and broadcast:
             # broadcast
             for i, v in enumerate(values):
                 if v is None:
@@ -566,15 +590,23 @@
                         assert len(v) == max_len
                 else:
                     values[i] = [v] * max_len
         return values
 
     def _compute_on_sympy_data(self, node, op_func):
         assert len(node.output) == 1
-        values = self._get_int_values(node, broadcast=True)
+
+        # Before mul & div operations
+        # cast inputs into interger might lose decimal part and reduce precision
+        # keep them as float, finish the operation, then cast the result into integer
+        if node.op_type in ["Mul", "Div"]:
+            values = self._get_int_or_float_values(node, broadcast=True, allow_float_values=True)
+        else:
+            values = self._get_int_or_float_values(node, broadcast=True)
+
         if all([v is not None for v in values]):
             is_list = [type(v) == list for v in values]
             as_list = any(is_list)
             if as_list:
                 self.sympy_data_[node.output[0]] = [op_func(vs) for vs in zip(*values)]
             else:
                 self.sympy_data_[node.output[0]] = op_func(values)
@@ -594,15 +626,15 @@
                 node.output[0],
                 get_elem_type_from_type_proto(self.known_vi_[node.input[0]].type),
                 self._get_shape(node, 0),
             )
         )
 
     def _new_symbolic_dim(self, prefix, dim):
-        new_dim = "{}_d{}".format(prefix, dim)
+        new_dim = f"{prefix}_d{dim}"
         if new_dim in self.suggested_merge_:
             v = self.suggested_merge_[new_dim]
             new_symbolic_dim = sympy.Integer(int(v)) if is_literal(v) else v
         else:
             new_symbolic_dim = sympy.Symbol(new_dim, integer=True, nonnegative=True)
             self.symbolic_dims_[new_dim] = new_symbolic_dim
         return new_symbolic_dim
@@ -620,20 +652,20 @@
 
     def _new_symbolic_shape(self, rank, node, out_idx=0):
         return [self._new_symbolic_dim_from_output(node, out_idx, i) for i in range(rank)]
 
     def _compute_conv_pool_shape(self, node, channels_last=False):
         sympy_shape = self._get_sympy_shape(node, 0)
         if len(node.input) > 1:
-            W_shape = self._get_sympy_shape(node, 1)
+            W_shape = self._get_sympy_shape(node, 1)  # noqa: N806
             rank = len(W_shape) - 2  # number of spatial axes
             kernel_shape = W_shape[-rank - 1 : -1] if channels_last else W_shape[-rank:]
             sympy_shape[3 if channels_last else 1] = W_shape[0]
         else:
-            W_shape = None
+            W_shape = None  # noqa: N806
             kernel_shape = get_attribute(node, "kernel_shape")
             rank = len(kernel_shape)
 
         assert len(sympy_shape) == rank + 2
 
         # only need to symbolic shape inference if input has symbolic dims in spatial axes
         spatial_shape = sympy_shape[-rank - 1 : -1] if channels_last else sympy_shape[-rank:]
@@ -710,15 +742,15 @@
             new_shape = rhs_shape[:rhs_reduce_dim] + [rhs_shape[-1]]
         elif rhs_rank == 1:
             lhs_reduce_dim = -1
             new_shape = lhs_shape[:lhs_reduce_dim]
         else:
             lhs_reduce_dim = -1
             rhs_reduce_dim = -2
-            new_shape = self._broadcast_shapes(lhs_shape[:-2], rhs_shape[:-2]) + [lhs_shape[-2]] + [rhs_shape[-1]]
+            new_shape = [*self._broadcast_shapes(lhs_shape[:-2], rhs_shape[:-2]), lhs_shape[-2]] + [rhs_shape[-1]]
         # merge reduce dim
         self._check_merged_dims(
             [lhs_shape[lhs_reduce_dim], rhs_shape[rhs_reduce_dim]],
             allow_broadcast=False,
         )
         if output_dtype is None:
             # infer output_dtype from input type when not specified
@@ -751,83 +783,85 @@
                     new_dim = onnx.TensorShapeProto.Dimension()
                     if not is_sequence(dst_type):
                         new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, out_idx, di))
                     dst_tensor_type.shape.dim[di].CopyFrom(new_dim)
         else:
             dst_tensor_type.CopyFrom(src_tensor_type)
 
-    def _infer_ArrayFeatureExtractor(self, node):
+    def _infer_ArrayFeatureExtractor(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 data_shape[:-1] + indices_shape,
             )
         )
 
     def _infer_symbolic_compute_ops(self, node):
         funcs = {
-            "Add": lambda l: l[0] + l[1],
-            "Div": lambda l: l[0] // l[1],  # integer div in sympy
-            "Equal": lambda l: l[0] == l[1],
-            "Floor": lambda l: sympy.floor(l[0]),
-            "Max": lambda l: l[1]
+            "Add": lambda l: l[0] + l[1],  # noqa: E741
+            "Div": lambda l: int(l[0] // l[1])  # noqa: E741
+            if isinstance(l[0] // l[1], float)
+            else l[0] // l[1],  # integer div in sympy
+            "Equal": lambda l: l[0] == l[1],  # noqa: E741
+            "Floor": lambda l: sympy.floor(l[0]),  # noqa: E741
+            "Max": lambda l: l[1]  # noqa: E741
             if is_literal(l[0]) and int(l[0]) < -self.int_max_
             else (l[0] if is_literal(l[1]) and int(l[1]) < -self.int_max_ else sympy.Max(l[0], l[1])),
-            "Min": lambda l: l[1]
+            "Min": lambda l: l[1]  # noqa: E741
             if is_literal(l[0]) and int(l[0]) > self.int_max_
             else (l[0] if is_literal(l[1]) and int(l[1]) > self.int_max_ else sympy.Min(l[0], l[1])),
-            "Mul": lambda l: l[0] * l[1],
-            "Sub": lambda l: l[0] - l[1],
-            "Where": lambda l: l[1] if l[0] else l[2],
-            "Neg": lambda l: -l[0],
+            "Mul": lambda l: int(l[0] * l[1]) if isinstance(l[0] * l[1], float) else l[0] * l[1],  # noqa: E741
+            "Sub": lambda l: l[0] - l[1],  # noqa: E741
+            "Where": lambda l: l[1] if l[0] else l[2],  # noqa: E741
+            "Neg": lambda l: -l[0],  # noqa: E741
         }
         assert node.op_type in funcs
         self._compute_on_sympy_data(node, funcs[node.op_type])
 
-    def _infer_Cast(self, node):
+    def _infer_Cast(self, node):  # noqa: N802
         self._pass_on_sympy_data(node)
 
-    def _infer_CategoryMapper(self, node):
+    def _infer_CategoryMapper(self, node):  # noqa: N802
         input_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         if input_type == onnx.TensorProto.STRING:
             output_type = onnx.TensorProto.INT64
         else:
             output_type = onnx.TensorProto.STRING
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_type, self._get_shape(node, 0)))
 
-    def _infer_Compress(self, node):
+    def _infer_Compress(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         # create a new symbolic dimension for Compress output
         compress_len = str(self._new_symbolic_dim_from_output(node))
         axis = get_attribute(node, "axis")
-        if axis == None:
+        if axis is None:
             # when axis is not specified, input is flattened before compress so output is 1D
             output_shape = [compress_len]
         else:
             output_shape = input_shape
             output_shape[handle_negative_axis(axis, len(input_shape))] = compress_len
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 output_shape,
             )
         )
 
-    def _infer_Concat(self, node):
+    def _infer_Concat(self, node):  # noqa: N802
         if any([i in self.sympy_data_ or i in self.initializers_ for i in node.input]):
-            values = self._get_int_values(node)
+            values = self._get_int_or_float_values(node)
             if all([v is not None for v in values]):
-                assert 0 == get_attribute(node, "axis")
+                assert get_attribute(node, "axis") == 0
                 self.sympy_data_[node.output[0]] = []
                 for i in range(len(node.input)):
                     value = values[i]
                     if type(value) == list:
                         self.sympy_data_[node.output[0]].extend(value)
                     else:
                         self.sympy_data_[node.output[0]].append(value)
@@ -856,15 +890,15 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(sympy_shape),
             )
         )
 
-    def _infer_ConcatFromSequence(self, node):
+    def _infer_ConcatFromSequence(self, node):  # noqa: N802
         seq_shape = self._get_shape(node, 0)
         new_axis = 1 if get_attribute(node, "new_axis") else 0
         axis = handle_negative_axis(get_attribute(node, "axis"), len(seq_shape) + new_axis)
         concat_dim = str(self._new_symbolic_dim_from_output(node, 0, axis))
         new_shape = seq_shape
         if new_axis:
             new_shape = seq_shape[:axis] + [concat_dim] + seq_shape[axis:]
@@ -875,20 +909,20 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.sequence_type.elem_type.tensor_type.elem_type,
                 new_shape,
             )
         )
 
-    def _infer_Constant(self, node):
+    def _infer_Constant(self, node):  # noqa: N802
         t = get_attribute(node, "value")
         self.sympy_data_[node.output[0]] = numpy_helper.to_array(t)
 
-    def _infer_ConstantOfShape(self, node):
-        sympy_shape = self._get_int_values(node)[0]
+    def _infer_ConstantOfShape(self, node):  # noqa: N802
+        sympy_shape = self._get_int_or_float_values(node)[0]
         vi = self.known_vi_[node.output[0]]
         if sympy_shape is not None:
             if type(sympy_shape) != list:
                 sympy_shape = [sympy_shape]
             self._update_computed_dims(sympy_shape)
             # update sympy data if output type is int, and shape is known
             if vi.type.tensor_type.elem_type == onnx.TensorProto.INT64 and all([is_literal(x) for x in sympy_shape]):
@@ -904,39 +938,39 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 vi.type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(sympy_shape),
             )
         )
 
-    def _infer_Conv(self, node):
+    def _infer_Conv(self, node):  # noqa: N802
         sympy_shape = self._compute_conv_pool_shape(node)
         self._update_computed_dims(sympy_shape)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 vi.type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(sympy_shape),
             )
         )
 
-    def _infer_NhwcConv(self, node):
+    def _infer_NhwcConv(self, node):  # noqa: N802
         sympy_shape = self._compute_conv_pool_shape(node, channels_last=True)
         self._update_computed_dims(sympy_shape)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(sympy_shape),
             )
         )
 
-    def _infer_Einsum(self, node):
+    def _infer_Einsum(self, node):  # noqa: N802
         # ref:https://github.com/onnx/onnx/blob/623dfaa0151b2e4ce49779c3ec31cbd78c592b80/onnx/defs/math/defs.cc#L3275
         equation = get_attribute(node, "equation")
         equation = equation.replace(b" ", b"")
         mid_index = equation.find(b"->")
         left_equation = equation[:mid_index] if mid_index != -1 else equation
 
         num_operands = 0
@@ -990,15 +1024,15 @@
                 if value == 1:
                     new_sympy_shape.append(letter_to_dim[key])
 
         output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_sympy_shape))
 
-    def _infer_Expand(self, node):
+    def _infer_Expand(self, node):  # noqa: N802
         expand_to_shape = as_list(self._try_get_value(node, 1), keep_none=True)
         if expand_to_shape is not None:
             # new_shape's dim can come from shape value
             self._update_computed_dims(expand_to_shape)
             shape = self._get_shape(node, 0)
             new_shape = self._broadcast_shapes(shape, get_shape_from_sympy_shape(expand_to_shape))
             vi = self.known_vi_[node.output[0]]
@@ -1006,69 +1040,69 @@
                 helper.make_tensor_value_info(
                     node.output[0],
                     self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                     new_shape,
                 )
             )
 
-    def _infer_Gather(self, node):
+    def _infer_Gather(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         axis = handle_negative_axis(get_attribute(node, "axis", 0), len(data_shape))
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 data_shape[:axis] + indices_shape + data_shape[axis + 1 :],
             )
         )
         # for 1D input, do some sympy compute
-        if node.input[0] in self.sympy_data_ and len(data_shape) == 1 and 0 == get_attribute(node, "axis", 0):
+        if node.input[0] in self.sympy_data_ and len(data_shape) == 1 and get_attribute(node, "axis", 0) == 0:
             idx = self._try_get_value(node, 1)
             if idx is not None:
                 data = self.sympy_data_[node.input[0]]
                 if type(data) == list:
                     if type(idx) == np.ndarray and len(idx.shape) == 1:
                         self.sympy_data_[node.output[0]] = [data[int(i)] for i in idx]
                     else:
                         self.sympy_data_[node.output[0]] = data[int(idx)]
                 else:
                     assert idx == 0 or idx == -1
                     self.sympy_data_[node.output[0]] = data
 
-    def _infer_GatherElements(self, node):
+    def _infer_GatherElements(self, node):  # noqa: N802
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 indices_shape,
             )
         )
 
-    def _infer_GatherND(self, node):
+    def _infer_GatherND(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         data_rank = len(data_shape)
         indices_shape = self._get_shape(node, 1)
-        indices_rank = len(indices_shape)
+        len(indices_shape)
         last_index_dimension = indices_shape[-1]
         assert is_literal(last_index_dimension) and last_index_dimension <= data_rank
         new_shape = indices_shape[:-1] + data_shape[last_index_dimension:]
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 new_shape,
             )
         )
 
-    def _infer_If(self, node):
+    def _infer_If(self, node):  # noqa: N802
         # special case for constant condition, in case there are mismatching shape from the non-executed branch
         subgraphs = [
             get_attribute(node, "then_branch"),
             get_attribute(node, "else_branch"),
         ]
         cond = self._try_get_value(node, 0)
         if cond is not None:
@@ -1088,15 +1122,15 @@
                     self._fuse_tensor_type(node, i_out, vi.type, subgraph.output[i_out].type)
 
                 # pass on sympy data from subgraph, if cond is constant
                 if cond is not None and i_sub == (0 if as_scalar(cond) > 0 else 1):
                     if subgraph.output[i_out].name in subgraph_infer.sympy_data_:
                         self.sympy_data_[vi.name] = subgraph_infer.sympy_data_[subgraph.output[i_out].name]
 
-    def _infer_Loop(self, node):
+    def _infer_Loop(self, node):  # noqa: N802
         subgraph = get_attribute(node, "body")
         assert len(subgraph.input) == len(node.input)
         num_loop_carried = len(node.input) - 2  # minus the length and initial loop condition
         # when sequence_type is used as loop carried input
         # needs to run subgraph infer twice if the tensor shape in sequence contains None
         for i, si in enumerate(subgraph.input):
             si_name = si.name
@@ -1149,33 +1183,33 @@
                 subgraph_vi_dim = subgraph.output[i + 1].type.tensor_type.shape.dim
                 vi.type.tensor_type.shape.ClearField("dim")
                 vi_dim = vi.type.tensor_type.shape.dim
                 vi_dim.add().dim_param = loop_iter_dim
                 vi_dim.extend(list(subgraph_vi_dim))
             vi.name = node.output[i]
 
-    def _infer_MatMul(self, node):
+    def _infer_MatMul(self, node):  # noqa: N802
         self._compute_matmul_shape(node)
 
-    def _infer_MatMulInteger(self, node):
+    def _infer_MatMulInteger(self, node):  # noqa: N802
         self._compute_matmul_shape(node, onnx.TensorProto.INT32)
 
-    def _infer_NonMaxSuppression(self, node):
+    def _infer_NonMaxSuppression(self, node):  # noqa: N802
         selected = str(self._new_symbolic_dim_from_output(node))
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, [selected, 3]))
 
-    def _infer_NonZero(self, node):
+    def _infer_NonZero(self, node):  # noqa: N802
         input_rank = self._get_shape_rank(node, 0)
         # create a new symbolic dimension for NonZero output
         nz_len = str(self._new_symbolic_dim_from_output(node, 0, 1))
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type, [input_rank, nz_len]))
 
-    def _infer_OneHot(self, node):
+    def _infer_OneHot(self, node):  # noqa: N802
         sympy_shape = self._get_sympy_shape(node, 0)
         depth = self._try_get_value(node, 1)
         axis = get_attribute(node, "axis", -1)
         axis = handle_negative_axis(axis, len(sympy_shape) + 1)
         new_shape = get_shape_from_sympy_shape(
             sympy_shape[:axis]
             + [self._new_symbolic_dim_from_output(node) if not is_literal(depth) else depth]
@@ -1186,15 +1220,15 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[2]].type.tensor_type.elem_type,
                 new_shape,
             )
         )
 
-    def _infer_Pad(self, node):
+    def _infer_Pad(self, node):  # noqa: N802
         if get_opset(self.out_mp_) <= 10:
             pads = get_attribute(node, "pads")
         else:
             pads = self._try_get_value(node, 1)
 
         sympy_shape = self._get_sympy_shape(node, 0)
         rank = len(sympy_shape)
@@ -1211,15 +1245,15 @@
         output_tp = self.known_vi_[node.input[0]].type.tensor_type.elem_type
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], output_tp, get_shape_from_sympy_shape(new_sympy_shape))
         )
 
-    def _infer_Pool(self, node):
+    def _infer_Pool(self, node):  # noqa: N802
         sympy_shape = self._compute_conv_pool_shape(node)
         self._update_computed_dims(sympy_shape)
         for o in node.output:
             if not o:
                 continue
             vi = self.known_vi_[o]
             vi.CopyFrom(
@@ -1355,15 +1389,15 @@
                     self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                     get_shape_from_sympy_shape(sympy_shape),
                 )
             )
 
     def _infer_aten_argmax(self, node):
         new_shape = None
-        if node.input[1] == "":
+        if not node.input[1]:
             # The argmax of the flattened input is returned.
             new_shape = []
         else:
             dim = self._try_get_value(node, 1)
             keepdim = self._try_get_value(node, 2)
             if keepdim is not None:
                 sympy_shape = self._get_sympy_shape(node, 0)
@@ -1381,15 +1415,15 @@
         if node.output[0] and new_shape is not None:
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, new_shape))
 
     def _infer_aten_group_norm(self, node):
         self._propagate_shape_and_type(node)
         input_shape = self._get_shape(node, 0)
-        N = input_shape[0] if input_shape is not None and len(input_shape) != 0 else None
+        N = input_shape[0] if input_shape is not None and len(input_shape) != 0 else None  # noqa: N806
         group = self._try_get_value(node, 6)
         output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         for i in [1, 2]:
             if node.output[i]:
                 vi = self.known_vi_[node.output[i]]
                 vi.CopyFrom(
                     helper.make_tensor_value_info(
@@ -1400,42 +1434,42 @@
                             as_scalar(group)
                             if group is not None
                             else str(self._new_symbolic_dim_from_output(node, i, 1)),
                         ],
                     )
                 )
 
-    def _infer_aten_upsample_nearest(self, node):
+    def _infer_aten_upsample(self, node):
         new_shape = None
         input_shape = self._get_shape(node, 0)
         if input_shape is not None:
             new_shape = input_shape[:2]
             output_size = self._try_get_value(node, 1)
             if output_size is not None:
-                new_shape += [dim_size.item() for dim_size in output_size]
+                new_shape += [dim_size.item() if type(dim_size) == np.int64 else dim_size for dim_size in output_size]
             else:
                 rank = len(input_shape)
                 new_shape += [str(self._new_symbolic_dim_from_output(node, 0, i)) for i in range(2, rank)]
         if node.output[0] and new_shape is not None:
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_shape))
 
-    def _infer_BatchNormalization(self, node):
+    def _infer_BatchNormalization(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
         # this works for opsets < 14 and 14 since we check i < len(node.output) in the loop
         for i in [1, 2, 3, 4]:
-            if i < len(node.output) and node.output[i] != "":
+            if i < len(node.output) and node.output[i]:
                 # all of these parameters have the same shape as the 1st input
                 self._propagate_shape_and_type(node, input_index=1, output_index=i)
 
-    def _infer_Range(self, node):
+    def _infer_Range(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
-        input_data = self._get_int_values(node)
+        input_data = self._get_int_or_float_values(node)
         if all([i is not None for i in input_data]):
             start = as_scalar(input_data[0])
             limit = as_scalar(input_data[1])
             delta = as_scalar(input_data[2])
             new_sympy_shape = [sympy.Max(sympy.ceiling((limit - start) / delta), 0)]
         else:
             new_sympy_shape = [self._new_symbolic_dim_from_output(node)]
@@ -1444,15 +1478,15 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(new_sympy_shape),
             )
         )
 
-    def _infer_ReduceSum(self, node):
+    def _infer_ReduceSum(self, node):  # noqa: N802
         keep_dims = get_attribute(node, "keepdims", 1)
         if get_opset(self.out_mp_) >= 13 and len(node.input) > 1:
             # ReduceSum changes axes to input[1] in opset 13
             axes = self._try_get_value(node, 1)
             vi = self.known_vi_[node.output[0]]
             if axes is None:
                 assert keep_dims  # can only handle keep_dims==True when axes is unknown, by generating new ranks
@@ -1477,23 +1511,36 @@
                     helper.make_tensor_value_info(
                         node.output[0],
                         self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                         output_shape,
                     )
                 )
 
-    def _infer_ReduceProd(self, node):
+    def _infer_ReduceProd(self, node):  # noqa: N802
         axes = get_attribute(node, "axes")
         keep_dims = get_attribute(node, "keepdims", 1)
         if keep_dims == 0 and axes == [0]:
-            data = self._get_int_values(node)[0]
+            data = self._get_int_or_float_values(node)[0]
             if data is not None:
                 self.sympy_data_[node.output[0]] = sympy_reduce_product(data)
 
-    def _infer_Reshape(self, node):
+    def _infer_RelativePositionBias(self, node):  # noqa: N802
+        seq_len = self._try_get_value(node, 1)
+        real_seq_len = self._try_get_value(node, 2)
+        if seq_len is None or real_seq_len is None:
+            return
+        num_heads = self._get_sympy_shape(node, 0)[1]
+
+        new_shape = [1, num_heads, str(seq_len), str(real_seq_len)]
+
+        output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_shape))
+
+    def _infer_Reshape(self, node):  # noqa: N802
         shape_value = self._try_get_value(node, 1)
         vi = self.known_vi_[node.output[0]]
         if shape_value is None:
             shape_shape = self._get_shape(node, 1)
             assert len(shape_shape) == 1
             shape_rank = shape_shape[0]
             assert is_literal(shape_rank)
@@ -1537,15 +1584,15 @@
                     vi.type.tensor_type.elem_type,
                     get_shape_from_sympy_shape(new_sympy_shape),
                 )
             )
 
         self._pass_on_sympy_data(node)
 
-    def _infer_Resize(self, node):
+    def _infer_Resize(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
         input_sympy_shape = self._get_sympy_shape(node, 0)
         if get_opset(self.out_mp_) <= 10:
             scales = self._try_get_value(node, 1)
             if scales is not None:
                 new_sympy_shape = [sympy.simplify(sympy.floor(d * s)) for d, s in zip(input_sympy_shape, scales)]
                 self._update_computed_dims(new_sympy_shape)
@@ -1585,15 +1632,15 @@
                 helper.make_tensor_value_info(
                     node.output[0],
                     self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                     get_shape_from_sympy_shape(new_sympy_shape),
                 )
             )
 
-    def _infer_Scan(self, node):
+    def _infer_Scan(self, node):  # noqa: N802
         subgraph = get_attribute(node, "body")
         num_scan_inputs = get_attribute(node, "num_scan_inputs")
         scan_input_axes = get_attribute(node, "scan_input_axes", [0] * num_scan_inputs)
         num_scan_states = len(node.input) - num_scan_inputs
         scan_input_axes = [
             handle_negative_axis(ax, self._get_shape_rank(node, i + num_scan_states))
             for i, ax in enumerate(scan_input_axes)
@@ -1620,57 +1667,87 @@
                 new_dim = handle_negative_axis(scan_output_axes[i - num_scan_states], len(shape) + 1)
                 shape = shape[:new_dim] + [scan_input_dim] + shape[new_dim:]
                 vi.CopyFrom(helper.make_tensor_value_info(o, subgraph.output[i].type.tensor_type.elem_type, shape))
             else:
                 vi.CopyFrom(subgraph.output[i])
             vi.name = o
 
-    def _infer_ScatterElements(self, node):
+    def _infer_ScatterElements(self, node):  # noqa: N802
         data_shape = self._get_shape(node, 0)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 data_shape,
             )
         )
 
-    def _infer_SequenceAt(self, node):
+    def _infer_SequenceAt(self, node):  # noqa: N802
         # need to create new symbolic dimension if sequence shape has None:
         seq_shape = self._get_shape(node, 0)
         vi = self.known_vi_[node.output[0]]
         if seq_shape is not None:
             for di, d in enumerate(seq_shape):
                 if d is not None:
                     continue
                 new_dim = onnx.TensorShapeProto.Dimension()
                 new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, 0, di))
                 vi.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
 
-    def _infer_SequenceInsert(self, node):
+    def _infer_SequenceInsert(self, node):  # noqa: N802
         # workaround bug in onnx's shape inference
         vi_seq = self.known_vi_[node.input[0]]
         vi_tensor = self.known_vi_[node.input[1]]
         vi_out_seq = self.known_vi_[node.output[0]]
         vi_out_seq.CopyFrom(vi_seq)
         vi_out_seq.name = node.output[0]
         self._fuse_tensor_type(node, 0, vi_out_seq.type, vi_tensor.type)
 
-    def _infer_Shape(self, node):
+    def _infer_Shape(self, node):  # noqa: N802
         self.sympy_data_[node.output[0]] = self._get_sympy_shape(node, 0)
 
-    def _infer_Size(self, node):
+    def _infer_Size(self, node):  # noqa: N802
         sympy_shape = self._get_sympy_shape(node, 0)
         self.sympy_data_[node.output[0]] = sympy_reduce_product(sympy_shape)
         self.known_vi_[node.output[0]].CopyFrom(
             helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, [])
         )
 
-    def _infer_Slice(self, node):
+    def _infer_Slice(self, node):  # noqa: N802
+        # SymPy fails to prove that `x_0 + ... + x_n >= 0` if one of `x_i` is a `sympy.Min(a, b)`,
+        # even when the relation holds for both `a` and `b`.
+        #
+        # When given `expr` of form `min(a, b) + ...`, this function returns `[a + ..., b + ...]`,
+        # so that we can prove inequalities for both expressions separately.
+        #
+        # If the number of `min(...)` subexpressions is not exactly one, this function just returns `[expr]`.
+        def flatten_min(expr):
+            assert isinstance(expr, sympy.Add), f"Expected a sum of two arguments, got {expr}"
+            min_positions = [idx for idx in range(len(expr.args)) if isinstance(expr.args[idx], sympy.Min)]
+            if len(min_positions) == 1:
+                min_pos = min_positions[0]
+
+                def replace_min_with_arg(arg_idx):
+                    replaced = list(expr.args)
+                    assert isinstance(
+                        replaced[min_pos], sympy.Min
+                    ), f"Expected a sympy.Min() at position {min_pos}, got {replaced[min_pos]}"
+                    assert (
+                        len(replaced[min_pos].args) == 2
+                    ), f"Expected a sympy.Min() with exactly 2 arguments, got {replaced[min_pos]}"
+                    replaced[min_pos] = replaced[min_pos].args[arg_idx]
+                    return sympy.Add(*replaced)
+
+                return [
+                    replace_min_with_arg(0),
+                    replace_min_with_arg(1),
+                ]
+            return [expr]
+
         def less_equal(x, y):
             try:
                 return bool(x <= y)
             except TypeError:
                 pass
             try:
                 return bool(y >= x)
@@ -1679,27 +1756,31 @@
             try:
                 return bool(-x >= -y)
             except TypeError:
                 pass
             try:
                 return bool(-y <= -x)
             except TypeError:
-                # the last attempt; this may raise TypeError
+                pass
+            try:
                 return bool(y - x >= 0)
+            except TypeError:
+                # the last attempt; this may raise TypeError
+                return all(bool(d >= 0) for d in flatten_min(y - x))
 
         def handle_negative_index(index, bound):
             """normalizes a negative index to be in [0, bound)"""
             try:
                 if not less_equal(0, index):
                     if is_literal(index) and index <= -self.int_max_:
                         # this case is handled separately
                         return index
                     return bound + index
             except TypeError:
-                logger.warning("Cannot determine if {} < 0".format(index))
+                logger.warning(f"Cannot determine if {index} < 0")
             return index
 
         if get_opset(self.out_mp_) <= 9:
             axes = get_attribute(node, "axes")
             starts = get_attribute(node, "starts")
             ends = get_attribute(node, "ends")
             if not axes:
@@ -1724,45 +1805,43 @@
                     new_sympy_shape[i] = self._new_symbolic_dim_from_output(node, 0, i)
             else:
                 new_sympy_shape = get_shape_from_sympy_shape(new_sympy_shape)
                 for i in axes:
                     new_sympy_shape[i] = self._new_symbolic_dim_from_output(node, 0, i)
         else:
             for i, s, e, t in zip(axes, starts, ends, steps):
-                e = handle_negative_index(e, new_sympy_shape[i])
+                e = handle_negative_index(e, new_sympy_shape[i])  # noqa: PLW2901
                 if is_literal(e):
                     if e >= self.int_max_:
-                        e = new_sympy_shape[i]
+                        e = new_sympy_shape[i]  # noqa: PLW2901
                     elif e <= -self.int_max_:
-                        e = 0 if s > 0 else -1
+                        e = 0 if s > 0 else -1  # noqa: PLW2901
                     elif is_literal(new_sympy_shape[i]):
                         if e < 0:
-                            e = max(0, e + new_sympy_shape[i])
-                        e = min(e, new_sympy_shape[i])
+                            e = max(0, e + new_sympy_shape[i])  # noqa: PLW2901
+                        e = min(e, new_sympy_shape[i])  # noqa: PLW2901
                     else:
                         if e > 0:
-                            e = (
+                            e = (  # noqa: PLW2901
                                 sympy.Min(e, new_sympy_shape[i]) if e > 1 else e
                             )  # special case for slicing first to make computation easier
                 else:
                     if is_literal(new_sympy_shape[i]):
-                        e = sympy.Min(e, new_sympy_shape[i])
+                        e = sympy.Min(e, new_sympy_shape[i])  # noqa: PLW2901
                     else:
                         try:
                             if not less_equal(e, new_sympy_shape[i]):
-                                e = new_sympy_shape[i]
+                                e = new_sympy_shape[i]  # noqa: PLW2901
                         except Exception:
-                            logger.warning(
-                                "Unable to determine if {} <= {}, treat as equal".format(e, new_sympy_shape[i])
-                            )
-                            e = new_sympy_shape[i]
+                            logger.warning(f"Unable to determine if {e} <= {new_sympy_shape[i]}, treat as equal")
+                            e = new_sympy_shape[i]  # noqa: PLW2901
 
-                s = handle_negative_index(s, new_sympy_shape[i])
+                s = handle_negative_index(s, new_sympy_shape[i])  # noqa: PLW2901
                 if is_literal(new_sympy_shape[i]) and is_literal(s):
-                    s = max(0, min(s, new_sympy_shape[i]))
+                    s = max(0, min(s, new_sympy_shape[i]))  # noqa: PLW2901
 
                 new_sympy_shape[i] = sympy.simplify((e - s + t + (-1 if t > 0 else 1)) // t)
 
             self._update_computed_dims(new_sympy_shape)
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
@@ -1783,26 +1862,32 @@
         ):
             input_sympy_data = self.sympy_data_[node.input[0]]
             if type(input_sympy_data) == list or (
                 type(input_sympy_data) == np.array and len(input_sympy_data.shape) == 1
             ):
                 self.sympy_data_[node.output[0]] = input_sympy_data[starts[0] : ends[0] : steps[0]]
 
-    def _infer_SoftmaxCrossEntropyLoss(self, node):
+    def _infer_SoftmaxCrossEntropyLoss(self, node):  # noqa: N802
         vi = self.known_vi_[node.output[0]]
         elem_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+
+        # If output type is explicit specified in attribute, we use it as output tensor type.
+        specified_output_type = get_attribute(node, "output_type", None)
+        if specified_output_type is not None:
+            elem_type = specified_output_type
+
         vi.type.tensor_type.elem_type = elem_type
         vi.type.tensor_type.shape.CopyFrom(onnx.TensorShapeProto())
 
         if len(node.output) > 1:
             data_shape = self._get_shape(node, 0)
             vi = self.known_vi_[node.output[1]]
             vi.CopyFrom(helper.make_tensor_value_info(vi.name, elem_type, data_shape))
 
-    def _infer_Split_Common(self, node, make_value_info_func):
+    def _infer_Split_Common(self, node, make_value_info_func):  # noqa: N802
         input_sympy_shape = self._get_sympy_shape(node, 0)
         axis = handle_negative_axis(get_attribute(node, "axis", 0), len(input_sympy_shape))
         split = get_attribute(node, "split")
         if not split:
             num_outputs = len(node.output)
             split = [input_sympy_shape[axis] / sympy.Integer(num_outputs)] * num_outputs
             self._update_computed_dims(split)
@@ -1816,21 +1901,21 @@
                     node.output[i_o],
                     self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                     get_shape_from_sympy_shape(input_sympy_shape[:axis] + [split[i_o]] + input_sympy_shape[axis + 1 :]),
                 )
             )
             self.known_vi_[vi.name] = vi
 
-    def _infer_Split(self, node):
+    def _infer_Split(self, node):  # noqa: N802
         self._infer_Split_Common(node, helper.make_tensor_value_info)
 
-    def _infer_SplitToSequence(self, node):
+    def _infer_SplitToSequence(self, node):  # noqa: N802
         self._infer_Split_Common(node, helper.make_sequence_value_info)
 
-    def _infer_Squeeze(self, node):
+    def _infer_Squeeze(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         op_set = get_opset(self.out_mp_)
 
         # Depending on op-version 'axes' are provided as attribute or via 2nd input
         if op_set < 13:
             axes = get_attribute(node, "axes")
             assert self._try_get_value(node, 1) is None
@@ -1844,41 +1929,41 @@
             # For symbolic dimensions we guess they are !=1.
             output_shape = [s for s in input_shape if s != 1]
             if self.verbose_ > 0:
                 symbolic_dimensions = [s for s in input_shape if type(s) != int]
                 if len(symbolic_dimensions) > 0:
                     logger.debug(
                         f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. "
-                        + f"Assuming the following dimensions are never equal to 1: {symbolic_dimensions}"
+                        f"Assuming the following dimensions are never equal to 1: {symbolic_dimensions}"
                     )
         else:
             axes = [handle_negative_axis(a, len(input_shape)) for a in axes]
             output_shape = []
             for i in range(len(input_shape)):
                 if i not in axes:
                     output_shape.append(input_shape[i])
                 else:
                     assert input_shape[i] == 1 or type(input_shape[i]) != int
                     if self.verbose_ > 0 and type(input_shape[i]) != int:
                         logger.debug(
                             f"Symbolic dimensions in input shape of op: '{node.op_type}' node: '{node.name}'. "
-                            + f"Assuming the dimension '{input_shape[i]}' at index {i} of the input to be equal to 1."
+                            f"Assuming the dimension '{input_shape[i]}' at index {i} of the input to be equal to 1."
                         )
 
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(
                 node.output[0],
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 output_shape,
             )
         )
         self._pass_on_sympy_data(node)
 
-    def _infer_Tile(self, node):
+    def _infer_Tile(self, node):  # noqa: N802
         repeats_value = self._try_get_value(node, 1)
         new_sympy_shape = []
         if repeats_value is not None:
             input_sympy_shape = self._get_sympy_shape(node, 0)
             for i, d in enumerate(input_sympy_shape):
                 new_dim = d * repeats_value[i]
                 new_sympy_shape.append(new_dim)
@@ -1890,25 +1975,25 @@
             helper.make_tensor_value_info(
                 node.output[0],
                 vi.type.tensor_type.elem_type,
                 get_shape_from_sympy_shape(new_sympy_shape),
             )
         )
 
-    def _infer_TopK(self, node):
+    def _infer_TopK(self, node):  # noqa: N802
         rank = self._get_shape_rank(node, 0)
         axis = handle_negative_axis(get_attribute(node, "axis", -1), rank)
         new_shape = self._get_shape(node, 0)
 
         if get_opset(self.out_mp_) <= 9:
             k = get_attribute(node, "k")
         else:
-            k = self._get_int_values(node)[1]
+            k = self._get_int_or_float_values(node)[1]
 
-        if k == None:
+        if k is None:
             k = self._new_symbolic_dim_from_output(node)
         else:
             k = as_scalar(k)
 
         if type(k) in [int, str]:
             new_shape[axis] = k
         else:
@@ -1919,24 +2004,24 @@
             )  # note that TopK dim could be computed in sympy_data, so need to update computed_dims when it enters shape
             new_shape = get_shape_from_sympy_shape(new_sympy_shape)
 
         for i_o in range(len(node.output)):
             vi = self.known_vi_[node.output[i_o]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[i_o], vi.type.tensor_type.elem_type, new_shape))
 
-    def _infer_Transpose(self, node):
+    def _infer_Transpose(self, node):  # noqa: N802
         if node.input[0] in self.sympy_data_:
             data_shape = self._get_shape(node, 0)
             perm = get_attribute(node, "perm", reversed(list(range(len(data_shape)))))
             input_data = self.sympy_data_[node.input[0]]
             self.sympy_data_[node.output[0]] = (
                 np.transpose(np.array(input_data).reshape(*data_shape), axes=tuple(perm)).flatten().tolist()
             )
 
-    def _infer_Unsqueeze(self, node):
+    def _infer_Unsqueeze(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         op_set = get_opset(self.out_mp_)
 
         # Depending on op-version 'axes' are provided as attribute or via 2nd input
         if op_set < 13:
             axes = get_attribute(node, "axes")
             assert self._try_get_value(node, 1) is None
@@ -1963,180 +2048,308 @@
                 self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                 output_shape,
             )
         )
 
         self._pass_on_sympy_data(node)
 
-    def _infer_ZipMap(self, node):
+    def _infer_ZipMap(self, node):  # noqa: N802
         map_key_type = None
         if get_attribute(node, "classlabels_int64s") is not None:
             map_key_type = onnx.TensorProto.INT64
         elif get_attribute(node, "classlabels_strings") is not None:
             map_key_type = onnx.TensorProto.STRING
 
         assert map_key_type is not None
         new_vi = onnx.ValueInfoProto()
         new_vi.name = node.output[0]
         new_vi.type.sequence_type.elem_type.map_type.value_type.tensor_type.elem_type = onnx.TensorProto.FLOAT
         new_vi.type.sequence_type.elem_type.map_type.key_type = map_key_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(new_vi)
 
-    def _infer_Attention(self, node):
+    def _infer_Attention(self, node):  # noqa: N802
         shape = self._get_shape(node, 0)
-        shape_bias = self._get_shape(node, 2)
-        if shape and len(shape) == 3 and shape_bias and len(shape_bias) == 1:
+        shape_weights = self._get_shape(node, 1)
+        shape_bias = self._try_get_shape(node, 2)
+        if shape_bias is not None:
+            assert len(shape_bias) == 1
+        tripled_hidden_size = shape_bias[0] if shape_bias is not None else shape_weights[1]
+        if shape and len(shape) == 3:
             qkv_hidden_sizes_attr = get_attribute(node, "qkv_hidden_sizes")
             if qkv_hidden_sizes_attr is not None:
                 assert len(qkv_hidden_sizes_attr) == 3
                 shape[2] = int(qkv_hidden_sizes_attr[2])
-            elif isinstance(shape_bias[0], int):
-                shape[2] = int(shape_bias[0] / 3)
+            elif isinstance(tripled_hidden_size, int):
+                shape[2] = int(tripled_hidden_size / 3)
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, shape))
 
             if len(node.output) > 1:
                 # input shape: (batch_size, sequence_length, hidden_size)
                 # past shape: (2, batch_size, num_heads, past_sequence_length, head_size)
                 # mask shape: (batch_size, total_sequence_length) or (batch_size, sequence_length, total_sequence_length) or (batch_size, 1, max_seq_len, max_seq_len)
                 # present shape: (2, batch_size, num_heads, total_sequence_length, head_size), where total_sequence_length=sequence_length+past_sequence_length
                 input_shape = self._get_shape(node, 0)
-                past_shape = self._get_shape(node, 4)
-                mask_shape = self._get_shape(node, 3)
+                past_shape = self._get_shape(node, 4) if len(node.input) > 4 and node.input[4] else []
+                mask_shape = self._get_shape(node, 3) if len(node.input) > 3 and node.input[3] else []
+
                 if past_shape and len(past_shape) == 5:
                     if mask_shape and len(mask_shape) in [2, 3]:
                         past_shape[3] = mask_shape[-1]
                     elif input_shape and len(input_shape) == 3:
                         if isinstance(input_shape[1], int) and isinstance(past_shape[3], int):
                             past_shape[3] = input_shape[1] + past_shape[3]
                         else:
                             past_shape[3] = f"{past_shape[3]}+{input_shape[1]}"
                     vi = self.known_vi_[node.output[1]]
                     vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, past_shape))
+                # No past input but present output still exists
+                else:
+                    num_heads = get_attribute(node, "num_heads")
+                    head_size = input_shape[2] // num_heads
+                    present_shape = [2, input_shape[0], num_heads, input_shape[1], head_size]
+                    vi = self.known_vi_[node.output[1]]
+                    vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, present_shape))
 
-    def _infer_BiasGelu(self, node):
-        self._propagate_shape_and_type(node)
+    def _infer_PackedAttention(self, node):  # noqa: N802
+        shape = self._get_shape(node, 0)
+        shape_weights = self._get_shape(node, 1)
+        shape_bias = self._try_get_shape(node, 2)
+        if shape_bias is not None:
+            assert len(shape_bias) == 1
+        tripled_hidden_size = shape_bias[0] if shape_bias is not None else shape_weights[1]
+        if shape and len(shape) == 2:
+            qkv_hidden_sizes_attr = get_attribute(node, "qkv_hidden_sizes")
+            if qkv_hidden_sizes_attr is not None:
+                assert len(qkv_hidden_sizes_attr) == 3
+                shape[1] = int(qkv_hidden_sizes_attr[2])
+            elif isinstance(tripled_hidden_size, int):
+                shape[1] = int(tripled_hidden_size / 3)
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, shape))
 
-    def _infer_MultiHeadAttention(self, node):
-        # Input 0 (query) has shape (batch_size, sequence_length, hidden_size)
-        # Without packed KV:
-        #   Input 1 (key) has shape (batch_size, kv_sequence_length, hidden_size)
-        #   Input 2 (value) has shape (batch_size, kv_sequence_length, v_hidden_size)
-        # With packed KV:
-        #   Input 1 (key) has shape (batch_size, kv_sequence_length, num_heads, 2, head_size)
-        #   Input 2 (value) is nullptr
-        # Output 0 has shape (batch_size, sequence_length, v_hidden_size)
-        query_shape = self._get_shape(node, 0)
-        key_shape = self._get_shape(node, 1)
-        if query_shape is not None and len(query_shape) == 3:
+    def _infer_RemovePadding(self, node):  # noqa: N802
+        shape = self._get_shape(node, 0)
+        if shape and len(shape) == 3:
+            output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, ["token_count", shape[2]]))
+
+            vi_token_offset = self.known_vi_[node.output[1]]
+            vi_token_offset.CopyFrom(
+                helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, [shape[0], shape[1]])
+            )
 
-            # By default, hidden size is same for Q/K/V. Only need check v_hidden_size when value is provided.
-            output_shape = query_shape
-            if key_shape and len(key_shape) == 3:
-                value_shape = self._get_shape(node, 2)
-                if value_shape and len(value_shape) == 3:
-                    output_shape[2] = value_shape[2]
+            vi_cumulated_seq_len = self.known_vi_[node.output[2]]
+            vi_cumulated_seq_len.CopyFrom(
+                helper.make_tensor_value_info(node.output[2], onnx.TensorProto.INT32, ["batch_size + 1"])
+            )
+
+            vi_max_seq_len = self.known_vi_[node.output[3]]
+            vi_max_seq_len.CopyFrom(helper.make_tensor_value_info(node.output[3], onnx.TensorProto.INT32, [1]))
 
+    def _infer_RestorePadding(self, node):  # noqa: N802
+        shape_input = self._get_shape(node, 0)
+        shape_token_offset = self._get_shape(node, 1)
+        if shape_input and len(shape_input) == 2 and shape_token_offset and len(shape_token_offset) == 2:
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
             vi = self.known_vi_[node.output[0]]
+
+            output_shape = [shape_token_offset[0], shape_token_offset[1], shape_input[1]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
 
-    def _infer_FastGelu(self, node):
+    def _infer_BiasGelu(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node)
+
+    def _infer_MultiHeadAttention(self, node):  # noqa: N802
+        # Output 0 has shape (batch_size, sequence_length, v_hidden_size)
+        # Q, K and V without packing:
+        #   Input 0 (query) has shape (batch_size, sequence_length, hidden_size)
+        #   Input 1 (key) has shape (batch_size, kv_sequence_length, hidden_size) or (batch_size, num_heads, kv_sequence_length, head_size)
+        #   Input 2 (value) has shape (batch_size, kv_sequence_length, v_hidden_size) or (batch_size, num_heads, kv_sequence_length, head_size)
+        # Packed KV:
+        #   Input 0 (query) has shape (batch_size, sequence_length, hidden_size)
+        #   Input 1 (batch_size, kv_sequence_length, num_heads, 2, head_size)
+        #   Input 2  nullptr
+        # Packed QKV:
+        #   Input 0 (batch_size, sequence_length, num_heads, 3, head_size)
+        #   Input 1  nullptr
+        #   Input 2  nullptr
+
+        query_shape = self._get_shape(node, 0)
+        total_sequence_length = None
+        output_dtype = None
+        if query_shape is not None:
+            if len(query_shape) == 3:
+                key_shape = self._try_get_shape(node, 1)
+                # By default, hidden size is same for Q/K/V. Only need check v_hidden_size when value is provided.
+                output_shape = query_shape
+                if key_shape is not None and len(key_shape) == 3:
+                    value_shape = self._try_get_shape(node, 2)
+                    if value_shape is not None and len(value_shape) == 3:
+                        output_shape[2] = value_shape[2]
+                    total_sequence_length = key_shape[1]
+
+                output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+                vi = self.known_vi_[node.output[0]]
+                vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
+
+            elif len(query_shape) == 5:
+                if isinstance(query_shape[2], int) and isinstance(query_shape[4], int):
+                    output_shape = [query_shape[0], query_shape[1], query_shape[2] * query_shape[4]]
+                else:
+                    output_shape = [query_shape[0], query_shape[1], f"{query_shape[2]}*{query_shape[4]}"]
+
+                total_sequence_length = query_shape[1]
+
+                output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+                vi = self.known_vi_[node.output[0]]
+                vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, output_shape))
+
+            if len(node.output) > 1:
+                batch_size = query_shape[0]
+                num_heads = get_attribute(node, "num_heads")
+
+                head_size = None
+                if len(query_shape) == 3:
+                    head_size = (
+                        int(query_shape[2] / num_heads)
+                        if isinstance(query_shape[2], int)
+                        else f"{query_shape[2]}/{num_heads}"
+                    )
+                else:
+                    head_size = query_shape[4]
+
+                past_shape = self._try_get_shape(node, 6)
+
+                if past_shape is not None:
+                    if isinstance(past_shape[2], int) and isinstance(total_sequence_length, int):
+                        total_sequence_length = past_shape[2] + total_sequence_length
+                    else:
+                        total_sequence_length = f"{past_shape[2]}+{total_sequence_length}"
+
+                present_shape = [batch_size, num_heads, total_sequence_length, head_size]
+
+                assert output_dtype is not None
+                vi = self.known_vi_[node.output[1]]
+                vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, present_shape))
+                vi = self.known_vi_[node.output[2]]
+                vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, present_shape))
+
+    def _infer_FastGelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_Gelu(self, node):
+    def _infer_Gelu(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_GemmFastGelu(self, node):
+    def _infer_GemmFastGelu(self, node):  # noqa: N802
         self._compute_matmul_shape(node)
 
-    def _infer_LayerNormalization(self, node):
+    def _infer_LayerNormalization(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_LongformerAttention(self, node):
+    def _infer_LongformerAttention(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_EmbedLayerNormalization(self, node):
+    def _infer_EmbedLayerNormalization(self, node):  # noqa: N802
         input_ids_shape = self._get_shape(node, 0)
         word_embedding_shape = self._get_shape(node, 2)
         assert len(input_ids_shape) == 2 and len(word_embedding_shape) == 2
-        output_shape = input_ids_shape + [word_embedding_shape[1]]
+        output_shape = [*input_ids_shape, word_embedding_shape[1]]
 
         word_embedding_dtype = self.known_vi_[node.input[2]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], word_embedding_dtype, output_shape))
 
-        mask_index_shape = [input_ids_shape[0]]
-        vi = self.known_vi_[node.output[1]]
-        vi.CopyFrom(helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, mask_index_shape))
+        if len(node.output) > 1 and node.output[1]:
+            mask_index_shape = [input_ids_shape[0]]
+            vi = self.known_vi_[node.output[1]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, mask_index_shape))
 
         if len(node.output) > 2:
-            # Optional output of add before layer nomalization is done
+            # Optional output of add before layer normalization is done
             # shape is same as the output
             vi = self.known_vi_[node.output[2]]
             vi.CopyFrom(helper.make_tensor_value_info(node.output[2], word_embedding_dtype, output_shape))
 
-    def _infer_SkipLayerNormalization(self, node):
+    def _infer_SkipLayerNormalization(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
-        if len(node.output) > 3:
-            self._propagate_shape_and_type(node, 0, 3)
 
         # If the SkipLayerNormalization node contains the optional
         # output for inference, infer the shape and type for it too
         if len(node.output) > 3:
             self._propagate_shape_and_type(node, 0, 3)
 
-    def _infer_GroupNorm(self, node):
+    def _infer_GroupNorm(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
-    def _infer_BiasSplitGelu(self, node):
+    def _infer_BiasSplitGelu(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
         bias_shape = self._get_shape(node, 1)
         if input_shape and bias_shape and isinstance(bias_shape[0], int):
             output_shape = input_shape
             output_shape[2] = int(bias_shape[0] / 2)
             vi = self.known_vi_[node.output[0]]
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
             vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, output_shape))
 
-    def _infer_PythonOp(self, node):
+    def _infer_BiasAdd(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node)
+
+    def _infer_PythonOp(self, node):  # noqa: N802
         output_tensor_types = get_attribute(node, "output_tensor_types")
         assert output_tensor_types
         output_tensor_ranks = get_attribute(node, "output_tensor_ranks")
         assert output_tensor_ranks
 
-        # set the context output seperately.
+        # set the context output separately.
         # The first output is autograd's context.
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, []))
-
-        # Outputs after autograd's context are tensors.
-        # We assume their ranks are fixed for different model inputs.
-        for i in range(len(node.output) - 1):
-            # Process the i-th tensor outputs.
-            vi = self.known_vi_[node.output[i + 1]]
-            sympy_shape = self._new_symbolic_shape(output_tensor_ranks[i], node)
-            shape = get_shape_from_sympy_shape(sympy_shape)
-            value_info = helper.make_tensor_value_info(node.output[i + 1], output_tensor_types[i], shape)
-            vi.CopyFrom(value_info)
+        if get_attribute(node, "name").decode() in ["_InspectActivation", "_IncrementStep"]:
+            # PythonOp with name being "_InspectActivation" or "_IncrementStep" will behave exactly same as a normal
+            # PythonOp when execution. The only difference is that
+            # 1). those ops having same number of tensor inputs and tensor outputs;
+            # 2). and the i-th output tensor's shape is same as i-th input tensor's shape.
+            # Be noted, the count of custom autograd function might be bigger than output count, because there might
+            # be other non-tensor constant inputs (string, object, int, tuple, etc). But we did not make those constant
+            # inputs as ONNX op's input, instead they are stored in the attributes.
+            assert len(node.output) == len(node.input) + 1  # The output contains one extra context info.
+            for input_index in range(len(node.output) - 1):
+                # Process the i-th tensor outputs.
+                vi = self.known_vi_[node.output[input_index + 1]]
+                shape = self._get_shape(node, input_index)
+                output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
+                vi.CopyFrom(helper.make_tensor_value_info(node.output[input_index + 1], output_dtype, shape))
+        else:
+            # Outputs after autograd's context are tensors.
+            # We assume their ranks are fixed for different model inputs.
+            for i in range(len(node.output) - 1):
+                # Process the i-th tensor outputs.
+                vi = self.known_vi_[node.output[i + 1]]
+                sympy_shape = self._new_symbolic_shape(output_tensor_ranks[i], node)
+                shape = get_shape_from_sympy_shape(sympy_shape)
+                value_info = helper.make_tensor_value_info(node.output[i + 1], output_tensor_types[i], shape)
+                vi.CopyFrom(value_info)
 
     def _propagate_shape_and_type(self, node, input_index=0, output_index=0):
         shape = self._get_shape(node, input_index)
         output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[output_index]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[output_index], output_dtype, shape))
 
     def _is_none_dim(self, dim_value):
         if type(dim_value) != str:
             return False
         if "unk__" not in dim_value:
             return False
-        if dim_value in self.symbolic_dims_.keys():
+        if dim_value in self.symbolic_dims_:
             return False
         return True
 
     def _is_shape_contains_none_dim(self, out_shape):
         for out in out_shape:
             if self._is_none_dim(out):
                 return out
@@ -2180,17 +2393,17 @@
         self.tmp_mp_.graph.ClearField("initializer")
 
         # compute prerequesite for node for topological sort
         # node with subgraphs may have dependency on implicit inputs, which will affect topological sort
         prereq_for_node = {}  # map from node to all its inputs, including implicit ones in subgraph
 
         def get_prereq(node):
-            names = set(i for i in node.input if i)
+            names = {i for i in node.input if i}
             subgraphs = []
-            if "If" == node.op_type:
+            if node.op_type == "If":
                 subgraphs = [
                     get_attribute(node, "then_branch"),
                     get_attribute(node, "else_branch"),
                 ]
             elif node.op_type in ["Loop", "Scan"]:
                 subgraphs = [get_attribute(node, "body")]
             for g in subgraphs:
@@ -2208,15 +2421,15 @@
             return names
 
         for n in self.tmp_mp_.graph.node:
             prereq_for_node[n.output[0]] = get_prereq(n)
 
         # topological sort nodes, note there might be dead nodes so we check if all graph outputs are reached to terminate
         sorted_nodes = []
-        sorted_known_vi = set([i.name for i in list(self.out_mp_.graph.input) + list(self.out_mp_.graph.initializer)])
+        sorted_known_vi = {i.name for i in list(self.out_mp_.graph.input) + list(self.out_mp_.graph.initializer)}
         if any([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
             # Loop/Scan will have some graph output in graph inputs, so don't do topological sort
             sorted_nodes = self.out_mp_.graph.node
         else:
             while not all([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
                 old_sorted_nodes_len = len(sorted_nodes)
                 for node in self.out_mp_.graph.node:
@@ -2227,14 +2440,15 @@
                         sorted_nodes.append(node)
                 if old_sorted_nodes_len == len(sorted_nodes) and not all(
                     [o.name in sorted_known_vi for o in self.out_mp_.graph.output]
                 ):
                     raise Exception("Invalid model with cyclic graph")
 
         for node in sorted_nodes:
+            # print(node.name)
             assert all([i in self.known_vi_ for i in node.input if i])
             self._onnx_infer_single_node(node)
             known_aten_op = False
             if node.op_type in self.dispatcher_:
                 self.dispatcher_[node.op_type](node)
             elif node.op_type in ["ConvTranspose"]:
                 # onnx shape inference ops like ConvTranspose may have empty shape for symbolic input
@@ -2280,40 +2494,42 @@
                     in_dims = [s[len(s) - out_rank + d] for s in in_shapes if len(s) + d >= out_rank]
                     if len(in_dims) > 1:
                         self._check_merged_dims(in_dims, allow_broadcast=True)
 
             for i_o in range(len(node.output)):
                 # Special case: We do not care about the training related
                 # outputs of SkipLayerNormalization
-                if node.op_type == "SkipLayerNormalization" and i_o in [1, 2]:
+                if (
+                    node.op_type == "SkipLayerNormalization" or node.op_type == "SkipSimplifiedLayerNormalization"
+                ) and i_o in [1, 2]:
                     continue
 
                 vi = self.known_vi_[node.output[i_o]]
                 out_type = vi.type
                 out_type_kind = out_type.WhichOneof("value")
 
                 # do not process shape for non-tensors
                 if out_type_kind not in ["tensor_type", "sparse_tensor_type", None]:
                     if self.verbose_ > 2:
                         if out_type_kind == "sequence_type":
                             seq_cls_type = out_type.sequence_type.elem_type.WhichOneof("value")
-                            if "tensor_type" == seq_cls_type:
+                            if seq_cls_type == "tensor_type":
                                 logger.debug(
                                     "  {}: sequence of {} {}".format(
                                         node.output[i_o],
                                         str(get_shape_from_value_info(vi)),
                                         onnx.TensorProto.DataType.Name(
                                             vi.type.sequence_type.elem_type.tensor_type.elem_type
                                         ),
                                     )
                                 )
                             else:
-                                logger.debug("  {}: sequence of {}".format(node.output[i_o], seq_cls_type))
+                                logger.debug(f"  {node.output[i_o]}: sequence of {seq_cls_type}")
                         else:
-                            logger.debug("  {}: {}".format(node.output[i_o], out_type_kind))
+                            logger.debug(f"  {node.output[i_o]}: {out_type_kind}")
                     continue
 
                 out_shape = get_shape_from_value_info(vi)
                 out_type_undefined = out_type.tensor_type.elem_type == onnx.TensorProto.UNDEFINED
                 if self.verbose_ > 2:
                     logger.debug(
                         "  {}: {} {}".format(
@@ -2391,15 +2607,15 @@
                             self.run_ = True
                         else:
                             self.run_ = False
                     else:
                         self.run_ = False
 
                     # create new dynamic dims for ops not handled by symbolic shape inference
-                    if self.run_ == False and not node.op_type in self.dispatcher_ and not known_aten_op:
+                    if self.run_ is False and node.op_type not in self.dispatcher_ and not known_aten_op:
                         is_unknown_op = out_type_undefined and (out_shape is None or len(out_shape) == 0)
                         if is_unknown_op:
                             # unknown op to ONNX, maybe from higher opset or other domain
                             # only guess the output rank from input 0 when using guess_output_rank option
                             out_rank = self._get_shape_rank(node, 0) if self.guess_output_rank_ else -1
                         else:
                             # valid ONNX op, but not handled by symbolic shape inference, just assign dynamic shape
```

## onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py

```diff
@@ -212,15 +212,15 @@
             "python -m onnxruntime.tools.onnx_model_utils.update_onnx_opset ..."
         )
 
         return False
 
     unsupported_ops = set()
     logger.debug(
-        "Checking if the data types and operators used in the model are supported " "in the pre-built ORT package..."
+        "Checking if the data types and operators used in the model are supported in the pre-built ORT package..."
     )
     unsupported = check_graph(
         model_with_type_info.graph,
         opsets,
         required_ops,
         global_onnx_tensorproto_types,
         special_types,
@@ -232,15 +232,15 @@
         logger.info("Unsupported operators:")
         for entry in sorted(unsupported_ops):
             logger.info("  " + entry)
 
     if unsupported:
         logger.info("\nModel is not supported by the pre-built package due to unsupported types and/or operators.")
         logger.info(
-            "Please see https://onnxruntime.ai/docs/reference/mobile/prebuilt-package/ for information "
+            "Please see https://onnxruntime.ai/docs/install/#install-on-web-and-mobile for information "
             "on what is supported in the pre-built package."
         )
         logger.info(
             "A custom build of ONNX Runtime will be required to run the model. Please see "
             "https://onnxruntime.ai/docs/build/custom.html for details on performing that."
         )
     else:
@@ -280,15 +280,15 @@
     parser = argparse.ArgumentParser(
         description="Check if model can be run using the ONNX Runtime Mobile Pre-Built Package",
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
     )
 
     parser.add_argument(
         "--config_path",
-        help="Path to required operators and types configuration used to build " "the pre-built ORT mobile package.",
+        help="Path to required operators and types configuration used to build the pre-built ORT mobile package.",
         required=False,
         type=pathlib.Path,
         default=get_default_config_path(),
     )
 
     parser.add_argument("model_path", help="Path to ONNX model to check", type=pathlib.Path)
```

## onnxruntime/tools/mobile_helpers/coreml_supported_ops.md

```diff
@@ -8,20 +8,28 @@
 |ai.onnx:AveragePool|Only 2D Pool is supported.|
 |ai.onnx:BatchNormalization||
 |ai.onnx:Cast||
 |ai.onnx:Clip||
 |ai.onnx:Concat||
 |ai.onnx:Conv|Only 1D/2D Conv is supported.<br/>Weights and bias should be constant.|
 |ai.onnx:DepthToSpace|Only DCR mode DepthToSpace is supported.|
+|ai.onnx:Div||
+|ai.onnx:Flatten||
 |ai.onnx:Gemm|Input B should be constant.|
 |ai.onnx:GlobalAveragePool|Only 2D Pool is supported.|
 |ai.onnx:GlobalMaxPool|Only 2D Pool is supported.|
+|ai.onnx:LeakyRelu||
+|ai.onnx:LRN||
 |ai.onnx:MatMul|Input B should be constant.|
 |ai.onnx:MaxPool|Only 2D Pool is supported.|
+|ai.onnx:Mul||
+|ai.onnx:Pad|Only constant mode and last two dim padding is supported.<br/>Input pads and constant_value should be constant.<br/>If provided, axes should be constant.|
+|ai.onnx:Pow|Only supports cases when both inputs are fp32.|
 |ai.onnx:PRelu|Input slope should be constant.<br/>Input slope should either have shape [C, 1, 1] or have 1 element.|
 |ai.onnx:Relu||
 |ai.onnx:Reshape||
 |ai.onnx:Resize||
 |ai.onnx:Sigmoid||
 |ai.onnx:Squeeze||
+|ai.onnx:Sub||
 |ai.onnx:Tanh||
 |ai.onnx:Transpose||
```

## onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md

```diff
@@ -2,14 +2,15 @@
 Keep in sync with doco generated from /docs/execution-providers/NNAPI-ExecutionProvider.md on the gh_pages branch
 -->
 |Operator|Note|
 |--------|------|
 |ai.onnx:Abs||
 |ai.onnx:Add||
 |ai.onnx:AveragePool|Only 2D Pool is supported.|
+|ai.onnx:BatchNormalization||
 |ai.onnx:Cast||
 |ai.onnx:Clip||
 |ai.onnx:Concat||
 |ai.onnx:Conv|Only 2D Conv is supported.<br/>Weights and bias should be constant.|
 |ai.onnx:DepthToSpace|Only DCR mode DepthToSpace is supported.|
 |ai.onnx:DequantizeLinear|All quantization scales and zero points should be constant.|
 |ai.onnx:Div||
@@ -19,14 +20,15 @@
 |ai.onnx:Floor||
 |ai.onnx:Gather|Input indices should be constant if not int32 type.|
 |ai.onnx:Gemm|If input B is not constant, transB should be 1.|
 |ai.onnx:GlobalAveragePool|Only 2D Pool is supported.|
 |ai.onnx:GlobalMaxPool|Only 2D Pool is supported.|
 |ai.onnx:Identity||
 |ai.onnx:Log||
+|ai.onnx:LRN||
 |ai.onnx:MatMul||
 |ai.onnx:MaxPool|Only 2D Pool is supported.|
 |ai.onnx:Max||
 |ai.onnx:Min||
 |ai.onnx:Mul||
 |ai.onnx:Neg||
 |ai.onnx:Pad|Only constant mode Pad is supported.<br/>Input pads and constant_value should be constant.<br/>Input pads values should be non-negative.|
```

## onnxruntime/tools/mobile_helpers/usability_checker.py

```diff
@@ -28,15 +28,15 @@
     """
 
     def __init__(self, filename):
         self._filename = filename
         self._ops = {}  # op to caveats
         self._ops_seen = set()
 
-        with open(filename, "r") as f:
+        with open(filename) as f:
             for line in f.readlines():
                 # we're looking for a markdown table with 2 columns. first is op name. second is caveats
                 # op name is domain:op
                 if line.startswith("|"):
                     pieces = line.strip().split("|")
                     if len(pieces) == 4:  # pre-first '|'. op, caveat, post-last '|'
                         domain_op = pieces[1]
@@ -81,15 +81,14 @@
         self.supported_ops_checker = None
         self.supported_groups = []
         self.unsupported_ops = set()
         self.nodes_unsupported_due_to_op = -1
         self.nodes_unsupported_due_to_dynamic_input = -1
 
     def suitability(self):
-
         # for now add up all the nodes. if there are subgraphs, the percentage of covered nodes will be reduced by all
         # nodes in the subgraphs.
         num_nodes = self.num_nodes + self.num_nodes_in_subgraphs
 
         # semi-arbitrary choices that err on the side of MAYBE.
         # having 1 partition is always preferred, but if that is small it may not be useful.
         # having 2 partitions may be okay if they cover most nodes
@@ -440,15 +439,15 @@
         if is_fixed_size_tensor(vi):
             num_fixed_values += 1
         else:
             num_dynamic_values += 1
 
     if logger:
         logger.info(
-            f"Num values with fixed shape={num_fixed_values}. " f"Num values with dynamic shape={num_dynamic_values}"
+            f"Num values with fixed shape={num_fixed_values}. Num values with dynamic shape={num_dynamic_values}"
         )
 
         if dynamic_inputs:
             if dynamic_outputs:
                 logger.info(
                     "Model has dynamic inputs and outputs. Consider re-exporting model with fixed sizes "
                     "if NNAPI or CoreML can be used with this model."
@@ -461,15 +460,14 @@
                        if applicable, should not be significantly impacted."""
                 )
 
     return dynamic_inputs, num_dynamic_values
 
 
 def checker(model_path, logger: logging.Logger):
-
     model = onnx.load(model_path)
     model_with_shape_info = onnx.shape_inference.infer_shapes(model)
 
     # create lookup map for efficiency
     value_to_shape = {}
     for v in model_with_shape_info.graph.input:
         value_to_shape[v.name] = v
```

## onnxruntime/tools/ort_format_model/__init__.py

```diff
@@ -14,14 +14,12 @@
 else:
     # running directly from ORT repo, so fbs bindings are under onnxruntime/core/flatbuffers
     ort_root = os.path.abspath(os.path.join(script_dir, "..", "..", "..", ".."))
     ort_fbs_py_parent_dir = os.path.join(ort_root, "onnxruntime", "core", "flatbuffers")
 
 sys.path.append(ort_fbs_py_parent_dir)
 
-from .operator_type_usage_processors import (  # noqa
-    GloballyAllowedTypesOpTypeImplFilter,
-    OperatorTypeUsageManager,
-    OpTypeImplFilterInterface,
-)
-from .ort_model_processor import OrtFormatModelProcessor  # noqa
-from .utils import create_config_from_models  # noqa
+from .operator_type_usage_processors import GloballyAllowedTypesOpTypeImplFilter  # noqa: E402, F401
+from .operator_type_usage_processors import OperatorTypeUsageManager  # noqa: E402, F401
+from .operator_type_usage_processors import OpTypeImplFilterInterface  # noqa: E402, F401
+from .ort_model_processor import OrtFormatModelProcessor  # noqa: E402, F401
+from .utils import create_config_from_models  # noqa: E402, F401
```

## onnxruntime/tools/ort_format_model/operator_type_usage_processors.py

```diff
@@ -7,30 +7,30 @@
 
 import ort_flatbuffers_py.fbs as fbs
 
 from .types import FbsTypeInfo, value_name_to_typestr
 
 
 def _create_op_key(domain: str, optype: str):
-    return "{}:{}".format(domain, optype)
+    return f"{domain}:{optype}"
 
 
 def _ort_constant_for_domain(domain: str):
     """
     Map a string domain value to the internal ONNX Runtime constant for that domain.
     :param domain: Domain string to map.
     :return: Internal ONNX Runtime constant
     """
 
     # constants are defined in <ORT root>/include/onnxruntime/core/graph/constants.h
     # This list is limited to just the domains we have processors for
     domain_to_constant_map = {"ai.onnx": "kOnnxDomain", "ai.onnx.ml": "kMLDomain", "com.microsoft": "kMSDomain"}
 
     if domain not in domain_to_constant_map:
-        raise ValueError("Domain {} not found in map to ONNX Runtime constant. Please update map.".format(domain))
+        raise ValueError(f"Domain {domain} not found in map to ONNX Runtime constant. Please update map.")
 
     return domain_to_constant_map[domain]
 
 
 def _reg_type_to_cpp_type(reg_type: str):
     if reg_type == "string":
         return "std::string"
@@ -72,15 +72,15 @@
         Given the string from a kernel registration, determine if the registration is required or not.
         :param type_in_registration: Type string from kernel registration
         :param globally_allowed_types: Optional set of globally allowed types. If provided, these types take precedence
                                        in determining the required types.
         :return: True is required. False if not.
         """
         # Not all operators have typed registrations, so this is optionally implemented by derived classes
-        raise RuntimeError("Did not expect processor for {} to have typed registrations.".format(self.name))
+        raise RuntimeError(f"Did not expect processor for {self.name} to have typed registrations.")
 
     def get_cpp_entry(self):
         """
         Get the C++ code that specifies this operator's required types.
         :return: List with any applicable C++ code for this operator's required types. One line per entry.
         """
         # Not applicable for some ops, so return no lines by default.
@@ -109,18 +109,18 @@
     Operator processor which tracks the types used for selected input/s and/or output/s.
     """
 
     def __init__(
         self,
         domain: str,
         optype: str,
-        inputs: [int] = [0],
-        outputs: [int] = [],
-        required_input_types: typing.Dict[int, typing.Set[str]] = {},
-        required_output_types: typing.Dict[int, typing.Set[str]] = {},
+        inputs: [int] = [0],  # noqa: B006
+        outputs: [int] = [],  # noqa: B006
+        required_input_types: typing.Dict[int, typing.Set[str]] = {},  # noqa: B006
+        required_output_types: typing.Dict[int, typing.Set[str]] = {},  # noqa: B006
     ):
         """
         Create DefaultTypeUsageProcessor. Types for one or more inputs and/or outputs can be tracked by the processor.
         The default is to track the types required for input 0, as this is the most common use case in ONNX.
 
         Required input and output types may be specified. These are only applicable to is_typed_registration_needed().
         If a registration type matches a required type, the typed registration is needed.
@@ -162,27 +162,27 @@
     def is_output_type_enabled(self, reg_type, index, allowed_type_set=None):
         """Whether output type is enabled based on required and allowed types."""
         if allowed_type_set is None:
             allowed_type_set = self._output_types[index]
         return self._is_type_enabled(reg_type, index, self._required_output_types, allowed_type_set)
 
     def process_node(self, node: fbs.Node, value_name_to_typeinfo: dict):
-        for i in self._input_types.keys():
+        for i in self._input_types:
             if i >= node.InputsLength():
                 # Some operators have fewer inputs in earlier versions where data that was as an attribute
                 # become an input in later versions to allow it to be dynamically provided. Allow for that.
                 # e.g. Slice-1 had attributes for the indices, and Slice-10 moved those to be inputs
                 # raise RuntimeError('Node has {} outputs. Tracker for {} incorrectly configured as it requires {}.'
                 #                    .format(node.OutputsLength(), self.name, o))
                 pass
             else:
                 type_str = value_name_to_typestr(node.Inputs(i), value_name_to_typeinfo)
                 self._input_types[i].add(type_str)
 
-        for o in self._output_types.keys():
+        for o in self._output_types:
             # Don't know of any ops where the number of outputs changed across versions, so require a valid length
             if o >= node.OutputsLength():
                 raise RuntimeError(
                     "Node has {} outputs. Tracker for {} incorrectly configured as it requires {}.".format(
                         node.OutputsLength(), self.name, o
                     )
                 )
@@ -192,15 +192,15 @@
 
     def is_typed_registration_needed(
         self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
     ):
         if 0 not in self._input_types.keys():
             # currently all standard typed registrations are for input 0.
             # custom registrations can be handled by operator specific processors (e.g. OneHotProcessor below).
-            raise RuntimeError("Expected typed registration to use type from input 0. Node:{}".format(self.name))
+            raise RuntimeError(f"Expected typed registration to use type from input 0. Node:{self.name}")
 
         return self.is_input_type_enabled(type_in_registration, 0, globally_allowed_types)
 
     def get_cpp_entry(self):
         entries = []
         domain = _ort_constant_for_domain(self.domain)
         for i in sorted(self._input_types.keys()):
@@ -323,15 +323,15 @@
         entry = json.dumps(aggregate_info)
         return entry
 
     def from_config_entry(self, entry: str):
         self._triples.clear()
         aggregate_info = json.loads(entry)
         if "custom" in aggregate_info:
-            self._triples = set([tuple(triple) for triple in aggregate_info["custom"]])
+            self._triples = {tuple(triple) for triple in aggregate_info["custom"]}
 
 
 def _create_operator_type_usage_processors():
     """
     Create a set of processors that determine the required types for all enabled operators.
     :return: Dictionary of operator key to processor. Key is 'domain:operator (e.g. ai.onnx:Cast)'.
     """
@@ -585,23 +585,22 @@
         """
         key = _create_op_key(domain, optype)
         op_processor = self._get_op_processor(key)
         if op_processor:
             op_processor.from_config_entry(config_entry)
 
     def debug_dump(self):
-
         print("C++ code that will be emitted:")
         [print(cpp_line) for cpp_line in self.get_cpp_entries()]
 
         print("Config file type information that will be returned by get_config_entry:")
         for key in sorted(self._operator_processors.keys()):
             entry = self._operator_processors[key].to_config_entry()
             if entry:
-                print("{} -> {}".format(key, entry))
+                print(f"{key} -> {entry}")
 
                 # roundtrip test to validate that we can initialize the processor from the entry and get the
                 # same values back
                 self._operator_processors[key].from_config_entry(entry)
                 assert entry == self._operator_processors[key].to_config_entry()
 
     class _OpTypeImplFilter(OpTypeImplFilterInterface):
```

## onnxruntime/tools/ort_format_model/ort_model_processor.py

```diff
@@ -13,23 +13,23 @@
         """
         Initialize ORT format model processor
         :param model_path: Path to model to load
         :param required_ops: Dictionary required operator information will be added to.
         :param processors: Operator type usage processors which will be called for each matching Node.
         """
         self._required_ops = required_ops  # dictionary of {domain: {opset:[operators]}}
-        self._file = open(model_path, "rb").read()
+        self._file = open(model_path, "rb").read()  # noqa: SIM115
         self._buffer = bytearray(self._file)
         if not fbs.InferenceSession.InferenceSession.InferenceSessionBufferHasIdentifier(self._buffer, 0):
-            raise RuntimeError("File does not appear to be a valid ORT format model: '{}'".format(model_path))
+            raise RuntimeError(f"File does not appear to be a valid ORT format model: '{model_path}'")
         self._model = fbs.InferenceSession.InferenceSession.GetRootAsInferenceSession(self._buffer, 0).Model()
         self._op_type_processors = processors
 
     @staticmethod
-    def _setup_type_info(graph: fbs.Graph, outer_scope_value_typeinfo={}):
+    def _setup_type_info(graph: fbs.Graph, outer_scope_value_typeinfo={}):  # noqa: B006
         """
         Setup the node args for this level of Graph.
         We copy the current list which represents the outer scope values, and add the local node args to that
         to create the valid list of values for the current Graph.
         :param graph: Graph to create NodeArg list for
         :param outer_scope_value_typeinfo: TypeInfo for outer scope values. Empty for the top-level graph in a model.
         :return: Dictionary of NodeArg name to TypeInfo
@@ -39,17 +39,17 @@
             n = graph.NodeArgs(j)
             value_name_to_typeinfo[n.Name()] = n.Type()  # TypeInfo for this NodeArg's name
 
         return value_name_to_typeinfo
 
     def _add_required_op(self, domain: str, opset: int, op_type: str):
         if domain not in self._required_ops:
-            self._required_ops[domain] = {opset: set([op_type])}
+            self._required_ops[domain] = {opset: {op_type}}
         elif opset not in self._required_ops[domain]:
-            self._required_ops[domain][opset] = set([op_type])
+            self._required_ops[domain][opset] = {op_type}
         else:
             self._required_ops[domain][opset].add(op_type)
 
     def _process_graph(self, graph: fbs.Graph, outer_scope_value_typeinfo: dict):
         """
         Process one level of the Graph, descending into any subgraphs when they are found
         :param outer_scope_value_typeinfo: Outer scope NodeArg dictionary from ancestor graphs
```

## onnxruntime/tools/ort_format_model/types.py

```diff
@@ -40,15 +40,15 @@
         elif value_type == fbs.TypeInfoValue.TypeInfoValue.map_type:
             map_type = fbs.MapType.MapType()
             map_type.init(value.Bytes, value.Pos)
             key_type = map_type.KeyType()  # TensorDataType
             key_type_str = FbsTypeInfo.tensordatatype_to_string[key_type]
             value_type = map_type.ValueType()  # TypeInfo
             value_type_str = FbsTypeInfo.typeinfo_to_str(value_type)
-            type_str = "std::map<{},{}>".format(key_type_str, value_type_str)
+            type_str = f"std::map<{key_type_str},{value_type_str}>"
 
         elif value_type == fbs.TypeInfoValue.TypeInfoValue.sequence_type:
             sequence_type = fbs.SequenceType.SequenceType()
             sequence_type.Init(value.Bytes, value.Pos)
             elem_type = sequence_type.ElemType()  # TypeInfo
             elem_type_str = FbsTypeInfo.typeinfo_to_str(elem_type)
             # TODO: Decide if we need to wrap the type in a std::vector. Issue is that the element type is internal
@@ -56,15 +56,15 @@
             # For now, return the element type (which will be the Tensor element type, or a map<A,B>) as
             # an operator input or output will either be a sequence or a not, so we don't need to disambiguate
             # between the two (i.e. we know if the returned value refers to the contents of a sequence, and can
             # handle whether it's the element type of a Tensor in the sequence, or the map type in a sequence of maps
             # due to this).
             type_str = elem_type_str
         else:
-            raise ValueError("Unknown or missing value type of {}".format(value_type))
+            raise ValueError(f"Unknown or missing value type of {value_type}")
 
         return type_str
 
 
 def get_typeinfo(name: str, value_name_to_typeinfo: dict) -> fbs.TypeInfo:
     "Lookup a name in a dictionary mapping value name to TypeInfo."
     if name not in value_name_to_typeinfo:
```

## onnxruntime/tools/ort_format_model/utils.py

```diff
@@ -43,15 +43,15 @@
         for model_file in sorted(model_files):
             out.write(f"# - {model_file}\n")
 
         for domain in sorted(required_ops.keys()):
             for opset in sorted(required_ops[domain].keys()):
                 ops = required_ops[domain][opset]
                 if ops:
-                    out.write("{};{};".format(domain, opset))
+                    out.write(f"{domain};{opset};")
                     if enable_type_reduction:
                         # type string is empty if op hasn't been seen
                         entries = [
                             "{}{}".format(op, op_type_processors.get_config_entry(domain, op) or "")
                             for op in sorted(ops)
                         ]
                     else:
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py

```diff
@@ -1,6 +1,7 @@
 from os.path import dirname, basename, isfile, join, splitext
 import glob
+
 modules = glob.glob(join(dirname(__file__), "*.py"))
-__all__ = [splitext(basename(f))[0] for f in modules if isfile(f) and not f.endswith('__init__.py')]
+__all__ = [splitext(basename(f))[0] for f in modules if isfile(f) and not f.endswith("__init__.py")]
 
-from . import *  # noqa
+from . import *
```

## onnxruntime/tools/qdq_helpers/optimize_qdq_model.py

```diff
@@ -4,34 +4,34 @@
 
 import argparse
 import os
 import pathlib
 
 import onnx
 
-from .qdq_model_utils import fix_dq_nodes_with_multiple_consumers
-
 
 def optimize_qdq_model():
     parser = argparse.ArgumentParser(
         os.path.basename(__file__),
-        description="""
-                                     Update a QDQ format ONNX model to ensure optimal performance when executed using
-                                     ONNX Runtime.
-                                     """,
+        description="Update a QDQ format ONNX model to ensure optimal performance when executed using ONNX Runtime.",
     )
 
     parser.add_argument("input_model", type=pathlib.Path, help="Provide path to ONNX model to update.")
     parser.add_argument("output_model", type=pathlib.Path, help="Provide path to write updated ONNX model to.")
 
     args = parser.parse_args()
 
     model = onnx.load(str(args.input_model.resolve(strict=True)))
 
-    # there's just one utility to run currently but we expect that will grow
-    fix_dq_nodes_with_multiple_consumers(model)
+    # run QDQ model optimizations here
+
+    # Originally, the fixing up of DQ nodes with multiple consumers was implemented as one such optimization.
+    # That was moved to an ORT graph transformer.
+    print("As of ORT 1.15, the fixing up of DQ nodes with multiple consumers is done by an ORT graph transformer.")
+
+    # There are no optimizations being run currently but we expect that there may be in the future.
 
     onnx.save(model, str(args.output_model.resolve()))
 
 
 if __name__ == "__main__":
     optimize_qdq_model()
```

## onnxruntime/transformers/__init__.py

```diff
@@ -4,13 +4,13 @@
 # --------------------------------------------------------------------------
 
 import os
 import sys
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
 
-import convert_to_onnx
+import convert_to_onnx  # noqa: E402, F401
 
 # added for backward compatible
-import gpt2_helper
+import gpt2_helper  # noqa: E402, F401
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "models", "t5"))
```

## onnxruntime/transformers/benchmark.py

```diff
@@ -41,24 +41,24 @@
 """
 
 import argparse
 import logging
 import os
 import timeit
 from datetime import datetime
-from enum import Enum
+from enum import Enum  # noqa: F401
 
 import numpy
-import onnx
+import onnx  # noqa: F401
 import psutil
+from benchmark_helper import allocateOutputBuffers  # noqa: F401
 from benchmark_helper import (
     ConfigModifier,
     OptimizerInfo,
     Precision,
-    allocateOutputBuffers,
     create_onnxruntime_session,
     get_latency_result,
     inference_ort,
     inference_ort_with_io_binding,
     output_details,
     output_fusion_statistics,
     output_summary,
@@ -72,24 +72,24 @@
     load_pretrained_model,
 )
 from packaging import version
 from quantize_helper import QuantizeHelper
 
 logger = logging.getLogger("")
 
-from huggingface_models import MODEL_CLASSES, MODELS
+from huggingface_models import MODEL_CLASSES, MODELS  # noqa: E402
 
 cpu_count = psutil.cpu_count(logical=False)
 
 # Set OMP environment variable before importing onnxruntime or torch.
 if "OMP_NUM_THREADS" not in os.environ:
     os.environ["OMP_NUM_THREADS"] = str(cpu_count)
 
-import torch
-from transformers import AutoConfig, AutoModel, AutoTokenizer, GPT2Model, LxmertConfig
+import torch  # noqa: E402
+from transformers import AutoConfig, AutoModel, AutoTokenizer, GPT2Model, LxmertConfig  # noqa: E402, F401
 
 
 def run_onnxruntime(
     use_gpu,
     provider,
     model_names,
     model_class,
@@ -174,15 +174,20 @@
                         validate_onnx,
                         use_raw_attention_mask,
                         overwrite,
                         model_fusion_statistics,
                         fusion_options,
                     )
             if "tf" in model_source:
-                (onnx_model_file, is_valid_onnx_model, vocab_size, max_sequence_length,) = export_onnx_model_from_tf(
+                (
+                    onnx_model_file,
+                    is_valid_onnx_model,
+                    vocab_size,
+                    max_sequence_length,
+                ) = export_onnx_model_from_tf(
                     model_name,
                     MODELS[model_name][1],
                     MODELS[model_name][2],
                     MODELS[model_name][3],
                     model_class,
                     config_modifier,
                     cache_dir,
@@ -253,17 +258,20 @@
                         "threads": num_threads,
                         "batch_size": batch_size,
                         "sequence_length": sequence_length,
                         "custom_layer_num": config_modifier.get_layer_num(),
                         "datetime": str(datetime.now()),
                     }
 
-                    logger.info(
-                        "Run onnxruntime on {} with input shape {}".format(model_name, [batch_size, sequence_length])
-                    )
+                    if config.model_type in ["vit", "swin"]:
+                        logger.info(
+                            f"Run onnxruntime on {model_name} with input shape {[batch_size, 3, config.image_size, config.image_size]}"
+                        )
+                    else:
+                        logger.info(f"Run onnxruntime on {model_name} with input shape {[batch_size, sequence_length]}")
 
                     if disable_ort_io_binding:
                         result = inference_ort(
                             ort_session,
                             ort_inputs,
                             result_template,
                             repeat_times,
@@ -329,19 +337,24 @@
         config_modifier.modify(config)
         model = load_pretrained_model(
             model_name,
             config=config,
             cache_dir=cache_dir,
             custom_model_class=model_class,
         )
-        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
 
-        max_input_size = (
-            tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
-        )
+        if config.model_type in ["vit", "swin"]:
+            # These models don't use sequence lengths, so just pick the first sequence length so that the summary still works
+            sequence_lengths = [sequence_lengths[0]]
+        else:
+            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
+
+            max_input_size = (
+                tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+            )
 
         logger.debug(f"Model {model}")
         logger.debug(f"Number of parameters {model.num_parameters()}")
 
         if precision == Precision.FLOAT16:
             model.half()
 
@@ -352,32 +365,42 @@
             model = QuantizeHelper.quantize_torch_model(model)
 
         for batch_size in batch_sizes:
             if batch_size <= 0:
                 continue
 
             for sequence_length in sequence_lengths:
-                if max_input_size is not None and sequence_length > max_input_size:
-                    continue
+                if config.model_type in ["vit", "swin"]:
+                    logger.info(
+                        f"Run PyTorch on {model_name} with input shape {[batch_size, 3, config.image_size, config.image_size]}"
+                    )
+                    input_ids = torch.randn(
+                        size=(batch_size, 3, config.image_size, config.image_size),
+                        dtype=torch.float16 if precision == Precision.FLOAT16 else torch.float32,
+                        device=device,
+                    )
+                else:
+                    if max_input_size is not None and sequence_length > max_input_size:
+                        continue
 
-                logger.info("Run PyTorch on {} with input shape {}".format(model_name, [batch_size, sequence_length]))
-                input_ids = torch.randint(
-                    low=0,
-                    high=config.vocab_size - 1,
-                    size=(batch_size, sequence_length),
-                    dtype=torch.long,
-                    device=device,
-                )
+                    logger.info(f"Run PyTorch on {model_name} with input shape {[batch_size, sequence_length]}")
+                    input_ids = torch.randint(
+                        low=0,
+                        high=config.vocab_size - 1,
+                        size=(batch_size, sequence_length),
+                        dtype=torch.long,
+                        device=device,
+                    )
                 try:
                     inference = (
                         torch.jit.trace(model, input_ids) if torchscript else torch.compile(model) if torch2 else model
                     )
                     inference(input_ids)
 
-                    runtimes = timeit.repeat(lambda: inference(input_ids), repeat=repeat_times, number=1)
+                    runtimes = timeit.repeat(lambda: inference(input_ids), repeat=repeat_times, number=1)  # noqa: B023
 
                     result = {
                         "engine": "torchscript" if torchscript else "torch2" if torch2 else "torch",
                         "version": torch.__version__,
                         "providers": "NA",
                         "device": "cuda" if use_gpu else "cpu",
                         "optimizer": "",
@@ -487,54 +510,52 @@
             if batch_size <= 0:
                 continue
 
             for sequence_length in sequence_lengths:
                 if max_input_size is not None and sequence_length > max_input_size:
                     continue
 
-                logger.info(
-                    "Run Tensorflow on {} with input shape {}".format(model_name, [batch_size, sequence_length])
-                )
+                logger.info(f"Run Tensorflow on {model_name} with input shape {[batch_size, sequence_length]}")
 
                 import random
 
                 rng = random.Random()
                 values = [rng.randint(0, config.vocab_size - 1) for i in range(batch_size * sequence_length)]
                 input_ids = tf.constant(values, shape=(batch_size, sequence_length), dtype=tf.int32)
 
                 try:
                     # Disable both for better inference perf
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def encoder_forward():
-                        return model(input_ids, training=False)
+                        return model(input_ids, training=False)  # noqa: B023
 
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def encoder_decoder_forward():
-                        return model(input_ids, decoder_input_ids=input_ids, training=False)
+                        return model(input_ids, decoder_input_ids=input_ids, training=False)  # noqa: B023
 
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def lxmert_forward():
-                        feats = tf.random.normal([1, 1, config.visual_feat_dim])
-                        pos = tf.random.normal([1, 1, config.visual_pos_dim])
-                        return model(
-                            input_ids,
+                        feats = tf.random.normal([1, 1, config.visual_feat_dim])  # noqa: B023
+                        pos = tf.random.normal([1, 1, config.visual_pos_dim])  # noqa: B023
+                        return model(  # noqa: B023
+                            input_ids,  # noqa: B023
                             visual_feats=feats,
                             visual_pos=pos,
                             training=False,
                         )
 
                     inference = encoder_forward
                     if config.is_encoder_decoder:
                         inference = encoder_decoder_forward
                     elif isinstance(config, LxmertConfig):
                         inference = lxmert_forward
 
                     inference()
 
-                    runtimes = timeit.repeat(lambda: inference(), repeat=repeat_times, number=1)
+                    runtimes = timeit.repeat(lambda: inference(), repeat=repeat_times, number=1)  # noqa: B023
 
                     result = {
                         "engine": "tensorflow",
                         "version": tf.__version__,
                         "providers": "NA",
                         "device": "cuda" if use_gpu else "cpu",
                         "optimizer": "",
@@ -762,15 +783,18 @@
         logger.error("fp16 is for GPU only")
         return
 
     if args.precision == Precision.INT8 and args.use_gpu:
         logger.error("int8 is for CPU only")
         return
 
-    args.num_threads = sorted(set(cpu_count if x <= 0 else x for x in args.num_threads))
+    if len(args.models) == 1 and MODELS[args.models[0]][3] in ["vit", "swim"]:
+        args.sequence_lengths = [""]
+
+    args.num_threads = sorted({cpu_count if x <= 0 else x for x in args.num_threads})
 
     logger.info(f"Arguments: {args}")
 
     if not os.path.exists(args.cache_dir):
         try:
             os.mkdir(args.cache_dir)
         except OSError:
@@ -887,16 +911,16 @@
                     args.overwrite,
                     args.disable_ort_io_binding,
                     use_raw_attention_mask,
                     model_fusion_statistics,
                     args.model_source,
                     args,
                 )
-            except:
-                logger.error(f"Exception", exc_info=True)
+            except Exception:
+                logger.error("Exception", exc_info=True)
 
     time_stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
     if model_fusion_statistics:
         csv_filename = args.fusion_csv or f"benchmark_fusion_{time_stamp}.csv"
         output_fusion_statistics(model_fusion_statistics, csv_filename)
 
     if len(results) == 0:
```

## onnxruntime/transformers/benchmark_helper.py

```diff
@@ -77,15 +77,15 @@
     onnx_model_path,
     use_gpu,
     provider=None,
     enable_all_optimization=True,
     num_threads=-1,
     enable_profiling=False,
     verbose=False,
-    provider_options={},  # map execution provider name to its option
+    provider_options={},  # map execution provider name to its option  # noqa: B006
 ):
     session = None
     try:
         sess_options = onnxruntime.SessionOptions()
 
         if enable_all_optimization:
             sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
@@ -129,15 +129,15 @@
         else:
             providers = ["CPUExecutionProvider"]
 
         if provider_options:
             providers = [(name, provider_options[name]) if name in provider_options else name for name in providers]
 
         session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=providers)
-    except:
+    except Exception:
         logger.error("Exception", exc_info=True)
 
     return session
 
 
 def setup_logger(verbose=True):
     if verbose:
@@ -181,20 +181,20 @@
 def get_latency_result(latency_list, batch_size):
     latency_ms = sum(latency_list) / float(len(latency_list)) * 1000.0
     latency_variance = numpy.var(latency_list, dtype=numpy.float64) * 1000.0
     throughput = batch_size * (1000.0 / latency_ms)
 
     return {
         "test_times": len(latency_list),
-        "latency_variance": "{:.2f}".format(latency_variance),
-        "latency_90_percentile": "{:.2f}".format(numpy.percentile(latency_list, 90) * 1000.0),
-        "latency_95_percentile": "{:.2f}".format(numpy.percentile(latency_list, 95) * 1000.0),
-        "latency_99_percentile": "{:.2f}".format(numpy.percentile(latency_list, 99) * 1000.0),
-        "average_latency_ms": "{:.2f}".format(latency_ms),
-        "QPS": "{:.2f}".format(throughput),
+        "latency_variance": f"{latency_variance:.2f}",
+        "latency_90_percentile": f"{numpy.percentile(latency_list, 90) * 1000.0:.2f}",
+        "latency_95_percentile": f"{numpy.percentile(latency_list, 95) * 1000.0:.2f}",
+        "latency_99_percentile": f"{numpy.percentile(latency_list, 99) * 1000.0:.2f}",
+        "average_latency_ms": f"{latency_ms:.2f}",
+        "QPS": f"{throughput:.2f}",
     }
 
 
 def output_details(results, csv_filename):
     with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
         column_names = [
             "engine",
@@ -241,16 +241,19 @@
             "precision",
             "optimizer",
             "io_binding",
             "threads",
         ]
         data_names = []
         for batch_size in args.batch_sizes:
-            for sequence_length in args.sequence_lengths:
-                data_names.append(f"b{batch_size}_s{sequence_length}")
+            if args.sequence_lengths == [""]:
+                data_names.append(f"b{batch_size}")
+            else:
+                for sequence_length in args.sequence_lengths:
+                    data_names.append(f"b{batch_size}_s{sequence_length}")
 
         csv_writer = csv.DictWriter(csv_file, fieldnames=header_names + data_names)
         csv_writer.writeheader()
         for model_name in args.models:
             for input_count in [1, 2, 3]:
                 for engine_name in args.engines:
                     for io_binding in [True, False, ""]:
@@ -269,29 +272,36 @@
                                         row.update(headers)
                                         row.update({k: "" for k in data_names})
                                     else:
                                         for k in header_names:
                                             assert row[k] == headers[k]
                                     b = result["batch_size"]
                                     s = result["sequence_length"]
-                                    row[f"b{b}_s{s}"] = result["average_latency_ms"]
+                                    if s:
+                                        row[f"b{b}_s{s}"] = result["average_latency_ms"]
+                                    else:
+                                        row[f"b{b}"] = result["average_latency_ms"]
                             if row:
                                 csv_writer.writerow(row)
 
     logger.info(f"Summary results are saved to csv file: {csv_filename}")
 
 
 def output_fusion_statistics(model_fusion_statistics, csv_filename):
     with open(csv_filename, mode="a", newline="", encoding="ascii") as csv_file:
-        column_names = ["model_filename", "datetime", "transformers", "torch"] + list(
-            next(iter(model_fusion_statistics.values())).keys()
-        )
+        column_names = [
+            "model_filename",
+            "datetime",
+            "transformers",
+            "torch",
+            *list(next(iter(model_fusion_statistics.values())).keys()),
+        ]
         csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
         csv_writer.writeheader()
-        for key in model_fusion_statistics.keys():
+        for key in model_fusion_statistics:
             model_fusion_statistics[key]["datetime"] = str(datetime.now())
             model_fusion_statistics[key]["transformers"] = transformers.__version__
             model_fusion_statistics[key]["torch"] = torch.__version__
             model_fusion_statistics[key]["model_filename"] = key
             csv_writer.writerow(model_fusion_statistics[key])
     logger.info(f"Fusion statistics is saved to csv file: {csv_filename}")
 
@@ -321,15 +331,15 @@
     warm_up_repeat=0,
 ):
     result = {}
 
     # Bind inputs and outputs to onnxruntime session
     io_binding = ort_session.io_binding()
     # Bind inputs to device
-    for name in ort_inputs.keys():
+    for name in ort_inputs:
         np_input = torch.from_numpy(ort_inputs[name]).to(device)
         input_type = (
             IO_BINDING_DATA_TYPE_MAP[str(ort_inputs[name].dtype)]
             if str(ort_inputs[name].dtype) in IO_BINDING_DATA_TYPE_MAP
             else data_type
         )
         io_binding.bind_input(
@@ -367,15 +377,15 @@
     )
     result.update(result_template)
     result.update({"io_binding": True})
     result.update(get_latency_result(latency_list, batch_size))
     return result
 
 
-def allocateOutputBuffers(output_buffers, output_buffer_max_sizes, device):
+def allocateOutputBuffers(output_buffers, output_buffer_max_sizes, device):  # noqa: N802
     # Allocate output tensors with the largest test size needed. So the allocated memory can be reused
     # for each test run.
 
     for i in output_buffer_max_sizes:
         output_buffers.append(torch.empty(i, dtype=torch.float32, device=device))
 
 
@@ -539,14 +549,15 @@
         return max_usage - memory_before_test
 
 
 def get_ort_environment_variables():
     # Environment variables might impact ORT performance on transformer models. Note that they are for testing only.
     env_names = [
         "ORT_DISABLE_FUSED_ATTENTION",
+        "ORT_ENABLE_FUSED_CAUSAL_ATTENTION",
         "ORT_DISABLE_FUSED_CROSS_ATTENTION",
         "ORT_DISABLE_TRT_FLASH_ATTENTION",
         "ORT_DISABLE_MEMORY_EFFICIENT_ATTENTION",
         "ORT_TRANSFORMER_OPTIONS",
         "ORT_CUDA_GEMM_OPTIONS",
     ]
     env = ""
```

## onnxruntime/transformers/bert_perf_test.py

```diff
@@ -10,22 +10,24 @@
 # See get_bert_inputs function in bert_test_data.py for more information.
 
 # Example command to run test on batch_size 1 and 2 for a model on GPU:
 #   python bert_perf_test.py --model bert.onnx --batch_size 1 2 --sequence_length 128 --use_gpu --samples 1000 --test_times 1
 
 import argparse
 import csv
+import json
 import multiprocessing
 import os
 import random
 import statistics
 import timeit
 from dataclasses import dataclass
 from datetime import datetime
 from pathlib import Path
+from typing import Optional
 
 import numpy as np
 import psutil
 import torch
 from bert_test_data import generate_test_data, get_bert_inputs
 
 
@@ -47,17 +49,27 @@
 @dataclass
 class ModelSetting:
     model_path: str
     input_ids_name: str
     segment_ids_name: str
     input_mask_name: str
     opt_level: int
+    input_tuning_results: Optional[str]
+    output_tuning_results: Optional[str]
 
 
-def create_session(model_path, use_gpu, provider, intra_op_num_threads, graph_optimization_level=None, log_severity=2):
+def create_session(
+    model_path,
+    use_gpu,
+    provider,
+    intra_op_num_threads,
+    graph_optimization_level=None,
+    log_severity=2,
+    tuning_results_path=None,
+):
     import onnxruntime
 
     onnxruntime.set_default_logger_severity(log_severity)
 
     if use_gpu and ("CUDAExecutionProvider" not in onnxruntime.get_available_providers()):
         print(
             "Warning: Please install onnxruntime-gpu package instead of onnxruntime, and use a machine with GPU for testing gpu performance."
@@ -123,14 +135,18 @@
             assert "TensorrtExecutionProvider" in session.get_providers()
             assert "CUDAExecutionProvider" in session.get_providers()
         else:
             assert "CUDAExecutionProvider" in session.get_providers()
     else:
         assert "CPUExecutionProvider" in session.get_providers()
 
+    if tuning_results_path is not None:
+        with open(tuning_results_path) as f:
+            session.set_tuning_results(json.load(f))
+
     return session
 
 
 def numpy_type(torch_type):
     type_map = {
         torch.float32: np.float32,
         torch.float16: np.float16,
@@ -169,15 +185,15 @@
     return io_binding
 
 
 def onnxruntime_inference_with_io_binding(session, all_inputs, output_names, test_setting):
     results = []
     latency_list = []
     device = "cuda" if test_setting.use_gpu else "cpu"
-    for test_case_id, inputs in enumerate(all_inputs):
+    for _test_case_id, inputs in enumerate(all_inputs):
         result = session.run(output_names, inputs)
         results.append(result)
         outputs = {}
         for i in range(len(output_names)):
             outputs[output_names[i]] = result[i]
 
         input_tensors, output_tensors = create_input_output_tensors(inputs, outputs, device)
@@ -197,26 +213,26 @@
 def onnxruntime_inference(session, all_inputs, output_names):
     if len(all_inputs) > 0:
         # Use a random input as warm up.
         session.run(output_names, random.choice(all_inputs))
 
     results = []
     latency_list = []
-    for test_case_id, inputs in enumerate(all_inputs):
+    for _test_case_id, inputs in enumerate(all_inputs):
         start_time = timeit.default_timer()
         result = session.run(output_names, inputs)
         latency = timeit.default_timer() - start_time
         results.append(result)
         latency_list.append(latency)
     return results, latency_list
 
 
 def to_string(model_path, session, test_setting):
     sess_options = session.get_session_options()
-    option = "model={},".format(os.path.basename(model_path))
+    option = f"model={os.path.basename(model_path)},"
     option += "graph_optimization_level={},intra_op_num_threads={},".format(
         sess_options.graph_optimization_level, sess_options.intra_op_num_threads
     ).replace("GraphOptimizationLevel.ORT_", "")
     option += f"batch_size={test_setting.batch_size},sequence_length={test_setting.sequence_length},test_cases={test_setting.test_cases},test_times={test_setting.test_times},use_gpu={test_setting.use_gpu}"
     return option
 
 
@@ -224,33 +240,34 @@
     session = create_session(
         model_setting.model_path,
         test_setting.use_gpu,
         test_setting.provider,
         intra_op_num_threads,
         model_setting.opt_level,
         log_severity=test_setting.log_severity,
+        tuning_results_path=model_setting.input_tuning_results,
     )
     output_names = [output.name for output in session.get_outputs()]
 
     key = to_string(model_setting.model_path, session, test_setting)
     if key in perf_results:
         print("skip duplicated test:", key)
         return
 
     print("Running test:", key)
 
     all_latency_list = []
     if test_setting.use_io_binding:
-        for i in range(test_setting.test_times):
+        for _i in range(test_setting.test_times):
             results, latency_list = onnxruntime_inference_with_io_binding(
                 session, all_inputs, output_names, test_setting
             )
             all_latency_list.extend(latency_list)
     else:
-        for i in range(test_setting.test_times):
+        for _i in range(test_setting.test_times):
             results, latency_list = onnxruntime_inference(session, all_inputs, output_names)
             all_latency_list.extend(latency_list)
 
     # latency in miliseconds
     latency_ms = np.array(all_latency_list) * 1000
 
     average_latency = statistics.mean(latency_ms)
@@ -271,14 +288,26 @@
         throughput,
     )
 
     print(
         "Average latency = {} ms, Throughput = {} QPS".format(format(average_latency, ".2f"), format(throughput, ".2f"))
     )
 
+    if model_setting.output_tuning_results:
+        output_path = os.path.abspath(model_setting.output_tuning_results)
+        if os.path.exists(output_path):
+            old_output_path = output_path
+            output_path = f"""{output_path.rsplit(".json", 1)[0]}.{datetime.now().timestamp()}.json"""
+            print("WARNING:", old_output_path, "exists, will write to", output_path, "instead.")
+
+        trs = session.get_tuning_results()
+        with open(output_path, "w") as f:
+            json.dump(trs, f)
+        print("Tuning results is saved to", output_path)
+
 
 def launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads):
     process = multiprocessing.Process(
         target=run_one_test,
         args=(
             model_setting,
             test_setting,
@@ -301,15 +330,15 @@
             test_setting.intra_op_num_threads,
         )
         return
 
     cpu_count = psutil.cpu_count(logical=False)
     logical_cores = psutil.cpu_count(logical=True)
 
-    candidate_threads = list(set([logical_cores, cpu_count]))
+    candidate_threads = list({logical_cores, cpu_count})
     for i in range(1, min(16, logical_cores)):
         if i not in candidate_threads:
             candidate_threads.append(i)
     candidate_threads.sort(reverse=True)
 
     for intra_op_num_threads in candidate_threads:
         launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads)
@@ -455,14 +484,27 @@
         "--input_mask_name",
         required=False,
         type=str,
         default=None,
         help="input name for attention mask",
     )
 
+    parser.add_argument(
+        "--input_tuning_results",
+        default=None,
+        type=str,
+        help="tuning results (json) to be loaded before benchmark",
+    )
+    parser.add_argument(
+        "--output_tuning_results",
+        default=None,
+        type=str,
+        help="tuning results (json) to be saved after benchmark",
+    )
+
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_arguments()
 
@@ -478,14 +520,16 @@
 
     model_setting = ModelSetting(
         args.model,
         args.input_ids_name,
         args.segment_ids_name,
         args.input_mask_name,
         args.opt_level,
+        args.input_tuning_results,
+        args.output_tuning_results,
     )
 
     for batch_size in batch_size_set:
         test_setting = TestSetting(
             batch_size,
             args.sequence_length,
             args.samples,
@@ -513,15 +557,15 @@
             args.sequence_length,
             datetime.now().strftime("%Y%m%d-%H%M%S"),
         ),
     )
     with open(summary_file, "w+", newline="") as tsv_file:
         tsv_writer = csv.writer(tsv_file, delimiter="\t", lineterminator="\n")
         headers = None
-        for (key, perf_result) in sorted_results:
+        for key, perf_result in sorted_results:
             params = key.split(",")
             if headers is None:
                 headers = [
                     "Latency(ms)",
                     "Latency_P50",
                     "Latency_P75",
                     "Latency_P90",
```

## onnxruntime/transformers/bert_test_data.py

```diff
@@ -130,15 +130,15 @@
             print("Successfully created the directory %s " % directory)
     else:
         print("Warning: directory %s existed. Files will be overwritten." % directory)
 
     index = 0
     for name, data in inputs.items():
         tensor = numpy_helper.from_array(data, name)
-        with open(os.path.join(directory, "input_{}.pb".format(index)), "wb") as file:
+        with open(os.path.join(directory, f"input_{index}.pb"), "wb") as file:
             file.write(tensor.SerializeToString())
         index += 1
 
 
 def fake_test_data(
     batch_size: int,
     sequence_length: int,
@@ -171,15 +171,15 @@
     """
     assert input_ids is not None
 
     np.random.seed(random_seed)
     random.seed(random_seed)
 
     all_inputs = []
-    for test_case in range(test_cases):
+    for _test_case in range(test_cases):
         input_1 = fake_input_ids_data(input_ids, batch_size, sequence_length, dictionary_size)
         inputs = {input_ids.name: input_1}
 
         if segment_ids:
             inputs[segment_ids.name] = fake_segment_ids_data(segment_ids, batch_size, sequence_length)
 
         if input_mask:
@@ -298,30 +298,30 @@
         expected_inputs = 1 + (1 if segment_ids else 0) + (1 if input_mask else 0)
         if len(graph_inputs) != expected_inputs:
             raise ValueError(f"Expect the graph to have {expected_inputs} inputs. Got {len(graph_inputs)}")
 
         return input_ids, segment_ids, input_mask
 
     if len(graph_inputs) != 3:
-        raise ValueError("Expect the graph to have 3 inputs. Got {}".format(len(graph_inputs)))
+        raise ValueError(f"Expect the graph to have 3 inputs. Got {len(graph_inputs)}")
 
     embed_nodes = onnx_model.get_nodes_by_op_type("EmbedLayerNormalization")
     if len(embed_nodes) == 1:
         embed_node = embed_nodes[0]
         input_ids = get_graph_input_from_embed_node(onnx_model, embed_node, 0)
         segment_ids = get_graph_input_from_embed_node(onnx_model, embed_node, 1)
         input_mask = get_graph_input_from_embed_node(onnx_model, embed_node, 7)
 
         if input_mask is None:
             for input in graph_inputs:
                 input_name_lower = input.name.lower()
                 if "mask" in input_name_lower:
                     input_mask = input
         if input_mask is None:
-            raise ValueError(f"Failed to find attention mask input")
+            raise ValueError("Failed to find attention mask input")
 
         return input_ids, segment_ids, input_mask
 
     # Try guess the inputs based on naming.
     input_ids = None
     segment_ids = None
     input_mask = None
@@ -498,28 +498,28 @@
 
     session = onnxruntime.InferenceSession(model)
     output_names = [output.name for output in session.get_outputs()]
 
     for i, inputs in enumerate(all_inputs):
         directory = os.path.join(output_dir, "test_data_set_" + str(i))
         result = session.run(output_names, inputs)
-        for i, output_name in enumerate(output_names):
+        for i, output_name in enumerate(output_names):  # noqa: PLW2901
             tensor_result = numpy_helper.from_array(np.asarray(result[i]), output_name)
-            with open(os.path.join(directory, "output_{}.pb".format(i)), "wb") as file:
+            with open(os.path.join(directory, f"output_{i}.pb"), "wb") as file:
                 file.write(tensor_result.SerializeToString())
 
 
 def main():
     args = parse_arguments()
 
     output_dir = args.output_dir
     if output_dir is None:
         # Default output directory is a sub-directory under the directory of model.
         p = Path(args.model)
-        output_dir = os.path.join(p.parent, "batch_{}_seq_{}".format(args.batch_size, args.sequence_length))
+        output_dir = os.path.join(p.parent, f"batch_{args.batch_size}_seq_{args.sequence_length}")
 
     if output_dir is not None:
         # create the output directory if not existed
         path = Path(output_dir)
         path.mkdir(parents=True, exist_ok=True)
     else:
         print("Directory existed. test data files will be overwritten.")
```

## onnxruntime/transformers/compare_bert_results.py

```diff
@@ -48,34 +48,34 @@
             max_abs_diff = max(max_abs_diff, abs_diff)
             if not np.allclose(results[i].tolist(), treatment_output.tolist(), rtol=rtol, atol=atol):
                 if case_passed:
                     case_passed = False
                     diff_count += 1
 
                     if verbose:
-                        print("case {} output {}".format(test_case_id, i))
-                        print("baseline={}\ntreatment={}".format(results[i].tolist(), treatment_output))
-                        print("rel_diff={} abs_diff={}".format(rel_diff, abs_diff))
+                        print(f"case {test_case_id} output {i}")
+                        print(f"baseline={results[i].tolist()}\ntreatment={treatment_output}")
+                        print(f"rel_diff={rel_diff} abs_diff={abs_diff}")
 
     if diff_count == 0:
         print(
             "100% passed for {} random inputs given thresholds (rtol={}, atol={}).".format(
                 len(baseline_results), rtol, atol
             )
         )
     else:
         print(
             "WARNING: {} out of {} results NOT passed for thresholds (rtol={}, atol={}).".format(
                 diff_count, len(baseline_results), rtol, atol
             )
         )
 
-    print("maximum absolute difference={}".format(max_abs_diff))
+    print(f"maximum absolute difference={max_abs_diff}")
 
-    print("maximum relative difference={}".format(max_rel_diff))
+    print(f"maximum relative difference={max_rel_diff}")
 
 
 def run_test(
     baseline_model,
     optimized_model,
     output_dir,
     batch_size,
@@ -86,15 +86,14 @@
     verbose,
     rtol,
     atol,
     input_ids_name,
     segment_ids_name,
     input_mask_name,
 ):
-
     # Try deduce input names from optimized model.
     input_ids, segment_ids, input_mask = get_bert_inputs(
         optimized_model, input_ids_name, segment_ids_name, input_mask_name
     )
 
     # Use random mask length for accuracy test. It might introduce slight inflation in latency reported in this script.
     all_inputs = generate_test_data(
@@ -123,15 +122,15 @@
         for i, inputs in enumerate(all_inputs):
             output_test_data(output_dir, i, inputs)
 
     treatment_results, treatment_latency, treatment_output_names = run_model(
         optimized_model, all_inputs, use_gpu, disable_optimization=False
     )
     if verbose:
-        print("treatment average latency: {} ms".format(statistics.mean(treatment_latency) * 1000))
+        print(f"treatment average latency: {statistics.mean(treatment_latency) * 1000} ms")
 
     # Validate the output of baseline and treatment, to make sure the results are similar.
     compare(baseline_results, treatment_results, verbose, rtol, atol)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
```

## onnxruntime/transformers/convert_generation.py

```diff
@@ -4,33 +4,44 @@
 # -------------------------------------------------------------------------
 """
 This converts GPT2 or T5 model to onnx with beam search operator.
 
 Example 1: convert gpt2 model with beam search:
     python convert_generation.py -m gpt2 --output gpt2_beam_search.onnx
 
-Example 2: convert T5 model with beam search in two steps:
+Example 2: convert gpt2 model with beam search containing specific cuda optimizations:
+    python convert_generation.py -m gpt2 --output gpt2_beam_search.onnx --use_gpu               \
+        --past_present_share_buffer --use_decoder_masked_attention
+
+Example 3: convert gpt2 model with beam search with mixed precision and enable SkipLayerNorm strict mode:
+    python convert_generation.py -m gpt2 --output gpt2_beam_search.onnx --use_gpu -p fp16 --use_sln_strict_mode
+
+Example 4: convert T5 model with beam search in two steps:
     cd ./models/t5
     python convert_to_onnx.py -m t5-small
     cd ../..
-    python convert_generation.py -m t5-small --model_type t5                                   \
+    python convert_generation.py -m t5-small --model_type t5                                    \
         --decoder_onnx ./models/t5/onnx_models/t5-small_decoder.onnx                            \
         --encoder_decoder_init_onnx ./models/t5/onnx_models/t5-small_encoder_decoder_init.onnx  \
         --output ./models/t5/onnx_models/t5_small_beam_search.onnx
 
-Example 3: convert T5 model with beam search. All in one step:
+Example 5: convert T5 model with beam search. All in one step:
     python convert_generation.py -m t5-small --model_type t5 --output ./models/t5/onnx_models/t5_small_beam_search.onnx
 
-Example 4: convert MT5 model with external data file like mt5-base-beamsearch.onnx.data in below example.
+Example 6: convert T5 model with beam search containing specific cuda optimizations. All in one step:
+    python convert_generation.py -m t5-small --model_type t5 --output ./models/t5/onnx_models/t5_small_beam_search.onnx   \
+        --use_gpu --past_present_share_buffer --use_decoder_masked_attention
+
+Example 7: convert MT5 model with external data file like mt5-base-beamsearch.onnx.data in below example.
     python convert_generation.py -m google/mt5-base --model_type mt5 --output mt5-base-beamsearch.onnx -e
 
-Example 5: convert gpt2 model with greedy search:
+Example 8: convert gpt2 model with greedy search:
     python convert_generation.py -m gpt2 --output gpt2_greedy_search.onnx --num_beams 1 --num_return_sequences 1
 
-Example 6: convert gpt2 model with sampling:
+Example 9: convert gpt2 model with sampling:
     python convert_generation.py -m gpt2 --output gpt2_sampling.onnx --num_beams 1 --num_return_sequences 1 --top_p 0.6
 """
 
 import argparse
 import logging
 import math
 import os
@@ -60,18 +71,18 @@
 from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_available_providers
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
 from gpt2_helper import PRETRAINED_GPT2_MODELS  # noqa: E402
 from models.gpt2.convert_to_onnx import main as convert_gpt2_to_onnx  # noqa: E402
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "models", "t5"))
-from benchmark_helper import setup_logger
+from benchmark_helper import setup_logger  # noqa: E402
 from models.t5.convert_to_onnx import export_onnx_models as export_t5_onnx_models  # noqa: E402
 from models.t5.t5_helper import PRETRAINED_MT5_MODELS, PRETRAINED_T5_MODELS  # noqa: E402
-from onnx_model import OnnxModel
+from onnx_model import OnnxModel  # noqa: E402
 
 logger = logging.getLogger("")
 
 
 class GenerationType(Enum):
     BEAMSEARCH = "beam_search"
     GREEDYSEARCH = "greedy_search"
@@ -160,14 +171,25 @@
         type=Precision,
         default=Precision.FLOAT32,
         choices=[Precision.FLOAT32, Precision.FLOAT16],
         help="Precision of model to run. fp32 for full precision, fp16 for half or mixed precision",
     )
 
     output_group.add_argument(
+        "-b",
+        "--op_block_list",
+        required=False,
+        nargs="*",
+        default=["auto"],
+        help="Disable certain onnx operators when exporting model to onnx format. When using default"
+        'value for gpt2 type of model fp16 precision, it will be set to ["Add", "LayerNormalization",'
+        ' "SkipLayerNormalization", "FastGelu"]. Other situation, it will be set to []',
+    )
+
+    output_group.add_argument(
         "-e",
         "--use_external_data_format",
         required=False,
         action="store_true",
         help="save external data for model > 2G",
     )
     output_group.set_defaults(use_external_data_format=False)
@@ -178,33 +200,36 @@
     output_group.set_defaults(run_shape_inference=False)
 
     output_group.add_argument(
         "-dpvs",
         "--disable_pad_vocab_size",
         required=False,
         action="store_true",
-        help="Do not pad logits MatMul weight to be a multiple of 8 along the dimension where dim value is the vocab size. The logits MatMul may hence be of poor performance for fp16 precision.",
+        help="Do not pad logits MatMul weight to be a multiple of 8 along the dimension where dim value is"
+        " the vocab size. The logits MatMul may hence be of poor performance for fp16 precision.",
     )
     output_group.set_defaults(disable_pad_vocab_size=False)
 
     output_group.add_argument(
         "-dsgd",
         "--disable_separate_gpt2_decoder_for_init_run",
         required=False,
         action="store_true",
-        help="Do not create separate decoder subgraphs for initial and remaining runs. This does not allow for optimizations based on sequence lengths in each subgraph",
+        help="Do not create separate decoder subgraphs for initial and remaining runs. This does not allow "
+        "for optimizations based on sequence lengths in each subgraph",
     )
     output_group.set_defaults(disable_separate_gpt2_decoder_for_init_run=False)
 
     output_group.add_argument(
         "-i",
         "--disable_shared_initializers",
         required=False,
         action="store_true",
-        help="do not share initializers in encoder and decoder for T5 or in the init decoder and decoder for GPT2. It will increase memory usage of t5/mt5/gpt2 models.",
+        help="do not share initializers in encoder and decoder for T5 or in the init decoder and decoder for "
+        "GPT2. It will increase memory usage of t5/mt5/gpt2 models.",
     )
     output_group.set_defaults(disable_shared_initializers=False)
 
     model_group = parser.add_argument_group("Beam search parameters that stored in the output model")
 
     model_group.add_argument(
         "--output_sequences_scores",
@@ -246,14 +271,23 @@
         required=False,
         action="store_true",
         help="Use shared buffer for past and present, currently work for gpt2 greedy/sampling search.",
     )
     model_group.set_defaults(past_present_share_buffer=False)
 
     model_group.add_argument(
+        "--use_decoder_masked_attention",
+        required=False,
+        action="store_true",
+        help="Uses `DecoderMaskedSelfAttention` or `DecoderMaskedMultiHeadAttention` to optimize the decoding Attention computation. "
+        "Must be used with `past_present_share_buffer`. Currently, only Attention head sizes of 32, 64 and 128 are supported.",
+    )
+    model_group.set_defaults(use_decoder_masked_attention=False)
+
+    model_group.add_argument(
         "--prefix_vocab_mask",
         required=False,
         action="store_true",
         help="Enable prefix_vocab_mask. This mask can be used to filter bad words in the first generated token only",
     )
     model_group.set_defaults(prefix_vocab_mask=False)
 
@@ -386,27 +420,43 @@
         default=-1,
         help="custom pad_token_id for generating model with existing onnx encoder/decoder",
     )
 
     test_group = parser.add_argument_group("Other options for testing parity and performance")
 
     test_group.add_argument(
+        "--use_sln_strict_mode",
+        required=False,
+        action="store_true",
+        help="Enable strict mode for SLN in CUDA provider. This ensures a better accuracy but will be slower.",
+    )
+    test_group.set_defaults(use_sln_strict_mode=False)
+
+    test_group.add_argument(
         "--use_gpu", required=False, action="store_true", help="use GPU for inference. Required for fp16."
     )
     test_group.set_defaults(use_gpu=False)
 
     test_group.add_argument(
         "--disable_parity",
         required=False,
         action="store_true",
         help="do not run parity test",
     )
     test_group.set_defaults(disable_parity=False)
 
     test_group.add_argument(
+        "--disable_perf_test",
+        required=False,
+        action="store_true",
+        help="do not run perf test",
+    )
+    test_group.set_defaults(disable_perf_test=False)
+
+    test_group.add_argument(
         "--torch_performance",
         required=False,
         action="store_true",
         help="test PyTorch performance",
     )
     test_group.set_defaults(torch_performance=False)
 
@@ -449,25 +499,30 @@
         "fp32" if args.precision == Precision.FLOAT32 else "fp16",
         "--test_runs",
         "1",
         "--test_cases",
         "10",
         "--overwrite",  # Overwrite onnx file if existed
     ]
+    if args.cache_dir:
+        arguments.extend(["--cache_dir", args.cache_dir])
     if args.use_gpu:
         arguments.append("--use_gpu")
     if args.use_external_data_format:
         arguments.append("--use_external_data_format")
 
+    if len(args.op_block_list):
+        arguments.extend(["--op_block_list"])
+        arguments.extend(args.op_block_list)
+
     if args.precision == Precision.FLOAT16:
         assert args.use_gpu, "fp16 or mixed precision model cannot run in CPU. Please add --use_gpu"
         # TODO(tianleiwu): Use auto mixed precision for fp16 conversion: arguments.append('--auto_mixed_precision')
         #       Need change cuda kernel to support a combination of fp32 logits and fp16 past state.
         #       Currently logits and past state shall be same data type.
-        arguments.extend(["--op_block_list", "Add", "LayerNormalization", "SkipLayerNormalization", "FastGelu"])
 
     if args.verbose:
         logger.info(f"arguments for convert_to_onnx:{arguments}")
 
     convert_gpt2_to_onnx(argv=arguments)
 
 
@@ -479,15 +534,15 @@
     """
     paths = export_t5_onnx_models(
         args.model_name_or_path,
         args.cache_dir,
         Path(args.output).parent,
         use_gpu=args.use_gpu,
         use_external_data_format=args.use_external_data_format,
-        optimize_onnx=False,
+        optimize_onnx=(args.precision != Precision.FLOAT16),
         precision=args.precision,
         verbose=False,
         use_decoder_start_token=False,
         merge_encoder_and_decoder_init=True,
         overwrite=True,
         disable_auto_mixed_precision=False,
         use_int32_inputs=True,
@@ -592,20 +647,21 @@
         return False
 
     # Save the model
     OnnxModel.save(decoder_model_proto, onnx_path, save_as_external_data=use_external_data_format)
     return True
 
 
-def create_ort_session(model_path: str, use_gpu: bool) -> InferenceSession:
+def create_ort_session(model_path: str, use_gpu: bool, use_sln_strict_mode: bool) -> InferenceSession:
     """Create OnnxRuntime session.
 
     Args:
         model_path (str): onnx model path
         use_gpu (bool): use GPU or not
+        use_sln_strict_mode (bool): use strict mode for skip layer normalization or not
 
     Raises:
         RuntimeError: CUDAExecutionProvider is not available when --use_gpu is specified.
 
     Returns:
         onnxruntime.InferenceSession: The created session.
     """
@@ -613,14 +669,20 @@
     sess_options.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL
     execution_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"] if use_gpu else ["CPUExecutionProvider"]
     if use_gpu:
         if "CUDAExecutionProvider" not in get_available_providers():
             raise RuntimeError("CUDAExecutionProvider is not available for --use_gpu!")
         else:
             logger.info("use CUDAExecutionProvider")
+        if use_sln_strict_mode:
+            cuda_provider_options = {"enable_skip_layer_norm_strict_mode": True}
+            provider_options = {"CUDAExecutionProvider": cuda_provider_options}
+            execution_providers = [
+                (name, provider_options[name]) if name in provider_options else name for name in execution_providers
+            ]
 
     ort_session = InferenceSession(model_path, sess_options, providers=execution_providers)
     return ort_session
 
 
 def verify_gpt2_subgraph(graph: onnx.GraphProto, precision: Precision):
     """Verify GPT-2 subgraph
@@ -633,15 +695,15 @@
         ValueError: Number of inputs not expected.
         ValueError: Input name is not expected.
         ValueError: Input data type is not expected.
         ValueError: Number of outputs not expected.
         ValueError: Output name is not expected.
         ValueError: Output data type is not expected.
     """
-    is_float16 = Precision.FLOAT16 == precision
+    is_float16 = precision == Precision.FLOAT16
 
     input_count = len(graph.input)
     layer_count = input_count - 3
     assert layer_count >= 1
 
     expected_inputs = ["input_ids", "position_ids", "attention_mask"] + [f"past_{i}" for i in range(layer_count)]
     if len(graph.input) != len(expected_inputs):
@@ -689,36 +751,35 @@
         ValueError: Number of inputs not expected.
         ValueError: Input name is not expected.
         ValueError: Input data type is not expected.
         ValueError: Number of outputs not expected.
         ValueError: Output name is not expected.
         ValueError: Output data type is not expected.
     """
-    is_float16 = Precision.FLOAT16 == precision
+    is_float16 = precision == Precision.FLOAT16
     float_type = TensorProto.FLOAT16 if is_float16 else TensorProto.FLOAT
 
     input_count = len(graph.input)
-    layer_count = (input_count - 3) // 4
+    layer_count = (input_count - 2) // 4
     assert layer_count >= 1
 
     # Expect inputs:
     #   input_ids: int32 (B, 1)
     #   encoder_attention_mask: int32 (B, encode_sequence_length)
-    #   encoder_hidden_states: (B, encode_sequence_length, encoder_hidden_size)
 
     #   past_key_self_0: (B, num_heads, past_decode_sequence_length, head_size)
     #   past_value_self_0: (B, num_heads, past_decode_sequence_length, head_size)
     #   ... (for each self attention layer)
 
     #   past_key_cross_0: (B, num_heads, encode_sequence_length, head_size)
     #   past_value_cross_0: (B, num_heads, encode_sequence_length, head_size)
     #   ... (for each cross attention layer)
 
     # TODO: encoder_hidden_states is optional
-    expected_inputs = ["input_ids", "encoder_attention_mask", "encoder_hidden_states"]
+    expected_inputs = ["input_ids", "encoder_attention_mask"]
     for i in range(layer_count):
         expected_inputs.append(f"past_key_self_{i}")
         expected_inputs.append(f"past_value_self_{i}")
     for i in range(layer_count):
         expected_inputs.append(f"past_key_cross_{i}")
         expected_inputs.append(f"past_value_cross_{i}")
 
@@ -766,15 +827,15 @@
         ValueError: Number of inputs not expected.
         ValueError: Input name is not expected.
         ValueError: Input data type is not expected.
         ValueError: Number of outputs not expected.
         ValueError: Output name is not expected.
         ValueError: Output data type is not expected.
     """
-    is_float16 = Precision.FLOAT16 == precision
+    is_float16 = precision == Precision.FLOAT16
     layer_count = (len(graph.output) - 2) // 4
     assert layer_count >= 1
 
     # Expect 3 inputs:
     #   encoder_input_ids:      int32 (B, encode_sequence_length)
     #   encoder_attention_mask: int32 (B, encode_sequence_length)
     #   decoder_input_ids:      int32 (B, 1)
@@ -970,15 +1031,15 @@
 def _attribute_to_pair(attribute):
     """
     Convert attribute to kwarg format for use with onnx.helper.make_node.
         :parameter attribute: attribute in AttributeProto format.
         :return: attribute in {key: value} format.
     """
     if attribute.type == 0:
-        raise ValueError("attribute {} does not have type specified.".format(attribute.name))
+        raise ValueError(f"attribute {attribute.name} does not have type specified.")
 
     # Based on attribute type definitions from AttributeProto
     # definition in https://github.com/onnx/onnx/blob/master/onnx/onnx.proto
     if attribute.type == 1:
         value = attribute.f
     elif attribute.type == 2:
         value = attribute.i
@@ -995,15 +1056,15 @@
     elif attribute.type == 8:
         value = attribute.strings
     elif attribute.type == 9:
         value = attribute.tensors
     elif attribute.type == 10:
         value = attribute.graphs
     else:
-        raise ValueError("attribute {} has unsupported type {}.".format(attribute.name, attribute.type))
+        raise ValueError(f"attribute {attribute.name} has unsupported type {attribute.type}.")
 
     return (attribute.name, value)
 
 
 def kwargs_of(node):
     kwargs = {}
     for attr in node.attribute:
@@ -1014,36 +1075,36 @@
     return kwargs
 
 
 def shape_of(vi):
     return tuple([d.dim_param if (d.dim_param) else d.dim_value for d in vi.type.tensor_type.shape.dim])
 
 
-def update_decoder_subgraph_past_present_share_buffer(subg):
+def update_decoder_subgraph_past_present_share_buffer(subg: GraphProto):
     input_past_0 = 3
     output_past_0 = 1
     new_inputs = []
     for i, vi in enumerate(subg.input):
         if i >= input_past_0:
             shape = shape_of(vi)
-            vi = onnx.helper.make_tensor_value_info(
+            vi = onnx.helper.make_tensor_value_info(  # noqa: PLW2901
                 vi.name,
                 elem_type=vi.type.tensor_type.elem_type,
                 shape=[shape[0], shape[1], shape[2], "max_seq_len", shape[4]],
             )
         new_inputs.extend([vi])
     new_inputs.extend([onnx.helper.make_tensor_value_info("past_sequence_length", onnx.TensorProto.INT32, shape=[1])])
     subg.ClearField("input")
     subg.input.extend(new_inputs)
 
     new_outputs = []
     for i, vi in enumerate(subg.output):
         if i >= output_past_0:
             shape = shape_of(vi)
-            vi = onnx.helper.make_tensor_value_info(
+            vi = onnx.helper.make_tensor_value_info(  # noqa: PLW2901
                 vi.name,
                 elem_type=vi.type.tensor_type.elem_type,
                 shape=[shape[0], shape[1], shape[2], "max_seq_len", shape[4]],
             )
         new_outputs.extend([vi])
     subg.ClearField("output")
     subg.output.extend(new_outputs)
@@ -1055,21 +1116,224 @@
             kwargs.update({"past_present_share_buffer": 1})
             nis = []
             nis.extend(node.input)
             while len(nis) < 6:
                 nis.extend([""])
             if len(nis) < 7:
                 nis.extend(["past_sequence_length"])
-            node = onnx.helper.make_node("Attention", nis, node.output, name=node.name, **kwargs)
+            node = onnx.helper.make_node("Attention", nis, node.output, name=node.name, **kwargs)  # noqa: PLW2901
         new_nodes.extend([node])
     subg.ClearField("node")
     subg.node.extend(new_nodes)
     return subg
 
 
+def update_decoder_subgraph_use_decoder_masked_attention(
+    subg: GraphProto, is_beam_search: bool, switch_attention: bool
+) -> bool:
+    """Update the Attention nodes to DecoderMaskedSelfAttention.
+
+    Args:
+        subg (GraphProto): GraphProto of the decoder subgraph
+        is_beam_search (bool): Boolean specifying if the sampling algo is BeamSearch
+        switch_attention (bool): Boolean specifying if `Attention` is to be switched with `DecoderMaskedSelfAttention`
+    """
+    if is_beam_search:
+        new_inputs = []
+        for _i, vi in enumerate(subg.input):
+            new_inputs.extend([vi])
+
+        # Add 2 BeamSearch specific inputs
+        new_inputs.extend([onnx.helper.make_tensor_value_info("beam_width", onnx.TensorProto.INT32, shape=[1])])
+        new_inputs.extend(
+            [
+                onnx.helper.make_tensor_value_info(
+                    "cache_indirection", onnx.TensorProto.INT32, shape=["batch_size", "beam_width", "max_seq_len"]
+                )
+            ]
+        )
+        subg.ClearField("input")
+        subg.input.extend(new_inputs)
+
+    if switch_attention:
+        decoder_masked_attention_supported_attr = [
+            "past_present_share_buffer",
+            "num_heads",
+            "scale",
+            "mask_filter_value",
+            "domain",
+        ]
+
+        new_nodes = []
+        for node in subg.node:
+            if node.op_type == "Attention":
+                kwargs = kwargs_of(node)
+                for k in kwargs.copy():
+                    # The Attention operator does not support different qkv hidden sizes when past/present
+                    # input/output exists (GPT2 model). Hence, we should never run into this.
+                    # But, if we do, do not go ahead with the optimization.
+                    if k == "qkv_hidden_sizes":
+                        return False
+
+                    if k not in decoder_masked_attention_supported_attr:
+                        # Log the fact that we are removing certain attributes from the node
+                        # We don't need to log it for "unidirectional" as we are aware that
+                        # decoding attention kernels are unidirectional by definition.
+                        if k != "unidirectional":
+                            logger.warning(
+                                f"Removing attribute: {k} from Attention node while switching to DecoderMaskedSelfAttention"
+                            )
+
+                        del kwargs[k]
+
+                nis = []
+                nis.extend(node.input)
+
+                # Add 2 BeamSearch specific inputs
+                if is_beam_search:
+                    while len(nis) < 7:
+                        nis.extend([""])
+                    if len(nis) < 8:
+                        nis.extend(["beam_width"])
+                    if len(nis) < 9:
+                        nis.extend(["cache_indirection"])
+
+                node = onnx.helper.make_node(  # noqa: PLW2901
+                    "DecoderMaskedSelfAttention", nis, node.output, name=node.name, **kwargs
+                )
+            new_nodes.extend([node])
+        subg.ClearField("node")
+        subg.node.extend(new_nodes)
+
+    return True
+
+
+def update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha(subg: GraphProto):
+    input_self_past_0 = 2
+    output_self_past_0 = 1
+
+    num_layers = int((len(subg.input) - input_self_past_0) / 4)
+    input_cross_past_0 = 2 * num_layers + input_self_past_0
+
+    new_nodes = []
+    old_nodes = []
+    for node in subg.node:
+        if node.op_type == "MultiHeadAttention":
+            old_nodes.extend([node])
+
+    # If not all the MultiheadAttention nodes are fused, this optimization is not applicable
+    if len(old_nodes) < num_layers:
+        return False
+
+    # Redirect the RelativePositionBias node's input from past_key_self_0.shape[2] to past_sequence_length.
+    # There is only one RelativePositionBias node in T5 decoder subgraph.
+    rel_pos_bias_node = None
+    for node in subg.node:
+        if node.op_type == "RelativePositionBias":
+            rel_pos_bias_node = node
+            break
+
+    if rel_pos_bias_node is None:
+        return False
+
+    decoder_masked_attention_supported_attr = [
+        "past_present_share_buffer",
+        "num_heads",
+        "scale",
+        "mask_filter_value",
+        "domain",
+    ]
+
+    for node in subg.node:
+        if len(node.output) > 0 and node.output[0] == rel_pos_bias_node.input[1]:
+            cast_node = onnx.helper.make_node(
+                "Cast",
+                ["past_sequence_length"],
+                ["past_sequence_length_int64"],
+                name="past_sequence_length_cast",
+                to=TensorProto.INT64,
+            )
+            node.input[1] = cast_node.output[0]
+            new_nodes.extend([cast_node])
+
+        if node.op_type == "MultiHeadAttention":
+            kwargs = kwargs_of(node)
+            for k in kwargs.copy():
+                if k not in decoder_masked_attention_supported_attr:
+                    del kwargs[k]
+
+            # note: This logic only apply to T5 model where there is no bias in Attention node.
+            nis = [
+                node.input[0],  # query
+                node.input[1],  # key
+                node.input[2],  # value
+                node.input[4],  # 2D mask
+                node.input[5],  # relative_position_bias
+            ]
+
+            if len(node.input) > 6:
+                nis.extend([node.input[6]])  # past_key
+                nis.extend([node.input[7]])  # past_value
+            else:
+                nis.extend([""])  # past_key
+                nis.extend([""])  # past_value
+
+            nis.extend(["past_sequence_length"])  # past_sequence_length
+            nis.extend(["beam_width"])  # beam_width
+            nis.extend(["cache_indirection"])  # cache_indirection
+
+            kwargs["past_present_share_buffer"] = 1
+
+            node = onnx.helper.make_node(  # noqa: PLW2901
+                "DecoderMaskedMultiHeadAttention", nis, node.output, name=node.name, **kwargs
+            )
+
+        new_nodes.extend([node])
+
+    subg.ClearField("node")
+    subg.node.extend(new_nodes)
+
+    new_inputs = []
+    for i, vi in enumerate(subg.input):
+        if i >= input_self_past_0 and i < input_cross_past_0:
+            shape = shape_of(vi)
+            vi = onnx.helper.make_tensor_value_info(  # noqa: PLW2901
+                vi.name,
+                elem_type=vi.type.tensor_type.elem_type,
+                shape=[shape[0], shape[1], "max_seq_len", shape[3]],
+            )
+        new_inputs.extend([vi])
+    new_inputs.extend([onnx.helper.make_tensor_value_info("past_sequence_length", onnx.TensorProto.INT32, shape=[1])])
+    new_inputs.extend([onnx.helper.make_tensor_value_info("beam_width", onnx.TensorProto.INT32, shape=[1])])
+    new_inputs.extend(
+        [
+            onnx.helper.make_tensor_value_info(
+                "cache_indirection", onnx.TensorProto.INT32, shape=["batch_size", "beam_width", "max_seq_len"]
+            )
+        ]
+    )
+    subg.ClearField("input")
+    subg.input.extend(new_inputs)
+
+    new_outputs = []
+    for i, vi in enumerate(subg.output):
+        if i >= output_self_past_0:
+            shape = shape_of(vi)
+            vi = onnx.helper.make_tensor_value_info(  # noqa: PLW2901
+                vi.name,
+                elem_type=vi.type.tensor_type.elem_type,
+                shape=[shape[0], shape[1], "max_seq_len", shape[3]],
+            )
+        new_outputs.extend([vi])
+    subg.ClearField("output")
+    subg.output.extend(new_outputs)
+
+    return True
+
+
 def update_input_shapes_for_gpt2_decoder_model(decoder_onnx_path: str, use_external_data_format: bool = True):
     """Update the input shapes for the inputs "input_ids" and "position_ids" and make the sequence length dim value 1 for each of them.
        The decoder model will be over-written.
 
     Args:
         decoder_onnx_path (str): Path of GPT-2 decoder onnx model
         use_external_data_format(bool): output tensors to external data or not.
@@ -1161,15 +1425,14 @@
                 "SkipLayerNormalization",
             ],
             [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
         )
 
     # Try without the Casts before and after the MatMuls
     if logits_matmul_to_residual_add_path is None:
-
         # Normalization Node is : LayerNormalization
         logits_matmul_to_residual_add_path = gpt2_init_decoder_model.match_parent_path(
             logits_matmul_node,
             ["LayerNormalization", "Add", "Add", "MatMul", "FastGelu", "MatMul", "LayerNormalization", "Add"],
             [0, 0, 1, 0, 0, 0, 0, 0],
         )
 
@@ -1190,15 +1453,15 @@
     # TODO(hasesh): Are there more permutations to try before returning ?
     if logits_matmul_to_residual_add_path is None:
         return False
 
     residual_add_node = logits_matmul_to_residual_add_path[-1]
 
     # If the last node in the pattern is SkipLayerNormalization, we need to adjust our pattern searches accordingly
-    is_skiplayernorm_path = True if residual_add_node.op_type == "SkipLayerNormalization" else False
+    is_skiplayernorm_path = residual_add_node.op_type == "SkipLayerNormalization"
 
     # Regular LayerNormalization path
     if not is_skiplayernorm_path:
         residual_add_to_attention_parent_index = 0
         residual_add_to_attention_path = gpt2_init_decoder_model.match_parent_path(
             residual_add_node, ["Add", "Cast", "MatMul", "Attention"], [residual_add_to_attention_parent_index, 0, 0, 0]
         )
@@ -1359,42 +1622,105 @@
     gpt2_init_decoder_model.topological_sort()
 
     # Save the init decoder model
     OnnxModel.save(init_decoder_model_proto, init_decoder_onnx_path, save_as_external_data=use_external_data_format)
     return True
 
 
+def make_dim_proto_numeric_t5(model, config):
+    """Make dim_proto numeric.
+
+    Args:
+        model: T5 encoder and decoder model.
+        config: T5 config.
+    """
+    sequence_length = str(1)
+    num_heads = str(config.num_heads)
+    hidden_size = str(config.d_model)
+    head_size = str(config.d_kv)
+
+    for tensor in model.graph.output:
+        for dim_proto in tensor.type.tensor_type.shape.dim:
+            if dim_proto.HasField("dim_param") and dim_proto.dim_param in [
+                sequence_length,
+                num_heads,
+                hidden_size,
+                head_size,
+            ]:
+                dim_value = int(dim_proto.dim_param)
+                dim_proto.Clear()
+                dim_proto.dim_value = dim_value
+
+    for tensor in model.graph.input:
+        for dim_proto in tensor.type.tensor_type.shape.dim:
+            if dim_proto.HasField("dim_param") and dim_proto.dim_param in [
+                sequence_length,
+                num_heads,
+                hidden_size,
+                head_size,
+            ]:
+                dim_value = int(dim_proto.dim_param)
+                dim_proto.Clear()
+                dim_proto.dim_value = dim_value
+
+
 def convert_generation_model(args: argparse.Namespace, generation_type: GenerationType = GenerationType.BEAMSEARCH):
     """Convert model according to command line arguments.
 
     Args:
         args (argparse.Namespace): arguments parsed from command line
     """
     is_gpt2: bool = args.model_type == "gpt2"
     is_beamsearch: bool = generation_type == GenerationType.BEAMSEARCH
     is_greedysearch: bool = generation_type == GenerationType.GREEDYSEARCH
     is_sampling: bool = generation_type == GenerationType.SAMPLING
-    past_present_share_buffer: bool = args.past_present_share_buffer and (is_greedysearch or is_sampling)
+    past_present_share_buffer: bool = args.past_present_share_buffer
 
-    logger.info(f"**** past_present_share_buffer={past_present_share_buffer}, is_greedysearch={is_greedysearch}")
+    logger.info(f"**** past_present_share_buffer={past_present_share_buffer}")
+    if len(args.op_block_list) == 1 and args.op_block_list[0] == "auto":
+        if is_gpt2 and args.precision == Precision.FLOAT16:
+            args.op_block_list = ["Add", "LayerNormalization", "SkipLayerNormalization", "FastGelu"]
+            logger.info(f"**** Setting op_block_list to {args.op_block_list}")
+            logger.info("**** use --op_block_list if you want to override the block operator list.")
+        else:
+            args.op_block_list = []
 
     if is_greedysearch or is_sampling:
         if not is_gpt2:
             raise NotImplementedError("Currently only gpt2 with greedy search/sampling is supported")
         if args.output_sequences_scores:
             raise NotImplementedError("output_sequences_scores currently is not supported in greedy search/sampling")
         if args.output_token_scores:
             raise NotImplementedError("output_token_scores currently is not supported in greedy search/sampling")
 
+    # For BeamSearch, sharing buffers for past and present states is only supported
+    # when using `use_decoder_masked_attention`
+    if past_present_share_buffer and is_beamsearch and not args.use_decoder_masked_attention:
+        raise ValueError(
+            "`use_decoder_masked_attention` MUST be turned on to use `past_present_share_buffer` in case of BeamSearch"
+        )
+
+    # For any kind of sampling, using decoder masked multihead attention is only supported
+    # when using `past_present_share_buffer`
+    if args.use_decoder_masked_attention and not past_present_share_buffer:
+        raise ValueError("`past_present_share_buffer` MUST be turned on to use `use_decoder_masked_attention`")
+
+    # For any kind of sampling, using decoder masked multihead attention is only supported
+    # on GPUs
+    if args.use_decoder_masked_attention and not args.use_gpu:
+        raise ValueError("`use_decoder_masked_attention` option is only supported on GPUs")
+
     if is_gpt2:
         if args.decoder_onnx and os.path.exists(args.decoder_onnx):
             logger.info(f"skip convert_to_onnx since path existed: {args.decoder_onnx}")
         else:
             if not args.decoder_onnx:
-                onnx_filename = "gpt2_past_{}.onnx".format("fp16" if args.precision == Precision.FLOAT16 else "fp32")
+                onnx_filename = "{}_past_{}.onnx".format(
+                    args.model_name_or_path, "fp16" if args.precision == Precision.FLOAT16 else "fp32"
+                )
                 args.decoder_onnx = Path(Path(args.output).parent, onnx_filename).as_posix()
 
             logger.info(f"Convert GPT model {args.model_name_or_path} to onnx {args.decoder_onnx} ...")
             gpt2_to_onnx(args)
     else:  # t5 or mt5
         if args.decoder_onnx and args.encoder_decoder_init_onnx:
             logger.info(
@@ -1419,15 +1745,15 @@
         logger.info(
             f"Pad logits MatMul weights for optimal MatMul perf in fp16 on {args.decoder_onnx}. "
             "The file will be overwritten."
         )
         logits_matmul_weight_padded = pad_weights_of_logits_matmul(args.decoder_onnx, args.use_external_data_format)
         if not logits_matmul_weight_padded:
             logger.warning(
-                "Tried and failed to pad logits MatMul weights. " "Performance may be sub-optimal for this MatMul"
+                "Tried and failed to pad logits MatMul weights. Performance may be sub-optimal for this MatMul"
             )
 
     gpt2_init_decoder_generated = False
     gpt2_init_decoder_onnx_path = None
     if (
         not args.disable_separate_gpt2_decoder_for_init_run
         and is_gpt2
@@ -1453,15 +1779,15 @@
 
         # Update the graph input shapes for the non-initial decoder model to account
         # for the fact that the sequence length will always be 1
         if gpt2_init_decoder_generated and not update_input_shapes_for_gpt2_decoder_model(
             args.decoder_onnx, args.use_external_data_format
         ):
             # Can't proceed further - better to raise an exception
-            raise ValueError(f"Could not update the input shapes for the non-initial decoder subgraph.")
+            raise ValueError("Could not update the input shapes for the non-initial decoder subgraph.")
 
     # If the user explicitly requests running shape inference or if we padded/mutated
     # weight(s)/input shape(s) in the decoder, we want to run shape inference to capture the new
     # shapes
     if logits_matmul_weight_padded or args.run_shape_inference or gpt2_init_decoder_generated:
         logger.info(f"Run symbolic shape inference on {args.decoder_onnx}. The file will be overwritten.")
         shape_inference(args.decoder_onnx, args.use_external_data_format)
@@ -1616,14 +1942,15 @@
     # Explicitly pass in the vocab size via an attribute
     if logits_matmul_weight_padded:
         attr_to_extend.extend([onnx.helper.make_attribute("vocab_size", vocab_size)])
 
     node.attribute.extend(attr_to_extend)
 
     initializers = []
+
     if args.model_type in ["t5", "mt5"]:
         if args.run_shape_inference:
             logger.info(f"Symbolic shape inference on {args.encoder_decoder_init_onnx}. The file will be overwritten.")
             shape_inference(args.encoder_decoder_init_onnx, args.use_external_data_format)
         encoder_model = onnx.load_model(args.encoder_decoder_init_onnx, load_external_data=True)
         encoder_model.graph.name = f"{args.model_type} encoder and decoder init"
         verify_t5_encoder_decoder_init_subgraph(encoder_model.graph, args.precision)
@@ -1639,14 +1966,30 @@
             # Move initializer from subgraph to main graph could reduce memory usage in inference.
             # moved_initializers = move_initializers(encoder_model.graph)
             # logger.info(
             #     f"{len(moved_initializers)} initializers ({[i.name for i in moved_initializers]}) from the encoder are moved to the main graph"
             # )
             # initializers.extend(moved_initializers)
 
+        make_dim_proto_numeric_t5(encoder_model, config)
+        make_dim_proto_numeric_t5(decoder_model, config)
+
+        # Update decoder subgraph in preparation to use past present share buffer
+        if past_present_share_buffer:
+            if not args.use_decoder_masked_attention:
+                raise ValueError("past_present_share_buffer is only supported with use_decoder_masked_attention")
+
+            logger.info(
+                "*****update t5 decoder subgraph to share past/present buffer and use decoder_masked_multihead_attention*****"
+            )
+            if update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha(decoder_model.graph):
+                logger.info("*****update t5 decoder subgraph successfully!!!*****")
+            else:
+                logger.info("*****DecoderMaskedMultiHeadAttention is not applied to T5 decoder*****")
+
         node.attribute.extend(
             [
                 onnx.helper.make_attribute("encoder", encoder_model.graph),
                 onnx.helper.make_attribute("decoder", decoder_model.graph),
                 onnx.helper.make_attribute(
                     "decoder_start_token_id",
                     config.decoder_start_token_id if len(encoder_model.graph.input) == 3 else -1,
@@ -1659,26 +2002,46 @@
             # graph and remove them from these models
             if not args.disable_shared_initializers:
                 # Unique shared initializers from the decoder and decoder_init could reduce memory usage in inference.
                 initializers = get_shared_initializers(gpt2_init_decoder_model, decoder_model)
                 logger.info(
                     f"{len(initializers)} shared initializers ({[i.name for i in initializers]}) in decoder and init decoder subgraphs are moved to the main graph"
                 )
+
+            # Update init decoder subgraph in preparation to use past present share buffer
             if past_present_share_buffer:
                 logger.info("*****update init decoder subgraph to make past and present share buffer******************")
                 update_decoder_subgraph_past_present_share_buffer(gpt2_init_decoder_model.graph)
+
+            # Update init decoder subgraph in preparation to use DecoderMaskedSelfAttention
+            # NOTE: Even if we will not use DecoderMaskedSelfAttention in the init decoder subgraph
+            # it makes the runtime changes cleaner if we keep both the init decoder and decoder subgraphs
+            # same in terms of the subgraph inputs.
+            if args.use_decoder_masked_attention and not update_decoder_subgraph_use_decoder_masked_attention(
+                gpt2_init_decoder_model.graph, is_beamsearch, False
+            ):
+                raise ValueError("Could not update the init decoder subgraph to use DecoderMaskedSelfAttention")
+
             node.attribute.append(onnx.helper.make_attribute("init_decoder", gpt2_init_decoder_model.graph))
         else:
             # Move initializer from subgraph to main graph could reduce memory usage in inference.
             initializers = move_initializers(decoder_model.graph)
             logger.info(f"{len(initializers)} initializers from the decoder are moved to the main graph")
 
+        # Update decoder subgraph in preparation to use past present share buffer
         if past_present_share_buffer:
             logger.info("*****update decoder subgraph to make past and present share buffer******************")
             update_decoder_subgraph_past_present_share_buffer(decoder_model.graph)
+
+        # Update decoder subgraph in preparation to use DecoderMaskedSelfAttention
+        if args.use_decoder_masked_attention and not update_decoder_subgraph_use_decoder_masked_attention(
+            decoder_model.graph, is_beamsearch, True
+        ):
+            raise ValueError("Could not update the decoder subgraph to use DecoderMaskedSelfAttention")
+
         node.attribute.append(onnx.helper.make_attribute("decoder", decoder_model.graph))
 
     # graph inputs
     input_ids = onnx.helper.make_tensor_value_info("input_ids", TensorProto.INT32, ["batch_size", "sequence_length"])
     max_length = onnx.helper.make_tensor_value_info("max_length", TensorProto.INT32, [1])
     min_length = onnx.helper.make_tensor_value_info("min_length", TensorProto.INT32, [1])
     num_beams = onnx.helper.make_tensor_value_info("num_beams", TensorProto.INT32, [1])
@@ -1846,15 +2209,15 @@
             early_stopping=args.early_stopping,
             no_repeat_ngram_size=args.no_repeat_ngram_size,
             eos_token_id=eos_token_id,
             pad_token_id=pad_token_id,
             num_return_sequences=args.num_return_sequences,
             length_penalty=args.length_penalty,
             repetition_penalty=args.repetition_penalty,
-            bad_words_ids=bad_words_ids,
+            bad_words_ids=bad_words_ids if bad_words_ids else None,
             return_dict_in_generate=True,
             output_scores=args.output_sequences_scores or args.output_token_scores,
         )
         torch_latency.append(time.time() - start)
     batch_size = input_ids.shape[0]
     from benchmark_helper import get_latency_result
 
@@ -1907,15 +2270,15 @@
     input_ids = inputs["input_ids"]
     attention_mask = inputs["attention_mask"]
 
     bad_words = "walk in park"
     bad_words_ids = tokenizer.encode(bad_words, add_prefix_space=True)
     bad_words_ids = [[word_id] for word_id in bad_words_ids]  # Convert to list of list
     if args.vocab_mask:
-        logger.debug("bad_words_ids", bad_words_ids)
+        logger.debug("bad_words_ids", bad_words_ids)  # noqa: PLE1205
     else:
         bad_words_ids = []
 
     config = model.config
     eos_token_id = config.eos_token_id
     pad_token_id = config.eos_token_id
     vocab_size = config.vocab_size
@@ -1953,16 +2316,14 @@
             decoded_sequence = tokenizer.decode(sequence, skip_special_tokens=True)
             torch_decoded_sequences.append(decoded_sequence)
             print(f"{i}: {decoded_sequence}")
 
     print("-" * 50)
     print("Testing beam search with onnxruntime...")
 
-    ort_session = create_ort_session(args.output, args.use_gpu)
-
     if is_greedy:
         inputs = {
             "input_ids": input_ids.cpu().numpy().astype(np.int32),
             "max_length": np.array([args.max_length], dtype=np.int32),
             "min_length": np.array([args.min_length], dtype=np.int32),
             "repetition_penalty": np.array([args.repetition_penalty], dtype=np.float32),
         }
@@ -1989,27 +2350,37 @@
 
     batch_size = input_ids.shape[0]
     if args.prefix_vocab_mask:
         logger.info("Use prefix vocab mask with all ones in ORT, but no corresponding setting for Torch model.")
         prefix_vocab_mask = np.ones((batch_size, vocab_size), dtype=np.int32)
         inputs["prefix_vocab_mask"] = prefix_vocab_mask
 
-    logger.debug("ORT inputs", inputs)
-    result = ort_session.run(None, inputs)
-
     if args.save_test_data:
         test_data_dir = Path(args.output).parent.as_posix()
-        logger.debug("test_data_dir", test_data_dir)
+        logger.debug("test_data_dir", test_data_dir)  # noqa: PLE1205
         from bert_test_data import output_test_data
 
+        logger.info(f"Saving test_data to {test_data_dir}/test_data_set_* ...")
+
         all_inputs = [inputs]
         for i, inputs in enumerate(all_inputs):
             dir = os.path.join(test_data_dir, "test_data_set_" + str(i))
             output_test_data(dir, inputs)
 
+    logger.debug("ORT inputs", inputs)  # noqa: PLE1205
+
+    if args.disable_perf_test:
+        return
+
+    logger.debug("Creating ort session......")
+    ort_session = create_ort_session(args.output, args.use_gpu, args.use_sln_strict_mode)
+
+    logger.debug("Run ort session......")
+    result = ort_session.run(None, inputs)
+
     # Test performance
     latency = []
     for _ in range(args.total_runs):
         start = time.time()
         _ = ort_session.run(None, inputs)
         latency.append(time.time() - start)
 
@@ -2106,29 +2477,28 @@
             cache_dir=args.cache_dir,
         )
 
     # Use different length sentences to test batching
     if sentences is None:
         sentences = [
             "translate English to French: The product is released",
-            "summarize: research continues to show that pets bring real health benefits to their owners."
-            + "Having a dog around can lead to lower levels of stress for both adults and kids.",
+            "summarize: research continues to show that pets bring real health benefits to their owners. Having a dog around can lead to lower levels of stress for both adults and kids.",
             # "summarize: I enjoy walking in the park. It makes my mind feel calm and refreshed. "
             # + "I enjoy looking at the trees, flowers, and wildlife around me, and listening to sound from natural.",
         ]
 
     inputs = tokenizer(sentences, return_tensors="pt", padding=True)
     input_ids = inputs["input_ids"]
     attention_mask = inputs["attention_mask"]
 
     bad_words = "walk in park"
     bad_words_ids = tokenizer.encode(bad_words)[:-1]  # exclude the last token (EOS)
     bad_words_ids = [[word_id] for word_id in bad_words_ids]  # Convert to list of list
     if args.vocab_mask:
-        logger.debug("bad_words_ids", bad_words_ids)
+        logger.debug("bad_words_ids", bad_words_ids)  # noqa: PLE1205
     else:
         bad_words_ids = []
 
     config = model.config
     eos_token_id = config.eos_token_id
     pad_token_id = config.pad_token_id
     vocab_size = config.vocab_size
@@ -2162,21 +2532,19 @@
         if args.output_sequences_scores:
             print("sequences_scores", beam_outputs.sequences_scores)
         if args.output_token_scores:
             print("scores", beam_outputs.scores)
         for i, sequence in enumerate(beam_outputs.sequences):
             decoded_sequence = tokenizer.decode(sequence, skip_special_tokens=True)
             torch_decoded_sequences.append(decoded_sequence)
-            print("{}: {}".format(i, decoded_sequence))
+            print(f"{i}: {decoded_sequence}")
 
     print("-" * 50)
     print("Testing beam search with onnxruntime...")
 
-    ort_session = create_ort_session(args.output, args.use_gpu)
-
     vocab_mask = np.ones((vocab_size), dtype=np.int32)
     if args.vocab_mask:
         for bad_word_id in bad_words_ids:
             vocab_mask[bad_word_id] = 0
 
     inputs = {
         "input_ids": input_ids.cpu().numpy().astype(np.int32),
@@ -2192,23 +2560,25 @@
         inputs["vocab_mask"] = vocab_mask
 
     if args.custom_attention_mask:
         inputs["attention_mask"] = create_attention_mask(input_ids, pad_token_id)
 
     if args.save_test_data:
         test_data_dir = Path(args.output).parent.as_posix()
-        logger.debug("test_data_dir", test_data_dir)
+        logger.debug("test_data_dir", test_data_dir)  # noqa: PLE1205
         from bert_test_data import output_test_data
 
         all_inputs = [inputs]
         for i, inputs in enumerate(all_inputs):
             dir = os.path.join(test_data_dir, "test_data_set_" + str(i))
             output_test_data(dir, inputs)
 
-    logger.debug("ORT inputs", inputs)
+    logger.debug("ORT inputs", inputs)  # noqa: PLE1205
+
+    ort_session = create_ort_session(args.output, args.use_gpu, args.use_sln_strict_mode)
 
     # Test performance
     latency = []
     for _ in range(args.total_runs):
         start = time.time()
         result = ort_session.run(None, inputs)
         latency.append(time.time() - start)
```

## onnxruntime/transformers/convert_tf_models_to_pytorch.py

```diff
@@ -86,27 +86,27 @@
     if not os.path.exists(ckpt_dir):
         os.makedirs(ckpt_dir)
 
     tf_ckpt_url = TFMODELS[model_name][3]
 
     import re
 
-    if re.search(".zip$", tf_ckpt_url) != None:
+    if re.search(".zip$", tf_ckpt_url) is not None:
         zip_dir = download_compressed_file(tf_ckpt_url, ckpt_dir)
 
         # unzip file
         import zipfile
 
         with zipfile.ZipFile(zip_dir, "r") as zip_ref:
             zip_ref.extractall(ckpt_dir)
             os.remove(zip_dir)
 
         return get_ckpt_prefix_path(ckpt_dir)
 
-    elif re.search(".tar.gz$", tf_ckpt_url) != None:
+    elif re.search(".tar.gz$", tf_ckpt_url) is not None:
         tar_dir = download_compressed_file(tf_ckpt_url, ckpt_dir)
 
         # untar file
         import tarfile
 
         with tarfile.open(tar_dir, "r") as tar_ref:
             tar_ref.extractall(ckpt_dir)
@@ -133,15 +133,15 @@
     config_module = __import__("transformers", fromlist=[config_name])
     model_config = getattr(config_module, config_name)
 
     parent_path = tf_checkpoint_path.rpartition("/")[0]
     config_path = glob.glob(parent_path + "/*config.json")
     config = model_config() if len(config_path) == 0 else model_config.from_json_file(str(config_path[0]))
 
-    if TFMODELS[model_name][2] == "":
+    if not TFMODELS[model_name][2]:
         from transformers import AutoModelForPreTraining
 
         init_model = AutoModelForPreTraining.from_config(config)
     else:
         model_categroy_name = TFMODELS[model_name][2]
         module = __import__("transformers", fromlist=[model_categroy_name])
         model_categroy = getattr(module, model_categroy_name)
@@ -186,15 +186,15 @@
 def tf2pt_pipeline_test():
     # For test on linux only
     import logging
 
     import torch
 
     logger = logging.getLogger("")
-    for model_name in TFMODELS.keys():
+    for model_name in TFMODELS:
         config, model = tf2pt_pipeline(model_name)
         assert config.model_type is TFMODELS[model_name][0]
 
         input = torch.randint(low=0, high=config.vocab_size - 1, size=(4, 128), dtype=torch.long)
         try:
             model(input)
         except RuntimeError as e:
```

## onnxruntime/transformers/float16.py

```diff
@@ -1,23 +1,32 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 # This file is modified from https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py
-# Modifications: keep_io_types can be list of names; convert initializers if needed to preserve precision; add force_fp16_initializers option.
+# Modifications:
+# (1) Update default value of min_positive_val and max_finite_val
+# (2) keep_io_types can be list of names
+# (3) convert initializers if needed to preserve precision
+# (4) add force_fp16_initializers option
+# (5) handle Resize and GroupNorm with mixed float inputs
+# (6) allow convert_float_to_float16 to accept model path
 
 import itertools
 import logging
-from typing import Dict, List
+import os
+import tempfile
+from typing import Dict
 
 import numpy as np
 import onnx
 from onnx import helper, numpy_helper
 from onnx import onnx_pb as onnx_proto
+from onnx.shape_inference import infer_shapes, infer_shapes_path
 from packaging import version
 
 logger = logging.getLogger(__name__)
 
 
 def _npfloat16_to_int(np_list):
     """
@@ -36,14 +45,30 @@
     Positive finite values greater than max_finite_val are mapped to max_finite_val.
     Similar for negative values. NaN, 0, inf, and -inf are unchanged.
     """
 
     def between(a, b, c):
         return np.logical_and(a < b, b < c)
 
+    if np_array[np.where(np_array > 0)].shape[0] > 0:
+        positive_max = np_array[np.where(np_array > 0)].max()
+        positive_min = np_array[np.where(np_array > 0)].min()
+        if positive_max >= max_finite_val:
+            logger.info(f"the float32 number {positive_max} will be truncated to {max_finite_val}")
+        if positive_min <= min_positive_val:
+            logger.info(f"the float32 number {positive_min} will be truncated to {min_positive_val}")
+
+    if np_array[np.where(np_array < 0)].shape[0] > 0:
+        negative_max = np_array[np.where(np_array < 0)].max()
+        negative_min = np_array[np.where(np_array < 0)].min()
+        if negative_min <= -max_finite_val:
+            logger.info(f"the float32 number {negative_min} will be truncated to {-max_finite_val}")
+        if negative_max >= -min_positive_val:
+            logger.info(f"the float32 number {negative_max} will be truncated to {-min_positive_val}")
+
     np_array = np.where(between(0, np_array, min_positive_val), min_positive_val, np_array)
     np_array = np.where(between(-min_positive_val, np_array, 0), -min_positive_val, np_array)
     np_array = np.where(between(max_finite_val, np_array, float("inf")), max_finite_val, np_array)
     np_array = np.where(between(float("-inf"), np_array, -max_finite_val), -max_finite_val, np_array)
     return np.float16(np_array)
 
 
@@ -59,15 +84,15 @@
         ValueError: input type is not TensorProto.
 
     Returns:
         TensorProto: the converted tensor.
     """
 
     if not isinstance(tensor, onnx_proto.TensorProto):
-        raise ValueError("Expected input type is an ONNX TensorProto but got %s" % type(tensor))
+        raise ValueError(f"Expected input type is an ONNX TensorProto but got {type(tensor)}")
 
     if tensor.data_type == onnx_proto.TensorProto.FLOAT:
         tensor.data_type = onnx_proto.TensorProto.FLOAT16
         # convert float_data (float type) to float16 and write to int32_data
         if tensor.float_data:
             float16_data = convert_np_to_float16(np.array(tensor.float_data), min_positive_val, max_finite_val)
             int_list = _npfloat16_to_int(float16_data)
@@ -98,32 +123,36 @@
     "FeatureVectorizer",
     "Imputer",
     "LabelEncoder",
     "LinearClassifier",
     "LinearRegressor",
     "Normalizer",
     "OneHotEncoder",
+    "RandomUniformLike",
     "SVMClassifier",
     "SVMRegressor",
     "Scaler",
     "TreeEnsembleClassifier",
     "TreeEnsembleRegressor",
     "ZipMap",
     "NonMaxSuppression",
     "TopK",
     "RoiAlign",
-    "Resize",
     "Range",
     "CumSum",
     "Min",
     "Max",
     "Upsample",
 ]
 
 
+# Some operators has data type fixed as float for some inputs. Key is op_type, value is list of input indices
+ALWAYS_FLOAT_INPUTS = {"Resize": [2], "GroupNorm": [1, 2]}
+
+
 class InitializerTracker:
     """Class for keeping track of initializer."""
 
     def __init__(self, initializer: onnx_proto.TensorProto):
         self.initializer = initializer
         self.fp32_nodes = []
         self.fp16_nodes = []
@@ -141,51 +170,64 @@
     max_finite_val=65504.0,
     keep_io_types=False,
     disable_shape_infer=False,
     op_block_list=None,
     node_block_list=None,
     force_fp16_initializers=False,
 ):
-    """Convert model tensor float type in the ONNX ModelProto input to tensor float16.
+    """Convert tensor float type in the input ONNX model to tensor float16.
 
     Args:
-        model (ModelProto): The ONNX model to convert.
+        model (ModelProto or str): The ONNX model or path of the model to convert.
         min_positive_val (float, optional): minimal positive value. Defaults to 5.96e-08.
         max_finite_val (float, optional): maximal finite value of float16. Defaults to 65504.
         keep_io_types (Union[bool, List[str]], optional): It could be boolean or a list of float32 input/output names.
-                                                          If True, model inputs/outputs should be left as float32. Defaults to False.
-        disable_shape_infer (bool, optional): Skips running onnx shape/type inference. Useful if shape inference has been done. Defaults to False.
+                                                          If True, model inputs/outputs should be left as float32.
+                                                          Defaults to False.
+        disable_shape_infer (bool, optional): Skips running onnx shape/type inference.
+                                              Useful if shape inference has been done. Defaults to False.
         op_block_list (List[str], optional): List of op types to leave as float32.
-                                             Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST` as default.
+                                             Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST`.
         node_block_list (List[str], optional): List of node names to leave as float32. Defaults to None.
         force_fp16_initializers(bool): force converting all float initializers to float16.
                                        Default to false, which will convert only the one needed to avoid precision loss.
     Raises:
         ValueError: input type is not ModelProto.
 
     Returns:
         ModelProto: converted model.
     """
     assert (
         min_positive_val >= 5.96e-08
     ), "invalid min_positive_val. smallest positive float16 value: subnormal 5.96e-08, and normalized 6.104e-05"
     assert max_finite_val <= float(np.finfo(np.float16).max), "invalid max_finite_val. largest float16 value: 65504"
 
+    if isinstance(model, str):
+        model_path = model
+        if version.parse(onnx.__version__) >= version.parse("1.8.0") and not disable_shape_infer:
+            # shape_infer_model_path should be in the same folder of model_path
+            with tempfile.NamedTemporaryFile(dir=os.path.dirname(model_path)) as tmpfile:
+                shape_infer_model_path = tmpfile.name
+                # infer_shapes_path can be used for model >2GB, and infer_shapes cannot.
+                infer_shapes_path(model_path, shape_infer_model_path)
+                model = onnx.load(shape_infer_model_path)
+                disable_shape_infer = True
+        else:
+            model = onnx.load(model_path)
+
+    if not isinstance(model, onnx_proto.ModelProto):
+        raise ValueError(f"Expected an ONNX ModelProto but got {type(model)}")
+
     func_infer_shape = None
     if not disable_shape_infer and version.parse(onnx.__version__) >= version.parse("1.2.0"):
         try:
-            from onnx.shape_inference import infer_shapes
-
             func_infer_shape = infer_shapes
         finally:
             pass
 
-    if not isinstance(model, onnx_proto.ModelProto):
-        raise ValueError("Expected model type is an ONNX ModelProto but got %s" % type(model))
-
     # create blocklists
     if op_block_list is None:
         op_block_list = DEFAULT_OP_BLOCK_LIST
     if node_block_list is None:
         node_block_list = []
     op_block_list = set(op_block_list)
     node_block_list = set(node_block_list)
@@ -194,14 +236,20 @@
         f"fp16 parameters: min_positive_val={min_positive_val} max_finite_val={max_finite_val} keep_io_types={keep_io_types} disable_shape_infer={disable_shape_infer} op_block_list={op_block_list} node_block_list={node_block_list} force_fp16_initializers={force_fp16_initializers}"
     )
 
     # create a queue for BFS
     queue = []
     value_info_list = []
     node_list = []
+
+    # Some operators (Like Resize or GroupNorm) have data type fixed as float for some input.
+    # When it is converted to float16, there are mixed types: some inputs are float32 and some are float16.
+    # This list keeps track of such nodes that are not in block list.
+    mixed_float_type_node_list = []
+
     # type inference on input model
     if func_infer_shape is not None:
         model = func_infer_shape(model)
     queue.append(model)
     name_mapping = {}
     graph_io_to_skip = set()
     io_casts = set()
@@ -272,37 +320,45 @@
                         if n.input[i] in name_mapping:
                             n.input[i] = name_mapping[n.input[i]]
                     for i in range(len(n.output)):
                         if n.output[i] in name_mapping:
                             n.output[i] = name_mapping[n.output[i]]
 
                     is_node_blocked = n.op_type in op_block_list or n.name in node_block_list
-                    for input in n.input:
-                        if input in fp32_initializers:
-                            fp32_initializers[input].add_node(n, is_node_blocked)
+                    for i, input_name in enumerate(n.input):
+                        if input_name in fp32_initializers:
+                            # For Resize/GroupNorm, only the first input can be float16
+                            use_fp32_weight = is_node_blocked or (n.op_type in ["Resize", "GroupNorm"] and i != 0)
+                            fp32_initializers[input_name].add_node(n, use_fp32_weight)
 
                     if is_node_blocked:
                         node_list.append(n)
                     else:
                         if n.op_type == "Cast":
                             for attr in n.attribute:
                                 if attr.name == "to" and attr.i == 1:
                                     attr.i = 10
                                     break
-                        for attr in n.attribute:
-                            next_level.append(attr)
+
+                        # For Resize/GroupNorm, attribute data type cannot be changed
+                        if n.op_type not in ["Resize", "GroupNorm"]:
+                            for attr in n.attribute:
+                                next_level.append(attr)
+                        else:
+                            mixed_float_type_node_list.append(n)
+
             # if q is model.graph.node.attribute, push q.g and q.graphs (GraphProto)
             # and process node.attribute.t and node.attribute.tensors (TensorProto)
             if isinstance(q, onnx_proto.AttributeProto):
                 next_level.append(q.g)
                 for n in q.graphs:
                     next_level.append(n)
                 q.t.CopyFrom(convert_tensor_float_to_float16(q.t, min_positive_val, max_finite_val))
                 for n in q.tensors:
-                    n = convert_tensor_float_to_float16(n, min_positive_val, max_finite_val)
+                    n = convert_tensor_float_to_float16(n, min_positive_val, max_finite_val)  # noqa: PLW2901
             # if q is graph, process input, output and value_info (ValueInfoProto)
             if isinstance(q, onnx_proto.GraphProto):
                 # Note that float initializers tracked by fp32_initializers will be processed later.
                 # for all ValueInfoProto with tensor(float) type in input, output and value_info, convert them to
                 # tensor(float16) except map and seq(map). And save them in value_info_list for further processing
                 for n in itertools.chain(q.input, q.output, q.value_info):
                     if n.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
@@ -313,44 +369,65 @@
                         if n.type.sequence_type.elem_type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:
                             if n.name not in graph_io_to_skip:
                                 n.type.sequence_type.elem_type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16
                                 value_info_list.append(n)
 
         queue = next_level
 
-    for key, value in fp32_initializers.items():
+    for _key, value in fp32_initializers.items():
         # By default, to avoid precision loss, do not convert an initializer to fp16 when it is used only by fp32 nodes.
         if force_fp16_initializers or value.fp16_nodes:
             value.initializer = convert_tensor_float_to_float16(value.initializer, min_positive_val, max_finite_val)
             value_info_list.append(make_value_info_from_tensor(value.initializer))
             if value.fp32_nodes and not force_fp16_initializers:
                 logger.info(
                     "initializer is used by both fp32 and fp16 nodes. Consider add these nodes to block list:{}".format(
                         value.fp16_nodes
                     )
                 )
 
+    # Some operators have data type fixed as float for some input. Add a float16 to float cast for those inputs.
+    for node in mixed_float_type_node_list:
+        for i, input_name in enumerate(node.input):
+            if i not in ALWAYS_FLOAT_INPUTS[node.op_type]:
+                continue
+            for value_info in value_info_list:
+                if input_name == value_info.name:
+                    # create new value_info for current node's new input name
+                    new_value_info = model.graph.value_info.add()
+                    new_value_info.CopyFrom(value_info)
+                    output_name = node.name + "_input_cast_" + str(i)
+                    new_value_info.name = output_name
+                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
+                    # add Cast node (from tensor(float16) to tensor(float) before current node
+                    node_name = node.name + "_input_cast" + str(i)
+                    new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
+                    model.graph.node.extend(new_node)
+                    # change current node's input name
+                    node.input[i] = output_name
+                    break
+
     # process the nodes in block list that doesn't support tensor(float16)
     for node in node_list:
         # if input's name is in the value_info_list meaning input is tensor(float16) type,
         # insert a float16 to float Cast node before the node,
         # change current node's input name and create new value_info for the new name
         for i in range(len(node.input)):
-            input = node.input[i]
+            input_name = node.input[i]
             for value_info in value_info_list:
-                if input == value_info.name:
+                if input_name == value_info.name:
                     # create new value_info for current node's new input name
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
                     output_name = node.name + "_input_cast_" + str(i)
                     new_value_info.name = output_name
                     new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT
                     # add Cast node (from tensor(float16) to tensor(float) before current node
                     node_name = node.name + "_input_cast" + str(i)
-                    new_node = [helper.make_node("Cast", [input], [output_name], to=1, name=node_name)]
+                    new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.input[i] = output_name
                     break
         # if output's name is in the value_info_list meaning output is tensor(float16) type, insert a float to
         # float16 Cast node after the node, change current node's output name and create new value_info for the new name
         for i in range(len(node.output)):
@@ -372,15 +449,15 @@
                     break
     return model
 
 
 def float_to_float16_max_diff(tensor, min_positive_val=5.96e-08, max_finite_val=65504.0):
     """Measure the maximum absolute difference after converting a float tensor to float16."""
     if not isinstance(tensor, onnx_proto.TensorProto):
-        raise ValueError("Expected input type is an ONNX TensorProto but got %s" % type(tensor))
+        raise ValueError(f"Expected input type is an ONNX TensorProto but got {type(tensor)}")
     if tensor.data_type != onnx_proto.TensorProto.FLOAT:
         raise ValueError("Expected tensor data type is float.")
 
     float32_data = None
     if tensor.float_data:
         float32_data = np.array(tensor.float_data)
```

## onnxruntime/transformers/fusion_attention.py

```diff
@@ -1,24 +1,20 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-from enum import Enum
 from logging import getLogger
-from os import name
-from sys import path
-from typing import Tuple, Union
+from typing import List, Optional, Tuple, Union
 
 import numpy as np
 from fusion_base import Fusion
 from fusion_options import AttentionMaskFormat
 from fusion_utils import FusionUtils, NumpyHelper
 from onnx import NodeProto, TensorProto, helper, numpy_helper
 from onnx_model import OnnxModel
-from shape_infer_helper import SymbolicShapeInferenceHelper, get_shape_from_type_proto
 
 logger = getLogger(__name__)
 
 
 class AttentionMask:
     """
     Fuse Attention subgraph into one Attention node.
@@ -28,14 +24,15 @@
         self.model = model
         # A lookup table with mask input as key, and mask index output as value
         self.mask_indice = {}
         # A lookup table with mask input as key, and cast (to int32) output as value
         self.mask_casted = {}
         self.utils = FusionUtils(model)
         self.mask_format = AttentionMaskFormat.MaskIndexEnd
+        self.opset_version = model.get_opset_version()
 
     def set_mask_format(self, mask_format: AttentionMaskFormat):
         self.mask_format = mask_format
 
     def set_mask_indice(self, mask, mask_index):
         if mask in self.mask_indice:
             assert mask_index == self.mask_indice[mask]
@@ -65,21 +62,42 @@
         # Attention supports int32 attention mask (2D) since 1.4.0
         if self.mask_format == AttentionMaskFormat.AttentionMask:
             self.mask_indice[input] = input_name
             return input_name
 
         # Add a mask processing node to convert attention mask to mask index (1D)
         output_name = self.model.create_node_name("mask_index")
-        mask_index_node = helper.make_node(
-            "ReduceSum",
-            inputs=[input_name],
-            outputs=[output_name],
-            name=self.model.create_node_name("ReduceSum", "MaskReduceSum"),
-        )
-        mask_index_node.attribute.extend([helper.make_attribute("axes", [1]), helper.make_attribute("keepdims", 0)])
+        if self.opset_version < 13:
+            mask_index_node = helper.make_node(
+                "ReduceSum",
+                inputs=[input_name],
+                outputs=[output_name],
+                name=self.model.create_node_name("ReduceSum", "MaskReduceSum"),
+            )
+            mask_index_node.attribute.extend([helper.make_attribute("axes", [1]), helper.make_attribute("keepdims", 0)])
+        else:
+            # ReduceSum-13: axes is moved from attribute to input
+            axes_name = "ort_const_1_reduce_sum_axes"
+            if self.model.get_initializer(axes_name) is None:
+                self.model.add_initializer(
+                    helper.make_tensor(
+                        name=axes_name,
+                        data_type=TensorProto.INT64,
+                        dims=[1],
+                        vals=[1],
+                    )
+                )
+            mask_index_node = helper.make_node(
+                "ReduceSum",
+                inputs=[input_name, axes_name],
+                outputs=[output_name],
+                name=self.model.create_node_name("ReduceSum", "MaskReduceSum"),
+            )
+            mask_index_node.attribute.extend([helper.make_attribute("keepdims", 0)])
+
         self.model.add_node(mask_index_node)
 
         self.mask_indice[input] = output_name
         return output_name
 
 
 class FusionAttention(Fusion):
@@ -90,17 +108,18 @@
     def __init__(
         self,
         model: OnnxModel,
         hidden_size: int,
         num_heads: int,
         attention_mask: AttentionMask,
         use_multi_head_attention: bool = False,
+        search_op_types: List[str] = ["SkipLayerNormalization", "LayerNormalization"],  # noqa: B006
     ):
         attention_op_name = "MultiHeadAttention" if use_multi_head_attention else "Attention"
-        super().__init__(model, attention_op_name, ["SkipLayerNormalization", "LayerNormalization"])
+        super().__init__(model, attention_op_name, search_op_types)
         self.hidden_size = hidden_size
         self.num_heads = num_heads
         self.attention_mask = attention_mask
         self.use_multi_head_attention = use_multi_head_attention
         self.mask_filter_value = None
 
         # Flags to show warning only once
@@ -193,68 +212,519 @@
 
         if input_0_shape != input_1_shape:
             logger.debug(f"the shape of two inputs of {add_qk} is not same")
             return None
 
         return add_qk.input[1]
 
+    def concat_kv(self, past_k: str, past_v: str) -> str:
+        """Concatenate past_k and past_v inputs to create past_kv input.
+
+        Args:
+            past_k (str): name of past K value
+            past_v (str): name of past V value
+
+        Returns:
+            kv_output_name (str): name of past KV value
+        """
+        # Unsqueeze K and V nodes from (B,N,P,H) to (1,B,N,P,H)
+        # B = batch size, N = num heads, P = past sequence length, H = head size
+        unsqueeze_k_name = self.model.create_node_name("Unsqueeze")
+        unsqueeze_v_name = self.model.create_node_name("Unsqueeze")
+        k_5d_name = (past_k + "_5d").replace(".", "_")
+        v_5d_name = (past_v + "_5d").replace(".", "_")
+
+        k_5d = helper.make_node(
+            "Unsqueeze",
+            inputs=[past_k],
+            outputs=[k_5d_name],
+            name=unsqueeze_k_name,
+            axes=[0],
+        )
+        v_5d = helper.make_node(
+            "Unsqueeze",
+            inputs=[past_v],
+            outputs=[v_5d_name],
+            name=unsqueeze_v_name,
+            axes=[0],
+        )
+
+        # Add unsqueeze nodes to graph
+        self.nodes_to_add.append(k_5d)
+        self.nodes_to_add.append(v_5d)
+        self.node_name_to_graph_name[unsqueeze_k_name] = self.this_graph_name
+        self.node_name_to_graph_name[unsqueeze_v_name] = self.this_graph_name
+
+        # Concat K and V to get one node of size (2,B,N,P,H)
+        concat_node_name = self.model.create_node_name("Concat")
+        kv_output_name = past_v.replace(".value", ".kv").replace(".", "_").replace("_value", "_kv")
+        concat_kv = helper.make_node(
+            "Concat",
+            inputs=[k_5d_name, v_5d_name],
+            outputs=[kv_output_name],
+            name=concat_node_name,
+            axis=0,
+        )
+
+        # Add concat node to graph
+        self.nodes_to_add.append(concat_kv)
+        self.node_name_to_graph_name[concat_node_name] = self.this_graph_name
+
+        return kv_output_name
+
+    def reshape_kv(self, past_k: str, past_v: str) -> (str, str):
+        """Reshape past_k and past_v from 4D to 3D to use as inputs for multihead attention node.
+
+        Args:
+            past_k (str): name of past K value of shape 4D
+            past_v (str): name of past V value of shape 4D
+
+        Returns:
+            k_3d (str): name of past K value of shape 3D
+            v_3d (str): name of past V value of shape 3D
+        """
+        # Reshape past_k and past_v from (B,N,P,H) to (B,P,N*H)
+        # B = batch size, N = num heads, P = past seq len, H = head size
+
+        # Create initializer for reshaping past_k and past_v
+        new_dims_name = "kv_4d_to_3d"
+        new_dims = self.model.get_initializer(new_dims_name)
+        if new_dims is None:
+            new_dims = numpy_helper.from_array(
+                np.array([0, -1, self.model.hidden_size], dtype="int64"), name=new_dims_name
+            )
+            self.model.add_initializer(new_dims, self.this_graph_name)
+
+        reshape_k_name = self.model.create_node_name("Reshape")
+        reshape_v_name = self.model.create_node_name("Reshape")
+        k_3d_name = (past_k + "_3d").replace(".", "_")
+        v_3d_name = (past_v + "_3d").replace(".", "_")
+
+        k_3d = helper.make_node(
+            "Reshape",
+            inputs=[past_k, new_dims_name],
+            outputs=[k_3d_name],
+            name=reshape_k_name,
+        )
+        v_3d = helper.make_node(
+            "Reshape",
+            inputs=[past_v, new_dims_name],
+            outputs=[v_3d_name],
+            name=reshape_v_name,
+        )
+
+        # Add reshape nodes to graph
+        self.nodes_to_add.append(k_3d)
+        self.nodes_to_add.append(v_3d)
+        self.node_name_to_graph_name[reshape_k_name] = self.this_graph_name
+        self.node_name_to_graph_name[reshape_v_name] = self.this_graph_name
+
+        return k_3d_name, v_3d_name
+
+    def split_kv(self, present_k_name: str, present_v_name: str, kv_node: str):
+        """Split kv_node containing present KV values into separate present K and present V values.
+
+        Args:
+            present_k_name (str): name of output to store present K value in
+            present_v_name (str): name of output to store present V value in
+            kv_node (str): name of present KV values
+        """
+        # Split kv_node into present_k and present_v nodes
+
+        # Create initializers for indexing kv_node, whose shape is (2,B,N,P,H)
+        k_index, v_index = "index_0", "index_1"
+        k_dim = self.model.get_initializer(k_index)
+        v_dim = self.model.get_initializer(v_index)
+        if k_dim is None:
+            k_dim = numpy_helper.from_array(np.array(0, dtype="int64"), name=k_index)
+            self.model.add_initializer(k_dim, self.this_graph_name)
+        if v_dim is None:
+            v_dim = numpy_helper.from_array(np.array(1, dtype="int64"), name=v_index)
+            self.model.add_initializer(v_dim, self.this_graph_name)
+
+        # Create nodes to index kv_node
+        gather_k_name = self.model.create_node_name("Gather")
+        gather_v_name = self.model.create_node_name("Gather")
+        present_k = helper.make_node(
+            "Gather",
+            inputs=[kv_node, k_index],
+            outputs=[present_k_name],
+            name=gather_k_name,
+            axis=0,
+        )
+        present_v = helper.make_node(
+            "Gather",
+            inputs=[kv_node, v_index],
+            outputs=[present_v_name],
+            name=gather_v_name,
+            axis=0,
+        )
+
+        # Add gather nodes to graph
+        self.nodes_to_add.append(present_k)
+        self.nodes_to_add.append(present_v)
+        self.node_name_to_graph_name[gather_k_name] = self.this_graph_name
+        self.node_name_to_graph_name[gather_v_name] = self.this_graph_name
+
+    def transpose_kv(self, past_k: str, past_v: str):
+        """Transpose past_k and past_v from (B,N,P,H) to (B,P,N,H)
+
+        Args:
+            past_k (str): name of past K value of shape (B,N,P,H)
+            past_v (str): name of past V value of shape (B,N,P,H)
+
+        Returns:
+            past_k_transpose (str): name of past K value of shape (B,P,N,H)
+            past_v_transpose (str): name of past V value of shape (B,P,N,H)
+        """
+        past_k_transpose = (past_k + "_transposed").replace(".", "_")
+        past_v_transpose = (past_v + "_transposed").replace(".", "_")
+        transpose_k_name = self.model.create_node_name("Transpose")
+        transpose_v_name = self.model.create_node_name("Transpose")
+
+        transpose_k = helper.make_node(
+            "Transpose",
+            inputs=[past_k],
+            outputs=[past_k_transpose],
+            name=transpose_k_name,
+            perm=[0, 2, 1, 3],
+        )
+        transpose_v = helper.make_node(
+            "Transpose",
+            inputs=[past_v],
+            outputs=[past_v_transpose],
+            name=transpose_v_name,
+            perm=[0, 2, 1, 3],
+        )
+
+        # Add reshape nodes to graph
+        self.nodes_to_add.append(transpose_k)
+        self.nodes_to_add.append(transpose_v)
+        self.node_name_to_graph_name[transpose_k_name] = self.this_graph_name
+        self.node_name_to_graph_name[transpose_v_name] = self.this_graph_name
+
+        return past_k_transpose, past_v_transpose
+
+    def create_packed_qkv_matmul_node(
+        self,
+        q_matmul: NodeProto,
+        k_matmul: NodeProto,
+        v_matmul: NodeProto,
+        q_add: NodeProto,
+        k_add: Union[NodeProto, None],
+        v_add: Union[NodeProto, None],
+        num_heads: int,
+    ) -> Union[NodeProto, None]:
+        """Create packed QKV MatMul node before MultiHeadAttention node.
+           This is for the scenario where an Attention node should be created but cannot be created
+           because past_key and past_value are separate inputs and not one concatenated input.
+
+        Args:
+            q_matmul (NodeProto): name of MatMul from Q path - (batch_size, sequence_length, hidden_size)
+            k_matmul (NodeProto): name of MatMul from K path - (batch_size, sequence_length, hidden_size)
+            v_matmul (NodeProto): name of MatMul from V path - (batch_size, sequence_length, hidden_size)
+            q_add (NodeProto): name of Add from Q path
+            k_add (NodeProto): name of Add from K path
+            v_add (NodeProto): name of Add from V path
+            num_heads (int): number of heads
+
+        Returns:
+            Union[NodeProto, None]: the node created or None if failed.
+        """
+        matmul_node_name = self.model.create_node_name("MatMul")
+
+        # Check that input for Q, K, V is the same
+        assert q_matmul.input[0] == k_matmul.input[0] and k_matmul.input[0] == v_matmul.input[0]
+
+        # Created packed QKV weight
+        q_weight = self.model.get_initializer(q_matmul.input[1])
+        k_weight = self.model.get_initializer(k_matmul.input[1])
+        v_weight = self.model.get_initializer(v_matmul.input[1])
+
+        qw = NumpyHelper.to_array(q_weight)
+        kw = NumpyHelper.to_array(k_weight)
+        vw = NumpyHelper.to_array(v_weight)
+
+        assert qw.shape == kw.shape and kw.shape == vw.shape
+        d = qw.shape[0]
+
+        qkv_weight = np.stack((qw, kw, vw), axis=1).reshape((d, 3 * d))
+        qkv_weight_name = matmul_node_name + "_qkv_weight"
+        weight = helper.make_tensor(
+            name=qkv_weight_name,
+            data_type=TensorProto.FLOAT,
+            dims=[qkv_weight.shape[0], qkv_weight.shape[1]],
+            vals=qkv_weight.flatten().tolist(),
+        )
+        self.model.add_initializer(weight, self.this_graph_name)
+
+        # Created packed QKV MatMul with output (B, S, 3*D)
+        # Output is of the form:
+        #
+        # [[[Q Q ... Q Q K K ... K K V V ... V V]]]
+        #   [Q Q ... Q Q K K ... K K V V ... V V]
+        #                     .
+        #                     .
+        #                     .
+        #  [[Q Q ... Q Q K K ... K K V V ... V V]
+        #   [Q Q ... Q Q K K ... K K V V ... V V]]]
+        qkv_matmul_output = matmul_node_name + "_qkv_out"
+        qkv_matmul = helper.make_node(
+            "MatMul",
+            inputs=[q_matmul.input[0], qkv_weight_name],
+            outputs=[qkv_matmul_output],
+            name=matmul_node_name,
+        )
+        self.node_name_to_graph_name[matmul_node_name] = self.this_graph_name
+
+        # Create Slice nodes to access Q, K, V
+        q_slice_name = matmul_node_name + "_q_start_index"
+        q_start_tensor = helper.make_tensor(name=q_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[0])
+        k_slice_name = matmul_node_name + "_k_start_index"
+        k_start_tensor = helper.make_tensor(name=k_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[d])
+        v_slice_name = matmul_node_name + "_v_start_index"
+        v_start_tensor = helper.make_tensor(name=v_slice_name, data_type=TensorProto.INT64, dims=[1], vals=[2 * d])
+        end_of_qkv_name = matmul_node_name + "_end_of_qkv_index"
+        end_of_qkv_tensor = helper.make_tensor(
+            name=end_of_qkv_name, data_type=TensorProto.INT64, dims=[1], vals=[3 * d]
+        )
+        qkv_last_axis_name = matmul_node_name + "_qkv_last_axis"
+        qkv_axis_tensor = helper.make_tensor(name=qkv_last_axis_name, data_type=TensorProto.INT64, dims=[1], vals=[-1])
+
+        self.model.add_initializer(q_start_tensor, self.this_graph_name)
+        self.model.add_initializer(k_start_tensor, self.this_graph_name)
+        self.model.add_initializer(v_start_tensor, self.this_graph_name)
+        self.model.add_initializer(end_of_qkv_tensor, self.this_graph_name)
+        self.model.add_initializer(qkv_axis_tensor, self.this_graph_name)
+
+        q_slice_output = matmul_node_name + "_q_out"
+        q_slice = helper.make_node(
+            "Slice",
+            inputs=[qkv_matmul_output, q_slice_name, k_slice_name, qkv_last_axis_name],
+            outputs=[q_slice_output],
+            name=self.model.create_node_name("Slice"),
+        )
+        self.node_name_to_graph_name[q_slice.name] = self.this_graph_name
+        k_slice_output = matmul_node_name + "_k_out"
+        k_slice = helper.make_node(
+            "Slice",
+            inputs=[qkv_matmul_output, k_slice_name, v_slice_name, qkv_last_axis_name],
+            outputs=[k_slice_output],
+            name=self.model.create_node_name("Slice"),
+        )
+        self.node_name_to_graph_name[k_slice.name] = self.this_graph_name
+        v_slice_output = matmul_node_name + "_v_out"
+        v_slice = helper.make_node(
+            "Slice",
+            inputs=[qkv_matmul_output, v_slice_name, end_of_qkv_name, qkv_last_axis_name],
+            outputs=[v_slice_output],
+            name=self.model.create_node_name("Slice"),
+        )
+        self.node_name_to_graph_name[v_slice.name] = self.this_graph_name
+
+        # Add nodes to graph
+        self.nodes_to_add.extend([qkv_matmul, q_slice, k_slice, v_slice])
+        return q_slice, k_slice, v_slice
+
+    def create_multihead_attention_node(
+        self,
+        q_matmul: NodeProto,
+        k_matmul: Union[NodeProto, str, None],
+        v_matmul: Union[NodeProto, str, None],
+        q_add: NodeProto,
+        k_add: Union[NodeProto, None],
+        v_add: Union[NodeProto, None],
+        num_heads: int,
+        hidden_size: int,
+        output: str,
+        key_padding_mask: str = "",
+        add_qk: str = "",
+        past_k: str = "",
+        past_v: str = "",
+        present_k: str = "",
+        present_v: str = "",
+        packed_qkv: bool = False,
+    ) -> Union[NodeProto, None]:
+        """Create a MultiHeadAttention node.
+
+        Args:
+            q_matmul (NodeProto): name of MatMul from Q path - (batch_size, sequence_length, hidden_size)
+            k_matmul (NodeProto): name of MatMul from K path - (batch_size, sequence_length, hidden_size) or (batch_size, num_heads, past_sequence_length, head_size)
+            v_matmul (NodeProto): name of MatMul from V path - (batch_size, sequence_length, hidden_size) or (batch_size, num_heads, past_sequence_length, head_size)
+            q_add (NodeProto): name of Add from Q path
+            k_add (NodeProto): name of Add from K path
+            v_add (NodeProto): name of Add from V path
+            num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
+            hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
+            output (str): output name of MHA
+            key_padding_mask (str): name of key padding mask
+            add_qk (str): name of add after Q x K'
+            past_k (str): name of past K value - (batch_size, num_heads, past_sequence_length, head_size)
+            past_v (str): name of past V value - (batch_size, num_heads, past_sequence_length, head_size)
+            present_k (str): name of present K value - (batch_size, num_heads, sequence_length, head_size)
+            present_v (str): name of present V value - (batch_size, num_heads, sequence_length, head_size)
+            packed_qkv (bool): whether to combine MatMuls from Q, K, V paths
+                               Note: This is for the scenario where an Attention node should be created but cannot be created
+                               because past_key and past_value are separate inputs and not one concatenated input.
+
+        Returns:
+            Union[NodeProto, None]: the node created or None if failed.
+        """
+        # B = batch size, N = num heads, P = past seq len, H = head size
+        assert num_heads > 0
+
+        if hidden_size > 0 and (hidden_size % num_heads) != 0:
+            logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
+            return None
+
+        graph_input_names = set([node.name for node in self.model.graph().input])
+        graph_output_names = set([node.name for node in self.model.graph().output])
+        mha_node_name = self.model.create_node_name("Attention")
+
+        # Add initial Q/K/V inputs for MHA
+        mha_inputs = []
+        if packed_qkv:
+            q_slice, k_slice, v_slice = self.create_packed_qkv_matmul_node(
+                q_matmul, k_matmul, v_matmul, q_add, k_add, v_add, num_heads
+            )
+            mha_inputs.extend([q_slice.output[0], k_slice.output[0], v_slice.output[0]])
+        elif type(k_matmul) == NodeProto and type(v_matmul) == NodeProto:
+            mha_inputs.extend([q_matmul.output[0], k_matmul.output[0], v_matmul.output[0]])
+        elif (
+            type(k_matmul) == str
+            and type(v_matmul) == str
+            and k_matmul in graph_input_names
+            and v_matmul in graph_input_names
+        ):
+            mha_inputs.extend([q_matmul.output[0], k_matmul, v_matmul])
+        else:
+            return None
+
+        # Create combined Q/K/V bias
+        q_bias = self.model.get_initializer(q_add.input[1]) or self.model.get_initializer(q_add.input[0])
+        qb = NumpyHelper.to_array(q_bias)
+        kb = np.zeros_like(qb)
+        vb = np.zeros_like(qb)
+        if k_add is not None:
+            k_bias = self.model.get_initializer(k_add.input[1]) or self.model.get_initializer(k_add.input[0])
+            kb = NumpyHelper.to_array(k_bias)
+        if v_add is not None:
+            v_bias = self.model.get_initializer(v_add.input[1]) or self.model.get_initializer(v_add.input[0])
+            vb = NumpyHelper.to_array(v_bias)
+
+        qkv_bias = np.stack((qb, kb, vb), axis=0)
+        qkv_bias_dim = 3 * np.prod(qb.shape)
+
+        bias_name = mha_node_name + "_qkv_bias"
+        bias = helper.make_tensor(
+            name=bias_name,
+            data_type=TensorProto.FLOAT,
+            dims=[qkv_bias_dim],
+            vals=qkv_bias.flatten().tolist(),
+        )
+
+        # Convert bias to FP16 if model is using FP16
+        if q_bias.data_type == 10:
+            bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
+        self.model.add_initializer(bias, self.this_graph_name)
+
+        # Add bias to inputs for MHA
+        mha_inputs.append(bias_name)
+
+        # Add optional inputs for MHA
+        if past_k and past_v and past_k in graph_input_names and past_v in graph_input_names:
+            mha_inputs.extend([key_padding_mask, add_qk, past_k, past_v])
+
+        # Add outputs for MHA
+        mha_outputs = [output]
+        if present_k and present_v and present_k in graph_output_names and present_v in graph_output_names:
+            mha_outputs.extend([present_k, present_v])
+
+        mha_node = helper.make_node(
+            "MultiHeadAttention",
+            inputs=mha_inputs,
+            outputs=mha_outputs,
+            name=mha_node_name,
+        )
+        mha_node.domain = "com.microsoft"
+        mha_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
+        return mha_node
+
     def create_attention_node(
         self,
         mask_index: str,
         q_matmul: NodeProto,
         k_matmul: NodeProto,
         v_matmul: NodeProto,
         q_add: NodeProto,
         k_add: NodeProto,
         v_add: NodeProto,
         num_heads: int,
         hidden_size: int,
         input: str,
         output: str,
-        add_qk_str: str,
+        add_qk_str: str = "",
+        past_k: str = "",
+        past_v: str = "",
+        present_k: str = "",
+        present_v: str = "",
+        scale: Optional[float] = None,
     ) -> Union[NodeProto, None]:
         """Create an Attention node.
 
         Args:
             mask_index (str): mask input
             q_matmul (NodeProto): MatMul node in fully connection for Q
-            k_matmul (NodeProto): MatMul node in fully connection for  K
-            v_matmul (NodeProto): MatMul node in fully connection for  V
+            k_matmul (NodeProto): MatMul node in fully connection for K
+            v_matmul (NodeProto): MatMul node in fully connection for V
             q_add (NodeProto): Add bias node in fully connection for Q
             k_add (NodeProto): Add bias node in fully connection for K
             v_add (NodeProto): Add bias node in fully connection for V
             num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
             hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
             input (str): input name
             output (str): output name
+            add_qk_str (str): name of Add node after Q x K'
+            past_k (str): name of input for past K value
+            past_v (str): name of input for past V value
+            present_k (str): name of output to store present K value
+            present_v (str): name of output to store present V value
 
         Returns:
             Union[NodeProto, None]: the node created or None if failed.
         """
         assert num_heads > 0
 
         if hidden_size > 0 and (hidden_size % num_heads) != 0:
             logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
             return None
 
+        has_bias = True
+        if q_add is None and k_add is None and v_add is None:
+            has_bias = False
+
         q_weight = self.model.get_initializer(q_matmul.input[1])
         k_weight = self.model.get_initializer(k_matmul.input[1])
         v_weight = self.model.get_initializer(v_matmul.input[1])
-        q_bias = self.model.get_initializer(q_add.input[1]) or self.model.get_initializer(q_add.input[0])
-        k_bias = self.model.get_initializer(k_add.input[1]) or self.model.get_initializer(k_add.input[0])
-        v_bias = self.model.get_initializer(v_add.input[1]) or self.model.get_initializer(v_add.input[0])
+
+        q_bias, k_bias, v_bias = None, None, None
+        if has_bias:
+            q_bias = self.model.get_initializer(q_add.input[1]) or self.model.get_initializer(q_add.input[0])
+            k_bias = self.model.get_initializer(k_add.input[1]) or self.model.get_initializer(k_add.input[0])
+            v_bias = self.model.get_initializer(v_add.input[1]) or self.model.get_initializer(v_add.input[0])
+
+            if not (k_weight and v_weight and q_bias and k_bias):
+                return None
 
         if q_weight is None:
             print(
                 f"{q_matmul.input[1]} is not an initializer. "
                 "Please set do_constant_folding=True in torch.onnx.export to unblock attention fusion"
             )
             return None
-        if not (k_weight and v_weight and q_bias and k_bias):
-            return None
 
         qw = NumpyHelper.to_array(q_weight)
         kw = NumpyHelper.to_array(k_weight)
         vw = NumpyHelper.to_array(v_weight)
 
         # assert q and k have same shape as expected
         assert qw.shape == kw.shape
@@ -271,47 +741,48 @@
                 "Please provide a correct input hidden size or pass in 0"
             )
 
         is_qkv_diff_dims = False
         if qw.shape != vw.shape:
             is_qkv_diff_dims = True
 
-        # All the matrices can have the same shape or q, k matrics can have the same shape with v being different
+        # All the matrices can have the same shape or q, k matrices can have the same shape with v being different
         # For 2d weights, the shapes would be [in_size, out_size].
         # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
         qw_out_size = np.prod(qw.shape[1:])
         kw_out_size = np.prod(kw.shape[1:])
         vw_out_size = np.prod(vw.shape[1:])
 
         qkv_weight_dim = 0
         if is_qkv_diff_dims:
             qkv_weight = np.concatenate((qw, kw, vw), axis=1)
             qkv_weight_dim = qw_out_size + kw_out_size + vw_out_size
         else:
             qkv_weight = np.stack((qw, kw, vw), axis=1)
             qkv_weight_dim = 3 * qw_out_size
 
-        qb = NumpyHelper.to_array(q_bias)
-        kb = NumpyHelper.to_array(k_bias)
-        vb = NumpyHelper.to_array(v_bias)
-
-        q_bias_shape = np.prod(qb.shape)
-        k_bias_shape = np.prod(kb.shape)
-        v_bias_shape = np.prod(vb.shape)
-
-        assert q_bias_shape == k_bias_shape == qw_out_size
-        assert v_bias_shape == vw_out_size
-
-        qkv_bias_dim = 0
-        if is_qkv_diff_dims:
-            qkv_bias = np.concatenate((qb, kb, vb), axis=0)
-            qkv_bias_dim = q_bias_shape + k_bias_shape + v_bias_shape
-        else:
-            qkv_bias = np.stack((qb, kb, vb), axis=0)
-            qkv_bias_dim = 3 * q_bias_shape
+        if has_bias:
+            qb = NumpyHelper.to_array(q_bias)
+            kb = NumpyHelper.to_array(k_bias)
+            vb = NumpyHelper.to_array(v_bias)
+
+            q_bias_shape = np.prod(qb.shape)
+            k_bias_shape = np.prod(kb.shape)
+            v_bias_shape = np.prod(vb.shape)
+
+            assert q_bias_shape == k_bias_shape == qw_out_size
+            assert v_bias_shape == vw_out_size
+
+            qkv_bias_dim = 0
+            if is_qkv_diff_dims:
+                qkv_bias = np.concatenate((qb, kb, vb), axis=0)
+                qkv_bias_dim = q_bias_shape + k_bias_shape + v_bias_shape
+            else:
+                qkv_bias = np.stack((qb, kb, vb), axis=0)
+                qkv_bias_dim = 3 * q_bias_shape
 
         attention_node_name = self.model.create_node_name("Attention")
 
         if not self.use_multi_head_attention:
             weight = helper.make_tensor(
                 name=attention_node_name + "_qkv_weight",
                 data_type=TensorProto.FLOAT,
@@ -320,69 +791,104 @@
             )
 
             # Sometimes weights and bias are stored in fp16
             if q_weight.data_type == 10:
                 weight.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(weight).astype(np.float16), weight.name))
             self.model.add_initializer(weight, self.this_graph_name)
 
-        bias = helper.make_tensor(
-            name=attention_node_name + "_qkv_bias",
-            data_type=TensorProto.FLOAT,
-            dims=[qkv_bias_dim],
-            vals=qkv_bias.flatten().tolist(),
-        )
-        if q_bias.data_type == 10:
-            bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
-        self.model.add_initializer(bias, self.this_graph_name)
+        bias = None
+        if has_bias:
+            bias = helper.make_tensor(
+                name=attention_node_name + "_qkv_bias",
+                data_type=TensorProto.FLOAT,
+                dims=[qkv_bias_dim],
+                vals=qkv_bias.flatten().tolist(),
+            )
+            if q_bias.data_type == 10:
+                bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
+            self.model.add_initializer(bias, self.this_graph_name)
 
         # For MultiHeadAttention operator, use separated inputs for query, key and value, and no weights.
         if self.use_multi_head_attention:
             if add_qk_str is not None:
-                logger.debug("MultiHeadAttention does not support extra_add_qk: cannot fuse the attention.")
+                logger.debug("MultiHeadAttention does not support relative_position_bias: cannot fuse the attention.")
                 return None
 
             attention_inputs = [
                 q_matmul.output[0],
                 k_matmul.output[0],
                 v_matmul.output[0],
                 attention_node_name + "_qkv_bias",
             ]
+
             if mask_index is not None:
                 attention_inputs.append(mask_index)
 
             attention_node = helper.make_node(
                 "MultiHeadAttention",
                 inputs=attention_inputs,
                 outputs=[output],
                 name=attention_node_name,
             )
         else:
             attention_inputs = [
                 input,
                 attention_node_name + "_qkv_weight",
-                attention_node_name + "_qkv_bias",
+                attention_node_name + "_qkv_bias" if has_bias else "",
             ]
             if mask_index is not None:
                 attention_inputs.append(mask_index)
             else:
                 attention_inputs.append("")
 
+            past_exists = past_k and past_v
+            if past_exists:
+                past_kv = self.concat_kv(past_k, past_v)
+                attention_inputs.append(past_kv)
+
             if add_qk_str is not None:
-                attention_inputs.append("")  # no past
-                attention_inputs.append(add_qk_str)
+                # Convert 4d mask from (B,1,M,M) to (B,N,M,M)
+                # B = batch size, M = max sequence length, N = num heads
+                concat_node_name = self.model.create_node_name("Concat")
+                mask_output_name = add_qk_str + "_mask"
+                concat_add_qk_fp32 = helper.make_node(
+                    "Concat",
+                    inputs=[add_qk_str for _ in range(num_heads)],
+                    outputs=[mask_output_name],
+                    name=concat_node_name,
+                    axis=1,
+                )
+                # Add new nodes to graph
+                self.nodes_to_add.append(concat_add_qk_fp32)
+                self.node_name_to_graph_name[concat_node_name] = self.this_graph_name
+
+                # Add attention mask to attention node
+                if not past_exists:
+                    attention_inputs.append("")
+                attention_inputs.append(mask_output_name)
+
+            attention_outputs = [output]
+            if present_k and present_v:
+                present_kv = present_k.replace(".key", "").replace("_key", "").replace(".", "_")
+                attention_outputs.append(present_kv)
+                self.split_kv(present_k, present_v, present_kv)
 
             attention_node = helper.make_node(
                 "Attention",
                 inputs=attention_inputs,
-                outputs=[output],
+                outputs=attention_outputs,
                 name=attention_node_name,
             )
+
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
 
+        if scale is not None:
+            attention_node.attribute.extend([helper.make_attribute("scale", scale)])
+
         if is_qkv_diff_dims:
             attention_node.attribute.extend(
                 [helper.make_attribute("qkv_hidden_sizes", [qw_out_size, kw_out_size, vw_out_size])]
             )
 
         if self.mask_filter_value is not None:
             attention_node.attribute.extend([helper.make_attribute("mask_filter_value", float(self.mask_filter_value))])
@@ -416,15 +922,15 @@
             )
             if qkv_nodes is not None:
                 (_, einsum_node, transpose_qkv, matmul_qkv) = qkv_nodes
             else:
                 return
 
         other_inputs = []
-        for i, input in enumerate(start_node.input):
+        for _i, input in enumerate(start_node.input):
             if input not in output_name_to_node:
                 continue
 
             if input == qkv_nodes[0].output[0]:
                 continue
             other_inputs.append(input)
         if len(other_inputs) != 1:
@@ -454,56 +960,79 @@
                 return
         elif normalize_node.op_type == "LayerNormalization":
             children = input_name_to_nodes[root_input]
             for child in children:
                 if child.op_type == "LayerNormalization":
                     root_input = child.output[0]
 
+        """
+        When Add before the LayerNormalization produces an output
+        that is consumed by some other nodes other than the LayerNormalization itself,
+        fused SkipLayerNormalization will have several outputs.
+        In this case we need to pick the one used in Attention
+
+        For example, this is the case for ViT
+
+        SkipLayerNormalization --> Attention --> MatMul --> Add --> SkipLayerNormalization
+         |                                                                     |
+         |                                                                     |
+         +---------------------------------------------------------------------+
+        """
+        parent_node = output_name_to_node[root_input]
+        if parent_node.op_type == "SkipLayerNormalization" and len(parent_node.output) == 4:
+            root_input = parent_node.output[0]
+
         children = input_name_to_nodes[root_input]
         children_types = [child.op_type for child in children]
         if children_types.count("MatMul") != 3:
             return
 
         v_nodes = self.model.match_parent_path(matmul_qkv, ["Transpose", "Reshape", "Add", "MatMul"], [1, 0, 0, None])
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
         (_, _, add_v, matmul_v) = v_nodes
 
         is_distill = False
         is_distill_add = False
+        is_no_mask_attention = False
         qk_paths = {
             "path1": (["Softmax", "Add", "Div", "MatMul"], [0, 0, None, 0]),
             "path2": (["Softmax", "Add", "Mul", "MatMul"], [0, 0, None, 0]),
             "path3": (["Softmax", "Where", "MatMul", "Div"], [0, 0, 2, 0]),
             "path4": (["Softmax", "Add", "Where", "MatMul"], [0, 0, 0, 2]),
+            "path5": (["Softmax", "Div", "MatMul"], [0, 0, 0]),
         }
 
         qk_nodes = None
         for k, v in qk_paths.items():
             qk_nodes = self.model.match_parent_path(matmul_qkv, v[0], v[1])
             if qk_nodes is None:
                 continue
             if k == "path3":
                 is_distill = True
             if k == "path4":
                 is_distill_add = True
+            if k == "path5":
+                is_no_mask_attention = True
             break
 
         if qk_nodes is None:
             logger.debug("fuse_attention: failed to match qk path")
             return
 
         add_qk = None
         matmul_qk = None
         where_qk = None
         if is_distill:
             (_, where_qk, matmul_qk, _) = qk_nodes
         elif is_distill_add:
             (_, add_qk, where_qk, matmul_qk) = qk_nodes
+        elif is_no_mask_attention:
+            (_, _, matmul_qk) = qk_nodes
         else:
             (_, add_qk, _, matmul_qk) = qk_nodes
 
         q_nodes = self.model.match_parent_path(matmul_qk, ["Transpose", "Reshape", "Add", "MatMul"], [0, 0, 0, None])
         if q_nodes is None:
             q_nodes = self.model.match_parent_path(
                 matmul_qk,
@@ -553,37 +1082,39 @@
                 output_name_to_node,
             )
             if add_qk is not None:
                 add_qk_str = self.get_add_qk_str(add_qk)
                 if add_qk_str is None:
                     logger.debug(f"fuse_attention: failed to verify shape inference of {add_qk}")
                     return
+        elif is_no_mask_attention:
+            pass
         else:
             _, mask_nodes, _ = self.model.match_parent_paths(
                 add_qk,
                 [
                     (
                         ["Mul", "Sub", "Cast", "Unsqueeze", "Unsqueeze"],
                         [None, 0, 1, 0, 0],
                     ),
                     (["Mul", "Sub", "Unsqueeze", "Unsqueeze"], [None, 0, 1, 0]),
                 ],
                 output_name_to_node,
             )
-        if mask_nodes is None:
+        if not is_no_mask_attention and mask_nodes is None:
             logger.debug("fuse_attention: failed to match mask path")
             return
 
-        if len(mask_nodes) > 1 and mask_nodes[0].op_type == "Mul":
+        if not is_no_mask_attention and len(mask_nodes) > 1 and mask_nodes[0].op_type == "Mul":
             _, mul_val = self.model.get_constant_input(mask_nodes[0])
             if mul_val != -10000:
                 self.mask_filter_value = mul_val
 
         if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_k.input[0] == root_input:
-            mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
+            mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0]) if not is_no_mask_attention else None
 
             attention_last_node = reshape_qkv if einsum_node is None else transpose_qkv
 
             q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q)
             # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
             # the input_hidden_size represents the input hidden size, this is used as needed but hidden sizes for Q, K are extracted appropriately
             new_node = self.create_attention_node(
```

## onnxruntime/transformers/fusion_attention_unet.py

```diff
@@ -16,20 +16,27 @@
 
 class FusionAttentionUnet(Fusion):
     """
     Fuse Attention subgraph of UNet into one Attention node.
     """
 
     def __init__(
-        self, model: OnnxModel, hidden_size: int, num_heads: int, is_cross_attention: bool, enable_packed_kv: bool
+        self,
+        model: OnnxModel,
+        hidden_size: int,
+        num_heads: int,
+        is_cross_attention: bool,
+        enable_packed_qkv: bool,
+        enable_packed_kv: bool,
     ):
         super().__init__(model, "MultiHeadAttention" if is_cross_attention else "Attention", ["LayerNormalization"])
         self.hidden_size = hidden_size
         self.num_heads = num_heads
         self.is_cross_attention = is_cross_attention
+        self.enable_packed_qkv = enable_packed_qkv
         self.enable_packed_kv = enable_packed_kv
 
         # Flags to show warning only once
         self.num_heads_warning = True
         self.hidden_size_warning = True
 
     def get_num_heads_and_hidden_size(self, reshape_q: NodeProto, layernorm_node: NodeProto) -> Tuple[int, int]:
@@ -88,17 +95,14 @@
     ) -> Union[NodeProto, None]:
         """Create an Attention node.
 
         Args:
             q_matmul (NodeProto): MatMul node in fully connection for Q
             k_matmul (NodeProto): MatMul node in fully connection for K
             v_matmul (NodeProto): MatMul node in fully connection for V
-            q_add (NodeProto): Add bias node in fully connection for Q
-            k_add (NodeProto): Add bias node in fully connection for K
-            v_add (NodeProto): Add bias node in fully connection for V
             num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
             hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
             input (str): input name
             output (str): output name
 
         Returns:
             Union[NodeProto, None]: the node created or None if failed.
@@ -146,43 +150,88 @@
 
         # assert q and k have same shape as expected
         if is_self_attention:
             if qw.shape != kw.shape or qw.shape != vw.shape:
                 return None
 
             qw_in_size = qw.shape[0]
-            kw_in_size = kw.shape[0]
-            vw_in_size = vw.shape[0]
-
-            assert qw_in_size == kw_in_size and kw_in_size == vw_in_size
 
             if hidden_size > 0 and hidden_size != qw_in_size:
                 raise ValueError(
                     f"Input hidden size ({hidden_size}) is not same as weight dimension of q,k,v ({qw_in_size}). "
                     "Please provide a correct input hidden size or pass in 0"
                 )
 
             # All the matrices can have the same shape or q, k matrics can have the same shape with v being different
             # For 2d weights, the shapes would be [in_size, out_size].
             # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
-            qw_out_size = np.prod(qw.shape[1:])
+            qw_out_size = int(np.prod(qw.shape[1:]))
 
-            qkv_weight = np.stack((qw, kw, vw), axis=1)
-            qkv_weight_dim = 3 * qw_out_size
+            if self.enable_packed_qkv:
+                attention_node_name = self.model.create_node_name("MultiHeadAttention")
 
-            attention_node_name = self.model.create_node_name("Attention")
+                c = qw_in_size
+                n = num_heads
+                h = qw_out_size // num_heads
 
-            weight = helper.make_tensor(
-                name=attention_node_name + "_qkv_weight",
-                data_type=TensorProto.FLOAT,
-                dims=[qw_in_size, qkv_weight_dim],
-                vals=qkv_weight.flatten().tolist(),
-            )
+                # Concat and interleave weights so that the output of fused KV GEMM has [B, S_kv, N, 3, H] shape
+                qkv_weight = np.dstack([qw.reshape(c, n, h), kw.reshape(c, n, h), vw.reshape(c, n, h)]).reshape(
+                    c, n * 3 * h
+                )
 
-            self.model.add_initializer(weight, self.this_graph_name)
+                matmul_node_name = self.model.create_node_name("MatMul", name_prefix="MatMul_QKV")
+                weight = helper.make_tensor(
+                    name=matmul_node_name + "_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[qkv_weight.shape[0], qkv_weight.shape[1]],
+                    vals=qkv_weight.flatten().tolist(),
+                )
+
+                self.model.add_initializer(weight, self.this_graph_name)
+
+                matmul_node = helper.make_node(
+                    "MatMul",
+                    inputs=[k_matmul.input[0], matmul_node_name + "_weight"],
+                    outputs=[matmul_node_name + "_out"],
+                    name=matmul_node_name,
+                )
+                self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
+
+                shape_tensor = helper.make_tensor(
+                    name=matmul_node_name + "_reshape_shape",
+                    data_type=TensorProto.INT64,
+                    dims=[5],
+                    vals=[0, 0, n, 3, h],
+                )
+                self.model.add_initializer(shape_tensor, self.this_graph_name)
+
+                reshape_node = helper.make_node(
+                    "Reshape",
+                    inputs=[matmul_node_name + "_out", matmul_node_name + "_reshape_shape"],
+                    outputs=[attention_node_name + "_input"],
+                    name=matmul_node_name + "_reshape",
+                )
+                self.node_name_to_graph_name[reshape_node.name] = self.this_graph_name
+                self.nodes_to_add.extend([matmul_node, reshape_node])
+                self.nodes_to_remove.extend([q_matmul, k_matmul, v_matmul])
+
+            else:
+                qkv_weight = np.stack((qw, kw, vw), axis=1)
+                qkv_weight_dim = 3 * qw_out_size
+
+                attention_node_name = self.model.create_node_name("Attention")
+
+                weight = helper.make_tensor(
+                    name=attention_node_name + "_qkv_weight",
+                    data_type=TensorProto.FLOAT,
+                    dims=[qw_in_size, qkv_weight_dim],
+                    vals=qkv_weight.flatten().tolist(),
+                )
+
+                self.model.add_initializer(weight, self.this_graph_name)
         else:  # cross attention
             attention_node_name = self.model.create_node_name("MultiHeadAttention")
             if self.enable_packed_kv:
                 if kw.shape != vw.shape:
                     return None
 
                 kw_in_size = kw.shape[0]
@@ -246,19 +295,22 @@
             data_type=TensorProto.FLOAT,
             dims=[qkv_bias_dim],
             vals=qkv_bias.flatten().tolist(),
         )
         self.model.add_initializer(bias, self.this_graph_name)
 
         if is_self_attention:
-            attention_inputs = [
-                input,
-                attention_node_name + "_qkv_weight",
-                attention_node_name + "_qkv_bias",
-            ]
+            if not self.enable_packed_qkv:
+                attention_inputs = [
+                    input,
+                    attention_node_name + "_qkv_weight",
+                    attention_node_name + "_qkv_bias",
+                ]
+            else:
+                attention_inputs = [attention_node_name + "_input"]
         else:
             if not self.enable_packed_kv:
                 attention_inputs = [
                     q_matmul.output[0],
                     k_matmul.output[0],
                     v_matmul.output[0],
                     attention_node_name + "_qkv_bias",
@@ -266,27 +318,31 @@
             else:
                 attention_inputs = [
                     q_matmul.output[0],
                     k_matmul.output[0],
                 ]
 
         attention_node = helper.make_node(
-            "Attention" if is_self_attention else "MultiHeadAttention",
+            "Attention" if (is_self_attention and not self.enable_packed_qkv) else "MultiHeadAttention",
             inputs=attention_inputs,
             outputs=[output],
             name=attention_node_name,
         )
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
 
         counter_name = (
             "Attention (self attention)"
-            if is_self_attention
+            if is_self_attention and not self.enable_packed_qkv
             else "MultiHeadAttention ({})".format(
-                "cross attention with packed kv" if self.enable_packed_kv else "cross attention"
+                "self attention with packed qkv"
+                if self.enable_packed_qkv
+                else "cross attention with packed kv"
+                if self.enable_packed_kv
+                else "cross attention"
             )
         )
         self.increase_counter(counter_name)
         return attention_node
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
         node_before_layernorm = self.model.match_parent(normalize_node, "Add", 0)
```

## onnxruntime/transformers/fusion_base.py

```diff
@@ -48,19 +48,19 @@
                 self.this_graph_name = graph.name
                 self.fuse(node, input_name_to_nodes, output_name_to_node)
 
         op_list = [node.op_type for node in self.nodes_to_add]
         if self.fused_count:
             for key, value in self.fused_count.items():
                 if value:
-                    logger.info(f"Fused {key} count: {value}")
+                    logger.info(f"Fused {key}: {value}")
         else:
             count = op_list.count(self.fused_op_type)
             if count > 0:
-                logger.info(f"Fused {self.description} count: {count}")
+                logger.info(f"Fused {self.description}: {count}")
 
         self.model.remove_nodes(self.nodes_to_remove)
         self.model.add_nodes(self.nodes_to_add, self.node_name_to_graph_name)
 
         if self.prune_graph:
             self.model.prune_graph()
         elif self.nodes_to_remove or self.nodes_to_add:
```

## onnxruntime/transformers/fusion_biassplitgelu.py

```diff
@@ -75,15 +75,16 @@
             return
 
         if (
             slice_before_mul.input[2] != slice_before_gelu.input[1]
         ):  # end index of slice_before_mul is start index of slice_before_gelu
             return
 
-        subgraph_nodes = start_index_nodes + [
+        subgraph_nodes = [
+            *start_index_nodes,
             end_index_nodes[0],
             mul_after_gelu,
             gelu_node,
             slice_before_mul,
             slice_before_gelu,
         ]
         subgraph_output = mul_after_gelu.output[0]
```

## onnxruntime/transformers/fusion_embedlayer.py

```diff
@@ -108,15 +108,20 @@
                 children_types != ["MatMul", "MatMul", "MatMul", "Shape", "SkipLayerNormalization"]
                 and children_types != ["Add", "MatMul", "MatMul", "MatMul", "Shape", "Shape"]
                 and children_types != ["Add", "MatMul", "MatMul", "MatMul", "Shape"]
             ):
                 logger.debug("No Attention like subgraph in children of LayerNormalization")
                 return False
         else:
-            if children_types != ["Add", "MatMul", "MatMul", "MatMul",] and children_types != [
+            if children_types != [
+                "Add",
+                "MatMul",
+                "MatMul",
+                "MatMul",
+            ] and children_types != [
                 "MatMul",
                 "MatMul",
                 "MatMul",
                 "SkipLayerNormalization",
             ]:
                 logger.debug("No Attention like subgraph in children of LayerNormalization")
                 return False
@@ -241,19 +246,19 @@
                                  /          Shape
                                 /              |
                               /              Gather (indices=1)
                              /                  |
                             /                  Add (optional, B=0)
                            /                    |
                         Gather (segment_ids) Unsqueeze (axes=0)
-                           \        |           |
-                            \     Gather      Slice (data[1,512], starts=0, ends=*, axes=1, steps=1)
-                              \    /            |
+                           \\        |           |
+                            \\     Gather      Slice (data[1,512], starts=0, ends=*, axes=1, steps=1)
+                              \\    /            |
                                 Add          Gather
-                                   \       /
+                                   \\       /
                                       Add
                                        |
                                 LayerNormalization
         """
         path = self.model.match_parent_path(
             position_embedding_gather,
             ["Slice", "Unsqueeze"],
@@ -527,32 +532,34 @@
             bool: whether there is an extra output needed out of embed layer norm node
         """
 
         nodes = self.model.get_children(add_before_layer_norm)
 
         return len(nodes) > 1
 
-    def fuse_gpt2(self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node):
+    def fuse_gpt2(
+        self, layernorm, add_before_layernorm, input_name_to_nodes, output_name_to_node, optional_segment_gather=None
+    ):
         # graph checks
-        # gpt2 has no segment embedding, subgraph pattern is like
-        #     input_ids  position_ids
-        #        |        |
-        #     Gather    Gather
-        #          \   /
-        #           Add _ _ _ _ _
-        #            |           |
-        #    LayerNormalization  |
-        #            |           |
-        #         Attention      |
-        #            |           |
-        #          Matmul        |
-        #            |          /
-        #           Add        /
-        #             \       /
-        #                Add
+        # gpt2 has optional segment embedding, subgraph pattern is like
+        #                      input_ids  position_ids
+        #                         |        |
+        #  token_ids           Gather    Gather
+        #       |                   \   /
+        #   Gather (optional)        Add _ _ _ _ _
+        #                   \         |           |
+        #                     LayerNormalization  |
+        #                             |           |
+        #                          Attention      |
+        #                             |           |
+        #                           Matmul        |
+        #                             |          /
+        #                            Add        /
+        #                              \       /
+        #                                 Add
         two_gather = self.match_two_gather(add_before_layernorm)
         if two_gather is None:
             return False
 
         word_embedding_gather, position_embedding_gather = two_gather
         input_ids = word_embedding_gather.input[1]
         position_ids = position_embedding_gather.input[1]
@@ -582,15 +589,15 @@
 
         # make the fused node
         embed_node = self.create_fused_node(
             input_ids,
             layernorm,
             word_embedding_gather,
             position_embedding_gather,
-            None,
+            optional_segment_gather,
             position_ids,
             optional_embedding_sum_output,
         )
 
         # direct the output to another add too
         self.model.replace_input_of_all_nodes(layernorm.output[0], embed_node.output[0])
         if optional_embedding_sum_output:
@@ -686,23 +693,36 @@
             position_embedding_gather,
             segment_embedding_gather,
         )
         self.finish_fusion(layernorm, embed_node)
         return True
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
+        first_add_path = self.model.match_parent_path(node, ["Add"], [0])
         if node.op_type == "LayerNormalization":
-            first_add_path = self.model.match_parent_path(node, ["Add"], [0])
             if first_add_path is None:
                 return
             add_before_layernorm = first_add_path[0]
+            optional_segment_gather = None
         else:  # SkipLayerNormalization
-            add_before_layernorm = node  # Add is fused into SkipLayerNormalization
+            gather_0_path = self.model.match_parent_path(node, ["Gather"], [0])
+            gather_1_path = self.model.match_parent_path(node, ["Gather"], [1])
+            if gather_0_path is None and gather_1_path is not None:
+                add_before_layernorm = first_add_path[0]
+                optional_segment_gather = gather_1_path[0]
+            elif gather_0_path is not None and gather_1_path is None:
+                add_before_layernorm = first_add_path[0]
+                optional_segment_gather = gather_0_path[0]
+            else:
+                add_before_layernorm = node  # Add is fused into SkipLayerNormalization
+                optional_segment_gather = None
 
-        if self.fuse_gpt2(node, add_before_layernorm, input_name_to_nodes, output_name_to_node):
+        if self.fuse_gpt2(
+            node, add_before_layernorm, input_name_to_nodes, output_name_to_node, optional_segment_gather
+        ):
             return
 
         if self.fuse_distilbert(node, add_before_layernorm, input_name_to_nodes, output_name_to_node):
             return
 
         if self.fuse_bert(node, add_before_layernorm, input_name_to_nodes, output_name_to_node):
             return
@@ -716,15 +736,15 @@
     def replace_mask(self, mask_int32, attention_nodes):
         # Inputs of EmbedLayerNorm: input_ids, segment_ids (optional), word_embedding, position_embedding,
         #           segment_embedding (optional), gamma, beta, mask (optional), position_ids (optional)
         embed_node = self.embed_node
         if len(embed_node.input) == 7:
             embed_node.input.append(mask_int32)
             logger.debug("append mask to %s", embed_node.name)
-        elif len(embed_node.input) > 7 and embed_node.input[7] == "":
+        elif len(embed_node.input) > 7 and not embed_node.input[7]:
             embed_node.input[7] = mask_int32
             logger.debug("replace mask in %s", embed_node.name)
         else:
             logger.debug("skip mask in %s", embed_node.name)
             return
 
         for attention_node in attention_nodes:
```

## onnxruntime/transformers/fusion_gelu_approximation.py

```diff
@@ -1,13 +1,13 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-from logging import getLogger
+from logging import getLogger  # noqa: F401
 
 from fusion_base import Fusion
 from onnx import helper
 from onnx_model import OnnxModel
 
 
 class FusionGeluApproximation(Fusion):
```

## onnxruntime/transformers/fusion_gpt_attention.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 
 import numpy as np
 from fusion_base import Fusion
 from fusion_utils import FusionUtils
-from onnx import TensorProto, helper, numpy_helper
+from onnx import TensorProto, helper, numpy_helper  # noqa: F401
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionGptAttentionPastBase(Fusion):
     """Base class for GPT Attention Fusion with past state"""
@@ -39,34 +39,34 @@
         #  Unsqueeze        Unsqueeze
         #        \        /
         #         \      /
         #           Concat
         #             |
         #         {present}
         gather = self.model.get_parent(concat_v, 0, output_name_to_node)
-        if gather.op_type != "Gather":
+        if gather is None or gather.op_type != "Gather":
             logger.debug("match_past_pattern_1: expect Gather for past")
             return None
 
-        if not self.model.find_constant_input(gather, 1) == 1:
+        if self.model.find_constant_input(gather, 1) != 1:
             logger.debug("match_past_pattern_1: expect indices=1 for Gather of past")
             return None
         past = gather.input[0]
 
         parent = self.model.get_parent(concat_k, 0, output_name_to_node)
-        if parent.op_type == "Gather":
+        if parent and parent.op_type == "Gather":
             gather_past_k = parent
         else:
             past_k_nodes = self.model.match_parent_path(concat_k, ["Transpose", "Gather"], [0, 0])
             if past_k_nodes is None:
                 logger.debug("match_past_pattern_1: failed match Transpose and Gather")
                 return None
             gather_past_k = past_k_nodes[-1]
 
-        if not self.model.find_constant_input(gather_past_k, 0) == 1:
+        if self.model.find_constant_input(gather_past_k, 0) != 1:
             logger.debug("match_past_pattern_1: expect indices=0 for Gather k of past")
             return None
         past_k = gather_past_k.input[0]
         if past != past_k:
             logger.debug("match_past_pattern_1: expect past to be same")
             return None
 
@@ -91,20 +91,20 @@
         #          Unsqueeze    Unsqueeze
         #                \       /
         #                 Concat
         #                   |
         #               {present}
         #
         squeeze = self.model.get_parent(concat_v, 0, output_name_to_node)
-        if squeeze.op_type != "Squeeze":
+        if squeeze is None or squeeze.op_type != "Squeeze":
             logger.debug("match_past_pattern_2: expect Squeeze as parent of concat_v")
             return None
 
         split = self.model.get_parent(squeeze, 0, output_name_to_node)
-        if split.op_type != "Split":
+        if split is None or split.op_type != "Split":
             logger.debug("match_past_pattern_2: expect Split for past path")
             return None
 
         opset_version = self.model.get_opset_version()
         if opset_version < 13:
             if not FusionUtils.check_node_attribute(squeeze, "axes", [0]):
                 logger.debug("match_past_pattern_2: axes != [0] for Squeeze in past path")
@@ -331,15 +331,15 @@
         layernorm_before_attention = fc_nodes[-1]
 
         # `another_input` will be non-None only if
         # (1) SkipLayerNorm fusion wasn't turned ON
         # (2) SkipLayerNorm fusion was turned ON but upstream layer's LayerNorm + Add was not
         # fused into a SkipLayerNorm. This can happen if the shapes to the Add node are different.
         # So, keep the following check if SkipLayerNorm fusion is turned ON or OFF.
-        if another_input is not None and not another_input in layernorm_before_attention.input:
+        if another_input is not None and another_input not in layernorm_before_attention.input:
             logger.debug("Upstream Add and (Skip)LayerNormalization shall have one same input")
             return
 
         is_unidirectional = True
         slice_mask = None
         input_mask_nodes = None
         concat_k_to_match = None
```

## onnxruntime/transformers/fusion_gpt_attention_megatron.py

```diff
@@ -1,18 +1,18 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 
 import numpy as np
-from fusion_base import Fusion
+from fusion_base import Fusion  # noqa: F401
 from fusion_gpt_attention import FusionGptAttentionPastBase
-from fusion_utils import FusionUtils
-from onnx import TensorProto, helper, numpy_helper
+from fusion_utils import FusionUtils  # noqa: F401
+from onnx import TensorProto, helper, numpy_helper  # noqa: F401
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 def is_close(value, expected_value):
     return abs(value - expected_value) <= 1e-6
```

## onnxruntime/transformers/fusion_gpt_attention_no_past.py

```diff
@@ -1,17 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 
-import numpy as np
+import numpy as np  # noqa: F401
 from fusion_base import Fusion
-from fusion_utils import FusionUtils
-from onnx import TensorProto, helper, numpy_helper
+from fusion_utils import FusionUtils  # noqa: F401
+from onnx import TensorProto, helper, numpy_helper  # noqa: F401
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionGptAttentionNoPast(Fusion):
     """
@@ -142,17 +142,17 @@
 
         # `another_input` will be non-None only if
         # (1) SkipLayerNorm fusion wasn't turned ON
         # (2) SkipLayerNorm fusion was turned ON but upstream layer's LayerNorm + Add was not
         # fused into a SkipLayerNorm. This can happen if the shapes to the Add node are different.
         # So, keep the following check if SkipLayerNorm fusion is turned ON or OFF.
         if another_input is not None:
-            if not another_input in layernorm_before_attention.input:
+            if another_input not in layernorm_before_attention.input:
                 # match openai-gpt
-                if not another_input in layernorm_before_attention.output:
+                if another_input not in layernorm_before_attention.output:
                     logger.debug("Add and (Skip)LayerNormalization shall have one same input")
                     return
 
         qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "Sub", "Mul", "Div", "MatMul"], [0, 0, 0, 0, 0])
         if qk_nodes is not None:
             (softmax_qk, sub_qk, mul_qk, div_qk, matmul_qk) = qk_nodes
             mask_nodes = self.model.match_parent_path(
```

## onnxruntime/transformers/fusion_group_norm.py

```diff
@@ -3,15 +3,14 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from logging import getLogger
 from typing import Dict
 
 import numpy as np
 from fusion_base import Fusion
-from fusion_utils import FusionUtils
 from onnx import TensorProto, helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionGroupNorm(Fusion):
@@ -139,56 +138,43 @@
             self.nodes_to_remove.extend([last_node])
         else:
             self.nodes_to_remove.extend(subgraph_nodes)
 
         # instance_norm_scale might from Constant node. Use prune graph to clear it.
         self.prune_graph = True
 
-        # Right now GroupNorm only support float16 input. Need add a Cast in fp32 model.
-        utils = FusionUtils(self.model)
-
-        input = root
-        output = last_node.output[0]
-        if weight.dtype == np.float32:
-            # Add a Cast node to get float16 input for GroupNorm
-            cast_input, _cast_node = utils.cast_input(root, "float16")
-            input = cast_input
-
-            # Add a Cast node to convert back to float32 after GroupNorm
-            output = group_norm_name + "_out"
-            cast_node = helper.make_node("Cast", inputs=[group_norm_name + "_out"], outputs=[last_node.output[0]])
-            cast_node.attribute.extend([helper.make_attribute("to", int(TensorProto.FLOAT))])
-            self.model.add_node(cast_node)
+        input_name = root
+        output_name = last_node.output[0]
 
         # NCHW to NHWC
         transpose_input = helper.make_node(
             "Transpose",
-            [input],
-            [input + "_NHWC"],
+            [input_name],
+            [input_name + "_NHWC"],
             name=self.model.create_node_name("Transpose", name_prefix="Transpose_NCHW_to_NHWC"),
             perm=[0, 2, 3, 1],
         )
 
         new_node = helper.make_node(
             "GroupNorm",
-            inputs=[input + "_NHWC", group_norm_name + "_gamma", group_norm_name + "_beta"],
-            outputs=[output + "_NHWC"],
+            inputs=[input_name + "_NHWC", group_norm_name + "_gamma", group_norm_name + "_beta"],
+            outputs=[output_name + "_NHWC"],
             name=group_norm_name,
         )
 
         new_node.attribute.extend(instance_norm.attribute)
         new_node.attribute.extend([helper.make_attribute("groups", 32)])
         new_node.attribute.extend([helper.make_attribute("activation", 1 if has_swish_activation else 0)])
         new_node.domain = "com.microsoft"
 
         # NHWC to NCHW
         transpose_output = helper.make_node(
             "Transpose",
-            [output + "_NHWC"],
-            [output],
+            [output_name + "_NHWC"],
+            [output_name],
             name=self.model.create_node_name("Transpose", name_prefix="Transpose_NHWC_to_NCHW"),
             perm=[0, 3, 1, 2],
         )
 
         self.nodes_to_add.append(new_node)
         self.nodes_to_add.append(transpose_input)
         self.nodes_to_add.append(transpose_output)
```

## onnxruntime/transformers/fusion_layernorm.py

```diff
@@ -80,15 +80,15 @@
         second_add_node = parent_nodes[1]
         i, add_weight = self.model.get_constant_input(second_add_node)
         if add_weight is None or add_weight <= 0 or add_weight > 1.0e-4:
             logger.warning(f"epsilon value is not expeced: {add_weight}")
             return
 
         pow_node = parent_nodes[3]
-        if not self.model.find_constant_input(pow_node, 2.0) == 1:
+        if self.model.find_constant_input(pow_node, 2.0) != 1:
             return
 
         mul_node = input_name_to_nodes[div_node.output[0]][0]
         if mul_node.op_type != "Mul":
             return
 
         last_add_node = input_name_to_nodes[mul_node.output[0]][0]
@@ -102,15 +102,15 @@
         subgraph_nodes.extend([last_add_node, mul_node, div_node])
         if not self.model.is_safe_to_fuse_nodes(
             subgraph_nodes,
             last_add_node.output,
             input_name_to_nodes,
             output_name_to_node,
         ):
-            logger.debug(f"It is not safe to fuse LayerNormalization node. Skip")
+            logger.debug("It is not safe to fuse LayerNormalization node. Skip")
             return
 
         weight_input = mul_node.input[1 - self.model.input_index(div_node.output[0], mul_node)]
         if not self.model.is_constant_with_specified_dimension(weight_input, 1, "layernorm weight"):
             return
 
         bias_input = last_add_node.input[1 - self.model.input_index(mul_node.output[0], last_add_node)]
```

## onnxruntime/transformers/fusion_options.py

```diff
@@ -27,37 +27,42 @@
         self.enable_layer_norm = True
         self.enable_attention = True
 
         # Use MultiHeadAttention instead of Attention operator. The difference:
         # (1) Attention has merged weights for Q/K/V projection, which might be faster in some cases since 3 MatMul is
         #     merged into one.
         # (2) Attention could only handle self attention; MultiHeadAttention could handle both self and cross attention.
-        # (3) MultiHeadAttention has only cuda implementation right now.
         self.use_multi_head_attention = False
 
         self.enable_skip_layer_norm = True
         self.enable_embed_layer_norm = True
         self.enable_bias_skip_layer_norm = True
         self.enable_bias_gelu = True
         self.enable_gelu_approximation = False
         self.enable_qordered_matmul = True
 
         self.enable_shape_inference = True
         self.enable_gemm_fast_gelu = False
 
         # Set default to sequence length for BERT model to use fused attention to speed up.
         # Note that embed layer normalization will convert 2D mask to 1D when mask type is MaskIndexEnd.
-        self.attention_mask_format = (
-            AttentionMaskFormat.MaskIndexEnd if model_type == "bert" else AttentionMaskFormat.AttentionMask
-        )
+        self.attention_mask_format = AttentionMaskFormat.AttentionMask
+        if model_type == "bert":
+            self.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
+        elif model_type == "vit":
+            self.attention_mask_format = AttentionMaskFormat.NoMask
 
         # options for stable diffusion
-        self.enable_group_norm = model_type == "unet"
-        self.enable_bias_splitgelu = model_type == "unet"
-        self.enable_packed_kv = model_type == "unet"
+        if model_type in ["unet", "vae", "clip"]:
+            self.enable_nhwc_conv = True
+            self.enable_group_norm = True
+            self.enable_bias_splitgelu = True
+            self.enable_packed_qkv = True
+            self.enable_packed_kv = True
+            self.enable_bias_add = True
 
     def use_raw_attention_mask(self, use_raw_mask=True):
         if use_raw_mask:
             self.attention_mask_format = AttentionMaskFormat.AttentionMask
         else:
             self.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
 
@@ -91,18 +96,29 @@
             options.enable_gemm_fast_gelu = True
         if args.use_mask_index:
             options.use_raw_attention_mask(False)
         if args.use_raw_attention_mask:
             options.use_raw_attention_mask(True)
         if args.no_attention_mask:
             options.disable_attention_mask()
-        if args.disable_group_norm:
-            options.enable_group_norm = False
-        if args.disable_packed_kv:
-            options.enable_packed_kv = False
+
+        if args.model_type in ["unet", "vae", "clip"]:
+            if args.disable_nhwc_conv:
+                options.enable_nhwc_conv = False
+            if args.disable_group_norm:
+                options.enable_group_norm = False
+            if args.disable_bias_splitgelu:
+                options.enable_bias_splitgelu = False
+            if args.disable_packed_qkv:
+                options.enable_packed_qkv = False
+            if args.disable_packed_kv:
+                options.enable_packed_kv = False
+            if args.disable_bias_add:
+                options.enable_bias_add = False
+
         return options
 
     @staticmethod
     def add_arguments(parser: ArgumentParser):
         parser.add_argument(
             "--disable_attention",
             required=False,
@@ -208,27 +224,58 @@
         parser.set_defaults(no_attention_mask=False)
 
         parser.add_argument(
             "--use_multi_head_attention",
             required=False,
             action="store_true",
             help="Use MultiHeadAttention instead of Attention operator for testing purpose. "
-            "Note that MultiHeadAttention might be slower than Attention since MatMul of input projection is excluded. "
-            "MultiHeadAttention has only CUDA implementation so the model can only run with cuda execution provider.",
+            "Note that MultiHeadAttention might be slower than Attention when qkv are not packed. ",
         )
         parser.set_defaults(use_multi_head_attention=False)
 
         parser.add_argument(
             "--disable_group_norm",
             required=False,
             action="store_true",
-            help="not fuse GroupNorm. Only works for model_type=unet",
+            help="not fuse GroupNorm. Only works for model_type=unet or vae",
         )
         parser.set_defaults(disable_group_norm=False)
 
         parser.add_argument(
             "--disable_packed_kv",
             required=False,
             action="store_true",
-            help="not use packed kv in cross attention. Only works for model_type=unet",
+            help="not use packed kv for cross attention in MultiHeadAttention. Only works for model_type=unet",
         )
         parser.set_defaults(disable_packed_kv=False)
+
+        parser.add_argument(
+            "--disable_packed_qkv",
+            required=False,
+            action="store_true",
+            help="not use packed qkv for self attention in MultiHeadAttention. Only works for model_type=unet",
+        )
+        parser.set_defaults(disable_packed_qkv=False)
+
+        parser.add_argument(
+            "--disable_bias_add",
+            required=False,
+            action="store_true",
+            help="not fuse BiasAdd. Only works for model_type=unet",
+        )
+        parser.set_defaults(disable_bias_add=False)
+
+        parser.add_argument(
+            "--disable_bias_splitgelu",
+            required=False,
+            action="store_true",
+            help="not fuse BiasSplitGelu. Only works for model_type=unet",
+        )
+        parser.set_defaults(disable_bias_splitgelu=False)
+
+        parser.add_argument(
+            "--disable_nhwc_conv",
+            required=False,
+            action="store_true",
+            help="Do not use NhwcConv. Only works for model_type=unet or vae",
+        )
+        parser.set_defaults(disable_nhwc_conv=False)
```

## onnxruntime/transformers/fusion_qordered_attention.py

```diff
@@ -124,15 +124,15 @@
             return
 
         if not FusionUtils.check_qdq_node_for_fusion(dequantize_qkv, self.model):
             return
 
         # Identify the root input to the Attention node
         other_inputs = []
-        for i, input in enumerate(start_node.input):
+        for _i, input in enumerate(start_node.input):
             if input not in output_name_to_node:
                 continue
 
             if input == qkv_nodes[0].output[0]:
                 continue
 
             other_inputs.append(input)
```

## onnxruntime/transformers/fusion_qordered_gelu.py

```diff
@@ -77,15 +77,15 @@
             subgraph_nodes,
             [node.output[0], downstream_quantize_node.output[0]]
             if downstream_shape_node is not None
             else downstream_quantize_node.output,
             input_name_to_nodes,
             output_name_to_node,
         ):
-            logger.debug(f"It is not safe to fuse QOrderedGelu node. Skip")
+            logger.debug("It is not safe to fuse QOrderedGelu node. Skip")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
 
         ordered_gelu_node = helper.make_node(
             "QOrderedGelu",
             inputs=[
```

## onnxruntime/transformers/fusion_qordered_layernorm.py

```diff
@@ -79,15 +79,15 @@
             subgraph_nodes,
             [node.output[0], downstream_quantize_node.output[0]]
             if downstream_shape_node is not None
             else downstream_quantize_node.output,
             input_name_to_nodes,
             output_name_to_node,
         ):
-            logger.debug(f"It is not safe to fuse QOrderedLayerNormalization node. Skip")
+            logger.debug("It is not safe to fuse QOrderedLayerNormalization node. Skip")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
 
         normalize_node = helper.make_node(
             "QOrderedLayerNormalization",
             inputs=[
```

## onnxruntime/transformers/fusion_qordered_matmul.py

```diff
@@ -166,15 +166,15 @@
 
         subgraph_nodes.extend(weight_nodes)
         subgraph_nodes.extend([downstream_quantize_node])  # Downstream Q node
 
         if not self.model.is_safe_to_fuse_nodes(
             subgraph_nodes, downstream_quantize_node.output, input_name_to_nodes, output_name_to_node
         ):
-            logger.debug(f"It is not safe to fuse QOrderedMatMul node. Skip")
+            logger.debug("It is not safe to fuse QOrderedMatMul node. Skip")
             return
 
         # Deal with the case where-in the Attention subgraph is not fused
         if transpose_node_0 is not None:
             self.model.replace_node_input(transpose_node_0, transpose_node_0.input[0], dequantize_node_0.input[0])
 
         # Make inputs
```

## onnxruntime/transformers/fusion_shape.py

```diff
@@ -54,15 +54,15 @@
                    (2d_input)
                     /       \
                 Shape       shape
                 /             \
             Gather(indices=0)  Gather(indices=1)
                 |                |
             Unsqueeze(axes=0)   Unsqueeze(axes=0)
-                   \          /
+                   \\          /
                       Concat
                         |
 
         into  (2d_input) --> Shape -->
         """
         opset_version = self.model.get_opset_version()
```

## onnxruntime/transformers/fusion_skiplayernorm.py

```diff
@@ -15,16 +15,21 @@
 
 class FusionSkipLayerNormalization(Fusion):
     """
     Fuse Add + LayerNormalization into one node: SkipLayerNormalization
     Note: This fusion does not check the input shape of Add and LayerNormalization.
     """
 
-    def __init__(self, model: OnnxModel):
-        super().__init__(model, "SkipLayerNormalization", "LayerNormalization")
+    def __init__(
+        self,
+        model: OnnxModel,
+        fused_op_type: str = "SkipLayerNormalization",
+        search_op_types: str = "LayerNormalization",
+    ):
+        super().__init__(model, fused_op_type, search_op_types)
         # Update shape inference is needed since other fusions might add new edge which does not have shape info yet.
         self.shape_infer_helper = self.model.infer_runtime_shape({"batch_size": 4, "seq_len": 7}, update=True)
 
         if self.shape_infer_helper is None:
             # TODO(tianleiwu): support subgraph in shape inference or add broadcasting in SkipLayerNormalization op.
             logger.warning("symbolic shape inference disabled or failed.")
 
@@ -33,21 +38,24 @@
 
         # In some models there is input_ids->gather->add->LayerNorm and one of input of the
         # add node is initializer with fixed shape which should not be fused into SkipLayerNorm
         if add is None:
             return
 
         for add_input in add.input:
-            if self.model.get_initializer(add_input) != None:
+            if self.model.get_initializer(add_input) is not None:
                 return
 
         # The number of input node of add should be 2
         if len(self.model.get_parents(add)) != 2:
             return
 
+        # Root Mean Square Layer Normalization
+        simplified = node.op_type == "SimplifiedLayerNormalization"
+
         if self.shape_infer_helper is not None:
             if not self.shape_infer_helper.compare_shape(add.input[0], add.input[1]):
                 logger.debug(
                     "skip SkipLayerNormalization fusion since shape of inputs (%s, %s) are not same",
                     add.input[0],
                     add.input[1],
                 )
@@ -85,20 +93,24 @@
         if (
             add is not None
             and add.op_type == "Add"
             and self.model.is_safe_to_fuse_nodes([add, node], outputs_to_keep, input_name_to_nodes, output_name_to_node)
         ):
             self.nodes_to_remove.extend([add, node])
 
-            inputs = [add.input[0], add.input[1], node.input[1], node.input[2]]
+            inputs = (
+                [add.input[0], add.input[1], node.input[1], node.input[2]]
+                if not simplified
+                else [add.input[0], add.input[1], node.input[1]]
+            )
             normalize_node = helper.make_node(
-                "SkipLayerNormalization",
+                self.fused_op_type,
                 inputs=inputs,
                 outputs=outputs,
-                name=self.model.create_node_name("SkipLayerNormalization", name_prefix="SkipLayerNorm"),
+                name=self.model.create_node_name(self.fused_op_type, name_prefix="SkipLayerNorm"),
             )
             normalize_node.domain = "com.microsoft"
 
             # Pass attribute "epsilon" from layernorm node to SkipLayerNormalization
             for att in node.attribute:
                 if att.name == "epsilon":
                     normalize_node.attribute.extend([att])
@@ -134,31 +146,32 @@
         if add_input_index >= 2:
             return
 
         (add, matmul) = nodes
 
         # bias should be one dimension
         bias_index = -1
+        bias_weight = None
         for i, input in enumerate(add.input):
             initializer = self.model.get_initializer(input)
             if initializer is None:
                 continue
             bias_index = i
             bias_weight = NumpyHelper.to_array(initializer)
             break
         if bias_weight is None:
-            logger.debug(f"Bias weight not found")
+            logger.debug("Bias weight not found")
             return
         if len(bias_weight.shape) != 1:
-            logger.debug(f"Bias weight is not 1D")
+            logger.debug("Bias weight is not 1D")
             return
 
         subgraph_nodes = [node, add]
         if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, node.output, input_name_to_nodes, output_name_to_node):
-            logger.debug(f"Skip fusing SkipLayerNormalization with Bias since it is not safe")
+            logger.debug("Skip fusing SkipLayerNormalization with Bias since it is not safe")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
         inputs = [
             node.input[1 - add_input_index],
             matmul.output[0],
             node.input[2],
```

## onnxruntime/transformers/fusion_transpose.py

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------------------------
 
 from logging import getLogger
 from typing import Dict, List
 
 from fusion_base import Fusion
 from fusion_utils import FusionUtils
-from onnx import NodeProto, helper
+from onnx import NodeProto, TensorProto, helper
 from onnx_model import OnnxModel
 
 logger = getLogger(__name__)
 
 
 class FusionTranspose(Fusion):
     def __init__(self, model: OnnxModel):
@@ -21,29 +21,30 @@
     def fuse(
         self,
         transpose_node: NodeProto,
         input_name_to_nodes: Dict[str, List[NodeProto]],
         output_name_to_node: Dict[str, NodeProto],
     ):
         """
+        Note that onnxruntime will do comprehensive transpose optimization after loading model.
+        The purpose of this fusion is to make graph clean before running onnxruntime.
+
         Case 1:
               (input)-->Transpose(perm=a)-->Transpose(perm=b)-->
         After:
               (input)-->Transpose(perm=a)-->  (this path can be removed if the output is not used anymore)
                 |
                 +----->Transpose(perm=a*b)-->
 
         Case 2 (Cast has only one child):
               (input)-->Transpose(perm=a)--> Cast -->Transpose(perm=b)-->
         After:
               (input)-->Transpose(perm=a)-->  (this path can be removed if the output is not used anymore)
                 |
                 +----->Cast --> Transpose(perm=a*b)-->
-
-
         """
         transpose_b = transpose_node
         if transpose_b.input[0] not in output_name_to_node:
             return
 
         transpose_a = output_name_to_node[transpose_b.input[0]]
         if transpose_a.op_type != "Cast":
@@ -64,18 +65,98 @@
 
         parent_permutation = OnnxModel.get_node_attribute(transpose_a, "perm")
         assert isinstance(parent_permutation, list)
 
         assert len(parent_permutation) == len(permutation)
 
         output_permutation = []
-        for j, index in enumerate(permutation):
+        for _j, index in enumerate(permutation):
             output_permutation.append(parent_permutation[index])
 
         if cast_node is None:
             if FusionUtils.skip_parent(self.model, transpose_b, transpose_a, input_name_to_nodes):
                 self.nodes_to_remove.append(transpose_a)
         else:
             if FusionUtils.skip_parent(self.model, cast_node, transpose_a, input_name_to_nodes):
                 self.nodes_to_remove.append(transpose_a)
         transpose_b.ClearField("attribute")
         transpose_b.attribute.extend([helper.make_attribute("perm", output_permutation)])
+
+
+class FusionInsertTranspose(Fusion):
+    def __init__(self, model: OnnxModel):
+        super().__init__(model, "", "GroupNorm")
+
+    def create_transpose_node(self, input_name: str, perm: List[int], output_name=None):
+        """Append a Transpose node after an input"""
+        node_name = self.model.create_node_name("Transpose")
+        if output_name is None:
+            output_name = node_name + "_out" + "-" + input_name
+        transpose_node = helper.make_node("Transpose", inputs=[input_name], outputs=[output_name], name=node_name)
+        transpose_node.attribute.extend([helper.make_attribute("perm", perm)])
+        return transpose_node
+
+    def fuse(
+        self,
+        group_norm_node: NodeProto,
+        input_name_to_nodes: Dict[str, List[NodeProto]],
+        output_name_to_node: Dict[str, NodeProto],
+    ):
+        """
+        This optimization will insert an Transpose, and onnxruntime transpose optimizer will remove it together with
+        another Transpose so that we can get effect of reducing one Transpose after onnxruntime optimization.
+        Before:
+            --> Gemm --> Unsqueeze(axes=[2]) --> Unsqueeze(axes=[3]) --> Add --> Transpose([0,2,3,1]) --> GroupNorm
+        After:
+            --> Gemm --> Unsqueeze(axes=[1]) --> Unsqueeze(axes=[2]) -->Transpose([0,3,1,2]) --> Add --> Transpose([0,2,3,1]) --> GroupNorm
+        """
+        gemm_path = self.model.match_parent_path(
+            group_norm_node, ["Transpose", "Add", "Unsqueeze", "Unsqueeze", "Gemm"], [0, 0, None, 0, 0]
+        )
+        if gemm_path is None:
+            return
+        transpose, add, unsqueeze_3, unsqueeze_2, gemm = gemm_path
+        if self.model.find_graph_output(unsqueeze_3.output[0]):
+            return
+
+        permutation = OnnxModel.get_node_attribute(transpose, "perm")
+        assert isinstance(permutation, list)
+        if permutation != [0, 2, 3, 1]:
+            return
+
+        if not (
+            self.model.get_constant_value(unsqueeze_3.input[1]) == 3
+            and self.model.get_constant_value(unsqueeze_2.input[1]) == 2
+            and len(self.model.get_children(gemm, input_name_to_nodes)) == 1
+            and len(self.model.get_children(unsqueeze_3, input_name_to_nodes)) == 1
+            and len(self.model.get_children(unsqueeze_2, input_name_to_nodes)) == 1
+        ):
+            return
+
+        # Here we use hard-coded name so that it could be shared for the whole model.
+        axes_1 = "ort_const_unsqueeze_axes_1"
+        if self.model.get_initializer(axes_1) is None:
+            axes_1_tensor = helper.make_tensor(
+                name=axes_1,
+                data_type=TensorProto.INT64,
+                dims=[1],
+                vals=[1],
+            )
+            self.model.add_initializer(axes_1_tensor, self.this_graph_name)
+
+        axes_2 = "ort_const_unsqueeze_axes_2"
+        if self.model.get_initializer(axes_2) is None:
+            axes_2_tensor = helper.make_tensor(
+                name=axes_2,
+                data_type=TensorProto.INT64,
+                dims=[1],
+                vals=[2],
+            )
+            self.model.add_initializer(axes_2_tensor, self.this_graph_name)
+
+        unsqueeze_3.input[1] = "ort_const_unsqueeze_axes_2"
+        unsqueeze_2.input[1] = "ort_const_unsqueeze_axes_1"
+        transpose_output_name = self.model.create_node_name("Transpose") + "_NCHW"
+        self.model.replace_input_of_all_nodes(unsqueeze_3.output[0], transpose_output_name)
+        new_transpose = self.create_transpose_node(unsqueeze_3.output[0], [0, 3, 1, 2], transpose_output_name)
+        self.model.add_node(new_transpose, self.this_graph_name)
+        self.increase_counter("Insert Transpose")
```

## onnxruntime/transformers/fusion_utils.py

```diff
@@ -70,37 +70,49 @@
                         break
                 if is_int32:
                     output_name = node.output[0]
                     self.model.remove_node(node)
                     self.model.replace_input_of_all_nodes(output_name, input_name)
 
     @staticmethod
-    def skip_parent(model: OnnxModel, node, parent_node, input_name_to_nodes):
+    def update_node_input(node, i, new_input_name, input_name_to_nodes):
+        old_input_reference = 0
+        if (node.input[i] in input_name_to_nodes) and node in input_name_to_nodes[node.input[i]]:
+            input_name_to_nodes[node.input[i]].remove(node)
+            old_input_reference = len(input_name_to_nodes[node.input[i]])
+
+        node.input[i] = new_input_name
+
+        if new_input_name in input_name_to_nodes:
+            input_name_to_nodes[new_input_name].append(node)
+        else:
+            input_name_to_nodes[new_input_name] = [node]
+
+        return old_input_reference
+
+    @staticmethod
+    def skip_parent(model: OnnxModel, node, parent_node, input_name_to_nodes, node_input_index=0, parent_input_index=0):
         """
         Before:
               (input)-->parent-->node-->(output)
         After:
               (input)-->parent-->
                 |
                 +----->node-->(output)
 
-        This function returns a flag about whether the parent node can be removed.
-        Note that this function assumes the node has first input links from parent!
+        This function returns a flag whether the parent node can be removed.
         """
-        parent_can_be_removed = False
-        input_name_to_nodes[node.input[0]].remove(node)
+
+        old_input_name = node.input[node_input_index]
+        new_input_name = parent_node.input[parent_input_index]
+        old_input_reference = FusionUtils.update_node_input(node, node_input_index, new_input_name, input_name_to_nodes)
+
         # We can remove the first Transpose if its output is not used (linked to graph output or other nodes) anymore.
-        if len(input_name_to_nodes[node.input[0]]) == 0 and not model.find_graph_output(
-            node.input[0]
-        ):  # checks main graph output. TODO: deal with subgraph
-            parent_can_be_removed = True
-            # self.nodes_to_remove.append(transpose_a)
+        parent_can_be_removed = (old_input_reference == 0) and not model.find_graph_output(old_input_name)
 
-        input_name_to_nodes[parent_node.input[0]].append(node)
-        node.input[0] = parent_node.input[0]
         return parent_can_be_removed
 
     @staticmethod
     def check_node_attribute(node, attribute_name: str, expected_value, default_value=None):
         """Verify that a node has expected value for an attribute.
 
         Args:
@@ -114,17 +126,15 @@
         """
         value = default_value
         for attr in node.attribute:
             if attr.name == attribute_name:
                 value = helper.get_attribute_value(attr)
 
         if isinstance(expected_value, list):
-            return (isinstance(value, ndarray) or isinstance(value, list)) and array_equal(
-                expected_value, value, equal_nan=False
-            )
+            return (isinstance(value, (ndarray, list))) and array_equal(expected_value, value, equal_nan=False)
         else:
             return value == expected_value
 
     @staticmethod
     def transpose_2d_int8_tensor(tensor: onnx_proto.TensorProto):
         """Transpose a 2-D INT8 TensorProto
         Args:
@@ -156,15 +166,15 @@
            (2) The Q/DQ node should have constant scale
            (3) The Q/DQ node should have a zero point of 0
         Args:
             node (NodeProto): a Q/DQ node to check
         Returns:
             bool: whether the check is passed or not
         """
-        if not node.op_type in {"QuantizeLinear", "DequantizeLinear"}:
+        if node.op_type not in {"QuantizeLinear", "DequantizeLinear"}:
             logger.debug(f"Provided node is not a Q/DQ node. Op Type: {node.op_type}")
 
         scale = model.get_constant_value(node.input[1])
 
         # Scale is not constant
         if scale is None:
             return False
@@ -203,17 +213,15 @@
             bool: whether the check is passed or not
         """
         assert len(node.input) > input_index
 
         value = self.model.get_constant_value(node.input[input_index])
 
         if isinstance(expected_value, list):
-            return (isinstance(value, ndarray) or isinstance(value, list)) and array_equal(
-                expected_value, value, equal_nan=False
-            )
+            return (isinstance(value, (ndarray, list))) and array_equal(expected_value, value, equal_nan=False)
         else:
             return value == expected_value
 
     def remove_identity_nodes(self):
         """Remove Identity nodes, except those right before graph output."""
         nodes_to_remove = []
         for node in self.model.nodes():
```

## onnxruntime/transformers/huggingface_models.py

```diff
@@ -154,8 +154,14 @@
         ["input_ids", "visual_feats", "visual_pos"],
         11,
         False,
         "bert",
     ),
     # "google/pegasus-xsum": (["input_ids"], 11, False, "bert"),
     # "google/pegasus-large": (["input_ids"], 11, False, "bert"),
+    # ViT
+    "google/vit-base-patch16-224": (["pixel_values"], 12, False, "vit"),
+    # Swin
+    "microsoft/swin-base-patch4-window7-224": (["pixel_values"], 12, False, "swin"),
+    "microsoft/swin-small-patch4-window7-224": (["pixel_values"], 12, False, "swin"),
+    "microsoft/swin-tiny-patch4-window7-224": (["pixel_values"], 12, False, "swin"),
 }
```

## onnxruntime/transformers/io_binding_helper.py

```diff
@@ -1,29 +1,29 @@
 import logging
-from typing import Dict, List, Union
+from typing import Dict, List
 
 import numpy
 import torch
 
 from onnxruntime import InferenceSession
 
 logger = logging.getLogger(__name__)
 
 
 class TypeHelper:
     @staticmethod
     def get_input_type(ort_session: InferenceSession, name: str) -> str:
-        for i, input in enumerate(ort_session.get_inputs()):
+        for _i, input in enumerate(ort_session.get_inputs()):
             if input.name == name:
                 return input.type
         raise ValueError(f"input name {name} not found")
 
     @staticmethod
     def get_output_type(ort_session, name: str) -> str:
-        for i, output in enumerate(ort_session.get_outputs()):
+        for _i, output in enumerate(ort_session.get_outputs()):
             if output.name == name:
                 return output.type
 
         raise ValueError(f"output name {name} not found")
 
     @staticmethod
     def ort_type_to_numpy_type(ort_type: str):
```

## onnxruntime/transformers/machine_info.py

```diff
@@ -5,17 +5,17 @@
 
 # It is used to dump machine information for Notebooks
 
 import argparse
 import json
 import logging
 import platform
-import sys
+import sys  # noqa: F401
 from os import environ
-from typing import Dict, List, Tuple, Union
+from typing import Dict, List, Tuple, Union  # noqa: F401
 
 import cpuinfo
 import psutil
 from py3nvml.py3nvml import (
     NVMLError,
     nvmlDeviceGetCount,
     nvmlDeviceGetHandleByIndex,
@@ -98,15 +98,15 @@
     def get_gpu_info_by_nvml(self) -> Dict:
         """Get GPU info using nvml"""
         gpu_info_list = []
         driver_version = None
         try:
             nvmlInit()
             driver_version = nvmlSystemGetDriverVersion()
-            deviceCount = nvmlDeviceGetCount()
+            deviceCount = nvmlDeviceGetCount()  # noqa: N806
             for i in range(deviceCount):
                 handle = nvmlDeviceGetHandleByIndex(i)
                 info = nvmlDeviceGetMemoryInfo(handle)
                 gpu_info = {}
                 gpu_info["memory_total"] = info.total
                 gpu_info["memory_available"] = info.free
                 gpu_info["name"] = nvmlDeviceGetName(handle)
```

## onnxruntime/transformers/onnx_exporter.py

```diff
@@ -12,18 +12,18 @@
 import numpy
 import torch
 from affinity_helper import AffinitySetting
 from benchmark_helper import OptimizerInfo, Precision, create_onnxruntime_session
 from huggingface_models import MODEL_CLASSES
 from quantize_helper import QuantizeHelper
 from torch_onnx_export_helper import torch_onnx_export
-from transformers import AutoConfig, AutoTokenizer, LxmertConfig, TransfoXLConfig
+from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, LxmertConfig, TransfoXLConfig
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "models", "gpt2"))
-from gpt2_helper import PRETRAINED_GPT2_MODELS, GPT2ModelNoPastState, TFGPT2ModelNoPastState
+from gpt2_helper import PRETRAINED_GPT2_MODELS, GPT2ModelNoPastState, TFGPT2ModelNoPastState  # noqa: E402
 
 os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
 
 logger = logging.getLogger(__name__)
 
 # Workaround by replacing torch.triu using self-defined op
 # Since torch.triu cannot be exported to ONNX. See https://github.com/pytorch/pytorch/issues/32968
@@ -45,16 +45,20 @@
 
 
 def restore_torch_functions():
     torch.triu = torch_func["triu"]
 
 
 def create_onnxruntime_input(vocab_size, batch_size, sequence_length, input_names, config, data_type=numpy.int64):
-    input_ids = numpy.random.randint(low=0, high=vocab_size - 1, size=(batch_size, sequence_length), dtype=data_type)
+    if config.model_type in ["vit", "swin"]:
+        input_ids = numpy.random.rand(batch_size, 3, config.image_size, config.image_size).astype(numpy.float32)
+        inputs = {"pixel_values": input_ids}
+        return inputs
 
+    input_ids = numpy.random.randint(low=0, high=vocab_size - 1, size=(batch_size, sequence_length), dtype=data_type)
     inputs = {"input_ids": input_ids}
 
     if "attention_mask" in input_names:
         attention_mask = numpy.ones([batch_size, sequence_length], dtype=data_type)
         inputs["attention_mask"] = attention_mask
 
     if "token_type_ids" in input_names:
@@ -91,15 +95,15 @@
         res_list.append(i) if not isinstance(i, (list, tuple)) else update_flatten_list(i, res_list)
     return res_list
 
 
 def build_dynamic_axes(example_inputs, outputs_flatten):
     sequence_length = example_inputs["input_ids"].shape[-1]
 
-    dynamic_axes = {key: {0: "batch_size", 1: "seq_len"} for key in example_inputs.keys()}
+    dynamic_axes = {key: {0: "batch_size", 1: "seq_len"} for key in example_inputs}
 
     output_names = ["output_" + str(i + 1) for i in range(len(outputs_flatten))]
     for i, output_name in enumerate(output_names):
         dynamic_axes[output_name] = {0: "batch_size"}
         dims = outputs_flatten[i].shape
         for j, dim in enumerate(dims):
             if dim == sequence_length:
@@ -168,15 +172,15 @@
     if not optimized_by_script:
         filename = f"{normalized_model_name}_{input_count}"
     else:
         device = "gpu" if use_gpu else "cpu"
         filename = f"{normalized_model_name}_{input_count}_{precision}_{device}"
 
     if optimized_by_onnxruntime:
-        filename += f"_ort"
+        filename += "_ort"
 
     directory = onnx_dir
     # ONNXRuntime will not write external data so the raw and optimized models shall be in same directory.
     if use_external_data and not optimized_by_onnxruntime:
         directory = os.path.join(onnx_dir, filename)
         if not os.path.exists(directory):
             os.makedirs(directory)
@@ -232,19 +236,24 @@
 
         from fusion_options import FusionOptions
         from optimizer import optimize_model
 
         if optimization_options is None:
             optimization_options = FusionOptions(model_type)
         optimization_options.use_raw_attention_mask(use_raw_attention_mask)
-        if Precision.FLOAT16 == precision:
+        if precision == Precision.FLOAT16:
             optimization_options.enable_gelu_approximation = True
-        if Precision.INT8 == precision:
+        if precision == Precision.INT8:
             optimization_options.enable_embed_layer_norm = False
 
+        # For swin models, the num_attention_heads is a list, which isn't supported yet, so set to 0 for now
+        if model_type == "swin":
+            num_attention_heads = 0
+            hidden_size = 0
+
         # Use script to optimize model.
         # Use opt_level <= 1 for models to be converted to fp16, because some fused op (like FusedGemm) has only fp32 and no fp16.
         # It is better to be conservative so we use opt_level=0 here, in case MemcpyFromHost is added to the graph by OnnxRuntime.
         opt_model = optimize_model(
             onnx_model_path,
             model_type,
             num_heads=num_attention_heads,
@@ -255,39 +264,39 @@
             only_onnxruntime=False,
         )
         if model_type == "bert_keras" or model_type == "bert_tf":
             opt_model.use_dynamic_axes()
 
         model_fusion_statistics[optimized_model_path] = opt_model.get_fused_operator_statistics()
 
-        if Precision.FLOAT16 == precision:
+        if precision == Precision.FLOAT16:
             opt_model.convert_float_to_float16(keep_io_types=True)
 
         opt_model.save_model_to_file(optimized_model_path, use_external_data_format)
     else:
         logger.info(f"Skip optimization since model existed: {optimized_model_path}")
 
 
 def modelclass_dispatcher(model_name, custom_model_class):
-    if custom_model_class != None:
+    if custom_model_class is not None:
         if custom_model_class in MODEL_CLASSES:
             return custom_model_class
         else:
             raise Exception("Valid model class: " + " ".join(MODEL_CLASSES))
 
     if model_name in PRETRAINED_GPT2_MODELS:
         return "GPT2ModelNoPastState"
 
     import re
 
-    if re.search("-squad$", model_name) != None:
+    if re.search("-squad$", model_name) is not None:
         return "AutoModelForQuestionAnswering"
-    elif re.search("-mprc$", model_name) != None:
+    elif re.search("-mprc$", model_name) is not None:
         return "AutoModelForSequenceClassification"
-    elif re.search("gpt2", model_name) != None:
+    elif re.search("gpt2", model_name) is not None:
         return "AutoModelWithLMHead"
 
     return "AutoModel"
 
 
 def load_pretrained_model(model_name, config, cache_dir, custom_model_class, is_tf_model=False):
     model_class_name = modelclass_dispatcher(model_name, custom_model_class)
@@ -435,15 +444,19 @@
                 onnx_model_path,
                 ort_model_path,
                 use_gpu,
                 overwrite,
                 model_fusion_statistics,
             )
 
-    return onnx_model_path, is_valid_onnx_model, config.vocab_size
+    return (
+        onnx_model_path,
+        is_valid_onnx_model,
+        config.num_labels if model_type in ["vit", "swin"] else config.vocab_size,
+    )
 
 
 def export_onnx_model_from_pt(
     model_name,
     opset_version,
     use_external_data_format,
     model_type,
@@ -457,25 +470,35 @@
     optimizer_info,
     validate_onnx,
     use_raw_attention_mask,
     overwrite,
     model_fusion_statistics,
     fusion_options,
 ):
-
     config, model = load_pt_model(model_name, model_class, cache_dir, config_modifier)
     # config, model = load_pt_model_from_tf(model_name)
     model.cpu()
 
-    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
-    max_input_size = (
-        tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
-    )
+    example_inputs = None
+    max_input_size = None
+
+    if model_type in ["vit", "swin"]:
+        image_processor = AutoFeatureExtractor.from_pretrained(model_name, cache_dir=cache_dir)
+        data = numpy.random.randint(
+            low=0, high=256, size=config.image_size * config.image_size * 3, dtype=numpy.uint8
+        ).reshape(config.image_size, config.image_size, 3)
+
+        example_inputs = image_processor(data, return_tensors="pt")
+    else:
+        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
+        max_input_size = (
+            tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
+        )
 
-    example_inputs = tokenizer.encode_plus("This is a sample input", return_tensors="pt")
+        example_inputs = tokenizer.encode_plus("This is a sample input", return_tensors="pt")
 
     example_inputs = filter_inputs(example_inputs, input_names)
 
     example_outputs = model(**example_inputs)
 
     assert isinstance(example_outputs, (list, tuple)), f"type of output is not list or tuple: {type(example_outputs)}"
 
@@ -491,18 +514,24 @@
         use_gpu,
         precision,
         False,
         use_external_data_format,
     )
 
     if overwrite or not os.path.exists(onnx_model_path):
-        logger.info("Exporting ONNX model to {}".format(onnx_model_path))
+        logger.info(f"Exporting ONNX model to {onnx_model_path}")
         Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
-        dynamic_axes, output_names = build_dynamic_axes(example_inputs, example_outputs_flatten)
+        dynamic_axes = None
+        output_names = None
+
+        if model_type in ["vit", "swin"]:
+            dynamic_axes, output_names = {key: {0: "pixel_values"} for key in example_inputs}, ["logits"]
+        else:
+            dynamic_axes, output_names = build_dynamic_axes(example_inputs, example_outputs_flatten)
 
         replace_torch_functions()
         torch_onnx_export(
             model=model,
             args=tuple(example_inputs.values()),
             f=onnx_model_path,
             input_names=list(example_inputs.keys()),
@@ -596,15 +625,15 @@
         example_inputs["visual_feats"] = tf.random.normal([1, 1, config.visual_feat_dim])
         example_inputs["visual_pos"] = tf.random.normal([1, 1, config.visual_pos_dim])
 
     try:
         # Use no past state for these models
         if config.use_cache:
             config.use_cache = False
-    except:
+    except Exception:
         pass
 
     example_outputs = model(example_inputs, training=False)
     output_names = None
 
     # For xlnet models, only compare the last_hidden_state output.
     if model_name == "xlnet-base-cased" or model_name == "xlnet-large-cased":
@@ -625,15 +654,15 @@
         precision,
         False,
         use_external_data_format,
     )
     tf_internal_model_path = onnx_model_path[:-5] if use_external_data_format else onnx_model_path
 
     if overwrite or not os.path.exists(tf_internal_model_path):
-        logger.info("Exporting ONNX model to {}".format(onnx_model_path))
+        logger.info(f"Exporting ONNX model to {onnx_model_path}")
         if not use_external_data_format:
             Path(tf_internal_model_path).parent.mkdir(parents=True, exist_ok=True)
 
         import zipfile
 
         import tf2onnx
```

## onnxruntime/transformers/onnx_model.py

```diff
@@ -7,15 +7,25 @@
 import os
 import sys
 from collections import deque
 from pathlib import Path
 from typing import Dict, List, Optional, Tuple
 
 from float16 import convert_float_to_float16
-from onnx import AttributeProto, GraphProto, ModelProto, NodeProto, TensorProto, helper, numpy_helper, save_model
+from onnx import (
+    AttributeProto,
+    GraphProto,
+    ModelProto,
+    NodeProto,
+    TensorProto,
+    ValueInfoProto,
+    helper,
+    numpy_helper,
+    save_model,
+)
 from shape_infer_helper import SymbolicShapeInferenceHelper
 
 logger = logging.getLogger(__name__)
 
 
 class OnnxModel:
     def __init__(self, model):
@@ -27,43 +37,45 @@
         self.shape_infer_helper: SymbolicShapeInferenceHelper = None
         self.enable_shape_infer: bool = True
         self.all_graphs: Optional[List[GraphProto]] = None
 
     def disable_shape_inference(self):
         self.enable_shape_infer = False
 
-    def infer_runtime_shape(self, dynamic_axis_mapping={}, update=False):
+    def infer_runtime_shape(self, dynamic_axis_mapping={}, update=False):  # noqa: B006
         if self.enable_shape_infer:
             if self.shape_infer_helper is None or update:
                 self.shape_infer_helper = SymbolicShapeInferenceHelper(self.model)
 
             try:
                 if self.shape_infer_helper.infer(dynamic_axis_mapping):
                     return self.shape_infer_helper
-            except:  # noqa
+            except Exception:
                 self.enable_shape_infer = False  # disable shape inference to suppress same error message.
                 print("failed in shape inference", sys.exc_info()[0])
 
         return None
 
     def input_name_to_nodes(self):
         input_name_to_nodes = {}
         for node in self.nodes():
             for input_name in node.input:
-                if input_name not in input_name_to_nodes:
-                    input_name_to_nodes[input_name] = [node]
-                else:
-                    input_name_to_nodes[input_name].append(node)
+                if input_name:  # could be empty when it is optional
+                    if input_name not in input_name_to_nodes:
+                        input_name_to_nodes[input_name] = [node]
+                    else:
+                        input_name_to_nodes[input_name].append(node)
         return input_name_to_nodes
 
     def output_name_to_node(self):
         output_name_to_node = {}
         for node in self.nodes():
             for output_name in node.output:
-                output_name_to_node[output_name] = node
+                if output_name:  # could be empty when it is optional
+                    output_name_to_node[output_name] = node
         return output_name_to_node
 
     def nodes(self):
         all_nodes = []
         for graph in self.graphs():
             for node in graph.node:
                 all_nodes.append(node)
@@ -237,15 +249,15 @@
 
         input = node.input[i]
         if input not in output_name_to_node:
             return None
 
         return output_name_to_node[input]
 
-    def match_first_parent(self, node, parent_op_type, output_name_to_node, exclude=[]):
+    def match_first_parent(self, node, parent_op_type, output_name_to_node, exclude=[]):  # noqa: B006
         """
         Find parent node based on constraints on op_type.
 
         Args:
             node (str): current node name.
             parent_op_type (str): constraint of parent node op_type.
             output_name_to_node (dict): dictionary with output name as key, and node as value.
@@ -266,15 +278,15 @@
 
     def match_parent(
         self,
         node,
         parent_op_type,
         input_index=None,
         output_name_to_node=None,
-        exclude=[],
+        exclude=[],  # noqa: B006
         return_indice=None,
     ):
         """
         Find parent node based on constraints on op_type and index.
         When input_index is None, we will find the first parent node based on constraints,
         and return_indice will be appended the corresponding input index.
 
@@ -312,26 +324,26 @@
         if parent is not None:
             logger.debug(f"Expect {parent_op_type}, Got {parent.op_type}")
 
         return None
 
     def match_parent_paths(self, node, paths, output_name_to_node):
         for i, path in enumerate(paths):
-            assert isinstance(path, List) or isinstance(path, Tuple)
+            assert isinstance(path, (List, Tuple))
             return_indice = []
             matched = self.match_parent_path(node, path[0], path[1], output_name_to_node, return_indice)
             if matched:
                 return i, matched, return_indice
         return -1, None, None
 
     def match_parent_path(
         self,
         node,
         parent_op_types,
-        parent_input_index,
+        parent_input_index=None,
         output_name_to_node=None,
         return_indice=None,
     ):
         """
         Find a sequence of input edges based on constraints on parent op_type and index.
         When input_index is None, we will find the first parent node based on constraints,
         and return_indice will be appended the corresponding input index.
@@ -343,35 +355,39 @@
             output_name_to_node (dict): dictionary with output name as key, and node as value.
             return_indice (list): a list to append the input index
                                   When there is no constraint on input index of an edge.
 
         Returns:
             parents: a list of matched parent node.
         """
-        assert len(parent_input_index) == len(parent_op_types)
+        if parent_input_index is not None:
+            assert len(parent_input_index) == len(parent_op_types)
 
         if output_name_to_node is None:
             output_name_to_node = self.output_name_to_node()
 
         current_node = node
         matched_parents = []
         for i, op_type in enumerate(parent_op_types):
             matched_parent = self.match_parent(
                 current_node,
                 op_type,
-                parent_input_index[i],
+                parent_input_index[i] if parent_input_index is not None else None,
                 output_name_to_node,
                 exclude=[],
                 return_indice=return_indice,
             )
             if matched_parent is None:
-                logger.debug(
-                    f"Failed to match index={i} parent_input_index={parent_input_index[i]} op_type={op_type}",
-                    stack_info=True,
-                )
+                if parent_input_index is not None:
+                    logger.debug(
+                        f"Failed to match index={i} parent_input_index={parent_input_index[i]} op_type={op_type}",
+                        stack_info=True,
+                    )
+                else:
+                    logger.debug(f"Failed to match index={i} op_type={op_type}", stack_info=True)
                 return None
 
             matched_parents.append(matched_parent)
             current_node = matched_parent
 
         return matched_parents
 
@@ -580,26 +596,33 @@
         self.convert_float_to_float16(use_symbolic_shape_infer=True, keep_io_types=cast_input_output)
 
     def convert_float_to_float16(self, use_symbolic_shape_infer=True, **kwargs):
         """Convert a model to half (default) or mixed precision.
            To use mixed precision, user need specify which graph inputs, outputs, operator type
            or list of nodes shall keep in float32.
 
-           By default, we use symbolic shape inference to get shape and type information.
-           If not, ONNX shape inference will be used.
+           Note that the conversion might not proceed without type information for the whole graph.
 
-           Note that symbolic/ONNX shape inference might fail, and the conversion might not proceed
-           without shape and type information.
+           By default, we use symbolic shape inference to get type information. The benefit of symbolic shape inference
+           is that it could handle fused operators in com.microsoft domain. Those operators cannot be handled in onnx shape
+           inference so symbolic shape inference is recommended for optimized model.
+
+           When symbolic shape inference is used (even if it failed), ONNX shape inference will be disabled.
+
+           Note that onnx shape inference will fail for model larger than 2GB. For large model, you have to eanble
+           symbolic shape inference. If your model is not optimized, you can also use model path to call
+           convert_float_to_float16 in float16.py (see https://github.com/microsoft/onnxruntime/pull/15067) to
+           avoid the 2GB limit.
 
         Args:
             use_symbolic_shape_infer (bool, optional): use symbolic shape inference instead of onnx shape inference.
                                                        Defaults to True.
             keep_io_types (Union[bool, List[str]], optional): boolean or a list of float32 input/output names.
                                                               If True, model inputs/outputs should be left as float32.
-                                                              Defaults to False.
+                                                              Defaults to True.
             op_block_list (List[str], optional): List of operator types to leave as float32.
                                                  Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST`.
             node_block_list (List[str], optional): List of node names to leave as float32. Defaults to None.
             force_fp16_initializers(bool): force converting all float initializers to float16.
                                            Default to false.
             min_positive_val (float, optional): minimal positive value. Defaults to 1e-7.
             max_finite_val (float, optional): maximal finite value. Defaults to 1e4.
@@ -608,15 +631,43 @@
             kwargs["keep_io_types"] = True
 
         model = self.model
         if use_symbolic_shape_infer:
             # Use symbolic shape inference since custom operators (like Gelu, SkipLayerNormalization etc)
             # are not recognized by onnx shape inference.
             shape_infer_helper = SymbolicShapeInferenceHelper(model)
-            model = shape_infer_helper.infer_shapes(model, auto_merge=True, guess_output_rank=False)
+            try:
+                model_with_shape = shape_infer_helper.infer_shapes(model, auto_merge=True, guess_output_rank=False)
+
+                # auto_merge might cause issue (see https://github.com/microsoft/onnxruntime/issues/15521)
+                # we only merge tensor data type but not shape information back to the original onnx model.
+                # Note that float16 conversion need data type but not shape information.
+                if model_with_shape is not None:
+                    name_vi = {}
+                    for vi in model_with_shape.graph.value_info:
+                        if (
+                            hasattr(vi.type, "tensor_type")
+                            and hasattr(vi.type.tensor_type, "elem_type")
+                            and vi.type.tensor_type.elem_type != TensorProto.UNDEFINED
+                            and vi.name
+                        ):
+                            vi_copy = ValueInfoProto()
+                            vi_copy.CopyFrom(vi)
+                            if hasattr(vi_copy.type.tensor_type, "shape"):
+                                vi_copy.type.tensor_type.ClearField("shape")
+                            name_vi[vi.name] = vi_copy
+                    for vi in model.graph.value_info:
+                        if vi.name in name_vi:
+                            del name_vi[vi.name]
+                    for _, vi in name_vi.items():
+                        model.graph.value_info.append(vi)
+            except Exception:
+                logger.warning(
+                    "Failed to run symbolic shape inference. Please file an issue in https://github.com/microsoft/onnxruntime."
+                )
 
         parameters = {"disable_shape_infer": use_symbolic_shape_infer}
         parameters.update(
             {
                 key: kwargs[key]
                 for key in [
                     "keep_io_types",
@@ -743,22 +794,26 @@
                 unused_nodes.append(node)
 
         self.remove_nodes(unused_nodes)
 
         if len(unused_nodes) > 0:
             logger.debug(f"Removed unused constant nodes: {len(unused_nodes)}")
 
-    def prune_graph(self, outputs=None):
+    def prune_graph(self, outputs=None, allow_remove_graph_inputs=True):
         """
-        Prune graph to keep only required outputs. It removes unnecessary inputs and nodes.
-        Nodes are not linked (directly or indirectly) to any required output will be removed.
+        Prune graph to keep only required outputs. It removes unnecessary nodes that are not linked
+        (directly or indirectly) to any required output.
+
+        There is also an option to remove graph inputs that are not used to generate any required output.
 
         Args:
             outputs (list): a list of graph outputs to retain. If it is None, all graph outputs will be kept.
+            allow_remove_graph_inputs (bool): allow remove graph inputs.
         """
+
         if len(self.graphs()) > 1:
             logger.debug("Skip prune_graph since graph has subgraph")
             return
 
         if outputs is None:
             outputs = [output.name for output in self.model.graph.output]
 
@@ -785,32 +840,36 @@
         for output in self.model.graph.output:
             if output.name not in outputs:
                 output_to_remove.append(output)
         for output in output_to_remove:
             self.model.graph.output.remove(output)
 
         # remove inputs not used by any node.
-        input_name_to_nodes = self.input_name_to_nodes()
         input_to_remove = []
-        for input in self.model.graph.input:
-            if input.name not in input_name_to_nodes:
-                input_to_remove.append(input)
-        for input in input_to_remove:
-            self.model.graph.input.remove(input)
+        if allow_remove_graph_inputs:
+            input_name_to_nodes = self.input_name_to_nodes()
+            for input in self.model.graph.input:
+                if input.name not in input_name_to_nodes:
+                    input_to_remove.append(input)
+            for input in input_to_remove:
+                self.model.graph.input.remove(input)
 
         if input_to_remove or output_to_remove or nodes_to_remove:
-            logger.info(
-                "Graph pruned: {} inputs, {} outputs and {} nodes are removed".format(
-                    len(input_to_remove), len(output_to_remove), len(nodes_to_remove)
-                )
-            )
+            removed = []
+            if input_to_remove:
+                removed.append(f"{len(input_to_remove)} inputs")
+            if output_to_remove:
+                removed.append(f"{len(output_to_remove)} outputs")
+            if nodes_to_remove:
+                removed.append(f"{len(nodes_to_remove)} nodes")
+            logger.info("Removed %s", ", ".join(removed))
 
         self.update_graph()
 
-    def update_graph(self, verbose=False):
+    def update_graph(self, verbose=False, allow_remove_graph_inputs=False):
         graph = self.model.graph
 
         remaining_input_names = []
         for node in graph.node:
             if node.op_type in ["Loop", "Scan", "If"]:
                 # TODO: handle inner graph
                 logger.debug(f"Skip update_graph since graph has operator: {node.op_type}")
@@ -820,19 +879,20 @@
                     if input_name not in remaining_input_names:
                         remaining_input_names.append(input_name)
         if verbose:
             logger.debug(f"remaining input names: {remaining_input_names}")
 
         # remove graph input that is not used
         inputs_to_remove = []
-        for input in graph.input:
-            if input.name not in remaining_input_names:
-                inputs_to_remove.append(input)
-        for input in inputs_to_remove:
-            graph.input.remove(input)
+        if allow_remove_graph_inputs:
+            for input in graph.input:
+                if input.name not in remaining_input_names:
+                    inputs_to_remove.append(input)
+            for input in inputs_to_remove:
+                graph.input.remove(input)
 
         names_to_remove = [input.name for input in inputs_to_remove]
         logger.debug(f"remove {len(inputs_to_remove)} unused inputs: {names_to_remove}")
 
         # remove weights that are not used
         weights_to_remove = []
         weights_to_keep = []
@@ -865,74 +925,72 @@
                                 output_to_remove,
                                 impacted_node,
                             )
                             return False
         return True
 
     @staticmethod
-    def graph_topological_sort(graph):
-        deps_count = [0] * len(graph.node)  # dependency count of each node
-        deps_to_nodes = {}  # input to node indice
+    def graph_topological_sort(graph, is_deterministic=False):
+        deps_set = set()  # dependency set of all node
+        sorted_node_set = set()  # sorted node set
         sorted_nodes = []  # initialize sorted_nodes
-        for node_idx, node in enumerate(graph.node):
-            # CANNOT use len(node.input) directly because input can be optional
-            deps_count[node_idx] = sum(1 for _ in node.input if _)
-            if deps_count[node_idx] == 0:  # Constant doesn't depend on any inputs
-                sorted_nodes.append(graph.node[node_idx])
-                continue
 
-            for input_name in node.input:
-                if input_name not in deps_to_nodes:
-                    deps_to_nodes[input_name] = [node_idx]
-                else:
-                    deps_to_nodes[input_name].append(node_idx)
-
-        # Note: this logic only applies to top level graph since a sub graph could use intializer from parent graph
         initializer_names = [init.name for init in graph.initializer]
         graph_input_names = [input.name for input in graph.input]
         input_names = initializer_names + graph_input_names
-        input_names.sort()
-        prev_input_name = None
+
+        if is_deterministic:
+            input_names.sort()
+
         for input_name in input_names:
-            if prev_input_name == input_name:
-                continue
+            deps_set.add(input_name)
 
-            prev_input_name = input_name
-            if input_name in deps_to_nodes:
-                for node_idx in deps_to_nodes[input_name]:
-                    deps_count[node_idx] = deps_count[node_idx] - 1
-                    if deps_count[node_idx] == 0:
-                        sorted_nodes.append(graph.node[node_idx])
-
-        start = 0
-        end = len(sorted_nodes)
-
-        while start < end:
-            for output in sorted_nodes[start].output:
-                if output in deps_to_nodes:
-                    for node_idx in deps_to_nodes[output]:
-                        deps_count[node_idx] = deps_count[node_idx] - 1
-                        if deps_count[node_idx] == 0:
-                            sorted_nodes.append(graph.node[node_idx])
-                            end = end + 1
-            start = start + 1
+        sorted_node_set_len = -1
+        graph_nodes = graph.node if not is_deterministic else sorted(graph.node, key=lambda x: x.name)
+        last_node_name = None
+        while len(sorted_node_set) != len(graph_nodes):
+            if len(sorted_node_set) == sorted_node_set_len:
+                break
+            sorted_node_set_len = len(sorted_node_set)
+            for node_idx, node in enumerate(graph_nodes):
+                if node_idx in sorted_node_set:
+                    continue
+                input_count = sum(1 for _ in node.input if _)
+                if input_count == 0:
+                    sorted_nodes.append(node)
+                    sorted_node_set.add(node_idx)
+                    for output in node.output:
+                        deps_set.add(output)
+                    continue
+                failed = False
+                for input_name in node.input:
+                    if input_name and input_name not in deps_set:
+                        failed = True
+                        last_node_name = node.name
+                if not failed:
+                    sorted_nodes.append(node)
+                    sorted_node_set.add(node_idx)
+                    for output in node.output:
+                        deps_set.add(output)
+                else:
+                    continue
 
-        if end != len(graph.node):
+        if len(sorted_node_set) != len(graph.node):
             raise RuntimeError(
-                f"Graph is not a DAG: end={end}, len(graph.node)={len(graph.node)}, graph.node[end]={graph.node[end]}"
+                f"Graph is not a DAG: len(sorted_node_set)={len(sorted_node_set)}, len(graph.node)={len(graph.node)}, failed at node {last_node_name}"
             )
 
         graph.ClearField("node")
         graph.node.extend(sorted_nodes)
 
-    def topological_sort(self):
+    def topological_sort(self, is_deterministic=False):
         # TODO: support graph_topological_sort() in subgraphs
         # for graph in self.graphs():
         #    self.graph_topological_sort(graph)
-        OnnxModel.graph_topological_sort(self.model.graph)
+        OnnxModel.graph_topological_sort(self.model.graph, is_deterministic)
 
     @staticmethod
     def save(
         model,
         output_path,
         save_as_external_data=False,
         all_tensors_to_one_file=True,
@@ -1014,14 +1072,26 @@
             int: opset version of onnx domain.
         """
         for opset in self.model.opset_import:
             if opset.domain in ["", "ai.onnx"]:
                 return opset.version
         raise RuntimeError("ONNX model has no opset for default domain")
 
+    def get_operator_statistics(self, include_domain=False):
+        """
+        Returns node count of operators.
+        """
+        op_count = {}
+        for node in self.nodes():
+            op = (node.domain + ":" if include_domain and node.domain else "") + node.op_type
+            op_count[op] = 1 if op not in op_count else (op_count[op] + 1)
+
+        logger.info(f"Operators:{op_count}")
+        return op_count
+
     @staticmethod
     def has_same_value(tensor1: TensorProto, tensor2: TensorProto) -> bool:
         """Returns True when two tensors have same value.
            Note that name can be different.
 
         Args:
             tensor1 (TensorProto): initializer 1
@@ -1030,15 +1100,15 @@
         Returns:
             bool: True when two intializers has same value.
         """
         if tensor1.data_type != tensor2.data_type or tensor1.dims != tensor2.dims:
             return False
         if tensor1.HasField("raw_data") and tensor2.HasField("raw_data"):
             return tensor1.raw_data == tensor2.raw_data
-        return numpy_helper.to_array(tensor1) == numpy_helper.to_array(tensor2)
+        return (numpy_helper.to_array(tensor1) == numpy_helper.to_array(tensor2)).all()
 
     def remove_duplicated_initializer(self):
         """Remove initializers with duplicated values, and only keep the first one.
         It could help reduce size of models (like ALBert) with shared weights.
         Note: this function does not process subgraph.
         """
         if len(self.graphs()) > 1:
@@ -1095,7 +1165,10 @@
                 if node.output[j] not in excluded:
                     if prefix + node.output[j] not in excluded:
                         node.output[j] = prefix + node.output[j]
 
         for value_info in self.model.graph.value_info:
             if value_info.name not in excluded:
                 value_info.name = prefix + value_info.name
+
+    def clean_shape_infer(self):
+        self.model.graph.ClearField("value_info")
```

## onnxruntime/transformers/onnx_model_bart.py

```diff
@@ -1,210 +1,25 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
+from typing import Optional
 
-from fusion_attention import AttentionMask, FusionAttention
+from fusion_attention import AttentionMask
+from fusion_bart_attention import FusionBartAttention
+from fusion_options import FusionOptions
 from fusion_reshape import FusionReshape
 from onnx import numpy_helper
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
-class FusionBartEncoderAttention(FusionAttention):
-    """
-    Fuse Bart Attention subgraph into one Attention node.
-    """
-
-    def __init__(
-        self,
-        model: OnnxModel,
-        hidden_size: int,
-        num_heads: int,
-        attention_mask: AttentionMask,
-    ):
-        super().__init__(model, hidden_size, num_heads, attention_mask)
-
-    def check_runtime_shape_path(
-        self,
-        reshape_qkv_2,
-        reshape_qkv_1,
-        reshape_q_2,
-        reshape_k_2,
-        reshape_v_2,
-        root_input,
-    ):
-        concat_qkv_2_path = self.model.match_parent_path(reshape_qkv_2, ["Concat"], [1])
-        if concat_qkv_2_path is None:
-            return False
-        concat_qkv_2 = concat_qkv_2_path[0]
-
-        reshape_qkv_2_path_1 = self.model.match_parent_path(concat_qkv_2, ["Unsqueeze", "Gather", "Shape"], [0, 0, 0])
-        reshape_qkv_2_path_2 = self.model.match_parent_path(concat_qkv_2, ["Unsqueeze", "Gather", "Shape"], [1, 0, 0])
-        reshape_qkv_2_path_3 = self.model.match_parent_path(concat_qkv_2, ["Unsqueeze", "Gather", "Shape"], [2, 0, 0])
-        if reshape_qkv_2_path_1 is None or reshape_qkv_2_path_2 is None or reshape_qkv_2_path_3 is None:
-            return False
-
-        _, gather_1, shape_1 = reshape_qkv_2_path_1
-        _, gather_2, shape_2 = reshape_qkv_2_path_2
-        _, _, shape_3 = reshape_qkv_2_path_3
-
-        if shape_1.input[0] != root_input or shape_2.input[0] != root_input or shape_3.input[0] != root_input:
-            return False
-
-        reshape_qkv_1_path_1 = self.model.match_parent_path(reshape_qkv_1, ["Concat", "Unsqueeze", "Gather"], [1, 0, 0])
-        reshape_qkv_1_path_2 = self.model.match_parent_path(reshape_qkv_1, ["Concat", "Unsqueeze", "Gather"], [1, 2, 0])
-        if reshape_qkv_1_path_1 is None or reshape_qkv_1_path_2 is None:
-            return False
-        if reshape_qkv_1_path_1[-1].name != gather_1.name or reshape_qkv_1_path_2[-1].name != gather_2.name:
-            return False
-
-        reshape_q_2_path = self.model.match_parent_path(reshape_q_2, ["Concat", "Unsqueeze", "Mul"], [1, 0, 0])
-        reshape_k_2_path = self.model.match_parent_path(reshape_k_2, ["Concat", "Unsqueeze", "Mul"], [1, 0, 0])
-        reshape_v_2_path = self.model.match_parent_path(reshape_v_2, ["Concat", "Unsqueeze", "Mul"], [1, 0, 0])
-        if reshape_q_2_path is None or reshape_k_2_path is None or reshape_v_2_path is None:
-            return False
-
-        mul_q = reshape_q_2_path[-1]
-        mul_k = reshape_k_2_path[-1]
-        mul_v = reshape_v_2_path[-1]
-
-        gather_1_out = gather_1.output[0]
-        if mul_q.input[0] != gather_1_out or mul_k.input[0] != gather_1_out or mul_v.input[0] != gather_1_out:
-            return False
-
-        return True
-
-    def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
-        # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
-        qkv_nodes = self.model.match_parent_path(
-            normalize_node,
-            ["Add", "MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
-            [None, 1, 0, 0, 0, 0],
-        )
-        if qkv_nodes is not None:
-            (
-                add_out,
-                matmul_out,
-                reshape_qkv_2,
-                transpose_qkv,
-                reshape_qkv_1,
-                matmul_qkv,
-            ) = qkv_nodes
-        else:
-            return
-
-        other_inputs = []
-        for i, input in enumerate(normalize_node.input):
-            if input not in output_name_to_node:
-                continue
-            if input == qkv_nodes[0].output[0]:
-                continue
-            other_inputs.append(input)
-        if len(other_inputs) != 1:
-            return
-
-        root_input = other_inputs[0]
-        children = input_name_to_nodes[root_input]
-        children_types = [child.op_type for child in children]
-        if children_types.count("MatMul") != 3:
-            return
-
-        v_nodes = self.model.match_parent_path(
-            matmul_qkv,
-            ["Reshape", "Transpose", "Reshape", "Add", "MatMul"],
-            [1, 0, 0, 0, None],
-        )
-        if v_nodes is None:
-            logger.debug("fuse_attention: failed to match v path")
-            return
-        (reshape_v_2, transpose_v, reshape_v_1, add_v, matmul_v) = v_nodes
-
-        qk_nodes = self.model.match_parent_path(matmul_qkv, ["Softmax", "MatMul"], [0, 0])
-        if qk_nodes is not None:
-            _, matmul_qk = qk_nodes
-        else:
-            return
-
-        q_nodes = self.model.match_parent_path(
-            matmul_qk,
-            ["Reshape", "Transpose", "Reshape", "Mul", "Add", "MatMul"],
-            [0, 0, 0, 0, 0, 1],
-        )
-        if q_nodes is not None:
-            reshape_q_2, _, reshape_q_1, _, add_q, matmul_q = q_nodes
-        else:
-            return
-
-        k_nodes = self.model.match_parent_path(
-            matmul_qk,
-            ["Transpose", "Reshape", "Transpose", "Reshape", "Add", "MatMul"],
-            [1, 0, 0, 0, 0, 1],
-        )
-        if k_nodes is not None:
-            _, reshape_k_2, _, reshape_k_1, add_k, matmul_k = k_nodes
-        else:
-            return
-
-        if not self.check_runtime_shape_path(
-            reshape_qkv_2,
-            reshape_qkv_1,
-            reshape_q_2,
-            reshape_k_2,
-            reshape_v_2,
-            root_input,
-        ):
-            return
-
-        if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_v.input[0] == root_input:
-
-            mask_nodes = []
-            mask_index = None
-            attention_last_node = reshape_qkv_2
-
-            num_heads, hidden_size = self.get_num_heads_and_hidden_size(reshape_q_1)
-
-            if num_heads <= 0 or hidden_size <= 0 or (hidden_size % num_heads) != 0:
-                logger.debug("fuse_attention: failed to detect num_heads or hidden_size")
-                return
-
-            new_node = self.create_attention_node(
-                mask_index,
-                matmul_q,
-                matmul_k,
-                matmul_v,
-                add_q,
-                add_k,
-                add_v,
-                num_heads,
-                hidden_size,
-                root_input,
-                attention_last_node.output[0],
-                None,
-            )
-            if new_node is None:
-                return
-
-            self.nodes_to_add.append(new_node)
-            self.node_name_to_graph_name[new_node.name] = self.this_graph_name
-
-            self.nodes_to_remove.extend([attention_last_node, transpose_qkv, matmul_qkv])
-            self.nodes_to_remove.extend(qk_nodes)
-            self.nodes_to_remove.extend(q_nodes)
-            self.nodes_to_remove.extend(k_nodes)
-            self.nodes_to_remove.extend(v_nodes)
-
-            # Use prune graph to remove mask nodes since they are shared by all attention nodes.
-            self.nodes_to_remove.extend(mask_nodes)
-            self.prune_graph = True
-
-
 class FusionBartReshape(FusionReshape):
     def __init__(self, model: OnnxModel):
         super().__init__(model)
 
     def fuse(self, reshape_node, input_name_to_nodes, output_name_to_node):
         if reshape_node.input[1] not in output_name_to_node:
             return
@@ -250,22 +65,27 @@
 
             if not (input_1[0] == -1 and input_2[0] > 0 and input_3[0] > 0):
                 return
 
             shape.extend(input_1)
             shape.extend(input_2)
             shape.extend(input_3)
-            gemm_path = self.model.match_parent_path(reshape_node, ["Add", "MatMul"], [0, 1], output_name_to_node)
-            if gemm_path is None:
+            gemm_path_with_bias = self.model.match_parent_path(
+                reshape_node, ["Add", "MatMul"], [0, 1], output_name_to_node
+            )
+            gemm_path_no_bias = self.model.match_parent_path(reshape_node, ["MatMul"], [0], output_name_to_node)
+            if gemm_path_with_bias is not None:
+                gemm_path = gemm_path_with_bias
+            elif gemm_path_no_bias is not None:
+                gemm_path = gemm_path_no_bias
+            else:
                 return
 
             top_matmul = gemm_path[-1]
             root_input = top_matmul.input[0]
-            if shape_0.input[0] != root_input:
-                return
 
             self.replace_reshape_node(shape, reshape_node, concat_node)
         else:
             (_, gather_1, shape_1) = path1
 
             gather_value = self.model.get_constant_value(gather_1.input[1])
             if gather_value == 1:
@@ -300,16 +120,20 @@
             self.replace_reshape_node(shape, reshape_node, concat_node)
 
 
 class BartOnnxModel(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
         self.attention_mask = AttentionMask(self)
-        self.attention_fusion = FusionBartEncoderAttention(self, self.hidden_size, self.num_heads, self.attention_mask)
+        self.attention_fusion = FusionBartAttention(self, self.hidden_size, self.num_heads, self.attention_mask)
         self.bart_reshape_fusion_preprocess = FusionBartReshape(self)
 
+    def optimize(self, options: Optional[FusionOptions] = None, add_dynamic_axes: bool = False):
+        self.attention_fusion.use_multi_head_attention = False if options is None else options.use_multi_head_attention
+        super().optimize(options, add_dynamic_axes)
+
     def fuse_attention(self):
         self.attention_fusion.apply()
 
     def preprocess(self):
         self.adjust_reshape_and_expand()
         self.bart_reshape_fusion_preprocess.apply()
```

## onnxruntime/transformers/onnx_model_bert.py

```diff
@@ -2,15 +2,17 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from logging import getLogger
 from typing import List, Optional
 
+from convert_to_packing_mode import PackingMode
 from fusion_attention import AttentionMask, FusionAttention
+from fusion_bart_attention import FusionBartAttention
 from fusion_biasgelu import FusionBiasGelu
 from fusion_embedlayer import FusionEmbedLayerNormalization
 from fusion_fastgelu import FusionFastGelu
 from fusion_gelu import FusionGelu
 from fusion_gelu_approximation import FusionGeluApproximation
 from fusion_gemmfastgelu import FusionGemmFastGelu
 from fusion_layernorm import FusionLayerNormalization, FusionLayerNormalizationTF
@@ -29,15 +31,15 @@
 logger = getLogger(__name__)
 
 
 class BertOptimizationOptions(FusionOptions):
     """This class is deprecated"""
 
     def __init__(self, model_type):
-        logger.warning(f"BertOptimizationOptions is depreciated. Please use FusionOptions instead.")
+        logger.warning("BertOptimizationOptions is depreciated. Please use FusionOptions instead.")
         super().__init__(model_type)
 
 
 class BertOnnxModel(OnnxModel):
     def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
         """Initialize BERT ONNX Model.
 
@@ -231,15 +233,14 @@
         """
         Update input and output shape to use dynamic axes.
         """
         bert_graph_inputs = self.get_graph_inputs_from_fused_nodes(
             casted=True
         ) + self.get_graph_inputs_from_fused_nodes(casted=False)
 
-        dynamic_batch_inputs = {}
         for input in self.model.graph.input:
             if input.name in bert_graph_inputs:
                 dim_proto = input.type.tensor_type.shape.dim[0]
                 dim_proto.dim_param = dynamic_batch_dim
                 if dynamic_seq_len is not None:
                     dim_proto = input.type.tensor_type.shape.dim[1]
                     dim_proto.dim_param = dynamic_seq_len
@@ -252,15 +253,15 @@
         self.adjust_reshape_and_expand()
         return
 
     def adjust_reshape_and_expand(self):
         nodes_to_remove = []
         for node in self.nodes():
             if node.op_type == "Reshape":
-                # Clean up unneccessary reshape nodes.
+                # Clean up unnecessary reshape nodes.
                 # Find reshape nodes with no actually data in "shape" attribute and remove.
                 reshape_shape = self.get_constant_value(node.input[1])
                 if reshape_shape is not None and reshape_shape.size == 0:
                     nodes_to_remove.extend([node])
                     self.replace_input_of_all_nodes(node.output[0], node.input[0])
                     continue
 
@@ -320,15 +321,15 @@
                     ],
                     [i, 0, 0, 0, 0, 0],
                     output_name_to_node,
                 )
                 if parent_nodes is not None:
                     (
                         cast,
-                        constantOfShape,
+                        constantOfShape,  # noqa: N806
                         concat,
                         unsqueeze,
                         gather,
                         shape,
                     ) = parent_nodes
                     if shape.input[0] == self.graph().input[0].name:
                         constantOfShape.input[0] = shape.output[0]
@@ -383,15 +384,15 @@
         self.fuse_reshape()
 
         if (options is None) or options.enable_skip_layer_norm:
             self.fuse_skip_layer_norm()
 
         if options is not None:
             self.attention_mask.set_mask_format(options.attention_mask_format)
-            if options.use_multi_head_attention:
+            if options.use_multi_head_attention and not isinstance(self.attention_fusion, FusionBartAttention):
                 self.attention_fusion = FusionAttention(
                     self, self.hidden_size, self.num_heads, self.attention_mask, options.use_multi_head_attention
                 )
 
         if (options is None) or options.enable_attention:
             self.fuse_attention()
 
@@ -479,7 +480,11 @@
         if embed == 0:
             logger.debug("Embed Layer not fused")
 
         if attention == 0:
             logger.warning("Attention not fused")
 
         return is_perfect
+
+    def convert_to_packing_mode(self, use_symbolic_shape_infer: bool = False):
+        packing_mode = PackingMode(self)
+        packing_mode.convert(use_symbolic_shape_infer)
```

## onnxruntime/transformers/onnx_model_bert_keras.py

```diff
@@ -1,20 +1,20 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-import argparse
+import argparse  # noqa: F401
 import logging
-import sys
-from collections import deque
+import sys  # noqa: F401
+from collections import deque  # noqa: F401
 
-import numpy as np
+import numpy as np  # noqa: F401
 import onnx
-from onnx import ModelProto, TensorProto, numpy_helper
+from onnx import ModelProto, TensorProto, numpy_helper  # noqa: F401
 from onnx_model_bert_tf import BertOnnxModelTF
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelKeras(BertOnnxModelTF):
     def __init__(self, model, num_heads, hidden_size):
@@ -57,15 +57,15 @@
                 continue
             logger.debug(f"Check attention input failed:{root_input}, {parent.output[0]}")
             return False, []
 
         return True, reshape_nodes
 
     def fuse_attention(self):
-        input_name_to_nodes = self.input_name_to_nodes()
+        self.input_name_to_nodes()
         output_name_to_node = self.output_name_to_node()
 
         nodes_to_remove = []
         attention_count = 0
 
         skip_layer_norm_nodes = self.get_nodes_by_op_type("SkipLayerNormalization")
         for normalize_node in skip_layer_norm_nodes:
@@ -77,22 +77,18 @@
             ]:
                 if parent.op_type == "Add":
                     parent = self.get_parent(normalize_node, 1)
                     if parent is None or parent.op_type not in [
                         "SkipLayerNormalization",
                         "EmbedLayerNormalization",
                     ]:
-                        logger.debug(
-                            "First input for skiplayernorm: {}".format(parent.op_type if parent is not None else None)
-                        )
+                        logger.debug(f"First input for skiplayernorm: {parent.op_type if parent is not None else None}")
                         continue
                 else:
-                    logger.debug(
-                        "First input for skiplayernorm: {}".format(parent.op_type if parent is not None else None)
-                    )
+                    logger.debug(f"First input for skiplayernorm: {parent.op_type if parent is not None else None}")
                     continue
             else:
                 # TODO: shall we add back the checking of children op types.
                 pass
 
             qkv_nodes = self.match_parent_path(
                 normalize_node,
@@ -223,19 +219,16 @@
 
     def preprocess(self):
         self.process_embedding()
         self.fuse_mask()
         self.skip_reshape()
 
     def skip_reshape(self):
-        input_name_to_nodes = self.input_name_to_nodes()
-        output_name_to_node = self.output_name_to_node()
-
-        nodes_to_remove = []
-        attention_count = 0
+        self.input_name_to_nodes()
+        self.output_name_to_node()
 
         count = 0
         reshape_nodes = self.get_nodes_by_op_type("Reshape")
         for reshape_node in reshape_nodes:
             parent = self.get_parent(reshape_node, 0)
             if parent is not None and parent.op_type == "Reshape":
                 reshape_node.input[0] = parent.input[0]
@@ -257,32 +250,30 @@
         word_initializer = self.get_initializer(gather_node.input[0])
         if word_initializer is None:
             logger.debug("failed to get word initializer")
             return False
 
         temp = numpy_helper.to_array(word_initializer)
         if len(temp.shape) == 2:
-            logger.info("Found word embedding. name:{}, shape:{}".format(word_initializer.name, temp.shape))
+            logger.info(f"Found word embedding. name:{word_initializer.name}, shape:{temp.shape}")
             word_embedding = word_initializer.name
         else:
-            logger.info("Failed to find word embedding. name:{}, shape:{}".format(word_initializer.name, temp.shape))
+            logger.info(f"Failed to find word embedding. name:{word_initializer.name}, shape:{temp.shape}")
             return False
 
         pos_initializer = self.get_initializer(add_node.input[1])
         if pos_initializer is not None:
             temp = numpy_helper.to_array(pos_initializer)
             if len(temp.shape) == 3 and temp.shape[0] == 1:
                 tensor = numpy_helper.from_array(temp.reshape((temp.shape[1], temp.shape[2])), "position_embedding")
                 self.add_initializer(tensor)
-                logger.info("Found position embedding. name:{}, shape:{}".format(pos_initializer.name, temp.shape[1:]))
+                logger.info(f"Found position embedding. name:{pos_initializer.name}, shape:{temp.shape[1:]}")
                 position_embedding = "position_embedding"
             else:
-                logger.info(
-                    "Failed to find position embedding. name:{}, shape:{}".format(pos_initializer.name, temp.shape)
-                )
+                logger.info(f"Failed to find position embedding. name:{pos_initializer.name}, shape:{temp.shape}")
                 return False
         else:
             pos_embed_path = self.match_parent_path(add_node, ["Gather", "Slice"], [1, 1], output_name_to_node)
             if pos_embed_path is None:
                 logger.debug("failed to match pos_embed_path")
                 return False
 
@@ -290,40 +281,36 @@
             pos_initializer = self.get_initializer(pos_gather.input[0])
             if pos_initializer is None:
                 logger.debug("failed to get pos initializer")
                 return False
 
             temp = numpy_helper.to_array(pos_initializer)
             if len(temp.shape) == 2:
-                logger.info("Found word embedding. name:{}, shape:{}".format(pos_initializer.name, temp.shape))
+                logger.info(f"Found word embedding. name:{pos_initializer.name}, shape:{temp.shape}")
                 position_embedding = pos_initializer.name
             else:
-                logger.info(
-                    "Failed to find position embedding. name:{}, shape:{}".format(pos_initializer.name, temp.shape)
-                )
+                logger.info(f"Failed to find position embedding. name:{pos_initializer.name}, shape:{temp.shape}")
                 return False
 
         gather = self.get_parent(skip_node, 1, output_name_to_node)
         if gather is None or gather.op_type != "Gather":
             logger.debug("failed to get gather")
             return False
 
         segment_initializer = self.get_initializer(gather.input[0])
         if segment_initializer is None:
             logger.debug("failed to get segment initializer")
             return False
 
         temp = numpy_helper.to_array(segment_initializer)
         if len(temp.shape) == 2:
-            logger.info("Found segment embedding. name:{}, shape:{}".format(segment_initializer.name, temp.shape))
+            logger.info(f"Found segment embedding. name:{segment_initializer.name}, shape:{temp.shape}")
             segment_embedding = segment_initializer.name
         else:
-            logger.info(
-                "Failed to find segment embedding. name:{}, shape:{}".format(segment_initializer.name, temp.shape)
-            )
+            logger.info(f"Failed to find segment embedding. name:{segment_initializer.name}, shape:{temp.shape}")
             return False
 
         logger.info("Create Embedding node")
         self.create_embedding_subgraph(node, word_embedding, segment_embedding, position_embedding)
         return True
 
     def process_embedding(self):
@@ -345,15 +332,15 @@
                 mask_path = self.match_parent_path(node, ["Sub", "Cast", "Slice", "Unsqueeze"], [0, 1, 0, 0])
                 if mask_path is None:
                     continue
                 sub_node, cast_node, slice_node, unsqueeze_node = mask_path
 
                 mask_input_name = self.attention_mask.get_first_mask()
                 if unsqueeze_node.input[0] != mask_input_name:
-                    print("Cast input {} is not mask input {}".format(unsqueeze_node.input[0], mask_input_name))
+                    print(f"Cast input {unsqueeze_node.input[0]} is not mask input {mask_input_name}")
                     continue
 
                 unsqueeze_added_1 = onnx.helper.make_node(
                     "Unsqueeze",
                     inputs=[mask_input_name],
                     outputs=["mask_fuse_unsqueeze1_output"],
                     name="Mask_UnSqueeze_1",
```

## onnxruntime/transformers/onnx_model_bert_tf.py

```diff
@@ -1,20 +1,20 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
-import argparse
+import argparse  # noqa: F401
 import logging
-import sys
-from collections import deque
+import sys  # noqa: F401
+from collections import deque  # noqa: F401
 
 import numpy as np
 import onnx
-from onnx import ModelProto, TensorProto, helper, numpy_helper
+from onnx import ModelProto, TensorProto, helper, numpy_helper  # noqa: F401
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelTF(BertOnnxModel):
     def __init__(self, model, num_heads, hidden_size):
@@ -299,38 +299,38 @@
             add_node, reshape_node, slice_node = pos_embed_path
             initializer = self.get_initializer(slice_node.input[0])
             if initializer is None:
                 continue
 
             temp = numpy_helper.to_array(initializer)
             if len(temp.shape) == 2:
-                logger.info("Found position embedding. name:{}, shape:{}".format(initializer.name, temp.shape))
+                logger.info(f"Found position embedding. name:{initializer.name}, shape:{temp.shape}")
                 position_embedding = initializer.name
             else:
-                logger.info("Failed to find position embedding. name:{}, shape:{}".format(initializer.name, temp.shape))
+                logger.info(f"Failed to find position embedding. name:{initializer.name}, shape:{temp.shape}")
                 return
 
             first_parent = self.get_parent(add_node, 0, output_name_to_node)
             if first_parent is not None and first_parent.op_type == "Add":
                 embeddings = self.get_2d_initializers_from_parent_subgraphs(first_parent)
                 if len(embeddings) != 2:
                     logger.warning(
-                        "Failed to find two embeddings (word and segment) from Add node. Found {}".format(embeddings)
+                        f"Failed to find two embeddings (word and segment) from Add node. Found {embeddings}"
                     )
                     return
 
                 word_embedding = None
                 segment_embedding = None
                 for name, shape in embeddings.items():
                     if shape[0] == 2:
                         segment_embedding = name
-                        logger.info("Found segment embedding. name:{}, shape:{}".format(name, shape))
+                        logger.info(f"Found segment embedding. name:{name}, shape:{shape}")
                     else:
                         word_embedding = name
-                        logger.info("Found words embedding. name:{}, shape:{}".format(name, shape))
+                        logger.info(f"Found words embedding. name:{name}, shape:{shape}")
 
                 if word_embedding is None or segment_embedding is None:
                     logger.info("Failed to find both word and segment embedding")
                     return
 
                 logger.info("Create Embedding node")
                 self.create_embedding_subgraph(
@@ -370,15 +370,15 @@
 
         for normalize_node in start_nodes:
             graph_name = self.get_graph_by_node(normalize_node).name
             # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
             if normalize_node.op_type == "LayerNormalization":
                 add_before_layernorm = self.match_parent(normalize_node, "Add", 0)
                 if add_before_layernorm is not None:
-                    normalize_node = add_before_layernorm
+                    normalize_node = add_before_layernorm  # noqa: PLW2901
                 else:
                     continue
             parent = self.get_parent(normalize_node, 1)
             if parent is None or parent.op_type not in [
                 "SkipLayerNormalization",
                 "LayerNormalization",
                 "Reshape",
```

## onnxruntime/transformers/onnx_model_gpt2.py

```diff
@@ -27,26 +27,25 @@
             fusion = FusionGptAttentionMegatron(self, self.num_heads)
             fusion.apply()
 
     def postprocess(self):
         """
         Remove extra reshape nodes.
         """
-        logger.debug(f"start postprocessing...")
+        logger.debug("start postprocessing...")
 
         input_name_to_nodes = self.input_name_to_nodes()
         output_name_to_node = self.output_name_to_node()
 
         reshape_count = 0
         for gemm_node in self.get_nodes_by_op_type("Gemm"):
             reshape_after_gemm = self.find_first_child_by_type(
                 gemm_node, "Reshape", input_name_to_nodes, recursive=False
             )
 
-            return_indice = []
             nodes = self.match_parent_path(gemm_node, ["Reshape", "FastGelu"], [0, 0], output_name_to_node)
             if nodes is None:
                 nodes = self.match_parent_path(
                     gemm_node,
                     ["Reshape", "LayerNormalization"],
                     [0, 0],
                     output_name_to_node,
```

## onnxruntime/transformers/onnx_model_tnlr.py

```diff
@@ -1,14 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 from typing import Union
 
+import numpy as np
 from fusion_attention import AttentionMask, FusionAttention
 from fusion_utils import NumpyHelper
 from onnx import NodeProto, TensorProto, helper, numpy_helper
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
@@ -36,15 +37,14 @@
         add: NodeProto,
         num_heads: int,
         hidden_size: int,
         input: str,
         output: str,
         add_qk_str: str,
     ) -> Union[NodeProto, None]:
-
         assert num_heads > 0
         if hidden_size > 0 and (hidden_size % num_heads) != 0:
             logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
             return None
 
         weight = self.model.get_initializer(matmul.input[1])
         bias = self.model.get_initializer(add.input[1]) or self.model.get_initializer(add.input[0])
@@ -119,15 +119,15 @@
         )
         if qkv_nodes is not None:
             (_, _, matmul_below, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes
         else:
             return
 
         other_inputs = []
-        for i, input in enumerate(start_node.input):
+        for _i, input in enumerate(start_node.input):
             if input not in output_name_to_node:
                 continue
 
             if input == qkv_nodes[0].output[0]:
                 continue
             other_inputs.append(input)
         if len(other_inputs) != 1:
@@ -168,16 +168,16 @@
             [1, 0, 0, 0, 1],
         )
         if k_nodes is None:
             return
         add = k_nodes[-2]
         matmul = k_nodes[-1]
 
-        extra_add_qk_nodes = self.model.match_parent_path(add_qk, ["Reshape", "Where"], [1, 0])
-        if extra_add_qk_nodes is None:
+        relative_position_bias_nodes = self.model.match_parent_path(add_qk, ["Reshape", "Where"], [1, 0])
+        if relative_position_bias_nodes is None:
             return
 
         if matmul.input[0] == root_input:
             mask_index = None
             attention_last_node = reshape_qkv
             # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
             # the input_hidden_size represents the input hidden size, this is used as needed but hidden sizes for Q, K are extracted appropriately
@@ -185,15 +185,15 @@
                 mask_index,
                 matmul,
                 add,
                 self.num_heads,
                 self.hidden_size,
                 root_input,
                 attention_last_node.output[0],
-                extra_add_qk_nodes[0].input[0],
+                relative_position_bias_nodes[0].input[0],
             )
             if new_node is None:
                 return
 
             self.nodes_to_add.append(new_node)
             self.node_name_to_graph_name[new_node.name] = self.this_graph_name
```

## onnxruntime/transformers/onnx_model_unet.py

```diff
@@ -3,19 +3,20 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from logging import getLogger
 from typing import Optional
 
 from fusion_attention_unet import FusionAttentionUnet
+from fusion_bias_add import FusionBiasAdd
 from fusion_biassplitgelu import FusionBiasSplitGelu
 from fusion_group_norm import FusionGroupNorm
 from fusion_nhwc_conv import FusionNhwcConv
 from fusion_options import FusionOptions
-from fusion_transpose import FusionTranspose
+from fusion_transpose import FusionInsertTranspose, FusionTranspose
 from onnx import ModelProto
 from onnx_model import OnnxModel
 from onnx_model_bert import BertOnnxModel
 
 logger = getLogger(__name__)
 
 
@@ -32,15 +33,14 @@
 
         super().__init__(model, num_heads=num_heads, hidden_size=hidden_size)
 
     def preprocess(self):
         self.remove_useless_div()
 
     def postprocess(self):
-        self.merge_sequential_transpose()
         self.prune_graph()
         self.remove_unused_constant()
 
     def remove_useless_div(self):
         """Remove Div by 1"""
         div_nodes = [node for node in self.nodes() if node.op_type == "Div"]
 
@@ -50,22 +50,22 @@
                 nodes_to_remove.append(div)
 
         for node in nodes_to_remove:
             self.replace_input_of_all_nodes(node.output[0], node.input[0])
 
         if nodes_to_remove:
             self.remove_nodes(nodes_to_remove)
-            logger.info("Removed %d useless Div (by 1) nodes", len(nodes_to_remove))
+            logger.info("Removed %d Div nodes", len(nodes_to_remove))
 
     def convert_conv_to_nhwc(self):
         # Do not update weight here since save external data has a bug
         conv_to_nhwc_conv = FusionNhwcConv(self, update_weight=False)
         conv_to_nhwc_conv.apply()
 
-    def merge_sequential_transpose(self):
+    def merge_adjacent_transpose(self):
         fusion_transpose = FusionTranspose(self)
         fusion_transpose.apply()
 
         remove_count = 0
         nodes = self.get_nodes_by_op_type("Transpose")
         for node in nodes:
             permutation = OnnxModel.get_node_attribute(node, "perm")
@@ -85,14 +85,33 @@
             self.remove_node(node)
             remove_count += 1
 
         total = len(fusion_transpose.nodes_to_remove) + remove_count
         if total:
             logger.info("Removed %d Transpose nodes", total)
 
+    def fuse_multi_head_attention(self, options: Optional[FusionOptions] = None):
+        # Self Attention
+        enable_packed_qkv = (options is None) or options.enable_packed_qkv
+        self_attention_fusion = FusionAttentionUnet(
+            self, self.hidden_size, self.num_heads, False, enable_packed_qkv, False
+        )
+        self_attention_fusion.apply()
+
+        # Cross Attention
+        enable_packed_kv = (options is None) or options.enable_packed_kv
+        cross_attention_fusion = FusionAttentionUnet(
+            self, self.hidden_size, self.num_heads, True, False, enable_packed_kv
+        )
+        cross_attention_fusion.apply()
+
+    def fuse_bias_add(self):
+        fusion = FusionBiasAdd(self)
+        fusion.apply()
+
     def optimize(self, options: Optional[FusionOptions] = None):
         if (options is not None) and not options.enable_shape_inference:
             self.disable_shape_inference()
 
         self.utils.remove_identity_nodes()
 
         # Remove cast nodes that having same data type of input and output based on symbolic shape inference.
@@ -108,62 +127,65 @@
 
         self.fuse_reshape()
 
         if (options is None) or options.enable_group_norm:
             group_norm_fusion = FusionGroupNorm(self)
             group_norm_fusion.apply()
 
+            insert_transpose_fusion = FusionInsertTranspose(self)
+            insert_transpose_fusion.apply()
+
         if (options is None) or options.enable_bias_splitgelu:
             bias_split_gelu_fusion = FusionBiasSplitGelu(self)
             bias_split_gelu_fusion.apply()
 
         if (options is None) or options.enable_attention:
-            self_attention_fusion = FusionAttentionUnet(self, self.hidden_size, self.num_heads, False, False)
-            self_attention_fusion.apply()
-
-            enable_packed_kv = (options is None) or options.enable_packed_kv
-            cross_attention_fusion = FusionAttentionUnet(self, self.hidden_size, self.num_heads, True, enable_packed_kv)
-            cross_attention_fusion.apply()
+            self.fuse_multi_head_attention(options)
 
         if (options is None) or options.enable_skip_layer_norm:
             self.fuse_skip_layer_norm()
 
         self.fuse_shape()
 
         # Remove reshape nodes that having same shape of input and output based on symbolic shape inference.
         self.utils.remove_useless_reshape_nodes()
 
-        self.convert_conv_to_nhwc()
-
         if (options is None) or options.enable_bias_skip_layer_norm:
             # Fuse SkipLayerNormalization and Add Bias before it.
             self.fuse_add_bias_skip_layer_norm()
 
         if options is not None and options.enable_gelu_approximation:
             self.gelu_approximation()
 
+        if options is None or options.enable_nhwc_conv:
+            self.convert_conv_to_nhwc()
+
+            self.merge_adjacent_transpose()
+
+        if options is not None and options.enable_bias_add:
+            self.fuse_bias_add()
+
         self.postprocess()
 
         logger.info(f"opset version: {self.get_opset_version()}")
 
     def get_fused_operator_statistics(self):
         """
         Returns node count of fused operators.
         """
         op_count = {}
         ops = [
             "Attention",
             "MultiHeadAttention",
-            "Gelu",
-            "FastGelu",
             "LayerNormalization",
             "SkipLayerNormalization",
             "BiasSplitGelu",
             "GroupNorm",
             "NhwcConv",
+            "BiasAdd",
         ]
         for op in ops:
             nodes = self.get_nodes_by_op_type(op)
             op_count[op] = len(nodes)
 
         logger.info(f"Optimized operators:{op_count}")
         return op_count
```

## onnxruntime/transformers/optimizer.py

```diff
@@ -25,17 +25,20 @@
 import coloredlogs
 from fusion_options import FusionOptions
 from onnx import ModelProto, load_model
 from onnx_model_bart import BartOnnxModel
 from onnx_model_bert import BertOnnxModel
 from onnx_model_bert_keras import BertOnnxModelKeras
 from onnx_model_bert_tf import BertOnnxModelTF
+from onnx_model_clip import ClipOnnxModel
 from onnx_model_gpt2 import Gpt2OnnxModel
+from onnx_model_t5 import T5OnnxModel
 from onnx_model_tnlr import TnlrOnnxModel
 from onnx_model_unet import UnetOnnxModel
+from onnx_model_vae import VaeOnnxModel
 
 logger = logging.getLogger(__name__)
 
 # Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx), and default opt_level
 MODEL_TYPES = {
     "bart": (BartOnnxModel, "pytorch", 1),
     "bert": (BertOnnxModel, "pytorch", 1),
@@ -44,24 +47,31 @@
     "gpt2": (Gpt2OnnxModel, "pytorch", 1),
     "gpt2_tf": (
         Gpt2OnnxModel,
         "tf2onnx",
         0,
     ),  # might add a class for GPT2OnnxModel for TF later.
     "tnlr": (TnlrOnnxModel, "pytorch", 1),
+    "t5": (T5OnnxModel, "pytorch", 2),
+    # Stable Diffusion models
     "unet": (UnetOnnxModel, "pytorch", 1),
+    "vae": (VaeOnnxModel, "pytorch", 1),
+    "clip": (ClipOnnxModel, "pytorch", 1),
+    "vit": (BertOnnxModel, "pytorch", 1),
+    "swin": (BertOnnxModel, "pytorch", 1),
 }
 
 
 def optimize_by_onnxruntime(
     onnx_model_path: str,
     use_gpu: bool = False,
     optimized_model_path: Optional[str] = None,
     opt_level: Optional[int] = 99,
-    disabled_optimizers=[],
+    disabled_optimizers=[],  # noqa: B006
+    verbose=False,
 ) -> str:
     """
     Use onnxruntime to optimize model.
 
     Args:
         onnx_model_path (str): the path of input onnx model.
         use_gpu (bool): whether the optimized model is targeted to run in GPU.
@@ -92,32 +102,33 @@
 
     if optimized_model_path is None:
         path_prefix = onnx_model_path[:-5]  # remove .onnx suffix
         optimized_model_path = "{}_o{}_{}.onnx".format(path_prefix, opt_level, "gpu" if use_gpu else "cpu")
 
     sess_options.optimized_model_filepath = optimized_model_path
 
+    if verbose:
+        print("Using onnxruntime to optimize model - Debug level Set to verbose")
+        sess_options.log_severity_level = 0
+
     kwargs = {}
     if disabled_optimizers:
         kwargs["disabled_optimizers"] = disabled_optimizers
 
     if not use_gpu:
-        session = onnxruntime.InferenceSession(
-            onnx_model_path, sess_options, providers=["CPUExecutionProvider"], **kwargs
-        )
+        onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=["CPUExecutionProvider"], **kwargs)
     else:
         gpu_ep = []
 
         if torch_version.cuda:
             gpu_ep.append("CUDAExecutionProvider")
         elif torch_version.hip:
             gpu_ep.append("MIGraphXExecutionProvider")
             gpu_ep.append("ROCMExecutionProvider")
-
-        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=gpu_ep, **kwargs)
+        onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=gpu_ep, **kwargs)
         assert not set(onnxruntime.get_available_providers()).isdisjoint(
             ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
         )
 
     assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)
     logger.debug("Save optimized model by onnxruntime to %s", optimized_model_path)
     return optimized_model_path
@@ -131,30 +142,30 @@
     optimization_options: Optional[FusionOptions] = None,
 ):
     """Optimize Model by graph fusion logic.
 
     Note that ONNXRuntime graph optimizations (like constant folding) will not be applied. So it is better to enable
     constant folding during exporting ONNX model, or run optimize_by_onnxruntime on the model first like optimize_model.
 
-    For BERT model, num_heads and hidden_size are optional. For other model types, you need specify these parameters.
+    For BERT model, num_heads and hidden_size are optional. For other model types, you need to specify these parameters.
 
     Args:
         model (ModelProto): model object
         model_type (str, optional): model type - like bert, bert_tf, bert_keras or gpt2. Defaults to 'bert'.
         num_heads (int, optional): number of attention heads. Defaults to 0.
                                    0 allows detect the parameter from graph automatically.
         hidden_size (int, optional): hidden size. Defaults to 0.
                                      0 allows detect the parameter from graph automatically.
         optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions.
                                                         Defaults to None.
 
      Returns:
         object of an optimizer class.
     """
-    if model_type not in ["bert", "unet"] and (num_heads == 0 or hidden_size == 0):
+    if model_type not in ["bert", "swin", "unet", "vae", "clip"] and (num_heads == 0 or hidden_size == 0):
         logger.warning(f"Please specify parameters of num_heads and hidden_size for model_type {model_type}")
 
     (optimizer_class, producer, _) = MODEL_TYPES[model_type]
 
     if model.producer_name and producer != model.producer_name:
         logger.warning(
             f'Model producer not matched: Expected "{producer}", Got "{model.producer_name}".'
@@ -183,18 +194,19 @@
     model_type: str = "bert",
     num_heads: int = 0,
     hidden_size: int = 0,
     optimization_options: Optional[FusionOptions] = None,
     opt_level: Optional[int] = None,
     use_gpu: bool = False,
     only_onnxruntime: bool = False,
+    verbose=False,
 ):
     """Optimize Model by OnnxRuntime and/or python fusion logic.
 
-    ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/resources/graph-optimizations.html).
+    ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/performance/graph-optimizations.html).
     However, the coverage is limited. We also have graph fusions that implemented in Python to improve the coverage.
     They can combined: ONNX Runtime will run first when opt_level > 0, then graph fusions in Python will be applied.
 
     To use ONNX Runtime only and no Python fusion logic, use only_onnxruntime flag and a positive opt_level like
         optimize_model(input, opt_level=1, use_gpu=False, only_onnxruntime=True)
 
     When opt_level is None, we will choose default optimization level according to model type.
@@ -244,33 +256,35 @@
         # Disable some optimizers that might cause failure in symbolic shape inference or attention fusion.
         disabled_optimizers += (
             []
             if only_onnxruntime
             else [
                 "MatMulScaleFusion",
                 "MatMulAddFusion",
-                "SimplifiedLayerNormFusion",
+                "MatmulTransposeFusion",
                 "GemmActivationFusion",
                 "BiasSoftmaxFusion",
             ]
         )
         temp_model_path = optimize_by_onnxruntime(
             input,
             use_gpu=use_gpu,
             opt_level=opt_level,
             disabled_optimizers=disabled_optimizers,
+            verbose=verbose,
         )
     elif opt_level == 1:
         # basic optimizations (like constant folding and cast elimination) are not specified to execution provider.
         # CPU provider is used here so that there is no extra node for GPU memory copy.
         temp_model_path = optimize_by_onnxruntime(
             input,
             use_gpu=False,
             opt_level=1,
             disabled_optimizers=disabled_optimizers,
+            verbose=verbose,
         )
 
     if only_onnxruntime and not temp_model_path:
         logger.warning("Please specify a positive value for opt_level when only_onnxruntime is True")
 
     model = load_model(temp_model_path or input)
 
@@ -278,15 +292,15 @@
         optimizer = optimizer_class(model, num_heads, hidden_size)
     else:
         optimizer = optimize_by_fusion(model, model_type, num_heads, hidden_size, optimization_options)
 
     # Remove the temporary model.
     if temp_model_path:
         os.remove(temp_model_path)
-        logger.debug("Remove temporary model: {}".format(temp_model_path))
+        logger.debug(f"Remove temporary model: {temp_model_path}")
 
     return optimizer
 
 
 def get_fusion_statistics(optimized_model_path: str) -> Dict[str, int]:
     """
     Get counter of fused operators in optimized model.
@@ -394,14 +408,30 @@
         "--use_external_data_format",
         required=False,
         action="store_true",
         help="use external data format to store large model (>2GB)",
     )
     parser.set_defaults(use_external_data_format=False)
 
+    parser.add_argument(
+        "--disable_symbolic_shape_infer",
+        required=False,
+        action="store_true",
+        help="diable symoblic shape inference",
+    )
+    parser.set_defaults(disable_symbolic_shape_infer=False)
+
+    parser.add_argument(
+        "--convert_to_packing_mode",
+        required=False,
+        action="store_true",
+        help="convert the model to packing mode. Only available for BERT like model",
+    )
+    parser.set_defaults(convert_to_packing_mode=False)
+
     args = parser.parse_args()
 
     return args
 
 
 def _setup_logger(verbose):
     if verbose:
@@ -438,17 +468,24 @@
 
     if args.float16:
         optimizer.convert_float_to_float16(keep_io_types=True)
 
     if args.input_int32:
         optimizer.change_graph_inputs_to_int32()
 
-    optimizer.save_model_to_file(args.output, args.use_external_data_format)
+    if args.model_type in ["bert", "gpt2"]:
+        if optimizer.is_fully_optimized():
+            logger.info("The model has been fully optimized.")
+        else:
+            logger.info("The model has been optimized.")
+
+    if args.convert_to_packing_mode:
+        if args.model_type == "bert":
+            optimizer.convert_to_packing_mode(not args.disable_symbolic_shape_infer)
+        else:
+            logger.warning("Packing mode only supports BERT like models")
 
-    if optimizer.is_fully_optimized():
-        logger.info("The model has been fully optimized.")
-    else:
-        logger.info("The model has been optimized.")
+    optimizer.save_model_to_file(args.output, args.use_external_data_format)
 
 
 if __name__ == "__main__":
     main()
```

## onnxruntime/transformers/profiler.py

```diff
@@ -175,15 +175,15 @@
     profile_file = session.end_profiling()
     return profile_file
 
 
 def load_profile_json(profile_file):
     print(f"loading profile output {profile_file} ...")
 
-    with open(profile_file, "r") as opened_file:
+    with open(profile_file) as opened_file:
         sess_time = json.load(opened_file)
 
     assert isinstance(sess_time, list)
     return sess_time
 
 
 def parse_kernel_results(sess_time, threshold=0):
@@ -252,15 +252,15 @@
     for kernel_name, op_name in kernel_name_to_op_name.items():
         duration = kernel_time[kernel_name]
         if op_name in op_time:
             op_time[op_name] += duration
         else:
             op_time[op_name] = duration
 
-    lines.append(f"\nGroup kernel time by operator:")
+    lines.append("\nGroup kernel time by operator:")
     lines.append("-" * 64)
     lines.append("Total(μs)\tTime%\tOperator")
     for op_name, duration in sorted(op_time.items(), key=lambda x: x[1], reverse=True):
         ratio = duration / total
         lines.append(f"{duration:10d}\t{ratio * 100.0:5.2f}\t{op_name}")
 
     return lines
```

## onnxruntime/transformers/quantize_helper.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
 import os
 
-import onnx
+import onnx  # noqa: F401
 import torch
 from transformers.modeling_utils import Conv1D
 
 logger = logging.getLogger(__name__)
 
 
 def _conv1d_to_linear(module):
```

## onnxruntime/transformers/shape_infer_helper.py

```diff
@@ -11,15 +11,15 @@
 # In ORT Package the symbolic_shape_infer.py is in ../tools
 file_path = os.path.dirname(__file__)
 if os.path.exists(os.path.join(file_path, "../tools/symbolic_shape_infer.py")):
     sys.path.append(os.path.join(file_path, "../tools"))
 else:
     sys.path.append(os.path.join(file_path, ".."))
 
-from symbolic_shape_infer import SymbolicShapeInference, get_shape_from_type_proto, sympy
+from symbolic_shape_infer import SymbolicShapeInference, get_shape_from_type_proto, sympy  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
 
 class SymbolicShapeInferenceHelper(SymbolicShapeInference):
     def __init__(self, model, verbose=0, int_max=2**31 - 1, auto_merge=True, guess_output_rank=False):
         super().__init__(int_max, auto_merge, guess_output_rank, verbose)
```

## onnxruntime/transformers/shape_optimizer.py

```diff
@@ -6,20 +6,20 @@
 # This tool is not used directly in bert optimization. It could assist developing the optimization script on the following senarios:
 # (1) It could simplify graph by removing many sub-graphs related to reshape.
 # (2) It could reduce extra inputs and outputs to fit other tools. The script compare_bert_results.py or bert_perf_test.py requires 3 inputs.
 
 import argparse
 import logging
 import os
-import re
+import re  # noqa: F401
 import sys
 import tempfile
-from collections import deque
+from collections import deque  # noqa: F401
 from datetime import datetime
-from pathlib import Path
+from pathlib import Path  # noqa: F401
 from typing import List
 
 import numpy as np
 import onnx
 from onnx import ModelProto, TensorProto, numpy_helper
 from onnx_model import OnnxModel
 
@@ -69,15 +69,15 @@
 
         return outputs
 
     def get_reshape_shape_inputs(self):
         """
         Returns a list of shape input names of Reshape nodes.
         """
-        output_name_to_node = self.output_name_to_node()
+        self.output_name_to_node()
 
         shape_inputs = []
         for node in self.model.graph.node:
             if node.op_type == "Reshape":
                 shape_inputs.append(node.input[1])
 
         return shape_inputs
@@ -103,15 +103,14 @@
     def add_extra_graph_output(self, extra_outputs):
         """
         Add a list of output names to graph output.
         """
         names_to_evaluate = []
         output_names = [output.name for output in self.model.graph.output]
         for name in extra_outputs:
-
             if self.get_initializer(name) is not None:  # already a constant
                 continue
             names_to_evaluate.append(name)
 
             if name not in output_names:
                 output_info = onnx.helper.ValueInfoProto()
                 output_info.name = name
@@ -268,21 +267,21 @@
         shapes[shape_input] = new_target_shape
 
         logger.debug(f"source_shape={source_shape}, target_shape={target_shape}, new_target_shape={new_target_shape}")
 
     def validate_input(self, input: str):
         if not self.find_graph_input(input):
             valid_names = [input.name for input in self.model.graph.input]
-            raise Exception("Input {} does not exist in the graph inputs: {}".format(input, valid_names))
+            raise Exception(f"Input {input} does not exist in the graph inputs: {valid_names}")
 
     def validate_outputs(self, output_names: List[str]):
         valid_names = [output.name for output in self.model.graph.output]
         for name in output_names:
             if name not in valid_names:
-                raise Exception("Output {} does not exist in the graph outputs: {}".format(name, valid_names))
+                raise Exception(f"Output {name} does not exist in the graph outputs: {valid_names}")
 
     def optimize(
         self,
         output_path: str,
         input_ids: str,
         segment_ids: str,
         input_mask: str,
```

## onnxruntime/transformers/torch_onnx_export_helper.py

```diff
@@ -2,15 +2,15 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import torch
 
 TrainingMode = torch.onnx.TrainingMode
-from packaging.version import Version
+from packaging.version import Version  # noqa: E402
 
 
 def torch_onnx_export(
     model,
     args,
     f,
     export_params=True,
```

## onnxruntime/transformers/models/bart/export.py

```diff
@@ -28,15 +28,14 @@
 
 def print_args(args):
     for arg in vars(args):
         logger.info(f"{arg}: {getattr(args, arg)}")
 
 
 def user_command():
-
     parent_parser = argparse.ArgumentParser(add_help=False)
     parent_parser.add_argument("--max_length", type=int, default=20, help="default to 20")
     parent_parser.add_argument("--min_length", type=int, default=0, help="default to 0")
     parent_parser.add_argument("-o", "--output", type=str, default="onnx_models", help="default name is onnx_models.")
     parent_parser.add_argument("-i", "--input_text", type=str, default=None, help="input text")
     parent_parser.add_argument("-s", "--spm_path", type=str, default=None, help="tokenizer model from sentencepice")
     parent_parser.add_argument("-v", "--vocab_path", type=str, help="vocab dictionary")
@@ -62,20 +61,19 @@
     )
 
     print_args(parent_parser.parse_args())
     return parent_parser.parse_args()
 
 
 if __name__ == "__main__":
-
     args = user_command()
     if args.opset_version < 14:
         raise ValueError(f"The minimum supported opset version is 14! The given one was {args.opset_version}.")
 
-    isExist = os.path.exists(args.output)
+    isExist = os.path.exists(args.output)  # noqa: N816
     if not isExist:
         os.makedirs(args.output)
 
     # beam search op only supports CPU for now
     args.device = "cpu"
     logger.info("ENV: CPU ...")
 
@@ -83,18 +81,18 @@
         args.input_text = (
             "PG&E stated it scheduled the blackouts in response to forecasts for high winds "
             "amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were "
             "scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
         )
 
     if not args.no_encoder:
-        logger.info(f"========== EXPORTING ENCODER ==========")
+        logger.info("========== EXPORTING ENCODER ==========")
         export_summarization_edinit.export_encoder(args)
     if not args.no_decoder:
-        logger.info(f"========== EXPORTING DECODER ==========")
+        logger.info("========== EXPORTING DECODER ==========")
         export_summarization_enc_dec_past.export_decoder(args)
     if not args.no_chain:
-        logger.info(f"========== CONVERTING MODELS ==========")
+        logger.info("========== CONVERTING MODELS ==========")
         chain_enc_dec_with_beamsearch.convert_model(args)
     if not args.no_inference:
-        logger.info(f"========== INFERENCING WITH ONNX MODEL ==========")
+        logger.info("========== INFERENCING WITH ONNX MODEL ==========")
         onnx_inference.run_inference(args)
```

## onnxruntime/transformers/models/bert/eval_squad.py

```diff
@@ -134,21 +134,21 @@
             "pretrained_model_name",
             "onnx_path",
             "provider",
             "disable_fused_attention",
             "use_io_binding",
         ]
 
-        model_list = list(set([result["onnx_path"] for result in results]))
+        model_list = list({result["onnx_path"] for result in results})
         model_list.sort()
 
-        batch_sizes = list(set([result["batch_size"] for result in results]))
+        batch_sizes = list({result["batch_size"] for result in results})
         batch_sizes.sort()
 
-        sequence_lengths = list(set([result["sequence_length"] for result in results]))
+        sequence_lengths = list({result["sequence_length"] for result in results})
         sequence_lengths.sort()
 
         key_names = []
         for sequence_length in sequence_lengths:
             for batch_size in batch_sizes:
                 key_names.append(f"b{batch_size}_s{sequence_length}")
```

## onnxruntime/transformers/models/gpt2/benchmark_gpt2.py

```diff
@@ -17,22 +17,22 @@
 import torch
 from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
 from packaging import version
 from transformers import AutoConfig
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
 
-from benchmark_helper import (
+from benchmark_helper import (  # noqa: E402
     Precision,
     create_onnxruntime_session,
     get_ort_environment_variables,
     prepare_environment,
     setup_logger,
 )
-from quantize_helper import QuantizeHelper
+from quantize_helper import QuantizeHelper  # noqa: E402
 
 logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
@@ -400,16 +400,16 @@
                             "sequence_length": sequence_length,
                             "past_sequence_length": past_sequence_length,
                             "disable_io_binding": args.disable_io_binding,
                             "torch_latency": f"{torch_latency:.2f}" if torch_latency else "None",
                             "onnxruntime_latency": f"{ort_latency:.2f}",
                         }
                         csv_writer.writerow(row)
-                    except:
-                        logger.error(f"Exception", exc_info=True)
+                    except Exception:
+                        logger.error("Exception", exc_info=True)
                         return None
 
     logger.info(f"Results are saved to file {csv_filename}")
     return csv_filename
 
 
 if __name__ == "__main__":
```

## onnxruntime/transformers/models/gpt2/convert_to_onnx.py

```diff
@@ -26,22 +26,22 @@
 from gpt2_helper import DEFAULT_TOLERANCE, MODEL_CLASSES, PRETRAINED_GPT2_MODELS, Gpt2Helper
 from gpt2_tester import Gpt2Tester
 from packaging import version
 from transformers import AutoConfig
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
 
-from benchmark_helper import (
+from benchmark_helper import (  # noqa: E402
     Precision,
     create_onnxruntime_session,
     get_ort_environment_variables,
     prepare_environment,
     setup_logger,
 )
-from quantize_helper import QuantizeHelper
+from quantize_helper import QuantizeHelper  # noqa: E402
 
 logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
@@ -344,15 +344,15 @@
             args.use_external_data_format,
             auto_mixed_precision=args.auto_mixed_precision,
             stage=args.stage,
             **fp16_params,
         )
 
         nodes = m.nodes()
-        op_list = set([node.op_type for node in nodes])
+        op_list = {node.op_type for node in nodes}
         all_ops = ",".join(op_list)
 
         # print optimized operators
         optimized_op_counter = m.get_fused_operator_statistics()
         if optimized_op_counter:
             optimized_ops = ",".join([key for key in optimized_op_counter if optimized_op_counter[key] > 0])
     else:
@@ -368,15 +368,15 @@
     if args.output.endswith(".onnx") and output_path != args.output and not args.use_external_data_format:
         import shutil
 
         shutil.move(output_path, args.output)
         output_path = args.output
 
     logger.info(f"Output path: {output_path}")
-    model_size_in_MB = int(get_onnx_model_size(output_path, args.use_external_data_format) / 1024 / 1024)
+    model_size_in_MB = int(get_onnx_model_size(output_path, args.use_external_data_format) / 1024 / 1024)  # noqa: N806
 
     session = create_onnxruntime_session(
         output_path, args.use_gpu, args.provider, enable_all_optimization=True, verbose=args.verbose
     )
     if args.model_class == "GPT2LMHeadModel" and session is not None:
         parity_result = gpt2helper.test_parity(
             session,
@@ -492,27 +492,27 @@
                 "diff_90_percentile": parity_result["max_diff_percentile_90"],
                 "diff_95_percentile": parity_result["max_diff_percentile_95"],
                 "diff_99_percentile": parity_result["max_diff_percentile_99"],
                 "diff_pass_rate": parity_result["diff_pass_rate"],
                 "nan_rate": parity_result["nan_rate"],
                 "top1_match_rate": parity_result["top1_match_rate"],
                 "top1_match_rate_per_run": parity_result["top1_match_rate_per_run"],
-                "onnx_size_in_MB": "{}".format(model_size_in_MB),
+                "onnx_size_in_MB": f"{model_size_in_MB}",
             }
             logger.info(f"result: {row}")
             result.update(row)
             csv_writer.writerow(row)
 
     if args.input_test_file:
         test_inputs = []
         # Each line of test file is a JSON string like:
         # {"input_ids": [[14698, 257, 1310, 13688, 319, 326]]}
         with open(args.input_test_file) as read_f:
             for _, line in enumerate(read_f):
-                line = line.rstrip()
+                line = line.rstrip()  # noqa: PLW2901
                 data = json.loads(line)
                 input_ids = torch.from_numpy(numpy.asarray(data["input_ids"], dtype=numpy.int64)).to(device)
 
                 if use_padding:
                     if "attention_mask" in data:
                         numpy_float = numpy.float16 if is_io_float16 else numpy.float32
                         attention_mask = torch.from_numpy(numpy.asarray(data["attention_mask"], dtype=numpy_float)).to(
```

## onnxruntime/transformers/models/gpt2/gpt2_helper.py

```diff
@@ -18,20 +18,19 @@
 import numpy
 import onnx
 import torch
 from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model, TFGPT2Model
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
 
-from benchmark_helper import Precision
-from float16 import float_to_float16_max_diff
-from fusion_options import AttentionMaskFormat
-from io_binding_helper import IOBindingHelper
-from onnx_model import OnnxModel
-from torch_onnx_export_helper import torch_onnx_export
+from benchmark_helper import Precision  # noqa: E402
+from float16 import float_to_float16_max_diff  # noqa: E402
+from io_binding_helper import IOBindingHelper  # noqa: E402
+from onnx_model import OnnxModel  # noqa: E402
+from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
 PRETRAINED_GPT2_MODELS = ["distilgpt2", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]
 
 DEFAULT_TOLERANCE = {
     Precision.FLOAT32: 0.0005,
@@ -65,15 +64,15 @@
     """Here we wrap a class for Onnx model conversion for GPT2Model with past state."""
 
     def __init__(self, config):
         super().__init__(config)
 
     @staticmethod
     def post_process(result, num_layer):
-        if isinstance(result[1][0], tuple) or isinstance(result[1][0], list):
+        if isinstance(result[1][0], (tuple, list)):
             assert len(result[1]) == num_layer and len(result[1][0]) == 2
             # assert len(result[1][0][0].shape) == 4 and result[1][0][0].shape == result[1][0][1].shape
             present = []
             for i in range(num_layer):
                 # Since transformers v4.*, past key and values are separated outputs.
                 # Here we concate them into one tensor to be compatible with Attention operator.
                 present.append(
@@ -111,15 +110,15 @@
             past_key_values=past,
             return_dict=False,
         )
 
         return MyGPT2Model.post_process(result, self.config.n_layer)
 
 
-class MyGPT2LMHeadModel_NoPadding(GPT2LMHeadModel):
+class MyGPT2LMHeadModel_NoPadding(GPT2LMHeadModel):  # noqa: N801
     """Here we wrap a class for Onnx model conversion for GPT2LMHeadModel with past state and no padding.
     When you always use batch_size=1 in inference, there is no padding in inputs. In such case, position_ids
     and attention_mask need no be in inputs.
     """
 
     def __init__(self, config):
         super().__init__(config)
@@ -184,14 +183,15 @@
         device: torch.device,
         float16: bool = False,
         has_position_ids: bool = True,
         has_attention_mask: bool = True,
         input_ids_dtype: torch.dtype = torch.int32,
         position_ids_dtype: torch.dtype = torch.int32,
         attention_mask_dtype: torch.dtype = torch.int32,
+        left_side_padding: bool = True,
     ) -> Gpt2Inputs:
         """Create random inputs for GPT2 model.
         Returns torch tensors of input_ids, position_ids, attention_mask and a list of past state tensors.
         """
         float_type = torch.float16 if float16 else torch.float32
         past_shape = [
             2,
@@ -214,17 +214,22 @@
         if has_attention_mask:
             total_sequence_length = past_sequence_length + sequence_length
             attention_mask = torch.ones(
                 [batch_size, total_sequence_length],
                 dtype=attention_mask_dtype,
                 device=device,
             )
+
             if total_sequence_length >= 2:
-                padding_position = random.randint(0, total_sequence_length - 1)  # test input with padding.
-                attention_mask[:, padding_position] = 0
+                for i in range(batch_size):
+                    padding_length = random.randint(0, total_sequence_length - 1)
+                    if left_side_padding:
+                        attention_mask[i, :padding_length] = 0
+                    else:  # right side padding
+                        attention_mask[i, total_sequence_length - padding_length :] = 0
 
         # Deduce position_ids from attention mask
         position_ids = None
         if has_position_ids:
             position_ids = attention_mask.long().cumsum(-1) - 1
             position_ids.masked_fill_(position_ids < 0, 0)
             position_ids = position_ids[:, past_sequence_length:].to(position_ids_dtype)
@@ -417,15 +422,15 @@
             outputs = model(*input_list)
 
         past_names = [f"past_{i}" for i in range(num_layer)]
         present_names = [f"present_{i}" for i in range(num_layer)]
 
         # GPT2Model outputs last_state; GPT2LMHeadModel outputs logits (prediction_scores)
         assert outputs[0].shape[2] == config.vocab_size or outputs[0].shape[2] == config.hidden_size
-        output_names = ["logits" if outputs[0].shape[2] == config.vocab_size else "last_state"] + present_names
+        output_names = ["logits" if outputs[0].shape[2] == config.vocab_size else "last_state", *present_names]
 
         # Shape of input tensors:
         #    input_ids: (batch_size, seq_len)
         #    past_{i}:  (2, batch_size, num_heads, past_seq_len, hidden_size/num_heads)
         #    attention_mask: (batch_size, past_seq_len + seq_len)
         # Shape of output tensors:
         #    last_state: (batch_size, seq_len, hidden_size)
@@ -513,24 +518,14 @@
     ):
         """Optimize ONNX model with an option to convert it to use mixed precision."""
         from fusion_options import FusionOptions
         from optimizer import optimize_model
 
         optimization_options = FusionOptions("gpt2")
 
-        if is_float16 and stage == 1:
-            # For init_decoder, enable mask index to use fused causal cuda kernel.
-            # Potentially, we can add other optimization like unpad for effective transformer
-            optimization_options.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
-
-        # TODO(hasesh): Investigate parity issue for GPT-2 fp16 when SkipLayerNormalization
-        # is enabled
-        if is_float16:
-            optimization_options.enable_skip_layer_norm = False
-
         m = optimize_model(
             onnx_model_path,
             model_type="gpt2",
             num_heads=num_attention_heads,
             hidden_size=hidden_size,
             opt_level=0,
             optimization_options=optimization_options,
@@ -547,15 +542,15 @@
 
         m.save_model_to_file(optimized_model_path, use_external_data_format)
         return m
 
     @staticmethod
     def auto_mixed_precision(
         onnx_model: OnnxModel,
-        op_block_list: List[str] = [
+        op_block_list: List[str] = [  # noqa: B006
             "Add",
             "LayerNormalization",
             "SkipLayerNormalization",
             "FastGelu",
             "EmbedLayerNormalization",
         ],
     ):
@@ -564,15 +559,15 @@
         Args:
             onnx_model (OnnxModel): optimized ONNX model
             op_block_list (List[str], optional): operators to compute in fp32. Defaults to ["Add", "LayerNormalization",
                                                  "SkipLayerNormalization", "FastGelu", "EmbedLayerNormalization"]
         Returns:
             parameters(dict): a dictionary of parameters used in float16 conversion
         """
-        op_full_set = set([node.op_type for node in onnx_model.nodes()])
+        op_full_set = {node.op_type for node in onnx_model.nodes()}
         fp32_op_set = set(op_block_list)
         fp16_op_set = op_full_set.difference(fp32_op_set)
         logger.info(f"fp32 op: {fp32_op_set} fp16 op: {fp16_op_set}")
 
         # logits is the first output
         logits_output_name = onnx_model.graph().output[0].name
 
@@ -643,15 +638,15 @@
         logger.debug("PyTorch inference time = {} ms".format(format(average_latency, ".2f")))
 
         return outputs, average_latency
 
     @staticmethod
     def onnxruntime_inference(ort_session, inputs: Gpt2Inputs, total_runs: int = 0):
         """Run inference of ONNX model, and returns average latency in ms when total_runs > 0 besides outputs."""
-        logger.debug(f"start onnxruntime_inference")
+        logger.debug("start onnxruntime_inference")
 
         ort_inputs = {"input_ids": numpy.ascontiguousarray(inputs.input_ids.cpu().numpy())}
 
         if inputs.past is not None:
             for i, past_i in enumerate(inputs.past):
                 ort_inputs[f"past_{i}"] = numpy.ascontiguousarray(past_i.cpu().numpy())
 
@@ -711,15 +706,15 @@
         output_buffers: Dict[str, torch.Tensor],
         output_shapes: Dict[str, List[int]],
         total_runs: int = 0,
         return_numpy: bool = True,
         include_copy_output_latency: bool = False,
     ):
         """Inference with IO binding. Returns outputs, and optional latency when total_runs > 0."""
-        logger.debug(f"start onnxruntime_inference_with_binded_io")
+        logger.debug("start onnxruntime_inference_with_binded_io")
 
         # Bind inputs and outputs to onnxruntime session
         io_binding = Gpt2Helper.prepare_io_binding(
             ort_session,
             inputs.input_ids,
             inputs.position_ids,
             inputs.attention_mask,
@@ -837,14 +832,15 @@
                 device,
                 is_float16,
                 has_position_ids,
                 has_attention_mask,
                 input_ids_dtype=input_ids_dtype,
                 position_ids_dtype=position_ids_dtype,
                 attention_mask_dtype=attention_mask_dtype,
+                left_side_padding=True,
             )
             outputs = Gpt2Helper.pytorch_inference(model, dummy_inputs)
             if use_io_binding:
                 ort_outputs = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs)
             else:
                 output_shapes = Gpt2Helper.get_output_shapes(
                     batch_size,
@@ -864,34 +860,34 @@
                 messages,
                 is_top1_matched,
             ) = Gpt2Helper.compare_outputs_v2(outputs, ort_outputs, atol=atol)
             if not numpy.isnan(max_abs_diff):
                 max_abs_diff_list.append(max_abs_diff)
             if is_all_close:
                 passed_test_cases += 1
+
             if is_top1_matched:
                 top1_matched_cases += 1
                 top1_matched_cases_per_run[run_id] += 1
 
             if verbose and not is_all_close:
                 logger.info(
                     f"test_case={i} batch_size={batch_size} past_sequence_length={past_sequence_length} sequence_length={sequence_length} MaxDiff={max_abs_diff}"
                 )
-                for i, message in enumerate(messages):
+                for i, message in enumerate(messages):  # noqa: PLW2901
                     logger.info(f"\t{i}: Name={ort_session.get_outputs()[i].name}, {message}")
 
             # Collect data for debugging
             if enable_pickle_output and (numpy.isnan(max_abs_diff) or max_abs_diff > 100 * atol):
                 Gpt2Helper.save_inputs(i, dummy_inputs)
                 Gpt2Helper.save_outputs(i, ort_outputs, outputs)
 
         if max_abs_diff_list:
             result = {
-                f"max_diff_percentile_{p}": "{:.5f}".format(numpy.percentile(max_abs_diff_list, p))
-                for p in [50, 90, 95, 99]
+                f"max_diff_percentile_{p}": f"{numpy.percentile(max_abs_diff_list, p):.5f}" for p in [50, 90, 95, 99]
             }
         else:
             result = {f"max_diff_percentile_{p}": "nan" for p in [50, 90, 95, 99]}
 
         result["top1_match_rate"] = top1_matched_cases * 1.0 / total_test_cases
         result["top1_match_rate_per_run"] = [x * 1.0 / test_cases_per_run for x in top1_matched_cases_per_run]
         result["diff_pass_rate"] = passed_test_cases * 1.0 / total_test_cases
@@ -982,15 +978,15 @@
     @staticmethod
     def get_onnx_paths(
         output_dir,
         model_name_or_path,
         model_class: str = "GPT2LMHeadModel",
         has_past=True,
         new_folder=False,
-        remove_existing=["raw", "fp32", "fp16", "int8"],
+        remove_existing=["raw", "fp32", "fp16", "int8"],  # noqa: B006
     ):
         """Build a  path name for given model based on given attributes."""
         model_name = model_name_or_path
         if os.path.isdir(model_name_or_path):
             model_name = Path(model_name_or_path).parts[-1]
         else:
             model_name.split("/")[-1]
```

## onnxruntime/transformers/models/gpt2/gpt2_parity.py

```diff
@@ -22,15 +22,15 @@
 import scipy.stats
 from convert_to_onnx import main
 from gpt2_helper import PRETRAINED_GPT2_MODELS, Gpt2Helper
 from onnx_model import OnnxModel
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
 
-from benchmark_helper import get_ort_environment_variables, setup_logger
+from benchmark_helper import get_ort_environment_variables, setup_logger  # noqa: E402
 
 logger = logging.getLogger("")
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
@@ -109,22 +109,22 @@
     def run(self, argv, experiment_name):
         start_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
         run_id = f"{start_time}_{self.run_id}"
         self.run_id += 1
 
         try:
             result = main(
-                argv + ["-t", f"{self.test_cases}", "-r", f"{self.total_runs}"],
+                [*argv, "-t", f"{self.test_cases}", "-r", f"{self.total_runs}"],
                 experiment_name=experiment_name,
                 run_id=run_id,
                 csv_filename=self.csv_path,
             )
             if result:
                 self.results.append(result)
-        except:
+        except Exception:
             logger.exception(f"Failed to run experiment {experiment_name}")
             result = None
 
         return result
 
 
 def load_results_from_csv(csv_path):
@@ -146,15 +146,15 @@
     raise RuntimeError("Failed to get average_latency from output")
 
 
 def score(row):
     """Scoring function based on 3 metrics. The larger score is better."""
     latency_in_ms = get_latency(row)
     top1_match_rate = float(row["top1_match_rate"])
-    onnx_size_in_MB = float(row["onnx_size_in_MB"])
+    onnx_size_in_MB = float(row["onnx_size_in_MB"])  # noqa: N806
     # A simple scoring function: cost of 0.1ms latency ~ 0.1% match rate ~ 100MB size
     return top1_match_rate * 1000 - latency_in_ms * 10 - onnx_size_in_MB / 100
 
 
 def print_wins(wins, rows, test_name):
     print()
     print("*" * 10)
@@ -317,24 +317,24 @@
         "--io_block_list",
         "logits",
         "--node_block_list",
         last_matmul_node_name,
     ]
 
     if op_block_list:
-        parameters.extend(["--op_block_list"] + op_block_list)
+        parameters.extend(["--op_block_list", *op_block_list])
 
     return parameters
 
 
 def run_candidate(
     task: ParityTask,
     args,
     last_matmul_node_name,
-    op_block_list=["FastGelu", "LayerNormalization"],
+    op_block_list=["FastGelu", "LayerNormalization"],  # noqa: B006
 ):
     parameters = get_mixed_precision_parameters(args, last_matmul_node_name, op_block_list)
     op_block_list_str = ",".join(sorted(op_block_list))
 
     if op_block_list:
         name = f"Mixed precision baseline + {op_block_list_str} in FP32"
     else:
@@ -403,17 +403,17 @@
     """Assumed that you have run step 0 and 1 to figure out that Logits FP32 and some operators shall be in FP32,
     This step will try add one more operator.
     """
     candidate_fp32_ops = ["FastGelu", "LayerNormalization", "SkipLayerNormalization"]
     fp32_ops = [x for x in candidate_fp32_ops if x in optimized_ops]
     for op in optimized_ops:
         if op not in fp32_ops:
-            op_block_list = fp32_ops + [op]
+            op_block_list = [*fp32_ops, op]
             task.run(
-                mixed_precision_baseline + ["--op_block_list"] + op_block_list,
+                [*mixed_precision_baseline, "--op_block_list", *op_block_list],
                 "Mixed precision baseline + {},{} in FP32".format(",".join(fp32_ops), op),
             )
 
 
 def run_parity(task: ParityTask, args):
     onnx_model_paths = Gpt2Helper.get_onnx_paths(
         "onnx_models",
@@ -446,15 +446,16 @@
     task.run(fp16_baseline, "FP16 baseline")
 
     last_matmul_node_name = get_last_matmul_node_name(onnx_model_paths["raw"])
 
     # Mixed precision baseline
     run_candidate(task, args, last_matmul_node_name, op_block_list=[])
 
-    get_fp32_ops = lambda x: [op for op in x if op in all_ops]
+    def get_fp32_ops(x):
+        return [op for op in x if op in all_ops]
 
     if args.all:
         run_tuning_step0(task, fp16_baseline, all_ops, optimized_ops)
         mixed_precision_baseline = get_mixed_precision_parameters(args, last_matmul_node_name, op_block_list=[])
         run_tuning_step1(task, mixed_precision_baseline, optimized_ops)
         run_tuning_step2(task, mixed_precision_baseline, optimized_ops)
     else:
@@ -505,14 +506,14 @@
     task = ParityTask(args.test_cases, args.runs, args.csv)
 
     if not args.skip_test:
         run_parity(task, args)
 
     try:
         rows = load_results_from_csv(task.csv_path)
-    except:
+    except Exception:
         logger.exception(f"Failed to load csv {task.csv_path}")
         rows = task.results
 
     logger.info("Start running significance tests...")
     summary_csv = task.csv_path.replace(".csv", ".stats.csv")
     run_significance_test(rows, summary_csv)
```

## onnxruntime/transformers/models/gpt2/gpt2_tester.py

```diff
@@ -13,15 +13,15 @@
 
 import numpy
 import torch
 from gpt2_helper import Gpt2Helper, Gpt2Inputs
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
 
-from benchmark_helper import Precision
+from benchmark_helper import Precision  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
 
 class Gpt2Metric:
     def __init__(self, treatment_name, baseline_name="Torch", top_k=20):
         assert top_k > 1 and top_k <= 100
@@ -57,20 +57,20 @@
         if self.seq_len_latency:
             print("Past sequence length range and average latency:")
             total = 0
             count = 0
             for key in sorted(self.seq_len_latency.keys()):
                 average = statistics.mean(self.seq_len_latency[key]) * 1000.0
                 if key == 0:
-                    print("\t{}:         \t{:.2f} ms".format(key, average))
+                    print(f"\t{key}:         \t{average:.2f} ms")
                 else:
-                    print("\t[{}, {}]:\t{:.2f} ms".format(2**key, 2 ** (key + 1) - 1, average))
+                    print(f"\t[{2**key}, {2 ** (key + 1) - 1}]:\t{average:.2f} ms")
                 total += average * len(self.seq_len_latency[key])
                 count += len(self.seq_len_latency[key])
-            print("Average Latency: {:.2f} ms".format(total / count))
+            print(f"Average Latency: {total / count:.2f} ms")
 
     def diff_logits(self, baseline_logits, treatment_logits, is_empty_past: bool):
         diff = (baseline_logits - treatment_logits).abs().max()
         if is_empty_past:
             self.max_logits_diff_no_past = max(self.max_logits_diff_no_past, diff)
         else:
             self.max_logits_diff = max(self.max_logits_diff, diff)
@@ -126,15 +126,14 @@
         hidden_size,
         num_layer,
         device,
         is_fp16=False,
         top_k=20,
         top_k_required_order=False,
     ):
-
         self.batch_size = input_ids.shape[0]
         self.input_length = input_ids.shape[1]
         self.n_layer = num_layer
 
         self.input_ids = input_ids
         self.position_ids = position_ids
         self.attention_mask = attention_mask
@@ -147,15 +146,15 @@
         past_shape = [
             2,
             self.batch_size,
             num_attention_heads,
             0,
             hidden_size // num_attention_heads,
         ]
-        for i in range(num_layer):
+        for _i in range(num_layer):
             empty_past = torch.empty(past_shape).type(torch.float16 if is_fp16 else torch.float32)
             self.past.append(empty_past.to(device))
 
         self.logits = None
         self.top_1_tokens = None
         self.top_k_tokens = None
         self.top_k = top_k
@@ -186,23 +185,23 @@
         if self.has_attention_mask:
             add_tensor(input_tensors, self.attention_mask, "attention_mask")
 
         for i in range(self.n_layer):
             add_tensor(input_tensors, self.past[i], "past_" + str(i))
 
         for i, tensor in enumerate(input_tensors):
-            with open(os.path.join(path, "input_{}.pb".format(i)), "wb") as f:
+            with open(os.path.join(path, f"input_{i}.pb"), "wb") as f:
                 f.write(tensor.SerializeToString())
 
         output_names = [output.name for output in session.get_outputs()]
-        for i, name in enumerate(output_names):
+        for i, _name in enumerate(output_names):
             tensor = numpy_helper.from_array(
                 output[i] if isinstance(output[i], numpy.ndarray) else output[i].clone().cpu().numpy()
             )
-            with open(os.path.join(path, "output_{}.pb".format(i)), "wb") as f:
+            with open(os.path.join(path, f"output_{i}.pb"), "wb") as f:
                 f.write(tensor.SerializeToString())
 
         print(f"Test data saved to directory {path}")
 
     def update(self, output, step, device):
         """
         Update the inputs for next inference.
@@ -286,17 +285,17 @@
     def predict_next_token(logits, top_k=1, required_order=False):
         """
         Get top k topkens based on logits.
         """
 
         # logits has shape (batch_size, seq_len, vocab_size)
         # last token logits has shape (batch_size, vocab_size)
-        lastTokenLogits = logits[:, -1]
+        lastTokenLogits = logits[:, -1]  # noqa: N806
         if top_k == 1:
-            generatedTokens = torch.argmax(lastTokenLogits, 1, True)
+            generatedTokens = torch.argmax(lastTokenLogits, 1, True)  # noqa: N806
             return generatedTokens
         else:
             topk = torch.argsort(lastTokenLogits, -1, descending=True)[:, :top_k]
             if not required_order:
                 sorted_topk, _ = topk.sort()
                 return sorted_topk
             return topk
@@ -458,15 +457,18 @@
                         past_seq_len,
                         seq_len,
                         model.config,
                         model_class=model_class,
                     )
                     Gpt2Helper.auto_increase_buffer_size(output_buffers, output_shapes)
 
-                    (onnx_io_output, avg_latency_ms,) = Gpt2Helper.onnxruntime_inference_with_binded_io(
+                    (
+                        onnx_io_output,
+                        avg_latency_ms,
+                    ) = Gpt2Helper.onnxruntime_inference_with_binded_io(
                         session,
                         onnx_io_runner.get_inputs(),
                         output_buffers,
                         output_shapes,
                         total_runs=1,
                         return_numpy=False,
                         include_copy_output_latency=True,
```

## onnxruntime/transformers/models/gpt2/parity_check_helper.py

```diff
@@ -15,15 +15,15 @@
 import numpy
 import torch
 from gpt2_helper import Gpt2Helper
 from onnx import TensorProto, numpy_helper
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
 
-from benchmark_helper import create_onnxruntime_session
+from benchmark_helper import create_onnxruntime_session  # noqa: E402
 
 NON_ZERO_VALUE = str(1)
 ZERO_VALUE = str(0)
 
 
 def environ_setting_nodes(node_name_filter=None, node_type_filter=None):
     # Set I/O data as default
@@ -91,27 +91,27 @@
         filename_other = os.path.join(outputs_path_other, Path(filename).name)
         if not os.path.exists(filename_other):
             continue
         with open(filename, "rb") as f:
             tensor = TensorProto()
             tensor.ParseFromString(f.read())
             array = numpy_helper.to_array(tensor)
-            with open(filename_other, "rb") as f:
+            with open(filename_other, "rb") as f:  # noqa: PLW2901
                 tensor_other = TensorProto()
                 tensor_other.ParseFromString(f.read())
                 array_other = numpy_helper.to_array(tensor_other)
                 if array_other.size == 0:
                     continue
                 diff = numpy.average(numpy.abs(array_other - array) / (numpy.abs(array_other) + 1e-6))
                 if math.isnan(diff):
                     continue
                 record[Path(filename).name.split(".")[0]] = diff
                 if_close[Path(filename).name.split(".")[0]] = numpy.allclose(array, array_other, rtol=1e-04, atol=1e-04)
 
-    results = [f"Node\tDiff\tClose"]
+    results = ["Node\tDiff\tClose"]
     for k, v in sorted(record.items(), key=lambda x: x[1], reverse=True):
         results.append(f"{k}\t{v}\t{if_close[k]}")
     for line in results:
         print(line)
 
 
 if __name__ == "__main__":
```

## onnxruntime/transformers/models/longformer/benchmark_longformer.py

```diff
@@ -47,15 +47,15 @@
 import torch
 from longformer_helper import PRETRAINED_LONGFORMER_MODELS, LongformerHelper, LongformerInputs
 from transformers import LongformerModel
 
 import onnxruntime
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-import benchmark_helper
+import benchmark_helper  # noqa: E402
 
 logger = logging.getLogger("")
 
 
 def test_torch_latency(
     device,
     model,
@@ -76,15 +76,15 @@
                 logger.info(f"batch_size={batch_size} sequence_length={sequence_length} global_length={global_length}")
                 inputs: LongformerInputs = LongformerHelper.get_dummy_inputs(
                     batch_size, sequence_length, global_length, device
                 )
                 input_list = inputs.to_list()
 
                 _ = model(*input_list)
-                runtimes = timeit.repeat(lambda: model(*input_list), repeat=test_times, number=1)
+                runtimes = timeit.repeat(lambda: model(*input_list), repeat=test_times, number=1)  # noqa: B023
                 result = {
                     "engine": "torch",  # TODO: test torchscript
                     "version": torch.__version__,
                     "device": "cuda",
                     "optimizer": "",
                     "precision": "fp32",
                     "io_binding": "",
@@ -400,15 +400,15 @@
         args.global_lengths,
         args.test_times,
         args.num_threads,
     )
 
 
 def test_latency(args, device) -> List[Dict[str, Any]]:
-    if "onnxruntime" == args.engine:
+    if args.engine == "onnxruntime":
         return test_ort(args, device)
 
     return test_torch(args, device)
 
 
 def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
@@ -643,15 +643,15 @@
                             args = parse_arguments(f"{arguments} -t 10 --memory".split(" "))
                             memory_results = launch_test(args)
 
                         args = parse_arguments(f"{arguments} -t {test_times}".split(" "))
                         latency_results = launch_test(args)
                     except KeyboardInterrupt as exc:
                         raise RuntimeError("Keyboard Interrupted") from exc
-                    except:
+                    except Exception:
                         traceback.print_exc()
                         continue
 
                     if len(latency_results) == 1:
                         latency_results[0]["memory"] = memory_results[0]["memory"] if memory_results else "N/A"
                     else:
                         raise RuntimeError("length of latency_results should be 1")
@@ -671,21 +671,21 @@
             "version",
             "global_length",
             "use_compact_memory",
             "use_half4",
             "description",
         ]
 
-        description_list = list(set([result["description"] for result in results]))
+        description_list = list({result["description"] for result in results})
         description_list.sort()
 
-        batch_sizes = list(set([result["batch_size"] for result in results]))
+        batch_sizes = list({result["batch_size"] for result in results})
         batch_sizes.sort()
 
-        sequence_lengths = list(set([result["sequence_length"] for result in results]))
+        sequence_lengths = list({result["sequence_length"] for result in results})
         sequence_lengths.sort()
 
         data_names = []
         for sequence_length in sequence_lengths:
             for batch_size in batch_sizes:
                 data_names.append(f"b{batch_size}_s{sequence_length}")
```

## onnxruntime/transformers/models/longformer/convert_to_onnx.py

```diff
@@ -43,16 +43,16 @@
 from onnx import load_model
 from packaging import version
 from torch.onnx import register_custom_op_symbolic
 from torch.onnx.symbolic_helper import parse_args
 from transformers import LongformerModel, LongformerSelfAttention
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from onnx_model_bert import BertOnnxModel
-from torch_onnx_export_helper import torch_onnx_export
+from onnx_model_bert import BertOnnxModel  # noqa: E402
+from torch_onnx_export_helper import torch_onnx_export  # noqa: E402
 
 # Supports format 0 or 1
 weight_bias_format = 0
 
 
 @parse_args("v", "v", "v", "v", "v", "v", "v", "i", "i")
 def my_longformer_attention(
@@ -145,15 +145,14 @@
 
     args = parser.parse_args()
     return args
 
 
 # Create a dummy input for ONNX export.
 def get_dummy_inputs(config, export_padding, device):
-
     # When sequence length is multiple of windows size, there is no padding logic in ONNX graph
     sequence_length = config.attention_window[0] + 1 if export_padding else config.attention_window[0]
 
     # Create dummy inputs
     input_ids = torch.arange(sequence_length).unsqueeze(0).to(device)
 
     attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)
@@ -262,15 +261,15 @@
     hidden_states,
     attention_mask=None,
     is_index_masked=None,
     is_index_global_attn=None,
     is_global_attn=None,
     output_attentions=False,
 ):
-    assert output_attentions == False
+    assert output_attentions is False
     return my_longformer_self_attention_forward_4(
         self,
         hidden_states,
         attention_mask,
         is_index_masked,
         is_index_global_attn,
         is_global_attn,
@@ -284,15 +283,15 @@
     attention_mask=None,
     layer_head_mask=None,
     is_index_masked=None,
     is_index_global_attn=None,
     is_global_attn=None,
     output_attentions=False,
 ):
-    assert output_attentions == False
+    assert output_attentions is False
     assert layer_head_mask is None
     return my_longformer_self_attention_forward_4(
         self,
         hidden_states,
         attention_mask,
         is_index_masked,
         is_index_global_attn,
@@ -396,15 +395,15 @@
         print(f"optimized fp16 model saved to {fp16_model_path}")
 
 
 def main(args):
     model_name = args.model
     onnx_model_path = model_name + ".onnx"
 
-    global weight_bias_format
+    global weight_bias_format  # noqa: PLW0603
     weight_bias_format = 0 if args.no_merge_qkv else 1
 
     model = LongformerModel.from_pretrained(PRETRAINED_LONGFORMER_MODELS[model_name])
 
     export_longformer(model, onnx_model_path, args.export_padding)
 
     if args.optimize_onnx or args.precision != "fp32":
```

## onnxruntime/transformers/models/longformer/generate_test_data.py

```diff
@@ -8,19 +8,19 @@
 import argparse
 import os
 import random
 import sys
 from pathlib import Path
 
 import numpy as np
-from onnx import ModelProto, TensorProto, numpy_helper
+from onnx import ModelProto, TensorProto, numpy_helper  # noqa: F401
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from bert_test_data import fake_input_ids_data, fake_input_mask_data, output_test_data
-from onnx_model import OnnxModel
+from bert_test_data import fake_input_ids_data, fake_input_mask_data, output_test_data  # noqa: E402
+from onnx_model import OnnxModel  # noqa: E402
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
     parser.add_argument("--model", required=True, type=str, help="bert onnx model path.")
 
@@ -127,15 +127,15 @@
         expected_inputs = 1 + (1 if input_mask else 0) + (1 if global_mask else 0)
         if len(graph_inputs) != expected_inputs:
             raise ValueError(f"Expect the graph to have {expected_inputs} inputs. Got {len(graph_inputs)}")
 
         return input_ids, input_mask, global_mask
 
     if len(graph_inputs) != 3:
-        raise ValueError("Expect the graph to have 3 inputs. Got {}".format(len(graph_inputs)))
+        raise ValueError(f"Expect the graph to have 3 inputs. Got {len(graph_inputs)}")
 
     # Try guess the inputs based on naming.
     input_ids = None
     input_mask = None
     global_mask = None
     for input in graph_inputs:
         input_name_lower = input.name.lower()
@@ -260,15 +260,14 @@
     seed,
     verbose,
     input_ids_name,
     input_mask_name,
     global_mask_name,
     num_global_tokens,
 ):
-
     input_ids, input_mask, global_mask = get_longformer_inputs(model, input_ids_name, input_mask_name, global_mask_name)
     all_inputs = generate_test_data(
         batch_size,
         sequence_length,
         test_cases,
         seed,
         verbose,
@@ -286,15 +285,15 @@
     args = parse_arguments()
 
     output_dir = args.output_dir
     if output_dir is None:
         # Default output directory is a sub-directory under the directory of model.
         output_dir = os.path.join(
             Path(args.model).parent,
-            "b{}_s{}_g{}".format(args.batch_size, args.sequence_length, args.global_tokens),
+            f"b{args.batch_size}_s{args.sequence_length}_g{args.global_tokens}",
         )
 
     if output_dir is not None:
         # create the output directory if not existed
         path = Path(output_dir)
         path.mkdir(parents=True, exist_ok=True)
     else:
```

## onnxruntime/transformers/models/stable_diffusion/benchmark.py

```diff
@@ -1,45 +1,182 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import argparse
+import csv
 import os
+import statistics
+import sys
 import time
 
 SD_MODELS = {
     "1.5": "runwayml/stable-diffusion-v1-5",
     "2.0": "stabilityai/stable-diffusion-2",
     "2.1": "stabilityai/stable-diffusion-2-1",
 }
 
+PROVIDERS = {
+    "cuda": "CUDAExecutionProvider",
+    "rocm": "ROCMExecutionProvider",
+}
+
 
-def get_test_settings():
-    height = 512
-    width = 512
-    num_inference_steps = 50
+def example_prompts():
     prompts = [
         "a photo of an astronaut riding a horse on mars",
         "cute grey cat with blue eyes, wearing a bowtie, acrylic painting",
         "a cute magical flying dog, fantasy art drawn by disney concept artists, highly detailed, digital painting",
         "an illustration of a house with large barn with many cute flower pots and beautiful blue sky scenery",
         "one apple sitting on a table, still life, reflective, full color photograph, centered, close-up product",
         "background texture of stones, masterpiece, artistic, stunning photo, award winner photo",
         "new international organic style house, tropical surroundings, architecture, 8k, hdr",
         "beautiful Renaissance Revival Estate, Hobbit-House, detailed painting, warm colors, 8k, trending on Artstation",
         "blue owl, big green eyes, portrait, intricate metal design, unreal engine, octane render, realistic",
         "delicate elvish moonstone necklace on a velvet background, symmetrical intricate motifs, leaves, flowers, 8k",
     ]
 
-    return height, width, num_inference_steps, prompts
+    return prompts
 
 
-def get_ort_pipeline(model_name: str, directory: str, provider: str, disable_safety_checker: bool):
-    from diffusers import OnnxStableDiffusionPipeline
+class CudaMemoryMonitor:
+    def __init__(self, keep_measuring=True):
+        self.keep_measuring = keep_measuring
+
+    def measure_gpu_usage(self):
+        from py3nvml.py3nvml import (
+            NVMLError,
+            nvmlDeviceGetCount,
+            nvmlDeviceGetHandleByIndex,
+            nvmlDeviceGetMemoryInfo,
+            nvmlDeviceGetName,
+            nvmlInit,
+            nvmlShutdown,
+        )
+
+        max_gpu_usage = []
+        gpu_name = []
+        try:
+            nvmlInit()
+            device_count = nvmlDeviceGetCount()
+            if not isinstance(device_count, int):
+                print(f"nvmlDeviceGetCount result is not integer: {device_count}")
+                return None
+
+            max_gpu_usage = [0 for i in range(device_count)]
+            gpu_name = [nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(i)) for i in range(device_count)]
+            while True:
+                for i in range(device_count):
+                    info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
+                    if isinstance(info, str):
+                        print(f"nvmlDeviceGetMemoryInfo returns str: {info}")
+                        return None
+                    max_gpu_usage[i] = max(max_gpu_usage[i], info.used / 1024**2)
+                time.sleep(0.002)  # 2ms
+                if not self.keep_measuring:
+                    break
+            nvmlShutdown()
+            return [
+                {
+                    "device_id": i,
+                    "name": gpu_name[i],
+                    "max_used_MB": max_gpu_usage[i],
+                }
+                for i in range(device_count)
+            ]
+        except NVMLError as error:
+            print("Error fetching GPU information using nvml: %s", error)
+            return None
+
+
+class RocmMemoryMonitor:
+    def __init__(self, keep_measuring=True):
+        self.keep_measuring = keep_measuring
+        rocm_smi_path = "/opt/rocm/libexec/rocm_smi"
+        if os.path.exists(rocm_smi_path):
+            if rocm_smi_path not in sys.path:
+                sys.path.append(rocm_smi_path)
+        try:
+            import rocm_smi
+
+            self.rocm_smi = rocm_smi
+            self.rocm_smi.initializeRsmi()
+        except ImportError:
+            self.rocm_smi = None
+
+    def get_used_memory(self, dev):
+        if self.rocm_smi is None:
+            return -1
+        return self.rocm_smi.getMemInfo(dev, "VRAM")[0] / 1024 / 1024
+
+    def measure_gpu_usage(self):
+        device_count = len(self.rocm_smi.listDevices()) if self.rocm_smi is not None else 0
+        max_gpu_usage = [0 for i in range(device_count)]
+        gpu_name = [f"GPU{i}" for i in range(device_count)]
+        while True:
+            for i in range(device_count):
+                max_gpu_usage[i] = max(max_gpu_usage[i], self.get_used_memory(i))
+            time.sleep(0.002)  # 2ms
+            if not self.keep_measuring:
+                break
+        return [
+            {
+                "device_id": i,
+                "name": gpu_name[i],
+                "max_used_MB": max_gpu_usage[i],
+            }
+            for i in range(device_count)
+        ]
+
+
+def measure_gpu_memory(monitor_type, func, start_memory=None):
+    if monitor_type is None:
+        return None
+
+    monitor = monitor_type(False)
+    memory_before_test = monitor.measure_gpu_usage()
+
+    if start_memory is None:
+        start_memory = memory_before_test
+    if start_memory is None:
+        return None
+    if func is None:
+        return start_memory
+
+    from concurrent.futures import ThreadPoolExecutor
+
+    with ThreadPoolExecutor() as executor:
+        monitor = monitor_type()
+        mem_thread = executor.submit(monitor.measure_gpu_usage)
+        try:
+            fn_thread = executor.submit(func)
+            _ = fn_thread.result()
+        finally:
+            monitor.keep_measuring = False
+            max_usage = mem_thread.result()
+
+        if max_usage is None:
+            return None
+
+        print(f"GPU memory usage: before={memory_before_test}  peak={max_usage}")
+        if len(start_memory) >= 1 and len(max_usage) >= 1 and len(start_memory) == len(max_usage):
+            # When there are multiple GPUs, we will check the one with maximum usage.
+            max_used = 0
+            for i, memory_before in enumerate(start_memory):
+                before = memory_before["max_used_MB"]
+                after = max_usage[i]["max_used_MB"]
+                used = after - before
+                max_used = max(max_used, used)
+            return max_used
+    return None
+
+
+def get_ort_pipeline(model_name: str, directory: str, provider, disable_safety_checker: bool):
+    from diffusers import DPMSolverMultistepScheduler, OnnxStableDiffusionPipeline
 
     import onnxruntime
 
     if directory is not None:
         assert os.path.exists(directory)
         session_options = onnxruntime.SessionOptions()
         pipe = OnnxStableDiffusionPipeline.from_pretrained(
@@ -50,120 +187,312 @@
     else:
         pipe = OnnxStableDiffusionPipeline.from_pretrained(
             model_name,
             revision="onnx",
             provider=provider,
             use_auth_token=True,
         )
+    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
+    pipe.set_progress_bar_config(disable=True)
 
     if disable_safety_checker:
         pipe.safety_checker = None
         pipe.feature_extractor = None
 
     return pipe
 
 
-def get_torch_pipeline(model_name: str, disable_safety_checker: bool):
-    from diffusers import StableDiffusionPipeline
+def get_torch_pipeline(model_name: str, disable_safety_checker: bool, enable_torch_compile: bool, use_xformers: bool):
+    from diffusers import DPMSolverMultistepScheduler, StableDiffusionPipeline
     from torch import channels_last, float16
 
-    pipe = StableDiffusionPipeline.from_pretrained(
-        model_name, torch_dtype=float16, revision="fp16", use_auth_token=True
-    ).to("cuda")
+    pipe = StableDiffusionPipeline.from_pretrained(model_name, torch_dtype=float16).to("cuda")
 
     pipe.unet.to(memory_format=channels_last)  # in-place operation
 
+    if use_xformers:
+        pipe.enable_xformers_memory_efficient_attention()
+
+    if enable_torch_compile:
+        import torch
+
+        pipe.unet = torch.compile(pipe.unet)
+        pipe.vae = torch.compile(pipe.vae)
+        pipe.text_encoder = torch.compile(pipe.text_encoder)
+        print("Torch compiled unet, vae and text_encoder")
+
+    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
+    pipe.set_progress_bar_config(disable=True)
+
     if disable_safety_checker:
         pipe.safety_checker = None
         pipe.feature_extractor = None
 
     return pipe
 
 
 def get_image_filename_prefix(engine: str, model_name: str, batch_size: int, disable_safety_checker: bool):
     short_model_name = model_name.split("/")[-1].replace("stable-diffusion-", "sd")
     return f"{engine}_{short_model_name}_b{batch_size}" + ("" if disable_safety_checker else "_safe")
 
 
-def run_ort_pipeline(pipe, batch_size: int, image_filename_prefix: str):
+def run_ort_pipeline(
+    pipe,
+    batch_size: int,
+    image_filename_prefix: str,
+    height,
+    width,
+    steps,
+    num_prompts,
+    batch_count,
+    start_memory,
+    memory_monitor_type,
+):
     from diffusers import OnnxStableDiffusionPipeline
 
     assert isinstance(pipe, OnnxStableDiffusionPipeline)
 
-    height, width, num_inference_steps, prompts = get_test_settings()
+    prompts = example_prompts()
 
-    pipe("warm up", height, width, num_inference_steps=2)
+    def warmup():
+        pipe("warm up", height, width, num_inference_steps=steps, num_images_per_prompt=batch_size)
 
-    latency_list = []
-    for i, prompt in enumerate(prompts):
-        input_prompts = [prompt] * batch_size
-        inference_start = time.time()
-        image = pipe(input_prompts, height, width, num_inference_steps).images[0]
-        inference_end = time.time()
-
-        latency = inference_end - inference_start
-        latency_list.append(latency)
-        print(f"Inference took {latency} seconds")
-        image.save(f"{image_filename_prefix}_{i}.jpg")
-    print("Average latency in seconds:", sum(latency_list) / len(latency_list))
+    # Run warm up, and measure GPU memory of two runs
+    # cuDNN/MIOpen The first run has  algo search so it might need more memory)
+    first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+    second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
 
+    if memory_monitor_type is None:
+        warmup()
 
-def run_torch_pipeline(pipe, batch_size: int, image_filename_prefix: str):
+    latency_list = []
+    for i, prompt in enumerate(prompts):
+        if i >= num_prompts:
+            break
+        for j in range(batch_count):
+            inference_start = time.time()
+            images = pipe(
+                prompt,
+                height,
+                width,
+                num_inference_steps=steps,
+                negative_prompt=None,
+                guidance_scale=7.5,
+                num_images_per_prompt=batch_size,
+            ).images
+            inference_end = time.time()
+            latency = inference_end - inference_start
+            latency_list.append(latency)
+            print(f"Inference took {latency:.3f} seconds")
+            for k, image in enumerate(images):
+                image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
+
+    from onnxruntime import __version__ as ort_version
+
+    return {
+        "engine": "onnxruntime",
+        "version": ort_version,
+        "height": height,
+        "width": width,
+        "steps": steps,
+        "batch_size": batch_size,
+        "batch_count": batch_count,
+        "num_prompts": num_prompts,
+        "average_latency": sum(latency_list) / len(latency_list),
+        "median_latency": statistics.median(latency_list),
+        "first_run_memory_MB": first_run_memory,
+        "second_run_memory_MB": second_run_memory,
+    }
+
+
+def run_torch_pipeline(
+    pipe,
+    batch_size: int,
+    image_filename_prefix: str,
+    height,
+    width,
+    steps,
+    num_prompts,
+    batch_count,
+    start_memory,
+    memory_monitor_type,
+):
     import torch
 
-    height, width, num_inference_steps, prompts = get_test_settings()
+    prompts = example_prompts()
 
-    pipe("warm up", height, width, num_inference_steps=2)
+    # total 2 runs of warm up, and measure GPU memory for CUDA EP
+    def warmup():
+        pipe("warm up", height, width, num_inference_steps=steps, num_images_per_prompt=batch_size)
+
+    # Run warm up, and measure GPU memory of two runs (The first run has cuDNN algo search so it might need more memory)
+    first_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+    second_run_memory = measure_gpu_memory(memory_monitor_type, warmup, start_memory)
+
+    if memory_monitor_type is None:
+        warmup()
 
     torch.set_grad_enabled(False)
 
     latency_list = []
     for i, prompt in enumerate(prompts):
-        input_prompts = [prompt] * batch_size
-        torch.cuda.synchronize()
-        inference_start = time.time()
-        image = pipe(input_prompts, height, width, num_inference_steps).images[0]
+        if i >= num_prompts:
+            break
         torch.cuda.synchronize()
-        inference_end = time.time()
-
-        latency = inference_end - inference_start
-        latency_list.append(latency)
-        print(f"Inference took {latency} seconds")
-        image.save(f"{image_filename_prefix}_{i}.jpg")
-
-    print("Average latency in seconds:", sum(latency_list) / len(latency_list))
-
+        for j in range(batch_count):
+            inference_start = time.time()
+            images = pipe(
+                prompt=prompt,
+                height=height,
+                width=width,
+                num_inference_steps=steps,
+                guidance_scale=7.5,
+                negative_prompt=None,
+                num_images_per_prompt=batch_size,
+                generator=None,  # torch.Generator
+            ).images
+
+            torch.cuda.synchronize()
+            inference_end = time.time()
+            latency = inference_end - inference_start
+            latency_list.append(latency)
+            print(f"Inference took {latency:.3f} seconds")
+            for k, image in enumerate(images):
+                image.save(f"{image_filename_prefix}_{i}_{j}_{k}.jpg")
+
+    return {
+        "engine": "torch",
+        "version": torch.__version__,
+        "height": height,
+        "width": width,
+        "steps": steps,
+        "batch_size": batch_size,
+        "batch_count": batch_count,
+        "num_prompts": num_prompts,
+        "average_latency": sum(latency_list) / len(latency_list),
+        "median_latency": statistics.median(latency_list),
+        "first_run_memory_MB": first_run_memory,
+        "second_run_memory_MB": second_run_memory,
+    }
+
+
+def run_ort(
+    model_name: str,
+    directory: str,
+    provider: str,
+    batch_size: int,
+    disable_safety_checker: bool,
+    height,
+    width,
+    steps,
+    num_prompts,
+    batch_count,
+    start_memory,
+    memory_monitor_type,
+    tuning,
+):
+    provider_and_options = provider
+    if tuning and provider in ["CUDAExecutionProvider", "ROCMExecutionProvider"]:
+        provider_and_options = (provider, {"tunable_op_enable": 1, "tunable_op_tuning_enable": 1})
 
-def run_ort(model_name: str, directory: str, provider: str, batch_size: int, disable_safety_checker: bool):
     load_start = time.time()
-    pipe = get_ort_pipeline(model_name, directory, provider, disable_safety_checker)
+    pipe = get_ort_pipeline(model_name, directory, provider_and_options, disable_safety_checker)
     load_end = time.time()
     print(f"Model loading took {load_end - load_start} seconds")
 
     image_filename_prefix = get_image_filename_prefix("ort", model_name, batch_size, disable_safety_checker)
-    run_ort_pipeline(pipe, batch_size, image_filename_prefix)
+    result = run_ort_pipeline(
+        pipe,
+        batch_size,
+        image_filename_prefix,
+        height,
+        width,
+        steps,
+        num_prompts,
+        batch_count,
+        start_memory,
+        memory_monitor_type,
+    )
+
+    result.update(
+        {
+            "model_name": model_name,
+            "directory": directory,
+            "provider": provider,
+            "disable_safety_checker": disable_safety_checker,
+        }
+    )
+    return result
 
 
-def run_torch(model_name: str, batch_size: int, disable_safety_checker: bool):
+def run_torch(
+    model_name: str,
+    batch_size: int,
+    disable_safety_checker: bool,
+    enable_torch_compile: bool,
+    use_xformers: bool,
+    height,
+    width,
+    steps,
+    num_prompts,
+    batch_count,
+    start_memory,
+    memory_monitor_type,
+):
     import torch
 
     torch.backends.cudnn.enabled = True
     torch.backends.cudnn.benchmark = True
-    # torch.backends.cuda.matmul.allow_tf32 = True
 
     torch.set_grad_enabled(False)
 
     load_start = time.time()
-    pipe = get_torch_pipeline(model_name, disable_safety_checker)
+    pipe = get_torch_pipeline(model_name, disable_safety_checker, enable_torch_compile, use_xformers)
     load_end = time.time()
     print(f"Model loading took {load_end - load_start} seconds")
 
     image_filename_prefix = get_image_filename_prefix("torch", model_name, batch_size, disable_safety_checker)
-    with torch.inference_mode():
-        run_torch_pipeline(pipe, batch_size, image_filename_prefix)
+
+    if not enable_torch_compile:
+        with torch.inference_mode():
+            result = run_torch_pipeline(
+                pipe,
+                batch_size,
+                image_filename_prefix,
+                height,
+                width,
+                steps,
+                num_prompts,
+                batch_count,
+                start_memory,
+                memory_monitor_type,
+            )
+    else:
+        result = run_torch_pipeline(
+            pipe,
+            batch_size,
+            image_filename_prefix,
+            height,
+            width,
+            steps,
+            num_prompts,
+            batch_count,
+            start_memory,
+            memory_monitor_type,
+        )
+
+    result.update(
+        {
+            "model_name": model_name,
+            "directory": None,
+            "provider": "compile" if enable_torch_compile else "xformers" if use_xformers else "default",
+            "disable_safety_checker": disable_safety_checker,
+        }
+    )
+    return result
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
 
     parser.add_argument(
         "-e",
@@ -172,20 +501,39 @@
         type=str,
         default="onnxruntime",
         choices=["onnxruntime", "torch"],
         help="Engines to benchmark. Default is onnxruntime.",
     )
 
     parser.add_argument(
+        "-r",
+        "--provider",
+        required=False,
+        type=str,
+        default="cuda",
+        choices=list(PROVIDERS.keys()),
+        help="Provider to benchmark. Default is CUDAExecutionProvider.",
+    )
+
+    parser.add_argument(
+        "-t",
+        "--tuning",
+        action="store_true",
+        help="Enable TunableOp and tuning. "
+        "This will incur longer warmup latency, and is mandatory for some operators of ROCm EP.",
+    )
+
+    parser.add_argument(
         "-v",
         "--version",
-        required=True,
+        required=False,
         type=str,
         choices=list(SD_MODELS.keys()),
-        help="Stable diffusion version like 1.5, 2.0 or 2.1",
+        default="1.5",
+        help="Stable diffusion version like 1.5, 2.0 or 2.1. Default is 1.5.",
     )
 
     parser.add_argument(
         "-p",
         "--pipeline",
         required=False,
         type=str,
@@ -197,36 +545,161 @@
         "--enable_safety_checker",
         required=False,
         action="store_true",
         help="Enable safety checker",
     )
     parser.set_defaults(enable_safety_checker=False)
 
-    parser.add_argument("-b", "--batch_size", type=int, default=1)
+    parser.add_argument(
+        "--enable_torch_compile",
+        required=False,
+        action="store_true",
+        help="Enable compile unet for PyTorch 2.0",
+    )
+    parser.set_defaults(enable_torch_compile=False)
+
+    parser.add_argument(
+        "--use_xformers",
+        required=False,
+        action="store_true",
+        help="Use xformers for PyTorch",
+    )
+    parser.set_defaults(use_xformers=False)
+
+    parser.add_argument(
+        "-b",
+        "--batch_size",
+        type=int,
+        default=1,
+        choices=[1, 2, 4, 8, 10, 16, 32],
+        help="Number of images per batch. Default is 1.",
+    )
+
+    parser.add_argument(
+        "--height",
+        required=False,
+        type=int,
+        default=512,
+        help="Output image height. Default is 512.",
+    )
+
+    parser.add_argument(
+        "--width",
+        required=False,
+        type=int,
+        default=512,
+        help="Output image width. Default is 512.",
+    )
+
+    parser.add_argument(
+        "-s",
+        "--steps",
+        required=False,
+        type=int,
+        default=50,
+        help="Number of steps. Default is 50.",
+    )
+
+    parser.add_argument(
+        "-n",
+        "--num_prompts",
+        required=False,
+        type=int,
+        default=1,
+        help="Number of prompts. Default is 1.",
+    )
+
+    parser.add_argument(
+        "-c",
+        "--batch_count",
+        required=False,
+        type=int,
+        choices=range(1, 11),
+        default=5,
+        help="Number of batches to test. Default is 5.",
+    )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_arguments()
     print(args)
 
+    memory_monitor_type = None
+    if args.provider == "cuda":
+        memory_monitor_type = CudaMemoryMonitor
+    elif args.provider == "rocm":
+        memory_monitor_type = RocmMemoryMonitor
+
+    start_memory = measure_gpu_memory(memory_monitor_type, None)
+    print("GPU memory used before loading models:", start_memory)
+
     sd_model = SD_MODELS[args.version]
+    provider = PROVIDERS[args.provider]
     if args.engine == "onnxruntime":
         assert args.pipeline, "--pipeline should be specified for onnxruntime engine"
 
-        if args.batch_size > 1:
-            # Need remove a line https://github.com/huggingface/diffusers/blob/a66f2baeb782e091dde4e1e6394e46f169e5ba58/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py#L307
-            #    in diffuers to run batch_size > 1.
-            assert (
-                args.enable_safety_checker
-            ), "batch_size > 1 is not compatible with safety checker due to a bug in diffuers"
-
-        provider = "CUDAExecutionProvider"  # TODO: use ["CUDAExecutionProvider", "CPUExecutionProvider"] in diffuers
-        run_ort(sd_model, args.pipeline, provider, args.batch_size, not args.enable_safety_checker)
+        result = run_ort(
+            sd_model,
+            args.pipeline,
+            provider,
+            args.batch_size,
+            not args.enable_safety_checker,
+            args.height,
+            args.width,
+            args.steps,
+            args.num_prompts,
+            args.batch_count,
+            start_memory,
+            memory_monitor_type,
+            args.tuning,
+        )
     else:
-        run_torch(sd_model, args.batch_size, not args.enable_safety_checker)
+        result = run_torch(
+            sd_model,
+            args.batch_size,
+            not args.enable_safety_checker,
+            args.enable_torch_compile,
+            args.use_xformers,
+            args.height,
+            args.width,
+            args.steps,
+            args.num_prompts,
+            args.batch_count,
+            start_memory,
+            memory_monitor_type,
+        )
+
+    print(result)
+
+    with open("benchmark_result.csv", mode="a", newline="") as csv_file:
+        column_names = [
+            "model_name",
+            "directory",
+            "engine",
+            "version",
+            "provider",
+            "disable_safety_checker",
+            "height",
+            "width",
+            "steps",
+            "batch_size",
+            "batch_count",
+            "num_prompts",
+            "average_latency",
+            "median_latency",
+            "first_run_memory_MB",
+            "second_run_memory_MB",
+        ]
+        csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
+        csv_writer.writeheader()
+        csv_writer.writerow(result)
 
 
 if __name__ == "__main__":
-    main()
+    try:
+        main()
+    except Exception as e:
+        tb = sys.exc_info()
+        print(e.with_traceback(tb[2]))
```

## onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py

```diff
@@ -1,105 +1,183 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 #
 # This script converts stable diffusion onnx models from float to half (mixed) precision for GPU inference.
 #
-# Before running this script, you need convert checkpoint to float32 onnx models like the following
-#    export ONNX_ROOT=./sd_onnx
-#    pip install -r requirements.txt
-#    huggingface-cli login
-#    wget https://raw.githubusercontent.com/huggingface/diffusers/v0.12.1/scripts/convert_stable_diffusion_checkpoint_to_onnx.py
-#    python convert_stable_diffusion_checkpoint_to_onnx.py --model_path runwayml/stable-diffusion-v1-5  --output_path $ONNX_ROOT/stable-diffusion-v1-5-fp32
-# Note that this script might not be compatible with older or newer version of diffusers.
-
-# Then you can use this script to convert them to float16 like the following:
-#    python optimize_pipeline.py -i $ONNX_ROOT/stable-diffusion-v1-5-fp32 -o $ONNX_ROOT/stable-diffusion-v1-5-fp16 --float16
-# Or
-#    python -m onnxruntime.transformers.models.stable_diffusion.optimize_pipeline -i $ONNX_ROOT/stable-diffusion-v1-5-fp32 -o $ONNX_ROOT/stable-diffusion-v1-5-fp16 --float16
+# Before running this script, follow README.md to setup python environment and convert stable diffusion checkpoint to float32 onnx models.
 #
-# Note that output model is for CUDA Execution Provider. It might not run in CPU Execution Provider.
-# Stable diffusion 2.1 model will get black images using float16 Attention. It is a known issue that we are working on.
+# For example, the float32 ONNX pipeline is saved to ./sd-v1-5 directory, you can optimize and convert it to float16 like the following:
+#    python optimize_pipeline.py -i ./sd-v1-5 -o ./sd-v1-5-fp16 --float16
+#
+# Note that the optimizations are carried out for CUDA Execution Provider at first, other EPs may not have the support for the fused opeartors.
+# In this case, the users should disable the operator fusion manually to workaround.
+#
+# Stable diffusion 2.1 model will get black images using float16 Attention. A walkaround is to force Attention to run in float32 like the following:
+#    python optimize_pipeline.py -i ./sd-v2-1 -o ./sd-v2-1-fp16 --float16 --force_fp32_ops unet:Attention
+#
+# If you are using nightly package (or built from source), you can force MultiHeadAttention to run in float32:
+#    python optimize_pipeline.py -i ./sd-v2-1 -o ./sd-v2-1-fp16 --float16 --force_fp32_ops unet:MultiHeadAttention
 
 import argparse
 import logging
 import os
 import shutil
 import sys
+import tempfile
 from pathlib import Path
+from typing import List
 
 import coloredlogs
+import onnx
+from packaging import version
+
+import onnxruntime
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from fusion_options import FusionOptions
-from optimizer import optimize_model  # noqa: E402
+from fusion_options import FusionOptions  # noqa: E402
+from onnx_model_clip import ClipOnnxModel  # noqa: E402
+from onnx_model_unet import UnetOnnxModel  # noqa: E402
+from onnx_model_vae import VaeOnnxModel  # noqa: E402
+from optimizer import optimize_by_onnxruntime, optimize_model  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
 
 def optimize_sd_pipeline(
-    source_dir: Path, target_dir: Path, overwrite: bool, use_external_data_format: bool, float16: bool
+    source_dir: Path,
+    target_dir: Path,
+    overwrite: bool,
+    use_external_data_format: bool,
+    float16: bool,
+    force_fp32_ops: List[str],
+    enable_runtime_optimization: bool,
+    args,
 ):
     """Optimize onnx models used in stable diffusion onnx pipeline and optionally convert to float16.
 
     Args:
         source_dir (Path): Root of input directory of stable diffusion onnx pipeline with float32 models.
         target_dir (Path): Root of output directory of stable diffusion onnx pipeline with optimized models.
         overwrite (bool): Overwrite files if exists.
         use_external_data_format (bool): save onnx model to two files: one for onnx graph, another for weights
         float16 (bool): use half precision
+        force_fp32_ops(List[str]): operators that are forced to run in float32.
+        enable_runtime_optimization(bool): run graph optimization using Onnx Runtime.
 
     Raises:
         RuntimeError: input onnx model does not exist
         RuntimeError: output onnx model path existed
     """
-    dirs_with_onnx = ["unet", "vae_encoder", "vae_decoder", "text_encoder", "safety_checker"]
-    for name in dirs_with_onnx:
+    model_type_mapping = {
+        "unet": "unet",
+        "vae_encoder": "vae",
+        "vae_decoder": "vae",
+        "text_encoder": "clip",
+        "safety_checker": "unet",
+    }
+
+    model_type_class_mapping = {
+        "unet": UnetOnnxModel,
+        "vae": VaeOnnxModel,
+        "clip": ClipOnnxModel,
+    }
+
+    force_fp32_operators = {
+        "unet": [],
+        "vae_encoder": [],
+        "vae_decoder": [],
+        "text_encoder": [],
+        "safety_checker": [],
+    }
+
+    if force_fp32_ops:
+        for fp32_operator in force_fp32_ops:
+            parts = fp32_operator.split(":")
+            if len(parts) == 2 and parts[0] in force_fp32_operators and (parts[1] and parts[1][0].isupper()):
+                force_fp32_operators[parts[0]].append(parts[1])
+            else:
+                raise ValueError(
+                    f"--force_fp32_ops shall be in the format of module:operator like unet:Attention, got {fp32_operator}"
+                )
+
+    for name, model_type in model_type_mapping.items():
         onnx_model_path = source_dir / name / "model.onnx"
 
         if not os.path.exists(onnx_model_path):
             message = f"input onnx model does not exist: {onnx_model_path}."
-            if name not in ["safety_checker", "feature_extractor"]:
+            if name not in ["safety_checker"]:
                 raise RuntimeError(message)
             continue
 
+        # Prepare output directory
+        optimized_model_path = target_dir / name / "model.onnx"
+        output_dir = optimized_model_path.parent
+        if optimized_model_path.exists():
+            if not overwrite:
+                raise RuntimeError(f"output onnx model path existed: {optimized_model_path}")
+
+        if output_dir.exists():
+            shutil.rmtree(output_dir)
+        output_dir.mkdir(parents=True, exist_ok=True)
+
         # Graph fusion before fp16 conversion, otherwise they cannot be fused later.
         # Right now, onnxruntime does not save >2GB model so we use script to optimize unet instead.
-        logger.info(f"optimize {onnx_model_path}...")
+        logger.info(f"Optimize {onnx_model_path}...")
 
-        fusion_options = FusionOptions("unet")
-        fusion_options.enable_packed_kv = float16
+        args.model_type = model_type
+        fusion_options = FusionOptions.parse(args)
+
+        if model_type in ["unet"]:
+            # Some optimizations are not available in v1.14 or older version: packed QKV and BiasAdd
+            has_all_optimizations = version.parse(onnxruntime.__version__) >= version.parse("1.15.0")
+            fusion_options.enable_packed_kv = float16
+            fusion_options.enable_packed_qkv = float16 and has_all_optimizations
+            fusion_options.enable_bias_add = has_all_optimizations
 
         m = optimize_model(
             str(onnx_model_path),
-            model_type="unet",
+            model_type=model_type,
             num_heads=0,  # will be deduced from graph
             hidden_size=0,  # will be deduced from graph
             opt_level=0,
             optimization_options=fusion_options,
-            use_gpu=False,
+            use_gpu=True,
         )
 
         if float16:
-            logger.info("convert %s to float16 ...", name)
-            m.convert_float_to_float16(op_block_list=["RandomNormalLike", "Resize", "GroupNorm"])
-
-        optimized_model_path = target_dir / name / "model.onnx"
-        output_dir = optimized_model_path.parent
-        if optimized_model_path.exists():
-            if not overwrite:
-                raise RuntimeError(f"output onnx model path existed: {optimized_model_path}")
-
-        if output_dir.exists():
-            shutil.rmtree(output_dir)
-        output_dir.mkdir(parents=True, exist_ok=True)
+            logger.info("Convert %s to float16 ...", name)
+            op_block_list = ["RandomNormalLike"]
+            m.convert_float_to_float16(
+                keep_io_types=False,
+                op_block_list=op_block_list + force_fp32_operators[name],
+            )
+
+        if enable_runtime_optimization and (float16 or (name not in ["unet"])):
+            # Use this step to see the final graph that executed by Onnx Runtime.
+            # Note that ORT cannot save model larger than 2GB so we exclude unet float32 model.
+            # This step is optional since it has no impact on performance except model loading time.
+            with tempfile.TemporaryDirectory() as tmp_dir:
+                # Save to a temporary file so that we can load it with Onnx Runtime.
+                logger.info("Saving a temporary model to run OnnxRuntime graph optimizations...")
+                tmp_model_path = Path(tmp_dir) / "model.onnx"
+                m.save_model_to_file(str(tmp_model_path))
+                ort_optimized_model_path = tmp_model_path
+                optimize_by_onnxruntime(
+                    str(tmp_model_path), use_gpu=True, optimized_model_path=str(ort_optimized_model_path)
+                )
+                model = onnx.load(str(ort_optimized_model_path), load_external_data=True)
+                m = model_type_class_mapping[model_type](model)
 
+        m.get_operator_statistics()
+        m.get_fused_operator_statistics()
         m.save_model_to_file(str(optimized_model_path), use_external_data_format=use_external_data_format)
-        logger.info("%s => %s", onnx_model_path, optimized_model_path)
+        logger.info("%s is optimized", name)
+        logger.info("*" * 20)
 
 
 def copy_extra_directory(source_dir: Path, target_dir: Path, overwrite: bool):
     """Copy extra directory that does not have onnx model
 
     Args:
         source_dir (Path): source directory
@@ -113,15 +191,15 @@
     extra_dirs = ["scheduler", "tokenizer", "feature_extractor"]
 
     for name in extra_dirs:
         source_path = source_dir / name
 
         if not os.path.exists(source_path):
             message = f"source path does not exist: {source_path}"
-            if name not in ["safety_checker", "feature_extractor"]:
+            if name not in ["feature_extractor"]:
                 raise RuntimeError(message)
             continue
 
         target_path = target_dir / name
         if target_path.exists():
             if not overwrite:
                 raise RuntimeError(f"output path existed: {target_path}")
@@ -174,14 +252,30 @@
         required=False,
         action="store_true",
         help="Output models of half or mixed precision.",
     )
     parser.set_defaults(float16=False)
 
     parser.add_argument(
+        "--force_fp32_ops",
+        required=False,
+        nargs="+",
+        type=str,
+        help="Force given operators (like unet:Attention) to run in float32. It is case sensitive!",
+    )
+
+    parser.add_argument(
+        "--inspect",
+        required=False,
+        action="store_true",
+        help="Inspect the optimized graph from Onnx Runtime for debugging purpose. This option has no impact on model performance.",
+    )
+    parser.set_defaults(inspect=False)
+
+    parser.add_argument(
         "--overwrite",
         required=False,
         action="store_true",
         help="Overwrite exists files.",
     )
     parser.set_defaults(overwrite=False)
 
@@ -191,21 +285,31 @@
         required=False,
         action="store_true",
         help="Onnx model larger than 2GB need to use external data format. "
         "Save onnx model to two files: one for onnx graph, another for large weights.",
     )
     parser.set_defaults(use_external_data_format=False)
 
+    FusionOptions.add_arguments(parser)
+
     args = parser.parse_args()
     return args
 
 
 def main():
     coloredlogs.install(fmt="%(funcName)20s: %(message)s")
     args = parse_arguments()
+    logger.info("Arguments: %s", str(args))
     copy_extra_directory(Path(args.input), Path(args.output), args.overwrite)
     optimize_sd_pipeline(
-        Path(args.input), Path(args.output), args.overwrite, args.use_external_data_format, args.float16
+        Path(args.input),
+        Path(args.output),
+        args.overwrite,
+        args.use_external_data_format,
+        args.float16,
+        args.force_fp32_ops,
+        args.inspect,
+        args,
     )
 
 
 main()
```

## onnxruntime/transformers/models/t5/convert_to_onnx.py

```diff
@@ -199,14 +199,15 @@
                     onnx_path,
                     output_path,
                     precision == Precision.FLOAT16,
                     config.num_heads,
                     config.hidden_size,
                     use_external_data_format,
                     auto_mixed_precision=not disable_auto_mixed_precision,
+                    use_gpu=use_gpu,
                 )
             else:
                 logger.info(f"Skip optimizing: existed ONNX model {onnx_path}")
         else:
             output_path = onnx_path
 
         ort_session = create_onnxruntime_session(
```

## onnxruntime/transformers/models/t5/past_helper.py

```diff
@@ -1,14 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import logging
+from typing import List, Tuple
+
+import torch
 
 logger = logging.getLogger(__name__)
 
 
 class PastKeyValuesHelper:
     """Helper functions to process past key values for encoder-decoder model"""
 
@@ -34,15 +37,15 @@
         """Split present state from grouped by layer to grouped by self/cross attention.
         Before: (past_key_self_0, past_value_self_0, past_key_cross_0, past_value_cross_0), (past_key_self_1, past_value_self_1, past_key_cross_1, past_value_cross_1), ...
         After: (past_key_self_0, past_value_self_0, past_key_self_1, past_value_self_1, ...), (past_key_cross_0, past_value_cross_0, past_key_cross_1, past_value_cross_1, ...)
 
         """
         present_self = []
         present_cross = []
-        for i, present_layer_i in enumerate(present_key_values):
+        for _i, present_layer_i in enumerate(present_key_values):
             assert len(present_layer_i) == 4, f"Expected to have four items. Got {len(present_layer_i)}"
             (
                 present_key_self,
                 present_value_self,
                 present_key_cross,
                 present_value_cross,
             ) = present_layer_i
@@ -62,7 +65,86 @@
                 past[2 * i],
                 past[2 * i + 1],
                 past[2 * num_layers + 2 * i],
                 past[2 * num_layers + 2 * i + 1],
             ]
             for i in range(num_layers)
         )
+
+    @staticmethod
+    def back_group_by_layer(past_key_values: Tuple[Tuple[torch.Tensor]]):
+        """Categorize present_key_values from self and cross attention to layer by layer.
+
+        Reorder past state from grouped by self/cross attention to grouped by layer.
+        Before: past_key_self_0, past_value_self_0, past_key_self_1, past_value_self_1, ...,
+                past_key_cross_0, past_value_cross_0, past_key_cross_1, past_value_cross_1, ...
+        After: (past_key_self_0, past_value_self_0, past_key_cross_0, past_value_cross_0),
+                (past_key_self_1, past_value_self_1, past_key_cross_1, past_value_cross_1),
+
+        Args:
+            present_key_values: From past_key_values of a model (group by self and cross attention)
+
+        Returns:
+            past_tuples: present key and values grouped by layer.
+        """
+        past_tuples = ()
+        half_idx = len(past_key_values) // 2
+        for i in range(len(past_key_values) // 4):
+            idx = 2 * i
+            past_tuples += (
+                (
+                    past_key_values[idx],
+                    past_key_values[idx + 1],
+                    past_key_values[half_idx + idx],
+                    past_key_values[half_idx + idx + 1],
+                ),
+            )
+        return past_tuples
+
+    @staticmethod
+    def group_by_self_and_cross(present_key_values: Tuple[torch.Tensor], concat: bool = False):
+        """Categorize present_key_values into self and cross attention.
+
+        Split present state from grouped by layer to grouped by self/cross attention.
+        Before: (past_key_self_0, past_value_self_0, past_key_cross_0, past_value_cross_0),
+                (past_key_self_1, past_value_self_1, past_key_cross_1, past_value_cross_1), ...
+        After: (past_key_self_0, past_value_self_0, past_key_self_1, past_value_self_1, ...),
+                (past_key_cross_0, past_value_cross_0, past_key_cross_1, past_value_cross_1, ...)
+
+        Args:
+            present_key_values: From past_key_values of a model (group by layer)
+            concat: If concat self attention with cross attention key/value to return
+
+        Returns:
+            present_self (Tuple[torch.Tensor]): present key and values from self attention
+            present_cross (Tuple[torch.Tensor]): present key and values from cross attention
+        """
+        present_self: List[torch.Tensor] = []
+        present_cross: List[torch.Tensor] = []
+        for _, present_layer_i in enumerate(present_key_values):
+            assert len(present_layer_i) == 4, f"Expected to have four items. Got {len(present_layer_i)}"
+            present_key_self, present_value_self, present_key_cross, present_value_cross = present_layer_i
+            present_self.extend([present_key_self, present_value_self])
+            present_cross.extend([present_key_cross, present_value_cross])
+        if concat:
+            return present_self + present_cross
+        else:
+            return present_self, present_cross
+
+    @staticmethod
+    def get_input_names(past_key_values: Tuple[Tuple[torch.Tensor]], encoder=True):
+        """Process input names of model wrapper.
+
+        Args:
+            past_key_values: Consider `self` and `cross` past_key_values
+
+        Returns:
+            names (List[string]): input names
+        """
+        names = []
+        num_layers = len(past_key_values) // 4 if encoder else len(past_key_values)
+        prefix = "past_" if not encoder else "present_"
+        for i in range(num_layers):
+            names.extend([prefix + s for s in [f"key_self_{i}", f"value_self_{i}"]])
+        for i in range(num_layers):
+            names.extend([prefix + s for s in [f"key_cross_{i}", f"value_cross_{i}"]])
+        return names
```

## onnxruntime/transformers/models/t5/t5_decoder.py

```diff
@@ -43,14 +43,17 @@
         super().__init__()
         self.decoder = decoder
         self.lm_head = lm_head
         self.config = config
         self.decoder_start_token_id = (
             decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id
         )
+        self.tie_word_embeddings = (
+            self.config.tie_word_embeddings if hasattr(self.config, "tie_word_embeddings") else True
+        )
 
     def forward(
         self,
         decoder_input_ids: torch.Tensor,
         encoder_attention_mask: torch.Tensor,
         encoder_hidden_states: torch.FloatTensor,
     ):
@@ -72,66 +75,70 @@
             use_cache=True,
             return_dict=True,
         )
 
         sequence_output = decoder_outputs.last_hidden_state
         present_key_values = decoder_outputs.past_key_values
 
-        sequence_output = sequence_output * (self.config.d_model**-0.5)
+        if self.tie_word_embeddings:
+            sequence_output = sequence_output * (self.config.d_model**-0.5)
 
         lm_logits = self.lm_head(sequence_output)
         past_self, past_cross = PastKeyValuesHelper.group_by_self_or_cross(present_key_values)
         return lm_logits, past_self, past_cross
 
 
 class T5Decoder(torch.nn.Module):
     """A T5 decoder with LM head and past key values"""
 
     def __init__(self, decoder, lm_head, config):
         super().__init__()
         self.decoder = decoder
         self.lm_head = lm_head
         self.config = config
+        self.tie_word_embeddings = (
+            self.config.tie_word_embeddings if hasattr(self.config, "tie_word_embeddings") else True
+        )
 
-    def forward(self, decoder_input_ids, encoder_attention_mask, encoder_hidden_states, *past):
-
+    def forward(self, decoder_input_ids, encoder_attention_mask, *past):
         past_key_values = PastKeyValuesHelper.group_by_layer(past, self.config.num_layers)
 
+        # This is a hack since only the third dimension of encoder_hidden_states is used here
+        dummy_encoder_hidden_states = encoder_attention_mask.unsqueeze(2)
         decoder_outputs = self.decoder(
             input_ids=decoder_input_ids,
             past_key_values=past_key_values,
-            encoder_hidden_states=encoder_hidden_states,
+            encoder_hidden_states=dummy_encoder_hidden_states,
             encoder_attention_mask=encoder_attention_mask,
             use_cache=True,
             return_dict=True,
         )
 
         sequence_output = decoder_outputs.last_hidden_state
         present_key_values = decoder_outputs.past_key_values
 
-        sequence_output = sequence_output * (self.config.d_model**-0.5)
+        if self.tie_word_embeddings:
+            sequence_output = sequence_output * (self.config.d_model**-0.5)
 
         lm_logits = self.lm_head(sequence_output)
         present_self, _ = PastKeyValuesHelper.group_by_self_or_cross(present_key_values)
 
         # Do not return present_cross since they are identical to corresponding past_cross input
         return lm_logits, present_self
 
 
 class T5DecoderInputs:
     def __init__(
         self,
         decoder_input_ids,
         encoder_attention_mask,
-        encoder_hidden_states,
         past_key_values=None,
     ):
         self.decoder_input_ids: torch.LongTensor = decoder_input_ids
         self.encoder_attention_mask: torch.LongTensor = encoder_attention_mask
-        self.encoder_hidden_states: Union[torch.FloatTensor, torch.HalfTensor] = encoder_hidden_states
         self.past_key_values: Union[List[torch.FloatTensor], List[torch.HalfTensor], None] = past_key_values
 
     @staticmethod
     def create_dummy(
         config: Union[T5Config, MT5Config],
         batch_size: int,
         encode_sequence_length: int,
@@ -150,15 +157,14 @@
             device (torch.device): device of output tensors
             float16 (bool): whether the model uses float32 or float16 in input
             use_int32_inputs(bool): whether use int32 instead of int64 for some inputs
 
         Returns:
             T5DecoderInputs: dummy inputs for decoder
         """
-        hidden_size: int = config.d_model
         num_attention_heads: int = config.num_heads
         num_layers: int = config.num_layers
         vocab_size: int = config.vocab_size
 
         # Do not use head_size = hidden_size / num_attention_heads here.
         # For example, mt5-small, d_model=512 and num_heads=6
         head_size: int = config.d_kv
@@ -177,21 +183,14 @@
             encode_sequence_length,
             vocab_size,
             device,
             use_int32_inputs=use_int32_inputs,
         )
 
         float_type = torch.float16 if float16 else torch.float32
-        encoder_hidden_state = torch.rand(
-            batch_size,
-            encode_sequence_length,
-            hidden_size,
-            dtype=float_type,
-            device=device,
-        )
 
         if past_decode_sequence_length > 0:
             self_attention_past_shape = [
                 batch_size,
                 num_attention_heads,
                 past_decode_sequence_length,
                 head_size,
@@ -208,33 +207,30 @@
                 past.append(torch.rand(self_attention_past_shape, dtype=float_type, device=device))
 
             for _ in range(2 * num_layers):
                 past.append(torch.rand(cross_attention_past_shape, dtype=float_type, device=device))
         else:
             past = None
 
-        return T5DecoderInputs(decoder_input_ids, encoder_inputs.attention_mask, encoder_hidden_state, past)
+        return T5DecoderInputs(decoder_input_ids, encoder_inputs.attention_mask, past)
 
     def to_list(self) -> List:
         input_list = [
             self.decoder_input_ids,
             self.encoder_attention_mask,
-            self.encoder_hidden_states,
         ]
         if self.past_key_values:
             input_list.extend(self.past_key_values)
         return input_list
 
     def to_fp32(self):
-        encoder_hidden_state = self.encoder_hidden_states.to(dtype=torch.float32)
         past = [p.to(dtype=torch.float32) for p in self.past_key_values] if self.past_key_values else None
         return T5DecoderInputs(
             self.decoder_input_ids.clone(),
             self.encoder_attention_mask.clone(),
-            encoder_hidden_state,
             past,
         )
 
 
 class T5DecoderHelper:
     @staticmethod
     def export_onnx(
@@ -269,31 +265,29 @@
 
         past_names = PastKeyValuesHelper.get_past_names(decoder.config.num_layers, present=False)
         present_names = PastKeyValuesHelper.get_past_names(decoder.config.num_layers, present=True)
         present_self_names = present_names[: 2 * decoder.config.num_layers]
 
         input_past_names = past_names if isinstance(decoder, T5Decoder) else []
         output_present_names = present_self_names if isinstance(decoder, T5Decoder) else present_names
-        output_names = ["logits"] + output_present_names
+        output_names = ["logits", *output_present_names]
 
         # Shape of input tensors (sequence_length==1):
         #    input_ids: (batch_size, sequence_length)
         #    encoder_attention_mask: (batch_size, encode_sequence_length)
-        #    encoder_hidden_states: (batch_size, encode_sequence_length, hidden_size)
         #    past_self_*: (batch_size, num_heads, past_decode_sequence_length, head_size)
         #    past_cross_*: (batch_size, num_heads, encode_sequence_length, head_size)
 
         # Shape of output tensors:
         #    logits: (batch_size, sequence_length, vocab_size)
         #    past_self_*: (batch_size, num_heads, past_decode_sequence_length + sequence_length, head_size)
         #    past_cross_*: (batch_size, num_heads, encode_sequence_length, head_size)
 
         input_names = ["input_ids"]
         input_names.append("encoder_attention_mask")
-        input_names.append("encoder_hidden_states")
         input_names.extend(input_past_names)
 
         dynamic_axes = {
             "input_ids": {
                 0: "batch_size",
                 # 1: 'sequence_length'
             },
@@ -358,15 +352,14 @@
     def onnxruntime_inference(ort_session, inputs: T5DecoderInputs):
         """Run inference of ONNX model."""
         logger.debug("start onnxruntime_inference")
 
         ort_inputs = {
             "input_ids": numpy.ascontiguousarray(inputs.decoder_input_ids.cpu().numpy()),
             "encoder_attention_mask": numpy.ascontiguousarray(inputs.encoder_attention_mask.cpu().numpy()),
-            "encoder_hidden_states": numpy.ascontiguousarray(inputs.encoder_hidden_states.cpu().numpy()),
         }
 
         if inputs.past_key_values:
             assert len(inputs.past_key_values) % 4 == 0
             num_layers = int(len(inputs.past_key_values) / 4)
             past_names = PastKeyValuesHelper.get_past_names(num_layers)
             for i, past_tensor in enumerate(inputs.past_key_values):
@@ -380,25 +373,25 @@
         model: Union[T5Decoder, T5DecoderInit],
         ort_session: InferenceSession,
         device: torch.device,
         use_int32_inputs: bool,
         max_cases: int = 4,
     ):
         """Compare the result from PyTorch and OnnxRuntime to verify the ONNX model is good."""
-        float16: bool = TypeHelper.get_input_type(ort_session, "encoder_hidden_states") == "tensor(float16)"
+        float16: bool = TypeHelper.get_input_type(ort_session, "past_key_self_0") == "tensor(float16)"
 
         test_cases = [(4, 11, 3), (1, 2, 5), (3, 1, 1), (8, 5, 2)]
         test_cases_max_diff = []
         for (
             batch_size,
             encode_sequence_length,
             past_decode_sequence_length,
         ) in test_cases[:max_cases]:
             if isinstance(model, T5DecoderInit):
-                past_decode_sequence_length = 0
+                past_decode_sequence_length = 0  # noqa: PLW2901
 
             inputs = T5DecoderInputs.create_dummy(
                 model.config,
                 batch_size,
                 encode_sequence_length,
                 past_decode_sequence_length,
                 device=device,
@@ -430,12 +423,15 @@
                         numpy.abs(torch_outputs[2][i].cpu().numpy() - ort_outputs[1 + 2 * model.config.num_layers + i])
                     )
                     logger.debug(f"cross attention past state {i} max_diff={max_diff}")
                     max_diff_all = max(max_diff_all, max_diff)
 
             test_cases_max_diff.append(max_diff_all)
             logger.info(
-                f"batch_size={batch_size}, encode_sequence_length={encode_sequence_length}, "
-                + f"past_decode_sequence_length={past_decode_sequence_length}, max_diff={max_diff_all}"
+                "batch_size=%s, encode_sequence_length=%s, past_decode_sequence_length=%s, max_diff=%s",
+                batch_size,
+                encode_sequence_length,
+                past_decode_sequence_length,
+                max_diff_all,
             )
 
         return max_diff_all
```

## onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py

```diff
@@ -123,15 +123,15 @@
             device=device,
             use_int32_inputs=use_int32_inputs,
         )
         input_list = inputs.to_list()
 
         present_names = PastKeyValuesHelper.get_past_names(model.config.num_layers, present=True)
 
-        output_names = ["logits", "encoder_hidden_states"] + present_names
+        output_names = ["logits", "encoder_hidden_states", *present_names]
 
         # Shape of input tensors (sequence_length==1):
         #    input_ids: (batch_size, sequence_length)
         #    encoder_attention_mask: (batch_size, encode_sequence_length)
         #    encoder_hidden_states: (batch_size, encode_sequence_length, hidden_size)
         #    past_self_*: (batch_size, num_heads, past_decode_sequence_length, head_size)
         #    past_cross_*: (batch_size, num_heads, encode_sequence_length, head_size)
@@ -251,15 +251,15 @@
     ):
         """Compare the result from PyTorch and OnnxRuntime to verify the ONNX model is good."""
         ort_inputs = ort_session.get_inputs()
         use_decoder_input_ids = len(ort_inputs) == 3
 
         test_cases = [(4, 11), (1, 2), (3, 1), (8, 5)]
         test_cases_max_diff = []
-        for (batch_size, encode_sequence_length) in test_cases[:max_cases]:
+        for batch_size, encode_sequence_length in test_cases[:max_cases]:
             inputs = T5EncoderDecoderInitInputs.create_dummy(
                 model.config,
                 batch_size,
                 encode_sequence_length,
                 use_decoder_input_ids=use_decoder_input_ids,
                 device=device,
                 use_int32_inputs=use_int32_inputs,
```

## onnxruntime/transformers/models/t5/t5_helper.py

```diff
@@ -15,17 +15,17 @@
 from t5_encoder import T5Encoder, T5EncoderHelper
 from t5_encoder_decoder_init import T5EncoderDecoderInit, T5EncoderDecoderInitHelper
 from transformers import MT5ForConditionalGeneration, T5ForConditionalGeneration
 
 from onnxruntime import InferenceSession
 
 sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
-from float16 import float_to_float16_max_diff
-from onnx_model import OnnxModel
-from optimizer import optimize_model
+from float16 import float_to_float16_max_diff  # noqa: E402
+from onnx_model import OnnxModel  # noqa: E402
+from optimizer import optimize_model  # noqa: E402
 
 logger = logging.getLogger(__name__)
 
 PRETRAINED_T5_MODELS = ["t5-small", "t5-base", "t5-large", "t5-3b", "t5-11b"]
 PRETRAINED_MT5_MODELS = ["google/mt5-small", "google/mt5-base", "google/mt5-large", "google/mt5-xl", "google/mt5-xxl"]
 
 
@@ -146,34 +146,30 @@
                 use_external_data_format,
                 use_int32_inputs,
             )
 
     @staticmethod
     def auto_mixed_precision(
         onnx_model: OnnxModel,
-        op_block_list: List[str] = [
-            "Pow",
-            "ReduceMean",
-            "Add",
-            "Sqrt",
-            "Div",
-            "Mul",
-            "Softmax",
+        op_block_list: List[str] = [  # noqa: B006
+            "SimplifiedLayerNormalization",
+            "SkipSimplifiedLayerNormalization",
             "Relu",
+            "Add",
         ],
     ):
         """Convert model to mixed precision.
            It detects whether original model has fp16 precision weights, and set parameters for float16 conversion automatically.
         Args:
             onnx_model (OnnxModel): optimized ONNX model
-            op_block_list (List[str], optional): . Defaults to ["Pow", "ReduceMean", "Add", "Sqrt", "Div", "Mul", "Softmax", "Relu"]
+            op_block_list (List[str], optional): . Defaults to ["SimplifiedLayerNormalization", "SkipSimplifiedLayerNormalization", "Relu", "Add"]
         Returns:
             parameters(dict): a dictionary of parameters used in float16 conversion
         """
-        op_full_set = set([node.op_type for node in onnx_model.nodes()])
+        op_full_set = {node.op_type for node in onnx_model.nodes()}
         fp32_op_set = set(op_block_list)
         fp16_op_set = op_full_set.difference(fp32_op_set)
         logger.info(f"fp32 op: {fp32_op_set} fp16 op: {fp16_op_set}")
 
         # logits is the first output
         logits_output_name = onnx_model.graph().output[0].name
 
@@ -224,25 +220,36 @@
         onnx_model_path: str,
         optimized_model_path: str,
         is_float16: bool,
         num_attention_heads: int,
         hidden_size: int,
         use_external_data_format: bool = False,
         auto_mixed_precision: bool = True,
+        use_gpu: bool = False,
     ):
         """Optimize ONNX model with an option to convert it to use mixed precision."""
+
+        from fusion_options import FusionOptions
+
+        optimization_options = None
+        if is_float16:
+            optimization_options = FusionOptions("t5")
+            optimization_options.enable_skip_layer_norm = False
+
         m = optimize_model(
             onnx_model_path,
-            model_type="bert",  # TODO: support optimization for t5
+            model_type="t5",
             num_heads=num_attention_heads,
             hidden_size=hidden_size,
-            opt_level=0,
-            optimization_options=None,
+            opt_level=2 if not use_external_data_format else 0,
+            optimization_options=optimization_options,
             use_gpu=False,
+            only_onnxruntime=not use_gpu,
         )
+
         if is_float16:
             if auto_mixed_precision:
                 T5Helper.auto_mixed_precision(m)
             else:
                 m.convert_model_float32_to_float16(cast_input_output=False)
 
         m.save_model_to_file(optimized_model_path, use_external_data_format, all_tensors_to_one_file=True)
```

## Comparing `onnxruntime_azure-1.14.0.dist-info/METADATA` & `onnxruntime_azure-1.15.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: onnxruntime-azure
-Version: 1.14.0
+Version: 1.15.0
 Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
 Home-page: https://onnxruntime.ai
 Author: Microsoft Corporation
 Author-email: onnxruntime@microsoft.com
 License: MIT License
 Download-URL: https://github.com/microsoft/onnxruntime/tags
 Keywords: onnx machine learning
@@ -40,14 +40,19 @@
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_ or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 
 
 Changes
 -------
 
+1.15.0
+^^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.15.0
+
 1.14.0
 ^^^^^^
 
 Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.14.0
 
 1.13.0
 ^^^^^^
```

## Comparing `onnxruntime_azure-1.14.0.dist-info/RECORD` & `onnxruntime_azure-1.15.0.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,91 +1,93 @@
 onnxruntime/LICENSE,sha256=wlDWJ48LR6ZDn7dZKwi1ilXrn1NapJodtjIRw_mCtnQ,1094
 onnxruntime/Privacy.md,sha256=v7dxKwdfPwfj6-5dwqKW0d4y2_ca0oZj9z0VOMtsOwg,2490
-onnxruntime/ThirdPartyNotices.txt,sha256=sAGw25XZolotxsKwrcjeRGMX33laLM7QNMwpsg6f3L0,305548
-onnxruntime/__init__.py,sha256=7W8cxH1ygZgMviak3fnKAczYqkXmQfDrbcDICJMB9NA,2670
-onnxruntime/backend/__init__.py,sha256=SQO4TOpHR3W4ZBU-3a9AqvXV5jKLdirR9erp51IPS2Y,320
-onnxruntime/backend/backend.py,sha256=_pIh57nFnYtIPJuq9NTbcMCEYY9wUf-da7o6A5ojTEM,8145
-onnxruntime/backend/backend_rep.py,sha256=56w0IreQuda8o6KU37Xtk5zEgNHTRn6xQopOzr9s5fM,1816
+onnxruntime/ThirdPartyNotices.txt,sha256=5JKbIGOHGGRdPMp-y8rpLKWLBFnxjFL51k1QjO0G4fA,318315
+onnxruntime/__init__.py,sha256=xMLAGr4te5HbbG-PBn4tuIXiMPz4lZFE7Vmm41LjEnA,4246
+onnxruntime/backend/__init__.py,sha256=5I1Ylsawf9w6MNmK4RiN1wA-EEQqlKKwYTNZB-m_k6M,334
+onnxruntime/backend/backend.py,sha256=0jIj7NPKBFw-c0g6zNoWmmeimD6GaVetc0KHJhIO1nc,8141
+onnxruntime/backend/backend_rep.py,sha256=8Hid8lLPmcBtXsEUfpXsamX0pN5XATIIun-U7A6oNmk,1821
 onnxruntime/capi/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
-onnxruntime/capi/_ld_preload.py,sha256=j0atUD-MKKr1H6VafQ9g_hX8yC881BPF9SFR0v9EKnM,864
+onnxruntime/capi/_ld_preload.py,sha256=li6cbZ64hDfUndat4mprUWzowLa3RQdw0q2E56sXFwE,413
 onnxruntime/capi/_pybind_state.py,sha256=P8zE305Trv-939aEa5tfb9gdjeQAUer66JJnyzVwXqw,1544
-onnxruntime/capi/onnxruntime_collect_build_info.py,sha256=7daIDN7UN1qQuYQLny5K-qdFe-q9RZGrmIznmd6Vc1Y,4064
-onnxruntime/capi/onnxruntime_inference_collection.py,sha256=zYDdyKDs-nIADsz6plEt6I5U5-1RqQOD7NX79ZW-zIQ,38915
-onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=Q2jSJEeG_ZCQWgGjtWOU2wo9LQ7sV9c5w-WjU-9bvgM,21912
-onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=KwNPnAbRFGRSLoaPNgYAPxw1cJoLQY2eOoqB8zRt0p8,19383200
-onnxruntime/capi/onnxruntime_validation.py,sha256=0nNLuuVwIKfv3kLppu_lzIIK0iiss7-Yx06t9FyTSzI,6336
+onnxruntime/capi/onnxruntime_collect_build_info.py,sha256=N7ViCgTVKYLPiHXhf16ZkGK2FVNB3PzfWFLU4ykP28w,4068
+onnxruntime/capi/onnxruntime_inference_collection.py,sha256=x2Fd_LzvV8VJnjTyLsQx9DUvT6-UzwmCiTqfoyBTxt4,39683
+onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=8r85s2PWg8U1EwXLIuqTIo4p7vl23wXilHoyo4zWTCE,21936
+onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=VB1TuRVe58hQM_I4xf3RIDCu-0isFRQmBMZ_88J-suY,19976120
+onnxruntime/capi/onnxruntime_validation.py,sha256=ZF14DZc9hI5wVQjYzn4bWe0AauNLEX0qapeIH2MPino,6382
 onnxruntime/capi/version_info.py,sha256=g2pKZXlThlkMbp7JytLW6rOC3M9Scbx44P9WBLULNaQ,33
 onnxruntime/capi/training/__init__.py,sha256=V63zeaS2MljWyqwkvAq6Elo8phOZdPGZNa_1fbZrIig,326
-onnxruntime/datasets/__init__.py,sha256=nVYftV07eIY7anQBOdZ0RZOvqMSUdIQzK2hrdkLmydI,480
+onnxruntime/datasets/__init__.py,sha256=0D1rdhXK940JccUq3Sj4BBMqjDpAPOcxlGcwJR4X3wc,471
 onnxruntime/datasets/logreg_iris.onnx,sha256=giR4TJjXNBLZ_ZmrzVejhWi9WQmA0PvlkWRkUxxS6Pw,670
 onnxruntime/datasets/mul_1.onnx,sha256=cfQxxOkyHsb76xWNAu0kBFmn3MmGc_p5pPQ5zkLvrxA,130
 onnxruntime/datasets/sigmoid.onnx,sha256=U0Crpnp-NHUWKteUN4r1XxcY9V-aXXS0r2Dsx_emJLY,103
-onnxruntime/quantization/__init__.py,sha256=N2YgQnPFYf16CpnqEQHSLYGE8MAkQLRGd3yg5C7wHpo,441
-onnxruntime/quantization/calibrate.py,sha256=b8u--if3jjyHqNOo1C_Ca5qy_LfKv1RxxggQ1M7OgG0,38106
-onnxruntime/quantization/onnx_model.py,sha256=eXaXlMEptm34ev-O7uNWFQlc67T9QZ7heI20dMjh4L4,18572
-onnxruntime/quantization/onnx_quantizer.py,sha256=HeflSDEEpzTAg7nLQcsPGbAcwQvqQBxdm-KTu7Gf4K4,46368
+onnxruntime/quantization/__init__.py,sha256=eeIgS5jf18UjGelvD4Bf57Z6-Qxvg6J54V-PEtlcww0,686
+onnxruntime/quantization/calibrate.py,sha256=hIhZRSGKrwSPZ4aV45QGVCQdebz9TKH5mQObemCqJhE,38011
+onnxruntime/quantization/onnx_model.py,sha256=0h7jgqANlOrlKFzXdek9rYuMh6Fdo-m_epeeyByqcsM,18686
+onnxruntime/quantization/onnx_quantizer.py,sha256=K66CJpD7CitF7J-vJ7zmtRwQDHeWH-N50iRmrjmNRPo,46664
 onnxruntime/quantization/preprocess.py,sha256=gaoyDn-VYzWDvox65Wtu3ZnnYfUtgllucByj2O2kq6E,5045
-onnxruntime/quantization/qdq_loss_debug.py,sha256=s4duitYQdidqE6KEytAnEnmykV0vEULuOOSb3yG6Csw,15308
-onnxruntime/quantization/qdq_quantizer.py,sha256=pkLvqZkI4pTfGEaB5nhC_cCzsao7qsYu5iHbSLctzYY,18436
-onnxruntime/quantization/quant_utils.py,sha256=ZA2PEaoq0odagyDpOUoIGpHW_dVxOXRhPWcDb4jjV0Q,19882
-onnxruntime/quantization/quantize.py,sha256=HGOsVpVjTpKn1hVtF68y-kQf2MEytlJe0bEYRO19oj4,27812
-onnxruntime/quantization/registry.py,sha256=2BZv32XAehe6UTAIdVNivqG0XghbgxoK6tfhHGPKYTw,3498
-onnxruntime/quantization/shape_inference.py,sha256=4fGQfwovMXlzJrFEG9Jjnl9Fk22YazwrMQ05zyRGSyA,6154
-onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py,sha256=yf6xYwFVZxvaUpt5RrZ3YZU251UnNal5va6WG-JX4Js,2076
-onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py,sha256=50M8NCrH_HrUpvU3GDYVtYY9hl9WI8vOMOHs-fKGf-U,2477
+onnxruntime/quantization/qdq_loss_debug.py,sha256=sFweAtHoO0mclnSIN4tuUsRtv1msgY3gXzkrJRMm7tE,15307
+onnxruntime/quantization/qdq_quantizer.py,sha256=TbyW2zj_XLppuduaeK90Q3MVLMscKSk43js4C-un0Xs,19061
+onnxruntime/quantization/quant_utils.py,sha256=DZRb5H_RYow7txO5N9WovEERy801wh4288nU5yXtBRw,20102
+onnxruntime/quantization/quantize.py,sha256=HT2rMx-0EEfk1tAtZNs__DBftB6CSpwQJnYv9ihZrHA,28359
+onnxruntime/quantization/registry.py,sha256=-nAxLSzcb8pxBaJcwKfwsvdFt2DOihcMSqkNGWa5H6o,3671
+onnxruntime/quantization/shape_inference.py,sha256=wg9a5h95j7xzpP7VSreIKTUCoedR_FCbz8j25yY2bps,6149
+onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py,sha256=e-jJFhw9fb775fDCLnWdbRSdoJ6vGD0c7qTnkIG-vNs,2250
+onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py,sha256=QQ9_f60Wya8U-KQOMu0gXImfhiPN6jNkfjpoCdAFic4,2665
 onnxruntime/quantization/CalTableFlatBuffers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/quantization/operators/__init__.py,sha256=IfKXrFWtRSye1mkgD9lpwxio0fw9cVr_1CdV1cvefig,85
-onnxruntime/quantization/operators/activation.py,sha256=MUhUAHZ_5P0pquIRcfE6FCqCjxQSg6Plj1LJbIhx_jI,4495
+onnxruntime/quantization/operators/activation.py,sha256=JMkSthxHxIJe4wDnzhxi9nXmSdIG2Q98E7ahxXp3llM,4463
 onnxruntime/quantization/operators/argmax.py,sha256=pfE9_eSTZ2otTkUcWwlLi7HJKtN10kE5c2Lz0SeVADQ,589
-onnxruntime/quantization/operators/attention.py,sha256=mMERM7TgDXan1Cs86m5AHpDND9BK2QZY1kBNXMBalRk,2625
+onnxruntime/quantization/operators/attention.py,sha256=eH7-Z3MfP6xRZCdhDAyNxWG2s2nZILxIEFVAHtqj7EQ,2637
 onnxruntime/quantization/operators/base_operator.py,sha256=vrAVfKJXZvF7ZherKw4JUGonNyNuoU2TWnwBy-EQ3QE,1118
-onnxruntime/quantization/operators/binary_op.py,sha256=MRqGkeSbVvTE77FxxRv0lCYLPSZJ7Sh39fFzbcLYQbU,2536
-onnxruntime/quantization/operators/concat.py,sha256=zR3XQ8574QCIEgdVCzHwmy1oJWSC03u78KIHctQJkWI,2096
-onnxruntime/quantization/operators/conv.py,sha256=AwkCBV-J0VmNRhEhvz40ElhdsRgy69Dwzl5yUDhKAEk,9713
+onnxruntime/quantization/operators/binary_op.py,sha256=pEQHRAS75EMp7LG6jzWV7gDQt_vzEPLJEI00eIOuoiA,2544
+onnxruntime/quantization/operators/concat.py,sha256=F8hZfd6dcnU-J2BxMHJj2FL1AIxabHIuOyFybSh20Xk,2149
+onnxruntime/quantization/operators/conv.py,sha256=nXNW4t6PDlyl7yDlesGjkHWO7ZJyammfXVJILXXDZd4,9695
 onnxruntime/quantization/operators/direct_q8.py,sha256=jNL6DZGKcc1GjvBTlO5m3uO5hsMKZzwE_9_KIpdp4EI,3350
-onnxruntime/quantization/operators/embed_layernorm.py,sha256=Cz3sDA-aB7qoeu1SuUG1WmQTo2i-SRpuoTGlRR9UbJY,4046
+onnxruntime/quantization/operators/embed_layernorm.py,sha256=2LsZk5Um0ELaRESWjScgYyQioJelRZK6oQbzAclSgXI,4058
 onnxruntime/quantization/operators/gather.py,sha256=oYPW3XdwWo7kqPYbEvCqPC6njAC2_zJN7c46z1xp6QE,2166
-onnxruntime/quantization/operators/gavgpool.py,sha256=PA2jaqZhwsg3lcSjidPN_7G4czVRQyXlkY0wPK6B27I,2451
-onnxruntime/quantization/operators/gemm.py,sha256=0O_ODI52wg5wfaBOiw1SYabooo6Agl9kaf_QKe6li6c,5829
-onnxruntime/quantization/operators/lstm.py,sha256=gGbB9o5_eo8nG62BPxuhp2MZlKcRYkMe_c_uEbW1VGg,4786
-onnxruntime/quantization/operators/matmul.py,sha256=RV-5h7cyVB8EvxIc4Y0jiX36tdI3uuI2P7cgsY84Tho,7796
+onnxruntime/quantization/operators/gavgpool.py,sha256=wYyjEf3h-_QChWKnsZ2N-haBG1RSvqRitZ-Yvfwo9Dk,2445
+onnxruntime/quantization/operators/gemm.py,sha256=dN47axddU9tl39NviPccIRPGu2qmjHEMkVpIN-P--uA,5905
+onnxruntime/quantization/operators/instnorm.py,sha256=hQ4cAMAVL5FXVWrMyTey9fXEhBiCja8HtQcYWZgWHAk,1114
+onnxruntime/quantization/operators/lstm.py,sha256=UBaWDHQQ1_FVLerZ_vC8wJeHaVXUvuKtMKgt4shljVE,5050
+onnxruntime/quantization/operators/matmul.py,sha256=PujLylxLAgJ_g9pA_m3H-wB1Fnfy2Fg1qEUIgJONWiU,7762
 onnxruntime/quantization/operators/maxpool.py,sha256=QyDmHyBo0QKf6kNFbp2a9v6ThrBO-OL3tW0PFdN6bkI,961
-onnxruntime/quantization/operators/pad.py,sha256=nH8TUaLDWl4R2dh_rR0nyztTvNM5kzZps7PbyW3LzaQ,4297
-onnxruntime/quantization/operators/pooling.py,sha256=aBg0jJe_k0Z7rIxEWwRy_Cfg8olDKRsjeyI6IigLOIU,2291
-onnxruntime/quantization/operators/qdq_base_operator.py,sha256=ubJ_847GNihEItm0GnV27DrNo8QCUg6mepnYrXs4i04,840
+onnxruntime/quantization/operators/pad.py,sha256=wenZNSN64H3-X4kgBlZW-1lTg-iGVuq863v2bwRCQag,4277
+onnxruntime/quantization/operators/pooling.py,sha256=L0IT7G6-2XSx9-wUz5BX59Mc43FfJEg79NwW3yqEDhI,2285
+onnxruntime/quantization/operators/qdq_base_operator.py,sha256=Fco9JZxrXQoVgjKvmHFuzT0mogWo9-wHiDa51CjTioo,823
 onnxruntime/quantization/operators/resize.py,sha256=BMeym-7GHOSnGpZisa9BkdQkVmCXwKANA5NpnKRnaLI,962
-onnxruntime/quantization/operators/softmax.py,sha256=zHeDA2SXhvndS7DAdKX7t1FwiG3DTU7pc_oWmLon5kc,3165
-onnxruntime/quantization/operators/split.py,sha256=sH3RE_TaJHkEaudHy40XP_Wu_xCGtjQuhxJ4px_CsM4,2250
-onnxruntime/quantization/operators/where.py,sha256=13oh6FdrtzfJydzANfe04HHvXMj2_QrL4j-ri0VW7mk,3133
-onnxruntime/tools/__init__.py,sha256=yR6vBVITzTdUI8JnLTlj3zhkbR18L2kb1XKiYS09HOQ,522
-onnxruntime/tools/check_onnx_model_mobile_usability.py,sha256=1aepP1_RcWA8MRuxTzwYha2cLfo3mThNo7GDVJCzBbc,2868
-onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=qPlThp-PYUGbGdX3WdEVjZCyY3WfOCPKESaniRcLMCs,16132
+onnxruntime/quantization/operators/softmax.py,sha256=aUy4yDGdCQ-vptejnNowfae3H8R5P40HszeiXBCtBGk,3386
+onnxruntime/quantization/operators/split.py,sha256=ZY8aEpiF2xD0r5DTmm3wVlcpsepd-FOSYMZ86XCwUeI,2244
+onnxruntime/quantization/operators/where.py,sha256=wd6PQ7LlbrJTqamFMch_Fipnbt4IewMJSAPozMTrwKI,3127
+onnxruntime/tools/__init__.py,sha256=7up7iKcklVy6UcpIIIIlBaK690O32vaOxyaaTWvwyxU,528
+onnxruntime/tools/check_onnx_model_mobile_usability.py,sha256=h-xTaXu_uSVptpmx69FiYcwACzuvI-4sTqLKkKXMo08,2871
+onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=TnqL0j4AdgCXdR1Iu-hHBNI02vMt3vMFnBtSGG4hIWs,16064
 onnxruntime/tools/file_utils.py,sha256=ONHY-VlxAJ7mlrTNZYkRD4I00RqsSHMZb1rUUxceQss,1569
 onnxruntime/tools/logger.py,sha256=ikKm7kP-W4Hjl2UuDx-WUViFrWU7qak-WzdoIgNyAMc,286
 onnxruntime/tools/make_dynamic_shape_fixed.py,sha256=GkbUE5kH1pOua5EJVH4trXs7mJIPIQ8T2YTeKQxr6ak,2608
-onnxruntime/tools/onnx_model_utils.py,sha256=SUx5s3XLE960YZ9Gm5boDVQIJk7pUlvGZ0th2sPMDE4,14368
+onnxruntime/tools/offline_tuning.py,sha256=Gd120-LGX04OJZ8nvErr_8h-5XGdDOEmuJPCWVQC76E,6380
+onnxruntime/tools/onnx_model_utils.py,sha256=rAt-27nQgDJ_cQPvg9Pc3Sub6a9dYoIgn8ja6isKTSo,14355
 onnxruntime/tools/onnx_randomizer.py,sha256=9L96dzIf59cQ2oQsmR2EEsdrR4hHwEGrpZkajEgUPAY,3361
-onnxruntime/tools/onnxruntime_test.py,sha256=LjmEWa9-Bxs5cyXxrMmgyUbeMf7uMDO5MC_sowwY1rY,5667
+onnxruntime/tools/onnxruntime_test.py,sha256=cyb-WS4KNKY6iaeCwDpzipMM5Sw2YLgPckJqqtRuINk,5657
 onnxruntime/tools/optimize_onnx_model.py,sha256=S6-B-YorPT48PnkUZZLxPQnGVhhuO0mRiNbG26JAfxg,1949
-onnxruntime/tools/pytorch_export_contrib_ops.py,sha256=eEjWo1wSZNlLryHZ1OhKUmhSBcRGxgFsU_Ugk4J6CvA,4077
-onnxruntime/tools/pytorch_export_helpers.py,sha256=ckFYt_eUq9WiZdzrSzV6SvtYRBOjtoVSYrgNjG-xxb8,5970
-onnxruntime/tools/reduced_build_config_parser.py,sha256=Axc3UdJMXLBa1jIpjR7iPED8axvN60fxvjeYBqSBrl8,10160
-onnxruntime/tools/symbolic_shape_infer.py,sha256=ms9orzBZJOz7t3T7bVt8ze1Acnv_zIzWAKnuETMhuV8,116516
+onnxruntime/tools/pytorch_export_contrib_ops.py,sha256=xxlw5jPDy72tWEPPYn8Qhof4H-edK7RwpT0ZXtWYfC4,4091
+onnxruntime/tools/pytorch_export_helpers.py,sha256=MRegHn3z3VhVbZQ4O-kTGedIE-pufyxhq1A1GVIdCjY,5971
+onnxruntime/tools/reduced_build_config_parser.py,sha256=O9XtpCRKoFiPcXuwfyGH1zcvpVU0cbOq9JxFh0Jm-Fs,10137
+onnxruntime/tools/symbolic_shape_infer.py,sha256=-XVLniCkGUbEZRH84MfSTkavu7D3m14SMzhE4D_A0k8,129481
 onnxruntime/tools/update_onnx_opset.py,sha256=fplb1ypV-pFhu8Xsi5u_bDfI7EsC4zamJkTziccgQ2c,1182
 onnxruntime/tools/mobile_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py,sha256=I_nymtmLjh7uuCtLK0sEm8jnsnJ-PtLhd4vR6eOG6Ug,12697
-onnxruntime/tools/mobile_helpers/coreml_supported_ops.md,sha256=xkMcUteZlXdCCPLEktF-iwNRkdJo3vOx7-QCLENyzw8,982
+onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py,sha256=hKH_tiR90ctxNrutkA1_OCZs17l4xgircarz8Or_CN4,12691
+onnxruntime/tools/mobile_helpers/coreml_supported_ops.md,sha256=PDRe4fer6SAA9jIoMQrPX7I7CHBU1dbiH29FEQEdYHk,1319
 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config,sha256=nDi5sBRRAFxhelU7H6SJUEHuxiUfFRE8MIjw7sVJCXs,3069
-onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md,sha256=CGV_wu4u45-LCniJFFGyJ-LQe-CgIb7ePlk3JZ6pSVc,2179
-onnxruntime/tools/mobile_helpers/usability_checker.py,sha256=Jq0MZM0cCI7U9iS8WO5Vlrr8N4DU_QlyoEEezHEdC10,25781
-onnxruntime/tools/ort_format_model/__init__.py,sha256=xAd4FSiBLrRqJjrDxBr7jjKO2cJ1KjNAZQf0q96MOLM,1235
-onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=cr9lmN748dt5-YHUV2RkQAmjpxscUYNnT8ZGRNAPAIY,27375
-onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=ghIXifBx_Pzxv3cIckmfahLajcQYulxQT03HtjaSyuA,4472
-onnxruntime/tools/ort_format_model/types.py,sha256=zQKyZDzy7l5X2Emhjm6AXZC4UrgHHY_08CWiLkwPwek,4172
-onnxruntime/tools/ort_format_model/utils.py,sha256=ADjN1yZ7rgf0NAdyab7CL8G9RpiauvPJBbwHGLkQ1I0,2614
+onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md,sha256=j43sa6p93T_pswIMlzHRfXH3VHY4IrZFY9a0QYzfPbE,2226
+onnxruntime/tools/mobile_helpers/usability_checker.py,sha256=XddT0FQjfSdgp6XMoImbPdWbtKmEGoUnZWswDvgsl-Q,25768
+onnxruntime/tools/ort_format_model/__init__.py,sha256=gQqh9tWzGxeUllyIEF2FmIfee9ulji3mlJQNW7QrpJ0,1378
+onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=Oyjk4bhv6CYsAni7_crv-NcR08NlCsWzQznTECke8Xk,27366
+onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=sT2if_kb7cwwfLp3m1zXPTNqy5pxn2NnitMrXjSftos,4484
+onnxruntime/tools/ort_format_model/types.py,sha256=gpjqcHLTox5fxkxP6XA_Tvl2_NmUc-n-q_QPxgGTXZg,4154
+onnxruntime/tools/ort_format_model/utils.py,sha256=Ix5mFXZCnMEHf8Tg7Mwg2GFdy0d1l-zocT2fsE8_8sU,2604
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py,sha256=KNRBlqUVQKtG2E7c2TvSUb29R3UXcR9cPUpdtmnR9QI,149
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py,sha256=Tr3VGnO2r4ZGT-OP_96qqUzTap1DS_0lysfzgODa1nQ,1611
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py,sha256=wfv752tTginm2d0tr2QecHSZAGoCTLwwTifXIXJC33A,9310
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py,sha256=1_69-oW8C6M_Wq2_1TXwXL_ryHXUpxIC42CU_QPYFss,348
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py,sha256=cXdas8VWnTkcmOLfMcOVnMO3CPv6ETOkz4SizEiGAt4,3754
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py,sha256=ez5IgvSx0ds8ktg6u4_YtJ1KAjRbT7Wv203bHeEDZyk,1924
@@ -116,95 +118,109 @@
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py,sha256=AB9-zZAM9bJjrWNbe8QIOR37i92jPeBGlgd1uRp0vD0,1673
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py,sha256=KYzRQPUQ3INAPl7SoTZhALOVWDyQGdXg-Gm5qjfUvHE,5144
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py,sha256=Q45onOhkRAMjauV4eytYveheZsnItgPOjKUKUI-dgzA,408
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py,sha256=IGgnA7Kgz00idyxMVDAy-m-CAxvcl4NtGIai7L9TEVw,1828
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py,sha256=BozLwO5lX5f___cutnpJDD2oJWOnJqDDaeJPJaqD_WQ,2039
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py,sha256=vLFbH5F9iaEEF5d0OSnt1bvlAzQ9dddPc7pDu5uhvLA,200
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py,sha256=oT0gX2UAHgZqIn7oyetzlJIvQmFgeSeNuibt0TLr3dQ,2118
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py,sha256=Q1xYqjyNFFo57GhsZg6sr4EiYhUuLuFjAFCkINYC3tk,259
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py,sha256=au-zd_Hu8RMFk1WMspyWvkkh09aN0gk-KnwF79ef5oo,253
 onnxruntime/tools/qdq_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime/tools/qdq_helpers/optimize_qdq_model.py,sha256=I9PZNo_cyS48bNLh9puZPP_MUssqMUaRsFHQrpcqSmM,1171
-onnxruntime/tools/qdq_helpers/qdq_model_utils.py,sha256=rneNH3VkXGJd5MtKwTwOfoz82RXxEi7V8B9GCp9Ofjg,4925
-onnxruntime/transformers/__init__.py,sha256=X3h9Eoh-GmVrRoSqGT6FLRClOYfd5qif1LnUNxSWSl4,512
+onnxruntime/tools/qdq_helpers/optimize_qdq_model.py,sha256=9kpU0Dukc0ZEHnjwO7NqNFUFVRtOx8xgFPVWZpXkEcQ,1279
+onnxruntime/transformers/__init__.py,sha256=fvi2cqSNrKFRuEnpSzac01zdg-O8py6bYOtnv988gDo,552
 onnxruntime/transformers/affinity_helper.py,sha256=KOKBvzoBr-wOk0QHMGKzY1uy1iI7E6eHpYwBdTHM-Y4,1442
-onnxruntime/transformers/benchmark.py,sha256=8ScKc9A8ZPQxo1IQjPAbSnhnjdohHNmzR91yMrAfDzI,31838
-onnxruntime/transformers/benchmark_helper.py,sha256=AlEfCA_0bVgmHZ_csDiXqKeSM1AC-vfWjEMKzMbaXZ8,20592
-onnxruntime/transformers/bert_perf_test.py,sha256=FV4Z4NELmnXTdSM6bGugicLFqXUcH9thTwA4xwHJuQE,18322
-onnxruntime/transformers/bert_test_data.py,sha256=d2qNBvlXMMiL54BqFfOLqunHOyFmVHCkTqfu0A8bl0I,19056
-onnxruntime/transformers/compare_bert_results.py,sha256=qI_WnUZcx3erMm5C-yrtxsRsZ48InCWyJdU_WJ2XE7E,7695
-onnxruntime/transformers/convert_generation.py,sha256=9gIocqASOlmLx3mzXbzRusX5dgzxEK3CKJU-sUgu_bs,92884
-onnxruntime/transformers/convert_tf_models_to_pytorch.py,sha256=0fTJM8O28L5lQwfU-bOoYocTXOd71FA0zGw8mLGqLYc,6706
-onnxruntime/transformers/float16.py,sha256=jhkP8GV-c2nKMLYCFemhI3lEXGKP7PGXjyJyBvyb4H8,18498
-onnxruntime/transformers/fusion_attention.py,sha256=5KyGcN5scabFqrK408_-uHhe1r4wFmRmkjGuwPjLnvM,26690
-onnxruntime/transformers/fusion_attention_unet.py,sha256=SMarapJNkgppy5b4TSL9UvM57RZHpH9dzcocX8BH6Us,16227
-onnxruntime/transformers/fusion_base.py,sha256=KgzS5Xr3w51pA-cJ3tM5Vc-0HluRWxP0-r3hz8epG2c,2832
+onnxruntime/transformers/benchmark.py,sha256=2RuzJwr1dIbqhYNidUxHTNa04gdOZLXFVlfxkWkBiKE,33334
+onnxruntime/transformers/benchmark_helper.py,sha256=3FthZnfsXmb8VsQgJwvrJiHw6RBbvDsMsox8iyE2Cus,20969
+onnxruntime/transformers/bert_perf_test.py,sha256=LuGJhil3bMT5W2dHd5B0dlXQj4QaIYkBF1cy0vzyLkg,19728
+onnxruntime/transformers/bert_test_data.py,sha256=6t5bq5KrTkGnx6MyNy95J3HTZd2Lqxh7mhc1svcFH64,19039
+onnxruntime/transformers/compare_bert_results.py,sha256=SxRP2gvGOy2w6_DoixftFdMeQwApRiTJrDTkqpDe9AY,7639
+onnxruntime/transformers/constants.py,sha256=enLKnm_Wlt7bu9OVa52lxwDBHJgbwhAXvSniP6EAMO8,740
+onnxruntime/transformers/convert_generation.py,sha256=2VhCTsrp9Mo-cfwaUalIVruPGJy8fzvpsiPiFU1HFVw,108650
+onnxruntime/transformers/convert_tf_models_to_pytorch.py,sha256=JrMGzUi_4cMfYoIYrxa1n0jnMDG-WYj-xmUXZmH8aJ0,6705
+onnxruntime/transformers/convert_to_packing_mode.py,sha256=Vpow2auDtyUhKHiIk39rPISvQ5s3F5NasNn9IGaZBSo,10057
+onnxruntime/transformers/float16.py,sha256=JXaX1Wh0KZyQMxyqHcwcxYqcN6CqiO7Mrtr_muJpwxE,22806
+onnxruntime/transformers/fusion_attention.py,sha256=twc3emyVmLGqv0hFOkrSIKYJAgEPd7ZYc_AFvFltB_w,50487
+onnxruntime/transformers/fusion_attention_unet.py,sha256=29oRxCAayOvytuobKmmI_XqWNDOITphwLYW-8A0cMrc,18666
+onnxruntime/transformers/fusion_attention_vae.py,sha256=C2JcT-ik-Fapc_lO21MBnNANpDHv1QQlWkaf1A2RQgI,12600
+onnxruntime/transformers/fusion_bart_attention.py,sha256=WSm8vOZhS4rXWM82It-dfRCd7Al03MzRrJFpJyUAFVE,18807
+onnxruntime/transformers/fusion_base.py,sha256=ld9YwmEMxw8syuEmoCOZuBNFXcSAwIzUgu-qokceTkg,2820
+onnxruntime/transformers/fusion_bias_add.py,sha256=7JRHl-p1M8GxNfa9cgHsES7dwburpaTWqYh921_8QjQ,2066
 onnxruntime/transformers/fusion_biasgelu.py,sha256=vGamxthOu6jXsxCRVdTFaP25-_tnjz9TVq91pIRV_Is,2300
-onnxruntime/transformers/fusion_biassplitgelu.py,sha256=pnJ2VT-Bk7SrDfNowFyhgyQmQJ43NHP65hkaVfGcq4g,4503
-onnxruntime/transformers/fusion_embedlayer.py,sha256=kWmVE0Gmjgs0UX7zWuneazDLv85bzjww55jN2A62XiM,34537
+onnxruntime/transformers/fusion_biassplitgelu.py,sha256=6G73bmAGM5y02Rm_Lupbn341O0Y5Sr-2Re_628Ez2Qo,4516
+onnxruntime/transformers/fusion_embedlayer.py,sha256=Kau9xeIRlbR21j6DIab5fzOulJxNxpwchxwzUz_Cbso,35646
 onnxruntime/transformers/fusion_fastgelu.py,sha256=pi2U93F4xWMThs6Yz9K1d6AZQ1kyWSZhjeGqs7WWAVI,13324
 onnxruntime/transformers/fusion_gelu.py,sha256=GrTB0LoVz_YRyTW-JoL4Fh_fz_IA01JweEP0Tj_Lwgs,10180
-onnxruntime/transformers/fusion_gelu_approximation.py,sha256=R9H6jd63dGCgzfZv_9p4bd7Ttg9-2LGDFZTBQPAKXiw,1062
+onnxruntime/transformers/fusion_gelu_approximation.py,sha256=MCNAXL0nIHweYrBe4euYUZnmzxvi3X2Cj1HKi6o4Nlc,1076
 onnxruntime/transformers/fusion_gemmfastgelu.py,sha256=bNQO5aVD2-H0Kqdr8meFGRmOi3A0AvVVTnCcrBF6WB8,4262
-onnxruntime/transformers/fusion_gpt_attention.py,sha256=CuGU8gFVvWHOzzPqZK1N0Zb3IprrNzFvWYuCB5XGY24,22256
-onnxruntime/transformers/fusion_gpt_attention_megatron.py,sha256=WIPsw7T8z6OZ77B2FJVr2DHDph3q94gJJx2pbanEwAA,13845
-onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=czqkOK4jLz5M7sKbF8gBEDE54IA5iFo1Pu5cB5yimVA,10981
-onnxruntime/transformers/fusion_group_norm.py,sha256=_AkbkGKua9V1bAnXBYy1m8vh7DuiwydFjrEqyMnwNvk,8412
-onnxruntime/transformers/fusion_layernorm.py,sha256=pq9G7jNGfrMxqjSb87jQC0PlHl99662jxTbU20KoKUg,12199
+onnxruntime/transformers/fusion_gpt_attention.py,sha256=dsF4lhKqHNIP9KtV0OSG4TJOW420dEa_r1_s2EGm3UI,22327
+onnxruntime/transformers/fusion_gpt_attention_megatron.py,sha256=A25O3V06F5e1c8sJLxh84wP3VAk2SfFZCVSQukOmuTQ,13887
+onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=NI9ySlwkHMHJEUOwyLZtiMMcpbifr6UFGhnBFwc3KFo,11023
+onnxruntime/transformers/fusion_group_norm.py,sha256=ASkDmrvsCVsTJlDi9pKj-PmBjxbFy3eAxSUDOWJgBY0,7693
+onnxruntime/transformers/fusion_layernorm.py,sha256=fQO0uU3yPE6KSvuvMYAKsnMu-qLAB4WPTVXoH85Bank,12194
 onnxruntime/transformers/fusion_nhwc_conv.py,sha256=VK4-I7ne5O_BkuTYR7TVP4FZaUUI2G6SXuVJWVG_GGI,3592
-onnxruntime/transformers/fusion_options.py,sha256=8-v75YBoo-lBjSj5pUtJNzhNkAEbWYXLiQvcGYmcHfY,8946
-onnxruntime/transformers/fusion_qordered_attention.py,sha256=6z5f59f62e7dnIPtmETmmuqn2i17qcwxiiuqUuG-ljE,17162
-onnxruntime/transformers/fusion_qordered_gelu.py,sha256=9SYxQXZ_B3HAkMGP85LSlg0hqh0lcKVYhbtGm_PLsZE,4394
-onnxruntime/transformers/fusion_qordered_layernorm.py,sha256=RJJZbi-Ydx8f5vbJsZ_KHzlWmMMuQO_poSbJR-RrqO4,4916
-onnxruntime/transformers/fusion_qordered_matmul.py,sha256=qbpd1tv3ROJdNg9FkfuBuvCsFIptpuQ32ybq_tDQJv8,8567
+onnxruntime/transformers/fusion_options.py,sha256=vFJONo287ZNRwpHVMYg92Hiyeaaks6tZ4h8xyq7PEIY,10607
+onnxruntime/transformers/fusion_qordered_attention.py,sha256=VutuLlHX0oDnDhcbzWhVSq-VXlyKNaOXu2hW8gdn21c,17163
+onnxruntime/transformers/fusion_qordered_gelu.py,sha256=aRBTRACUuXMctEfyL1GICG5hFRqiuybJQ0B7Psgz5dQ,4393
+onnxruntime/transformers/fusion_qordered_layernorm.py,sha256=5GndWDsK2_3wURps3R4tVuGD0IuDFAcWzX3FXiysoGM,4915
+onnxruntime/transformers/fusion_qordered_matmul.py,sha256=j85chtrY9YrGD1ERNIHqCBAxZW51I2Sk_EFU4jg8qdM,8566
 onnxruntime/transformers/fusion_reshape.py,sha256=RNfehwtwQYWvrBjXKkjso_L9nxfGUC1WRyGFwwWMrY4,6463
-onnxruntime/transformers/fusion_shape.py,sha256=1FvW377VEd1jwvrZhblFMGay2ME6LTP0cCvM55uvXP4,3844
-onnxruntime/transformers/fusion_skiplayernorm.py,sha256=EVuJ2HMWZ0vwYU6hIXf6-7j0RBSRHl9zBN1uQViGnwI,7878
-onnxruntime/transformers/fusion_transpose.py,sha256=RvcZmRx84pmw16WEFrA_sX7BmBh8LCItQq_rDG0DCwk,3019
-onnxruntime/transformers/fusion_utils.py,sha256=El2BTIig-zap5I8lb5QXihhq9Etxj_HmBbyk1IyfxF8,11883
-onnxruntime/transformers/huggingface_models.py,sha256=Wk-YUMyJ5n2QVJyOgllEP41AKD6rXrWYv1L0h1nbWmw,8774
-onnxruntime/transformers/io_binding_helper.py,sha256=eJx8qPExHb7mQEEQzXy-Ak1hnwrO9xcvVJmRSxCDHdM,7731
-onnxruntime/transformers/machine_info.py,sha256=qJh3m1BXfMy8c5Nxw3frl_kzOejl6aIgpzikcqnt3lU,7294
-onnxruntime/transformers/onnx_exporter.py,sha256=kBoT85ye9EVrG95HYr_aNbneyC84bpFp8sD0a4KwuYE,24077
-onnxruntime/transformers/onnx_model.py,sha256=vSqIt_BNigPNbnXu6pjnXagd83aYCtG7t6lPopQJ7ZQ,45624
-onnxruntime/transformers/onnx_model_bart.py,sha256=ZiJQG5TmsYs97MJGcrEzMAIHf26RSUZksf_rBdvtq58,11903
-onnxruntime/transformers/onnx_model_bert.py,sha256=5-VgAQuJMPphYYkBaeeVs9UdNjCeK9UpmAeigwR6NaM,20614
-onnxruntime/transformers/onnx_model_bert_keras.py,sha256=Hr1TKrVZK6m_7wGfbpjNx6DeB9MRnd5KXNjghYfl87g,19511
-onnxruntime/transformers/onnx_model_bert_tf.py,sha256=4_YzBzWbG1POgIhU4USN61P8GF49pppMg0kKu-gql9Q,25536
-onnxruntime/transformers/onnx_model_gpt2.py,sha256=lTs2Bb3TZuA0jD_GsnbeXnOZC_pCAI5B9MR6A5TyVaQ,3780
-onnxruntime/transformers/onnx_model_tnlr.py,sha256=2dHTsl-6dxtItb-tuTNoZFuy_e-uaKvT6kR6Tisyrxo,8633
-onnxruntime/transformers/onnx_model_unet.py,sha256=VOqO6VYoVKIA_-kr0FmEYfz5wQoeDrMyBxVWaGk65cQ,6382
-onnxruntime/transformers/optimizer.py,sha256=3PPFH2QRq6pPIzMYnm8GopMnFMmzK417WELmPaXf0pA,17781
-onnxruntime/transformers/profiler.py,sha256=Dk6nrffFUKiws9vorgginLjPreJ8RmjFkz6sm_KXL0U,25015
-onnxruntime/transformers/quantize_helper.py,sha256=-pw8a4ruZYcAgGZKrSVQVeRHKsMBV6RwhJ2V91Q6Uk4,2811
-onnxruntime/transformers/shape_infer_helper.py,sha256=G1HSeWMcxPGFJvC07fRAUnKe_WVxjzZSRO65_BYLAV4,4576
-onnxruntime/transformers/shape_optimizer.py,sha256=_NmONDq49wj0Uda2gA9PVGo0ca_Hln5KOw2kZbaTZ2E,15557
-onnxruntime/transformers/torch_onnx_export_helper.py,sha256=wVysoS9ZpAEAxeHg0irs3sjOgj7ERPHXQBfrpoq6Uc4,2493
-onnxruntime/transformers/models/bart/export.py,sha256=l5YbN4uL8I3-bzfkngJpdRCHgQUPJAsZvOcNLptsNmw,4279
+onnxruntime/transformers/fusion_shape.py,sha256=k6wcbIWY4FMxWqLuf0b768z5n2vwslys2yJ30ceek9A,3845
+onnxruntime/transformers/fusion_skiplayernorm.py,sha256=s379IDb_HjNnoWsHFSLP7lFb-EWtP8ic2-rVa3YxX4c,8269
+onnxruntime/transformers/fusion_transpose.py,sha256=4h9x8j7ERcT2TUhr6XgYICrtvaeL5JBDovYCoiB6tAw,6980
+onnxruntime/transformers/fusion_utils.py,sha256=7jdnakQTTJP1OiS6uzrX1lX1rPcXvKUu0NfZd6NmFSA,12229
+onnxruntime/transformers/huggingface_models.py,sha256=C0B3Lh52edigeZd_JZEBBl2x_hruGF0u5VmsNOke_J4,9130
+onnxruntime/transformers/io_binding_helper.py,sha256=KEy0QRl_3HmFU8sjSCbP2vsDNegSzHQ1CJejPp0as2k,7726
+onnxruntime/transformers/machine_info.py,sha256=qJXW4KGXgNONrtgBextAurr1K9QDFG3Pcyllxl7Xel4,7336
+onnxruntime/transformers/onnx_exporter.py,sha256=MyTxtBwesCK2jCVs0ql6m-AANbuPgZEKdFRBAZmC90A,25364
+onnxruntime/transformers/onnx_model.py,sha256=k_SGDsziPBDNHa0wJrZeYI27EwgX7pk4BH9-Opj2WxA,49231
+onnxruntime/transformers/onnx_model_bart.py,sha256=G5KQEf_ipKOD41ctZvqySSheBlHtbVF9gbpSGkAyfH4,5397
+onnxruntime/transformers/onnx_model_bert.py,sha256=wtxvd-nyW5nxKP4KTjApOkGNO_xodMMERAQWoWjKD2A,20938
+onnxruntime/transformers/onnx_model_bert_keras.py,sha256=vK8lv_3clLuLJ6AYEUCdv6ukoPdG_4jtLyhk7_lv0oA,19132
+onnxruntime/transformers/onnx_model_bert_tf.py,sha256=YCtvEy8sNBTESPOVp0gaTgES6R3PIIAnoSkSPPn4nJs,25561
+onnxruntime/transformers/onnx_model_clip.py,sha256=21bomWo0u0OWxhgP198OC8gzjnNoy2scvKz5Aw0R04g,1067
+onnxruntime/transformers/onnx_model_gpt2.py,sha256=HMVPDGBlU1QPp1dB5aKCDAnM1nPds8HfrzEiYEC-Yo4,3747
+onnxruntime/transformers/onnx_model_t5.py,sha256=7rbWKZDIGr0GV7a-qxrD-LmwKVoKEUl57u7pCK6L-3M,31346
+onnxruntime/transformers/onnx_model_tnlr.py,sha256=HNb6dBsWU8Fo-Trx_R9pgK0pKnDRZGJ3i54n_aNd16E,8682
+onnxruntime/transformers/onnx_model_unet.py,sha256=JvRHSsljqiZ0HEkGE9wh477RIM1grCBQiS20hufS5D0,7100
+onnxruntime/transformers/onnx_model_vae.py,sha256=_L2UlVv4CHNImsvQKmyW0QIMK5Hpsg8P8ZCyM8f9rEQ,1515
+onnxruntime/transformers/optimizer.py,sha256=hVR5nF-WcABMJaOyroXkfzyC1QNhIYOc3Bg1a20WvkU,19209
+onnxruntime/transformers/profiler.py,sha256=MAe1Oqamoh7ZQoZ08audv8XceEJURW-X72R9-HdB_-E,25009
+onnxruntime/transformers/quantize_helper.py,sha256=8u_pW1DHv7mh5KYOObG4ZP_qQKuFRGcxkotdvbkNy9Y,2825
+onnxruntime/transformers/shape_infer_helper.py,sha256=Six_K74VGMyJRf6npveQkn-_cLjcFIRdRvmuSIaC9W0,4590
+onnxruntime/transformers/shape_optimizer.py,sha256=BC3-JRbRrCohR0GJWkfk2Kdkkhmcg4_bBPKJ9y66R9Q,15555
+onnxruntime/transformers/torch_onnx_export_helper.py,sha256=5ml2odDKi6sA1iaHqdNMA8tC6MnslaCF35RZjPx6ib4,2507
+onnxruntime/transformers/models/bart/export.py,sha256=PNlhkbvrxTxSSLXpzqoa02Lektzf8rdZpcVFBxw-qcI,4285
 onnxruntime/transformers/models/bert/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/bert/eval_squad.py,sha256=6BWYrPuIOSRRa3DEmXuN8Ax5rsIGrRtyT0RpZRaKHJk,10798
+onnxruntime/transformers/models/bert/eval_squad.py,sha256=pr7M4yG0eJIFuPubrKta1vWg3MMlzKS2eoyUe4iLS0w,10783
 onnxruntime/transformers/models/gpt2/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/gpt2/benchmark_gpt2.py,sha256=051pgIYTuSiMA-u_5J1G-67UAGEfp0Gw7jm_lalLzvs,15999
-onnxruntime/transformers/models/gpt2/convert_to_onnx.py,sha256=cclYc3RJrZ1nHNOSxt2jMa0Ks1hU7yrReaNJ8meiSgA,20705
-onnxruntime/transformers/models/gpt2/gpt2_helper.py,sha256=sKO-B1gsJRR_MoRfsKyt0kOpuyARlfP3DcU3g9jAZX0,41748
-onnxruntime/transformers/models/gpt2/gpt2_parity.py,sha256=YKaocbFbm9LSiJ9VBpqhQQuiXZ2_SKlL0S5UTVLsI0w,18248
-onnxruntime/transformers/models/gpt2/gpt2_tester.py,sha256=jhmaTSdgA3-k1mLIsxFCs6P_HWZREp3hpLwHXfBlZv0,20109
-onnxruntime/transformers/models/gpt2/parity_check_helper.py,sha256=tneM3LQR38RLdic-WVG1ABW6d3QE0iqIHgW9Re8L8YM,5876
+onnxruntime/transformers/models/gpt2/benchmark_gpt2.py,sha256=JBR2lEewyswFErYN0C1R2gj9zdh1t0eDQK3rO97Nh0o,16036
+onnxruntime/transformers/models/gpt2/convert_to_onnx.py,sha256=6zxnf4usjOoC6tNjcg27V4QSxJVKTRmAPsirIHG_a40,20751
+onnxruntime/transformers/models/gpt2/gpt2_helper.py,sha256=FJbaiUkdBrXPLiF7hta_PUvEgoLFMsfezgpU5Cj8k6Y,41547
+onnxruntime/transformers/models/gpt2/gpt2_parity.py,sha256=9sVBRQwL3ELgN19eV3WXWeQMHY2P46ufm48vnF5WcRE,18322
+onnxruntime/transformers/models/gpt2/gpt2_tester.py,sha256=Qs1dOPLzYFGujpHGOobxsU4q-75INKqSHpJH5imDYcY,20178
+onnxruntime/transformers/models/gpt2/parity_check_helper.py,sha256=7tO0sHE8oLQ0smA83jfJ_JHlCrNyT289_tEAbEiJJUQ,5906
 onnxruntime/transformers/models/longformer/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/longformer/benchmark_longformer.py,sha256=8WnT_5_eNKfu675rHqSUPNuZ5VKENI1O2oYKviyTvuQ,30347
-onnxruntime/transformers/models/longformer/convert_to_onnx.py,sha256=Oy9P-31FlQdbf-corTdN2MYrsrM5Fy_f5B0JOQ0MViY,15299
-onnxruntime/transformers/models/longformer/generate_test_data.py,sha256=cJqh-ZkJGb3AeB98o2mQRewgNS1z8ENHLPlcF2eBYKc,9205
+onnxruntime/transformers/models/longformer/benchmark_longformer.py,sha256=iPyJoRG_pjoA5I0Qx47x8nY34_2yoNv8PXeSLBcPGp8,30370
+onnxruntime/transformers/models/longformer/convert_to_onnx.py,sha256=9dCRv9i9EAqCCPZSyof33cF7S5wkk2RmHYFQVL0QbXE,15342
+onnxruntime/transformers/models/longformer/generate_test_data.py,sha256=a05XRHyGTm6tnH5qhc-XPLrYQBZm_1VcDgua6gTqA3Q,9225
 onnxruntime/transformers/models/longformer/longformer_helper.py,sha256=FH7Uykc57rLNr1l0pr85OVgr9PZE_4x29xdE-t1riC4,3180
 onnxruntime/transformers/models/stable_diffusion/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/stable_diffusion/benchmark.py,sha256=CHGOWBvTrwJWvY5DXDZkyjodEY4wwkqXJjl9_z-js7A,8431
-onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py,sha256=pRkj2h-QDW39KsUD5XNKRGL_X6S66AJjdsyr4TJqEcQ,8062
+onnxruntime/transformers/models/stable_diffusion/benchmark.py,sha256=Fa4hc_TsaXuL2W-RRHxPMjUfemHNLPgjKXti4ID88M4,21893
+onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py,sha256=S-6sgND_NKJPMEzPj7nkSokBmchfZL2kRf9-z7aNDpA,11932
 onnxruntime/transformers/models/t5/__init__.py,sha256=Hrvad2_TcQ3bpd4b7QJsJV4SSilowg8Loy3e5FxNiEI,252
-onnxruntime/transformers/models/t5/convert_to_onnx.py,sha256=WUdMaoY-i5ZqgXln8lXe99xxf7yfIRH4Of493lxh2Io,8794
-onnxruntime/transformers/models/t5/past_helper.py,sha256=QRvjm3E2Ww2HX2HsySbaJke6YYnJGUfdjpUj9jnoDG4,3122
-onnxruntime/transformers/models/t5/t5_decoder.py,sha256=ComnODpc8CBl9e_2icQn_iZE0Q9H5O4UtxwPlBGwRLs,17415
+onnxruntime/transformers/models/t5/convert_to_onnx.py,sha256=e1t47mh0N16c0glXXlBhuWA21gko398YHjGjaE3caQM,8832
+onnxruntime/transformers/models/t5/past_helper.py,sha256=ounFkzTPTM0N9gjZ70jhh-grskckMQwCu2KsDupljpM,6987
+onnxruntime/transformers/models/t5/t5_decoder.py,sha256=jw21pKh49-kxxT3SqaAhVzH5hCTRJuWs7pBpmbGQrPk,17207
 onnxruntime/transformers/models/t5/t5_encoder.py,sha256=qxjvzbfPYIKtIoYBMRCW4vmjd9beSWqFg4dexSYUNyk,6407
-onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py,sha256=UoAz1Q32fpQ7QyPojHIVkbvQxLYADi5cs-TvDv3Xmi4,12326
-onnxruntime/transformers/models/t5/t5_helper.py,sha256=pljh1gGWuDu1-xSMDrxrGzEaXvrWvAuZ3ZtY8Tv0dF4,10669
-onnxruntime_azure-1.14.0.dist-info/METADATA,sha256=8jG7qoCwXWfcN4Gosep_JVn-vszk2mxDk1G2-v_U_u8,3957
-onnxruntime_azure-1.14.0.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
-onnxruntime_azure-1.14.0.dist-info/entry_points.txt,sha256=ziQA922fkGW-RIvlaEdwpnXp2SU5VxBrmcXUvL1g8iI,78
-onnxruntime_azure-1.14.0.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
-onnxruntime_azure-1.14.0.dist-info/RECORD,,
+onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py,sha256=_AGmW4ujewlPG8tV8kGQBsekFwl5psEj4mDxWFrcM9Q,12324
+onnxruntime/transformers/models/t5/t5_helper.py,sha256=EpYgznALmy32_Iz4A-WDGql1-kOY2M358c2L9rcGvaY,11027
+onnxruntime/transformers/models/whisper/__init__.py,sha256=2c213CqzXrc0N6Cqf__Te5d_SH_stfLdNdeNrugB7SQ,321
+onnxruntime/transformers/models/whisper/convert_to_onnx.py,sha256=lUnX4gxlWBJTHdU5oCOG4KpnGM7nqAMKaY9QIa7ahqY,11670
+onnxruntime/transformers/models/whisper/whisper_chain.py,sha256=6QTlpTbu68fp8A-0CIRVhz9bccmQbCVxnikHmwXoB0s,4441
+onnxruntime/transformers/models/whisper/whisper_decoder.py,sha256=SKIfYMP0mgp6xKdCTS2H5rW_avmaOEAEXFO8U7Rb77A,16818
+onnxruntime/transformers/models/whisper/whisper_encoder.py,sha256=PLKYGpFvvw0_Z7MaGv75Ccu5PHC_9yXVoWdLC7t80lw,5931
+onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py,sha256=ZSoj2mtvoYeQy3sh-4wgjPs9WcWbt_iOuMt9iTRsXb8,13107
+onnxruntime/transformers/models/whisper/whisper_helper.py,sha256=hQldNVOLQw3UjbAczdWXMjlIleye1dFNpwhQp2q6_wA,10529
+onnxruntime_azure-1.15.0.dist-info/METADATA,sha256=kyWPgnkSoFW-88pnl3ZHbH0S0mzfZlv6XAcd3pecwI8,4056
+onnxruntime_azure-1.15.0.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+onnxruntime_azure-1.15.0.dist-info/entry_points.txt,sha256=ziQA922fkGW-RIvlaEdwpnXp2SU5VxBrmcXUvL1g8iI,78
+onnxruntime_azure-1.15.0.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
+onnxruntime_azure-1.15.0.dist-info/RECORD,,
```

