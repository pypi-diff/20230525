# Comparing `tmp/qworker-1.6.6-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/qworker-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,32 @@
-Zip file size: 313812 bytes, number of entries: 30
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-05 21:56 qworker.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-05 21:56 qworker-1.6.6.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-05 21:56 qw/
--rw-r--r--  2.0 unx     3169 b- defN 23-May-05 21:56 qworker-1.6.6.dist-info/METADATA
--rw-r--r--  2.0 unx       40 b- defN 23-May-05 21:56 qworker-1.6.6.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        3 b- defN 23-May-05 21:56 qworker-1.6.6.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1951 b- defN 23-May-05 21:56 qworker-1.6.6.dist-info/RECORD
--rw-r--r--  2.0 unx      217 b- defN 23-May-05 21:56 qworker-1.6.6.dist-info/WHEEL
--rw-r--r--  2.0 unx     1070 b- defN 23-May-05 21:56 qworker-1.6.6.dist-info/LICENSE
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-05 21:56 qw/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-05 21:56 qw/wrappers/
--rwxr-xr-x  2.0 unx   568544 b- defN 23-May-05 21:56 qw/exceptions.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx     2028 b- defN 23-May-05 21:56 qw/__main__.py
--rw-r--r--  2.0 unx     4064 b- defN 23-May-05 21:56 qw/discovery.py
--rw-r--r--  2.0 unx     2172 b- defN 23-May-05 21:56 qw/conf.py
--rw-r--r--  2.0 unx      137 b- defN 23-May-05 21:56 qw/__init__.py
--rw-r--r--  2.0 unx      379 b- defN 23-May-05 21:56 qw/decorators.py
--rw-r--r--  2.0 unx     5282 b- defN 23-May-05 21:56 qw/process.py
--rw-r--r--  2.0 unx      593 b- defN 23-May-05 21:56 qw/version.py
--rw-r--r--  2.0 unx    15504 b- defN 23-May-05 21:56 qw/client.py
--rw-r--r--  2.0 unx    19834 b- defN 23-May-05 21:56 qw/server.py
--rw-r--r--  2.0 unx     1732 b- defN 23-May-05 21:56 qw/protocols.py
--rw-r--r--  2.0 unx      597 b- defN 23-May-05 21:56 qw/utils/versions.py
--rw-r--r--  2.0 unx      512 b- defN 23-May-05 21:56 qw/utils/functions.py
--rw-r--r--  2.0 unx       46 b- defN 23-May-05 21:56 qw/utils/__init__.py
--rwxr-xr-x  2.0 unx   434800 b- defN 23-May-05 21:56 qw/utils/json.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx     4089 b- defN 23-May-05 21:56 qw/wrappers/di_task.py
--rw-r--r--  2.0 unx      895 b- defN 23-May-05 21:56 qw/wrappers/base.py
--rw-r--r--  2.0 unx      853 b- defN 23-May-05 21:56 qw/wrappers/func.py
--rw-r--r--  2.0 unx      320 b- defN 23-May-05 21:56 qw/wrappers/__init__.py
-30 files, 1068831 bytes uncompressed, 310258 bytes compressed:  71.0%
+Zip file size: 314724 bytes, number of entries: 30
+drwxr-xr-x  2.0 unx        0 b- stor 23-May-24 23:49 qworker-1.7.0.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-May-24 23:49 qw/
+drwxr-xr-x  2.0 unx        0 b- stor 23-May-24 23:49 qworker.libs/
+-rw-rw-r--  2.0 unx     1951 b- defN 23-May-24 23:49 qworker-1.7.0.dist-info/RECORD
+-rw-r--r--  2.0 unx      217 b- defN 23-May-24 23:49 qworker-1.7.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        3 b- defN 23-May-24 23:49 qworker-1.7.0.dist-info/top_level.txt
+-rw-r--r--  2.0 unx     1070 b- defN 23-May-24 23:49 qworker-1.7.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx       40 b- defN 23-May-24 23:49 qworker-1.7.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx     3170 b- defN 23-May-24 23:49 qworker-1.7.0.dist-info/METADATA
+drwxr-xr-x  2.0 unx        0 b- stor 23-May-24 23:49 qw/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-May-24 23:49 qw/wrappers/
+-rw-r--r--  2.0 unx     2160 b- defN 23-May-24 23:49 qw/discovery.py
+-rw-r--r--  2.0 unx    15257 b- defN 23-May-24 23:49 qw/client.py
+-rw-r--r--  2.0 unx     2370 b- defN 23-May-24 23:49 qw/conf.py
+-rw-r--r--  2.0 unx     2388 b- defN 23-May-24 23:49 qw/__main__.py
+-rw-r--r--  2.0 unx      379 b- defN 23-May-24 23:49 qw/decorators.py
+-rw-r--r--  2.0 unx    21403 b- defN 23-May-24 23:49 qw/server.py
+-rw-r--r--  2.0 unx      618 b- defN 23-May-24 23:49 qw/version.py
+-rw-r--r--  2.0 unx     4129 b- defN 23-May-24 23:49 qw/protocols.py
+-rw-r--r--  2.0 unx     7462 b- defN 23-May-24 23:49 qw/process.py
+-rw-r--r--  2.0 unx      137 b- defN 23-May-24 23:49 qw/__init__.py
+-rwxr-xr-x  2.0 unx   568544 b- defN 23-May-24 23:49 qw/exceptions.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx      512 b- defN 23-May-24 23:49 qw/utils/functions.py
+-rw-r--r--  2.0 unx       46 b- defN 23-May-24 23:49 qw/utils/__init__.py
+-rw-r--r--  2.0 unx      597 b- defN 23-May-24 23:49 qw/utils/versions.py
+-rwxr-xr-x  2.0 unx   434800 b- defN 23-May-24 23:49 qw/utils/json.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx      895 b- defN 23-May-24 23:49 qw/wrappers/base.py
+-rw-r--r--  2.0 unx      320 b- defN 23-May-24 23:49 qw/wrappers/__init__.py
+-rw-r--r--  2.0 unx     4092 b- defN 23-May-24 23:49 qw/wrappers/di_task.py
+-rw-r--r--  2.0 unx      853 b- defN 23-May-24 23:49 qw/wrappers/func.py
+30 files, 1073413 bytes uncompressed, 311170 bytes compressed:  71.0%
```

## zipnote {}

```diff
@@ -1,91 +1,91 @@
-Filename: qworker.libs/
+Filename: qworker-1.7.0.dist-info/
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/
+Filename: qw/
 Comment: 
 
-Filename: qw/
+Filename: qworker.libs/
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/METADATA
+Filename: qworker-1.7.0.dist-info/RECORD
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/entry_points.txt
+Filename: qworker-1.7.0.dist-info/WHEEL
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/top_level.txt
+Filename: qworker-1.7.0.dist-info/top_level.txt
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/RECORD
+Filename: qworker-1.7.0.dist-info/LICENSE
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/WHEEL
+Filename: qworker-1.7.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: qworker-1.6.6.dist-info/LICENSE
+Filename: qworker-1.7.0.dist-info/METADATA
 Comment: 
 
 Filename: qw/utils/
 Comment: 
 
 Filename: qw/wrappers/
 Comment: 
 
-Filename: qw/exceptions.cpython-39-x86_64-linux-gnu.so
-Comment: 
-
-Filename: qw/__main__.py
+Filename: qw/discovery.py
 Comment: 
 
-Filename: qw/discovery.py
+Filename: qw/client.py
 Comment: 
 
 Filename: qw/conf.py
 Comment: 
 
-Filename: qw/__init__.py
+Filename: qw/__main__.py
 Comment: 
 
 Filename: qw/decorators.py
 Comment: 
 
-Filename: qw/process.py
+Filename: qw/server.py
 Comment: 
 
 Filename: qw/version.py
 Comment: 
 
-Filename: qw/client.py
+Filename: qw/protocols.py
 Comment: 
 
-Filename: qw/server.py
+Filename: qw/process.py
 Comment: 
 
-Filename: qw/protocols.py
+Filename: qw/__init__.py
 Comment: 
 
-Filename: qw/utils/versions.py
+Filename: qw/exceptions.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
 Filename: qw/utils/functions.py
 Comment: 
 
 Filename: qw/utils/__init__.py
 Comment: 
 
-Filename: qw/utils/json.cpython-39-x86_64-linux-gnu.so
+Filename: qw/utils/versions.py
 Comment: 
 
-Filename: qw/wrappers/di_task.py
+Filename: qw/utils/json.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
 Filename: qw/wrappers/base.py
 Comment: 
 
-Filename: qw/wrappers/func.py
+Filename: qw/wrappers/__init__.py
 Comment: 
 
-Filename: qw/wrappers/__init__.py
+Filename: qw/wrappers/di_task.py
+Comment: 
+
+Filename: qw/wrappers/func.py
 Comment: 
 
 Zip file comment:
```

## qw/__main__.py

```diff
@@ -1,64 +1,90 @@
 """Queue Worker server entry point."""
 import asyncio
 import argparse
 import uvloop
+import warnings
 from .conf import (
     WORKER_DEFAULT_HOST,
     WORKER_DEFAULT_PORT,
     WORKER_DEFAULT_QTY,
     WORKER_QUEUE_SIZE,
     WORKER_DISCOVERY_PORT
 
 )
-from .process import spawn_process
+from .process import SpawnProcess
 from .utils import cPrint
 
-asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
-uvloop.install()
+
+warnings.simplefilter("default", ResourceWarning)
 
 def main():
     """Main Worker Function."""
+    asyncio.set_event_loop_policy(
+        uvloop.EventLoopPolicy()
+    )
+    uvloop.install()
     parser = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter
     )
-    parser.add_argument('--host', dest='host', type=str,
-                        default=WORKER_DEFAULT_HOST,
-                        help='set server host'
-    )
-    parser.add_argument('--port', dest='port', type=int,
-                        default=WORKER_DEFAULT_PORT,
-                        help='set server port'
-    )
-    parser.add_argument('--workers', dest='workers', type=int,
-                        default=WORKER_DEFAULT_QTY,
-                        help='max number of workers'
-    )
-    parser.add_argument('--queue', dest='queue', type=int,
-                        default=WORKER_QUEUE_SIZE,
-                        help='Size of Queue on Worker'
-    )
-    parser.add_argument('--wkname', dest='wkname', type=str, default='Worker',
-                        help='Worker Name'
-    )
-    parser.add_argument('--discovery', dest='discovery', type=str, default=WORKER_DISCOVERY_PORT,
-                        help='UDP Port for Service discovery'
-    )
-    parser.add_argument('--debug', action="store_true", default=False,
-                        help="Start workers in Debug Mode"
+    parser.add_argument(
+        '--host', dest='host', type=str,
+        default=WORKER_DEFAULT_HOST,
+        help='set server host'
+    )
+    parser.add_argument(
+        '--port', dest='port', type=int,
+        default=WORKER_DEFAULT_PORT,
+        help='set server port'
+    )
+    parser.add_argument(
+        '--workers', dest='workers', type=int,
+        default=WORKER_DEFAULT_QTY,
+        help='max number of workers'
+    )
+    parser.add_argument(
+        '--queue', dest='queue', type=int,
+        default=WORKER_QUEUE_SIZE,
+        help='Size of Queue on Worker'
+    )
+    parser.add_argument(
+        '--wkname', dest='wkname', type=str,
+        default='Worker',
+        help='Worker Name'
+    )
+    parser.add_argument(
+        '--enable-discovery', dest='enable_discovery',
+        type=str.lower,
+        choices=["true", "false"],
+        default='true',
+        help='Start Discovery Service on this Worker'
+    )
+    parser.add_argument(
+        '--discovery', dest='discovery', type=str,
+        default=WORKER_DISCOVERY_PORT,
+        help='UDP Port for Service discovery'
+    )
+    parser.add_argument(
+        '--debug', action="store_true",
+        default=False,
+        help="Start workers in Debug Mode"
     )
     args = parser.parse_args()
     try:
         loop = asyncio.get_event_loop()
         cPrint('::: Starting Workers ::: ')
-        process = spawn_process(args, event_loop=loop)
+        process = SpawnProcess(args)
         process.start()
         loop.run_forever()
     except KeyboardInterrupt:
         process.terminate()
+    except Exception as ex:
+        # log the unexpected error
+        print(f"Unexpected error: {ex}")
+        process.terminate()
     finally:
         cPrint('Shutdown all workers ...', level='WARN')
-        loop.close()
+        loop.close()  # close the event loop
 
 
 if __name__ == '__main__':
     main()
```

## qw/discovery.py

```diff
@@ -1,91 +1,36 @@
 import asyncio
 from typing import Any
 from itertools import cycle
 import random
 import socket
-import struct
-from navconfig.logging import logging
 from qw.exceptions import QWException
 from qw.utils import cPrint
-from qw.utils.json import json_encoder, json_decoder
+from qw.utils.json import json_decoder
 from .conf import (
-    WORKER_DISCOVERY_HOST,
     WORKER_DISCOVERY_PORT,
     WORKER_DEFAULT_PORT,
     expected_message
 )
+from .protocols import DiscoveryProtocol
 
-MULTICAST_ADDRESS = "239.255.255.250"
-
-DEFAULT_HOST = WORKER_DISCOVERY_HOST
-if not DEFAULT_HOST:
-    DEFAULT_HOST = socket.gethostbyname(socket.gethostname())
-
-
-class DiscoveryProtocol(asyncio.DatagramProtocol):
-    """Basic Discovery Protocol for Workers."""
-
-    workers: dict = {}
-
-    def __init__(self):
-        self._loop = asyncio.get_event_loop()
-        self._loop.set_debug(True)
-        self.transport = None
-        super().__init__()
-
-    def connection_made(self, transport):
-        self.transport = transport
-        # Allow receiving multicast broadcasts
-        sock = self.transport.get_extra_info('socket')
-        group = socket.inet_aton(MULTICAST_ADDRESS)
-        mreq = struct.pack('4sL', group, socket.INADDR_ANY)
-        sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
-
-    def datagram_received(self, data: Any, addr: str):
-        data = data.decode('utf-8')
-        print(f'Received {data!r} from {addr!r}')
-        logging.debug("%s:%s > %s", *(addr + (data,)))
-        if data == 'list_workers':
-            data = json_encoder(self.workers).encode('utf-8')
-            self.transport.sendto(data, addr)
-        elif data == expected_message:
-            # send information Protocol:
-            data = expected_message.encode('utf-8')
-            self.transport.sendto(data, addr)
-        else:
-            # register a worker
-            try:
-                server, addr = zip(*json_decoder(data).items())
-                self.workers[server[0]] = tuple(addr[0])
-            except Exception as ex:
-                logging.warning(ex)
-
-
-    def error_received(self, exc):
-        print('Error received:', exc)
-
-    def connection_lost(self, exc):
-        print("Socket closed, stop the event loop", exc)
-
-    def register_worker(self, server: str, addr: tuple):
-        self.workers[server] = addr
-
-    def remove_worker(self, server: str):
-        del self.workers[server]
 
 async def get_server_discovery(event_loop: asyncio.AbstractEventLoop) -> Any:
     """Get Server Discovery.
     """
-    return await event_loop.create_datagram_endpoint(
-        DiscoveryProtocol,
-        local_addr=('0.0.0.0', WORKER_DISCOVERY_PORT),
-        family=socket.AF_INET,
-        allow_broadcast=True
-    )
+    try:
+        return await event_loop.create_datagram_endpoint(
+            DiscoveryProtocol,
+            local_addr=('0.0.0.0', WORKER_DISCOVERY_PORT),
+            family=socket.AF_INET,
+            allow_broadcast=True
+        )
+    except OSError:
+        return False
+
 
 def get_client_discovery() -> tuple:
     # Create a UDP socket
     sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
     sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
     sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
     sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
@@ -95,15 +40,15 @@
         server_list = []
         while True:
             sock.sendto(expected_message.encode(), srv_addr)
             # Receive response
             data, server = sock.recvfrom(4096)
             if data.decode('utf-8') == expected_message:
                 srv, port = server
-                cPrint(f':: Discovery Server: {srv}' )
+                cPrint(f':: Discovery Server: {srv}')
                 sock.sendto('list_workers'.encode(), (srv, port))
                 # ask for a list of servers:
                 # TODO: detect which port is used by this server:
                 ls, _ = sock.recvfrom(4096)
                 if ls:
                     server_list = [tuple(v) for v in json_decoder(ls).values()]
                 else:
@@ -112,8 +57,8 @@
     except socket.timeout as ex:
         raise QWException(
             "Unable to discover Workers on this Network."
         ) from ex
     finally:
         sock.close()
         random.shuffle(server_list)
-        return server_list, cycle(server_list) # pylint: disable=W0150
+        return server_list, cycle(server_list)  # pylint: disable=W0150
```

## qw/conf.py

```diff
@@ -1,14 +1,14 @@
 from navconfig import config
 
 def get_worker_list(workers: list):
     """Convert a list of workers in a tuple of worker:port for Scheduler."""
     wl = []
     for worker in workers:
-        w,p = worker.split(':')
+        w, p = worker.split(':')
         wl.append((w, p))
     return wl
 
 ### Worker Configuration
 WORKER_DEFAULT_HOST = config.get('WORKER_DEFAULT_HOST', fallback='0.0.0.0')
 WORKER_DEFAULT_PORT = config.getint('WORKER_DEFAULT_PORT', fallback=8888)
 WORKER_DEFAULT_QTY = config.getint('WORKER_DEFAULT_QTY', fallback=4)
@@ -20,14 +20,17 @@
 QW_WORKER_LIST = 'QW_WORKER_LIST'
 
 ## Network Discovery:
 USE_DISCOVERY = config.getboolean('USE_DISCOVERY', fallback=True)
 WORKER_DISCOVERY_HOST = config.get('WORKER_DISCOVERY_HOST')
 WORKER_DISCOVERY_PORT = config.getint('WORKER_DISCOVERY_PORT', fallback=8434)
 WORKER_DISCOVERY_BROADCAST = config.get('WORKER_DISCOVERY_BROADCAST', '255.255.255.255')
+WORKER_DEFAULT_MULTICAST = config.get(
+    'WORKER_DEFAULT_MULTICAST', fallback="239.255.255.250"
+)
 ## Word used by Discovery
 expected_message = config.get('WORKER_DISCOVERY_MESSAGE')
 WORKER_SECRET_KEY = config.get('WORKER_SECRET_KEY')
 
 REDIS_HOST = config.get('REDIS_HOST', fallback='localhost')
 REDIS_PORT = config.getint('REDIS_PORT', fallback=6379)
 REDIS_WORKER_DB = config.getint('REDIS_WORKER_DB', fallback=2)
@@ -41,13 +44,23 @@
 HIGH_LIST = [e.strip() for e in list(config.get(
     'WORKER_HIGH_LIST', fallback='127.0.0.1:8899').split(","))]
 WORKER_HIGH_LIST = get_worker_list(HIGH_LIST)
 
 # upgrade no-files
 NOFILES = config.getint('ULIMIT_NOFILES', fallback=16384)
 
-PACKAGE_LIST = config.getlist('PACKAGE_LIST', fallback=('asyncdb', 'qw', 'querysource', 'navconfig'))
+PACKAGE_LIST = config.getlist(
+    'PACKAGE_LIST', fallback=('asyncdb', 'qw', 'querysource', 'navconfig')
+)
 
 try:
-    from settings.settings import WORKER_LIST, WORKER_HIGH_LIST, WORKER_REDIS, WORKER_DEFAULT_HOST, WORKER_DEFAULT_PORT, PACKAGE_LIST # pylint: disable=W0611
+    from settings.settings import (
+        WORKER_LIST,
+        WORKER_HIGH_LIST,
+        WORKER_REDIS,
+        WORKER_DEFAULT_MULTICAST,
+        WORKER_DEFAULT_HOST,
+        WORKER_DEFAULT_PORT,
+        PACKAGE_LIST
+    )  # pylint: disable=W0611
 except ImportError:
     pass
```

## qw/process.py

```diff
@@ -37,74 +37,96 @@
         ## print(f'Setting soft & hard ulimit -n {soft} {hard}')
         res.setrlimit(res.RLIMIT_NOFILE, (soft, hard))
     except (ValueError, AttributeError) as err:
         logging.exception(err)
         try:
             ulimit = 'ulimit -{type} {value};'
             subprocess.Popen(ulimit.format(type='n', value=hard), shell=True)
-        except Exception as e: # pylint: disable=W0703
+        except Exception as e:  # pylint: disable=W0703
             print('Failed to set ulimit, giving up')
             logging.exception(e, stack_info=False)
     return 'nofile', (soft, hard)
 
 
-class spawn_process(object):
-    def __init__(self, args, event_loop):
-        self.loop: asyncio.AbstractEventLoop = event_loop
+class SpawnProcess(object):
+    def __init__(self, args):
+        try:
+            self.loop: asyncio.AbstractEventLoop = asyncio.get_event_loop()
+        except RuntimeError:
+            raise
         self.host: str = args.host
         self.address = socket.gethostbyname(socket.gethostname())
         self.id = str(uuid.uuid1())
         self.port: int = args.port
         self.worker: str = f"{args.wkname}-{args.port}"
         self.debug: bool = args.debug
         self.redis: Callable = None
+        self.enable_discovery: bool = True
+        if args.enable_discovery == 'false':
+            self.enable_discovery: bool = False
         self.discovery_server: Callable = None
         self.transport: asyncio.Transport = None
         # increase the ulimit of server
         raise_nofile(value=NOFILES)
+        ## Logger:
+        self.logger = logging.getLogger(
+            name='QW:WorkerProcess'
+        )
         for i in range(args.workers):
-            p = mp.Process(
-                target=start_server,
-                name=f'{self.worker}_{i}',
-                args=(i, args.host, args.port, args.debug, )
-            )
-            JOB_LIST.append(p)
-            p.start()
-
+            try:
+                p = mp.Process(
+                    target=start_server,
+                    name=f'{self.worker}_{i}',
+                    args=(i, args.host, args.port, args.debug, )
+                )
+                JOB_LIST.append(p)
+                p.start()
+            except (OSError, IOError) as ex:
+                self.logger.error(
+                    f"Error Dispatching Worker: {ex}"
+                )
+                raise
 
     async def start_redis(self):
         # starting redis:
         try:
             self.redis = aioredis.ConnectionPool.from_url(
-                    WORKER_REDIS,
-                    decode_responses=True,
-                    encoding='utf-8'
-            )
-        except Exception as e:
-            raise Exception(e) from e
+                WORKER_REDIS,
+                decode_responses=True,
+                encoding='utf-8'
+            )
+        except Exception as ex:
+            self.logger.exception(ex)
+            raise
 
     async def stop_redis(self):
         try:
             conn = aioredis.Redis(connection_pool=self.redis)
             await conn.delete(
                 QW_WORKER_LIST
             )
-            await self.redis.disconnect(inuse_connections = True)
-        except Exception as err: # pylint: disable=W0703
-            logging.exception(err)
+            await self.redis.disconnect(inuse_connections=True)
+        except Exception as err:  # pylint: disable=W0703
+            self.logger.exception(err)
 
     async def register_worker(self):
-        worker = json_encoder({
-            self.id: (self.address, self.port)
-        })
-        conn = aioredis.Redis(connection_pool=self.redis)
-        await conn.lpush(
-            QW_WORKER_LIST,
-            worker
-        )
+        try:
+            worker = json_encoder({
+                self.id: (self.address, self.port)
+            })
+            conn = aioredis.Redis(connection_pool=self.redis)
+            await conn.lpush(
+                QW_WORKER_LIST,
+                worker
+            )
+        except aioredis.ConnectionError as err:
+            self.logger.error(
+                f"Redis connection error: {err}"
+            )
+            raise
         if self.discovery_server:
             self.discovery_server.register_worker(
                 server=self.id, addr=(self.address, self.port)
             )
         else:
             # self-registration into Discovery Service:
             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
@@ -133,36 +155,78 @@
 
     def start(self):
         try:
             # start a redis connection
             self.loop.run_until_complete(
                 self.start_redis()
             )
-            try:
+        except aioredis.ConnectionError as err:
+            self.logger.error(
+                f"Redis connection error: {err}"
+            )
+            raise
+        except Exception as err:
+            self.logger.error(
+                f"Unexpected error when starting Redis: {err}"
+            )
+            raise
+        try:
+            if self.enable_discovery is True:
                 self.transport, self.discovery_server = self.loop.run_until_complete(
                     get_server_discovery(event_loop=self.loop)
                 )
                 cPrint(':: Starting Discovery Server ::', level='WARN')
-            except Exception:
-                pass
+        except asyncio.TimeoutError as err:
+            self.logger.error(
+                f"Timeout error when starting Discovery Server: {err}"
+            )
+            raise
+        except Exception as err:
+            self.logger.error(
+                f"Unexpected error when starting Discovery Server: {err}"
+            )
+            raise
+        try:
             # register worker in the worker list
             self.loop.run_until_complete(
                 self.register_worker()
             )
-        except Exception as err: # pylint: disable=W0703
-            logging.error(err)
+        except asyncio.TimeoutError as err:
+            self.logger.error(
+                f"Timeout error when registering worker: {err}"
+            )
+            raise
+        except Exception as err:
+            self.logger.error(
+                f"Unexpected error when registering worker: {err}"
+            )
+            raise
 
     def terminate(self):
         try:
             self.loop.run_until_complete(
                 self.remove_worker()
             )
             self.loop.run_until_complete(
                 self.stop_redis()
             )
-            if self.transport:
+            for j in JOB_LIST:
+                try:
+                    j.terminate()
+                except (OSError, AssertionError) as ex:
+                    self.logger.exception(ex)
+                try:
+                    j.join()
+                except TypeError as ex:
+                    self.logger.exception(ex)
+        except asyncio.TimeoutError as ex:
+            self.logger.warning(
+                f"Timeout error: {ex}"
+            )
+        except asyncio.CancelledError as exc:
+            self.logger.warning(str(exc))
+        except Exception as err:  # pylint: disable=W0703
+            self.logger.exception(err)
+            raise
+        finally:
+            if self.enable_discovery is True:
                 self.transport.close()
-        except Exception as err: # pylint: disable=W0703
-            logging.exception(err)
-        for j in JOB_LIST:
-            j.terminate()
-            j.join()
```

## qw/version.py

```diff
@@ -1,17 +1,17 @@
 """Queue Worker Meta information.
    QueueWorker is a asyncio-based Worker for distributed functions.
 """
 
 __title__ = 'qworker'
-__description__ = ('QueueWorker is asynchronous Task Queue implementation built on to of Asyncio.'
+__description__ = ('QueueWorker is asynchronous Task Queue implementation '
+                   'built on top of Asyncio.'
                    'Can you spawn distributed workers to run functions inside workers.')
-__version__ = '1.6.6'
+__version__ = '1.7.0'
 __author__ = 'Jesus Lara'
 __author_email__ = 'jesuslarag@gmail.com'
 __license__ = 'MIT'
 
-def get_version() -> tuple: # pragma: no cover
-   """
-   Get nav-auth version as tuple.
-   """
-   return tuple(x for x in __version__.split('.')) # pragma: no cover
+def get_version() -> tuple:  # pragma: no cover
+    """ Get nav-auth version as tuple.
+    """
+    return tuple(x for x in __version__.split('.'))  # pragma: no cover
```

## qw/client.py

```diff
@@ -74,34 +74,45 @@
         _retries_per_server: Defaultdict that stores number of retries for
                              each task queue server.
     """
     timeout: int = 5
     redis: Callable = None
 
     def __init__(self, worker_list: list = None, timeout: int = 5):
-        self._loop = asyncio.get_event_loop()
+        try:
+            self._loop = asyncio.get_event_loop()
+        except RuntimeError:
+            raise
+        ## logger:
+        self.logger = logging.getLogger('QW:Client')
         ## check if we use network discovery or redis list:
         if worker_list:
             self._workers = worker_list
             self._worker_list = itertools.cycle(worker_list)
         else:
             if USE_DISCOVERY is True:
                 # get worker list from discovery:
                 self._workers, self._worker_list = get_client_discovery()
             else:
-                # starting redis:
-                self.redis = aioredis.ConnectionPool.from_url(
+                try:
+                    # starting redis:
+                    self.redis = aioredis.ConnectionPool.from_url(
                         WORKER_REDIS,
                         decode_responses=True,
                         encoding='utf-8'
-                )
-                # getting the list from redis
-                self._workers, self._worker_list = self.get_workers()
+                    )
+                    # getting the list from redis
+                    self._workers, self._worker_list = self.get_workers()
+                except aioredis.ConnectionError as err:
+                    self.logger.error(
+                        f"Redis connection error: {err}"
+                    )
+                    raise
         if not self._worker_list:
-            logging.warning(
+            self.logger.warning(
                 'EMPTY WORKER LIST: Trying to connect to a default Worker'
             )
             # try to connect with the default worker
             self._workers = [(WORKER_DEFAULT_HOST, WORKER_DEFAULT_PORT)]
             self._worker_list = itertools.cycle(self._workers)
         self._num_retries = defaultdict(int)
         self._worker = None
@@ -111,17 +122,16 @@
         if USE_DISCOVERY is True:
             self._workers, self._worker_list = get_client_discovery()
 
     def get_workers(self):
         async def get_workers_list():
             conn = aioredis.Redis(connection_pool=self.redis)
             workers = []
-            l = await conn.lrange(QW_WORKER_LIST, 0, 100)
-            if l:
-                w = [orjson.loads(el) for el in l]
+            if (lrange := await conn.lrange(QW_WORKER_LIST, 0, 100)):
+                w = [orjson.loads(el) for el in lrange]
                 workers = [tuple(list(v.values())[0]) for v in w]
             return workers, itertools.cycle(workers)
         return self._loop.run_until_complete(get_workers_list())
 
     def get_servers(self) -> list:
         return self._workers
 
@@ -139,25 +149,24 @@
         if response == b'CONTINUE':
             return [reader, writer]
         else:
             response = cloudpickle.loads(response)
             if isinstance(response, BaseException):
                 raise response
 
-
     async def get_connection(self):
         retries = {}
         reader = None
         writer = None
         if not self._worker_list:
             # try discover again a worker list:
             self.discover_workers()
         while True:
             worker = round_robin_worker(self._worker_list)
-            logging.debug(f':: WORKER SELECTED: {worker!r}')
+            self.logger.debug(f':: WORKER SELECTED: {worker!r}')
             if not worker:
                 raise ConnectionAbortedError(
                     "Error: There is no workers to work with."
                 )
             try:
                 task = asyncio.open_connection(
                     *worker
@@ -167,15 +176,15 @@
                 )
                 retries[worker] = 0
                 # check the signature between server and client:
                 return await self.validate_connection(
                     reader, writer
                 )
             except DiscardedTask as exc:
-                logging.warning(
+                self.logger.warning(
                     f'Task was discarded, {exc!s}, retrying'
                 )
                 raise
             except asyncio.TimeoutError:
                 # removing this worker from the self workers
                 # TODO: removing bad worker:
                 warnings.warn(f"Timeout, skipping {worker!r}")
@@ -211,18 +220,18 @@
 
     async def close(self, writer: asyncio.StreamWriter):
         if writer.can_write_eof():
             writer.write_eof()
         await writer.drain()
         try:
             if self.redis:
-                await self.redis.disconnect(inuse_connections = True)
+                await self.redis.disconnect(inuse_connections=True)
         except (AttributeError, ConnectionError) as err:
-            logging.error(err)
-        logging.debug('Closing Socket')
+            self.logger.error(err)
+        self.logger.debug('Closing Socket')
         writer.close()
 
     async def run(self, fn: Any, *args, use_wrapper: bool = False, **kwargs):
         """Runs a function in Queue Worker
 
         Run function in the Queue Worker, returns the result or raises exception.
 
@@ -247,20 +256,20 @@
             ### ask again after wait for new connection:
             reader, writer = await self.get_connection()
         except ConnectionError as ex:
             raise ConnectionError(
                 f"Unable to Connect to Queue Worker: {ex}"
             ) from ex
         except Exception as err:
-            logging.error(err)
+            self.logger.error(err)
             raise
 
         host, *_ = writer.get_extra_info('sockname')
         # wrapping the function into Task Wrapper
-        logging.debug(f'Sending Object {fn!s} to Worker {host}')
+        self.logger.debug(f'Sending Object {fn!s} to Worker {host}')
         if isinstance(fn, (TaskWrapper, FuncWrapper)):
             # already wrapped
             func = fn
             func.queued = False
         elif use_wrapper is True:
             # wrap function into a Wrapper:
             func = FuncWrapper(
@@ -277,15 +286,15 @@
             serialized_task = cloudpickle.dumps(func)
             writer.write(serialized_task)
             # sending data to worker:
             if writer.can_write_eof():
                 writer.write_eof()
             await writer.drain()
         except Exception as err:
-            logging.error(f'Error Serializing Task: {err!s}')
+            self.logger.error(f'Error Serializing Task: {err!s}')
             raise
         # Then, got the result:
         serialized_result = None
         try:
             while True:
                 serialized_result = await reader.read(-1)
                 if reader.at_eof():
@@ -294,15 +303,15 @@
             raise QWException(
                 f"Error getting results from Worker: {ex}"
             ) from ex
         finally:
             await self.close(writer)
         try:
             task_result = cloudpickle.loads(serialized_result)
-            logging.debug(
+            self.logger.debug(
                 f'Data Received: {task_result!r}'
             )
             try:
                 if isinstance(task_result, str):
                     result = jsonpickle.decode(task_result)
                 else:
                     result = task_result
@@ -313,18 +322,18 @@
                 print(e)
             task_result = result
         except (ValueError, TypeError) as ex:
             raise ParserError(
                 f"Error Parsing serialized results: {ex}"
             ) from ex
         except EOFError as err:
-            logging.exception(f'No data was received from Server: {err!s}')
+            self.logger.exception(f'No data was received from Server: {err!s}')
             task_result = None
-        except Exception as err: # pylint: disable=W0703
-            logging.exception(f'Error receiving data from Worker Server: {err!s}')
+        except Exception as err:  # pylint: disable=W0703
+            self.logger.exception(f'Error receiving data from Worker Server: {err!s}')
             task_result = None
         if isinstance(task_result, BaseException):
             # raise task_result
             return task_result
         elif isinstance(task_result, list):
             # try to convert into object:
             res = []
@@ -337,35 +346,19 @@
             return res
         elif isinstance(task_result, dict):
             # check if result is and error:
             if 'exception' in task_result:
                 ex = task_result['exception']
                 if isinstance(ex, BaseException):
                     msg = task_result['error']
-                    ex.message = msg
-                    raise ex
+                    error = str(ex)
+                    raise ex(
+                        f"{error}: {msg}"
+                    )
             return task_result
-        # elif isinstance(task_result, str):
-        #     if newval:=is_parseable(task_result):
-        #         val = newval(task_result)
-        #         if isinstance(val, list):
-        #             new_val = []
-        #             for el in val:
-        #                 if nval:=is_parseable(el):
-        #                     new_val.append(nval(el))
-        #                 else:
-        #                     new_val.append(el)
-        #             val = new_val
-        #         return val
-        #     # try to de-serialize the result:
-        #     try:
-        #         return orjson.loads(task_result)
-        #     except Exception as err:
-        #         logging.error(err)
-        #         return task_result
         else:
             return task_result
 
     async def queue(self, fn: Any, *args, **kwargs):
         """Send a function to a Queue Worker and return.
 
         Send & Forget functionality to send a task to Queue Worker.
@@ -387,17 +380,17 @@
         try:
             reader, writer = await self.get_connection()
         except DiscardedTask:
             await asyncio.sleep(WAIT_TIME)
             ### ask again after wait for new connection:
             reader, writer = await self.get_connection()
         except Exception as err:
-            logging.error(err)
+            self.logger.error(err)
             raise
-        logging.debug(f'Sending function {fn!s} to Worker')
+        self.logger.debug(f'Sending function {fn!s} to Worker')
         host, *_ = writer.get_extra_info('sockname')
         if isinstance(fn, (FuncWrapper, TaskWrapper)):
             # Function was wrapped or is already wrapped
             func = fn
         else:
             func = FuncWrapper(
                 host,
@@ -430,11 +423,11 @@
             # we dont need the result, return true
             serialized_result = {
                 "status": "Queued",
                 "task": f"{func!r}",
                 "message": received
             }
             return serialized_result
-        except Exception as err: # pylint: disable=W0703
-            logging.exception(
+        except Exception as err:  # pylint: disable=W0703
+            self.logger.exception(
                 f'Error Serializing Task: {err!s}'
             )
```

## qw/server.py

```diff
@@ -1,79 +1,54 @@
 """QueueWorker Server Implementation"""
-import asyncio
-import inspect
-import multiprocessing as mp
-from collections.abc import Callable
 import os
-import queue
 import socket
 import uuid
+import asyncio
+import inspect
+from typing import Any
+from collections.abc import Callable
+import multiprocessing as mp
 from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
 from functools import partial
 import resource
 import psutil
 import cloudpickle
-import uvloop
 from navconfig.logging import logging
 from qw.exceptions import (
     QWException,
     ParserError,
     ConfigError,
     DiscardedTask
 )
 from qw.utils import make_signature
+from .protocols import QueueProtocol
 from .conf import (
     WORKER_DEFAULT_HOST,
     WORKER_DEFAULT_PORT,
     WORKER_DEFAULT_QTY,
     WORKER_QUEUE_SIZE,
     expected_message,
     WORKER_SECRET_KEY,
     RESOURCE_THRESHOLD,
     CHECK_RESOURCE_USAGE
 )
 from .utils.json import json_encoder
 from .utils.versions import get_versions
 from .utils import cPrint
-from .wrappers import QueueWrapper, FuncWrapper, TaskWrapper
-
-asyncio.set_event_loop_policy(
-    uvloop.EventLoopPolicy()
+from .wrappers import (
+    QueueWrapper,
+    FuncWrapper,
+    TaskWrapper
 )
-uvloop.install()
+
 
 DEFAULT_HOST = WORKER_DEFAULT_HOST
 if not DEFAULT_HOST:
     DEFAULT_HOST = socket.gethostbyname(socket.gethostname())
 
-
-def start_server(num_worker, host, port, debug: bool):
-    """thread worker function"""
-    loop = asyncio.new_event_loop()
-    asyncio.set_event_loop(loop)
-    worker = QWorker(
-        host=host,
-        port=port,
-        event_loop=loop,
-        debug=debug,
-        worker_id=num_worker
-    )
-    try:
-        loop.run_until_complete(
-            worker.start()
-        )
-    except KeyboardInterrupt:
-        print(f'Shutdown Worker {worker.name}')
-        loop.run_until_complete(
-            worker.shutdown()
-        )
-    finally:
-        loop.close()
-
-
 class QWorker:
     """Queue Task Worker server.
 
     Attributes:
         host: Hostname of the server.
         port: Port number of the server.
         loop: Event loop to run in.
@@ -83,15 +58,15 @@
             self,
             host: str = DEFAULT_HOST,
             port: int = WORKER_DEFAULT_PORT,
             worker_id: int = None,
             name: str = '',
             event_loop: asyncio.AbstractEventLoop = None,
             debug: bool = False,
-            protocol = None
+            protocol: Any = None
     ):
         self.host = host
         self.port = port
         self.debug = debug
         self.queue = None
         self.consumers = []
         self.executor = None
@@ -99,124 +74,138 @@
         if name:
             self._name = name
         else:
             self._name = mp.current_process().name
         self._executor = ThreadPoolExecutor(
             max_workers=WORKER_DEFAULT_QTY
         )
-        if not event_loop:
-            self._loop = asyncio.new_event_loop()
-            asyncio.set_event_loop(self._loop)
-        else:
-            self._loop = event_loop
+        self._loop = event_loop if event_loop else asyncio.new_event_loop()
         self._server: Callable = None
         self._pid = os.getpid()
         self._protocol = protocol
+        # logging:
+        self.logger = logging.getLogger(
+            f'QW:Server:{self._name}:{self._id}'
+        )
 
     @property
     def name(self):
         return self._name
 
-    def get_resource_usage(self):
-        if CHECK_RESOURCE_USAGE is True:
-            soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
-            processes = psutil.process_iter()
-            used = 0
-            try:
-                for ps in processes:
-                    try:
-                        num_fds = len(ps.open_files())
-                        used += num_fds
-                    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
-                        pass
-                return (used / soft) * 100
-            except (ValueError, RuntimeError):
-                pass
-        else:
-            return True
-
     async def start(self):
         """Starts Queue Manager."""
         self.queue = asyncio.Queue(maxsize=WORKER_QUEUE_SIZE)
         self.executor = ProcessPoolExecutor(
             max_workers=WORKER_DEFAULT_QTY
         )
-        if self._protocol:
-            coro = self._loop.create_server(
-                self._protocol,
-                host=self.host,
-                port=self.port,
-                family=socket.AF_INET,
-                reuse_port=True
-            )
-        else:
-            coro = asyncio.start_server(
-                self.connection_handler,
-                host=self.host,
-                port=self.port,
-                family=socket.AF_INET,
-                reuse_port=True,
-                # loop=self._loop
-            )
-        # server
-        self._server = await coro
-        self.server_address = (socket.gethostbyname(socket.gethostname()), self.port)
         try:
-            await self.fire_consumers()
+            if self._protocol:
+                self._server = await self._loop.create_server(
+                    self._protocol,
+                    host=self.host,
+                    port=self.port,
+                    family=socket.AF_INET,
+                    reuse_port=True
+                )
+            else:
+                self._server = await asyncio.start_server(
+                    self.connection_handler,
+                    host=self.host,
+                    port=self.port,
+                    family=socket.AF_INET,
+                    reuse_port=True,
+                    # loop=self._loop
+                )
+            self.server_address = (
+                socket.gethostbyname(socket.gethostname()), self.port
+            )
             sock = self._server.sockets[0].getsockname()
-            logging.info(
+            self.logger.info(
                 f'Serving {self._name}:{self._id} on {sock}, pid: {self._pid}'
             )
         except Exception as err:
-            logging.error(err)
             raise QWException(
                 f"Error: {err}"
             ) from err
         # Serve requests until Ctrl+C is pressed
         try:
+            await self.fire_consumers()
             async with self._server:
                 await self._server.serve_forever()
-        except RuntimeError as err:
-            logging.exception(err, stack_info=True)
+        except (RuntimeError, KeyboardInterrupt) as err:
+            self.logger.exception(err, stack_info=True)
 
     async def fire_consumers(self):
         """Fire up the Task consumers."""
         self.consumers = [
             asyncio.create_task(
-                self.task_handler(self.queue)) for _ in range(WORKER_QUEUE_SIZE)
+                self.task_handler(self.queue)) for _ in range(WORKER_QUEUE_SIZE - 1)
         ]
 
-    async def empty_queue(self, q: asyncio.Queue):
-        """Processing and shutting down the Queue."""
-        for _ in range(q.qsize()):
+    def get_resource_usage(self):
+        if CHECK_RESOURCE_USAGE is True:
+            soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
+            processes = psutil.process_iter()
+            used = 0
             try:
-                q.get_nowait()
-                q.task_done()
-            except queue.Empty:
+                for ps in processes:
+                    try:
+                        num_fds = len(ps.open_files())
+                        used += num_fds
+                    except (
+                        psutil.NoSuchProcess,
+                        psutil.AccessDenied,
+                        psutil.ZombieProcess
+                    ):
+                        pass
+                return (used / soft) * 100
+            except (ValueError, RuntimeError):
                 pass
+        else:
+            return True
+
+    async def empty_queue(self, q: asyncio.Queue):
+        """Processing and shutting down the Queue."""
+        while not q.empty():
+            q.get_nowait()
+            q.task_done()
         await q.join()
 
     async def shutdown(self):
         if self.debug is True:
-            cPrint(f'Shutting down worker {self.name!s}')
+            cPrint(
+                f'Shutting down worker {self.name!s}'
+            )
         try:
             # forcing close the queue
             await self.empty_queue(self.queue)
         except KeyboardInterrupt:
             pass
         # also: cancel the idle consumers:
         for c in self.consumers:
-            c.cancel()
+            try:
+                c.cancel()
+            except asyncio.CancelledError:
+                pass
         try:
             self._server.close()
             await self._server.wait_closed()
         except RuntimeError as err:
-            logging.exception(err, stack_info=True)
+            self.logger.exception(err, stack_info=True)
+        except Exception as exc:
+            raise QWException(
+                f"Error closing Worker: {exc}"
+            )
+        finally:
+            self._loop.stop()
         if self.debug is True:
-            cPrint('::: QueueWorker Server Closed ::: ', level='INFO')
+            cPrint(
+                '::: QueueWorker Server Closed ::: ',
+                level='INFO'
+            )
 
     def run_process(self, fn):
         """Unpickles task, runs it and pickles result."""
         loop = asyncio.new_event_loop()
         asyncio.set_event_loop(loop)
         fn.set_loop(loop)
         try:
@@ -229,319 +218,421 @@
                 f"Error: {err}"
             ) from err
         finally:
             loop.close()
 
     async def run_function(self, fn, event_loop: asyncio.AbstractEventLoop):
         result = None
-        print(f'Running Task {fn!s} in worker {self.name!s}')
+        self.logger.debug(
+            f'Running Task {fn!s} in worker {self.name!s}'
+        )
         try:
             asyncio.set_event_loop(event_loop)
             if isinstance(fn, FuncWrapper):
                 result = await fn()
             elif inspect.isawaitable(fn) or asyncio.iscoroutinefunction(fn):
                 result = await fn()
             else:
                 result = fn()
-        except Exception as err: # pylint: disable=W0703
+        except Exception as err:  # pylint: disable=W0703
             result = err
         return result
 
     async def run_task(self, task: TaskWrapper):
         result = None
         try:
             await task.create()
             result = await task.run()
-        except Exception as err: # pylint: disable=W0703
+        except Exception as err:  # pylint: disable=W0703
             result = err
         finally:
             await task.close()
-        print(f'RUN TASK {task!s} RESULT> ', result)
+        self.logger.debug(
+            f"Running Task: {task!s}"
+        )
         return result
 
     async def task_handler(self, q: asyncio.Queue):
         """Method for handling the tasks received by the connection handler."""
         while True:
             task = await q.get()
             if self.debug:
-                cPrint(f'Running Queued Task {task!s}', level='DEBUG')
+                cPrint(
+                    f'Running Queued Task {task!s}', level='DEBUG'
+                )
             # processing the task received
             if isinstance(task, TaskWrapper):
-                # Running a DataIntegrator Task
+                # Running a FlowTask Task
                 task.set_loop(self._loop)
                 task.debug = self.debug
-                result = await self.run_task(task)
-                logging.debug(f'{task!s} Result: {result!r}')
+                fn = partial(self.run_process, task)
+                result = await self._loop.run_in_executor(
+                    self._executor, fn
+                )
+                # result = await self.run_task(task)
+                self.logger.debug(
+                    f'{task!s} Result: {result!r}'
+                )
             elif isinstance(task, FuncWrapper):
                 # running a FuncWrapper
                 result = None
                 try:
                     result = await self.run_function(task, self._loop)
-                except Exception as err: # pylint: disable=W0703
+                except Exception as err:  # pylint: disable=W0703
                     result = err
-                logging.debug(f'{task!s} Result: {result!r}')
+                self.logger.debug(f'{task!s} Result: {result!r}')
             else:
                 # TODO: try to Execute the object deserialized
                 pass
             q.task_done()
-            logging.debug(f'consumed: {task}')
-            logging.debug(f'QUEUE Size after Work: {self.queue.qsize()}')
-
+            self.logger.debug(
+                f'consumed: {task}'
+            )
+            self.logger.debug(
+                f'QUEUE Size after Work: {self.queue.qsize()}'
+            )
 
     def check_signature(self, payload: bytes) -> bool:
         signature = make_signature(expected_message, WORKER_SECRET_KEY)
         if signature == payload:
             return True
         else:
             return False
 
-
-    async def response_keepalive(self, writer: asyncio.StreamWriter, status: dict = None) -> None:
+    async def response_keepalive(
+        self,
+        writer: asyncio.StreamWriter,
+        status: dict = None
+    ) -> None:
         addrs = ', '.join(str(sock.getsockname()) for sock in self._server.sockets)
         prct_used = self.get_resource_usage()
-        ### logging.debug(f'{self.name}: {prct_used:.2f}%')
         if not status:
             status = {
                 "pong": "Empty data",
                 "worker": {
                     "name": self.name,
                     "serving": addrs,
                     "resource": f"{prct_used:.2f}%"
                 }
             }
         result = json_encoder(status)
         await self.closing_writer(writer, result.encode('utf-8'))
 
-    async def connection_handler(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
-        """ Handler for Function/Task Execution.
+    async def worker_health(self, writer: asyncio.StreamWriter):
+        addrs = ', '.join(str(sock.getsockname()) for sock in self._server.sockets)
+        status = {
+            "queue": {
+                "size": self.queue.qsize(),
+                "full": self.queue.full(),
+                "empty": self.queue.empty(),
+                "consumers": len(self.consumers)
+            },
+            "worker": {
+                "name": self.name,
+                "address": self.server_address,
+                "serving": addrs
+            }
+        }
+        await self.response_keepalive(status=status, writer=writer)
 
-        receives the client request and run/queue the function..
+    async def worker_check_state(self, writer: asyncio.StreamWriter):
+        ## TODO: add last executed task
+        addrs = ', '.join(str(sock.getsockname()) for sock in self._server.sockets)
+        status = {
+            "versions": get_versions(),
+            "worker": {
+                "name": self.name,
+                "address": self.server_address,
+                "serving": addrs
+            },
+            "queue": {
+                "size": self.queue.qsize(),
+                "full": self.queue.full(),
+                "empty": self.queue.empty(),
+                "consumers": len(self.consumers)
+            },
+        }
+        await self.response_keepalive(
+            status=status,
+            writer=writer
+        )
 
-        Args:
-            reader: asyncio StreamReader, client information
-            writer: asyncio StreamWriter, infor to send to client.
-        Returns:
-            Task Result.
-        """
-        # # TODO: task can select which executor to use, else use default:
-        # print(f':: Starting Handler on worker {self.name!s} ::')
-        addr = writer.get_extra_info("peername")
-        # first time: check signature:
+    async def discard_task(self, message: str, writer: asyncio.StreamWriter):
+        exc = DiscardedTask(
+            message
+        )
+        result = cloudpickle.dumps(exc)
+        await self.closing_writer(
+            writer,
+            result
+        )
+        return False
+
+    async def signature_validation(
+            self,
+            reader: asyncio.StreamReader,
+            writer: asyncio.StreamWriter
+    ):
+        prefix = None
         try:
             prefix = await reader.readline()
             if not prefix:
                 # if no content on payload:
                 await self.response_keepalive(writer=writer)
                 return False
-            msglen = int(prefix)
+        except asyncio.IncompleteReadError as exc:
+            self.logger.error(exc)
+            return False
+        except (ConnectionResetError, ConnectionAbortedError, EOFError) as exc:
+            self.logger.error(exc)
+            raise
+        except asyncio.CancelledError:
+            return False
+        ###
+        if prefix == b'health':
+            ### sending a heartbeat
+            await self.worker_health(
+                writer=writer
+            )
+            return False
+        elif prefix == b'check_state':
+            await self.worker_check_state(
+                writer=writer
+            )
+            return False
+        else:
+            try:
+                msglen = int(prefix)
+            except ValueError:
+                raise
             payload = await reader.readexactly(msglen)
             if self.check_signature(payload) is False:
                 ### close transport inmediately:
                 exc = ConnectionRefusedError(
                     'Connection unsecured, Closing now.'
                 )
-                logging.error(f'Closing unsecured connection from {addr}')
+                self.logger.error(
+                    'Closing unsecured connection'
+                )
                 result = cloudpickle.dumps(exc)
                 await self.closing_writer(
                     writer,
                     result
                 )
                 return False
             else:
                 prct_used = self.get_resource_usage()
-                logging.debug(f'Current NFILE percent: {prct_used}')
+                self.logger.debug(f'Current NFILE percent: {prct_used}')
                 if prct_used >= int(RESOURCE_THRESHOLD):
-                    logging.error(
-                        f'Discarted Task from {addr} due Resource usage: {prct_used}'
+                    self.logger.error(
+                        f'Discarted Task due Resource usage: {prct_used}'
                     )
                     exc = DiscardedTask(
                         f'Too many Open Files: {prct_used:.2f}% usage.'
                     )
                     result = cloudpickle.dumps(exc)
                     await self.closing_writer(
                         writer,
                         result
                     )
                     return False
                 # passing a "continue" signal:
                 writer.write('CONTINUE'.encode('utf-8'))
                 await writer.drain()
-        except ValueError as err:
-            if prefix == b'health':
-                status = {
-                    "queue": {
-                        "size": self.queue.qsize(),
-                        "full": self.queue.full(),
-                        "empty": self.queue.empty(),
-                        "consumers": len(self.consumers)
-                    },
-                    "worker": {
-                        "name": self.name,
-                        "address": self.server_address
-                    }
-                }
-                await self.response_keepalive(status=status, writer=writer)
-            exc = ConfigError(
-                f"QW Server: invalid or empty Signature WORKER_SECRET_KEY, err: {err!s}"
-            )
-            result = cloudpickle.dumps(exc)
-            await self.closing_writer(writer, result)
-            return False
-        except Exception as err: # pylint: disable=W0703
-            logging.exception(f'Error Decoding Signature: {err}', stack_info=True)
-        ## after: deserialize Task:
+                return True
+
+    async def _read_task(self, reader: asyncio.StreamReader):
         serialized_task = b''
         while True:
-            serialized_task = await reader.read(-1)
+            serialized_task += await reader.read(-1)
             if reader.at_eof():
                 break
-        result = None
-        task_uuid = uuid.uuid4()
-        logging.debug(
-            f"Received Data from {addr!r} into worker {self.name!s} pid: {self._pid}"
-        )
+        return serialized_task
+
+    async def deserialize_task(self, serialized_task, writer: asyncio.StreamWriter):
         try:
             task = cloudpickle.loads(serialized_task)
-            logging.debug(f'TASK RECEIVED: {task}')
+            self.logger.debug(f'TASK RECEIVED: {task}')
+            return task
         except RuntimeError as ex:
-            ex = ParserError(f"Error decoding serialized task: {ex}")
+            ex = ParserError(f"Error Decoding Serialized Task: {ex}")
             result = cloudpickle.dumps(ex)
             await self.closing_writer(writer, result)
             return False
-        except EOFError:
-            # send a pong
-            result = "Pong: Empty Data"
-            await self.closing_writer(writer, result.encode('utf-8'))
-            return True
-        except Exception as err: # pylint: disable=W0703
-            exc = Exception(
-                f'No Valid Function was sent to Worker: {err}'
-            )
-            result = cloudpickle.dumps(exc)
-            await self.closing_writer(writer, result)
-            return False
-        # TODO: evaluate different kind of tasks
-        if not task or isinstance(task, str):
-            addrs = ', '.join(str(sock.getsockname()) for sock in self._server.sockets)
-            if task == 'health':
-                # can return health of worker
-                status = {
-                    "queue": {
-                        "size": self.queue.qsize(),
-                        "full": self.queue.full(),
-                        "empty": self.queue.empty(),
-                        "consumers": len(self.consumers)
-                    },
-                    "worker": {
-                        "name": self.name,
-                        "address": self.server_address,
-                        "serving": addrs
-                    }
-                }
-                await self.response_keepalive(status=status, writer=writer)
-            elif task == 'check_state':
-                ## checking the current state of Worker:
-                ## TODO: add last executed task
-                status = {
-                    "versions": get_versions(),
-                    "worker": {
-                        "name": self.name,
-                        "address": self.server_address,
-                        "serving": addrs
-                    },
-                    "queue": {
-                        "size": self.queue.qsize(),
-                        "full": self.queue.full(),
-                        "empty": self.queue.empty(),
-                        "consumers": len(self.consumers)
-                    },
-                }
-                await self.response_keepalive(status=status, writer=writer)
-            else:
-            # its a simple keepalive:
-                await self.response_keepalive(writer=writer)
-            return True
-        elif isinstance(task, QueueWrapper):
-            # Set Debug level of task:
-            task.debug = self.debug
-            if task.queued is True:
-                try:
-                    task.id = task_uuid
-                    await self.queue.put(task)
-                    logging.debug(f'Current QUEUE Size: {self.queue.qsize()}')
-                    result = f'Task {task!s} with id {task_uuid} was queued.'.encode('utf-8')
-                except asyncio.QueueFull:
-                    logging.debug(
-                        f"Worker {self.name!s} Queue is Full, discarding Task {task!r}"
-                    )
-                    result = {
-                        "error": f"Worker {self.name!s} Queue is Full, discarding Task {task!r}"
-                    }
-                    # result = cloudpickle.dumps(result)
-                    await self.closing_writer(writer, result)
-                    return False
-            else:
+
+    async def handle_queue_wrapper(
+        self,
+        task: QueueWrapper,
+        uid: uuid.UUID,
+        writer: asyncio.StreamWriter
+    ):
+        """Handle QueueWrapper Tasks.
+        """
+        # Set Debug level of task:
+        task.debug = self.debug
+        if task.queued is True:
+            try:
+                task.id = uid
+                await self.queue.put(task)
+                self.logger.debug(
+                    f'Current QUEUE Size: {self.queue.qsize()}'
+                )
+                return f'Task {task!s} with id {uid} was queued.'.encode('utf-8')
+            except asyncio.QueueFull:
+                return await self.discard_task(
+                    f"Worker {self.name!s} Queue is Full, discarding Task {task!r}"
+                )
+        else:
+            try:
+                # executed and send result to client
+                task.id = uid
+                fn = partial(self.run_process, task)
+                return await self._loop.run_in_executor(self._executor, fn)
+            except Exception as err:  # pylint: disable=W0703
                 try:
-                    # executed and send result to client
-                    task.id = task_uuid
-                    fn = partial(self.run_process, task)
-                    result = await self._loop.run_in_executor(self._executor, fn)
-                except Exception as err: # pylint: disable=W0703
-                    try:
-                        result = cloudpickle.dumps(err)
-                    except Exception as ex: # pylint: disable=W0703
-                        result = cloudpickle.dumps(
-                            Exception(f'Error on Worker: {ex!s}')
+                    result = cloudpickle.dumps(err)
+                except Exception as ex:  # pylint: disable=W0703
+                    result = cloudpickle.dumps(
+                        QWException(
+                            f'Error on Deal with Exception: {ex!s}'
                         )
-                    await self.closing_writer(writer, result)
-                    return False
+                    )
+                await self.closing_writer(writer, result)
+                return False
+
+    async def connection_handler(
+            self,
+            reader: asyncio.StreamReader,
+            writer: asyncio.StreamWriter
+    ):
+        """ Handler for Function/Task Execution.
+        receives the client request and run/queue the function.
+        Args:
+            reader: asyncio StreamReader, client information
+            writer: asyncio StreamWriter, infor to send to client.
+        Returns:
+            Task Result.
+        """
+        # # TODO: task can select which executor to use, else use default:
+        addr = writer.get_extra_info("peername")
+        # first time: check signature authentication of payload:
+        if not await self.signature_validation(reader, writer):
+            return False
+        self.logger.debug(
+            f"Received Data from {addr!r} to worker {self.name!s} pid: {self._pid}"
+        )
+        # after: deserialize Task:
+        serialized_task = await self._read_task(reader)
+        task = None
+        result = None
+        task_uuid = uuid.uuid4()
+        task = await self.deserialize_task(serialized_task, writer)
+        if not task:
+            return False
+        elif isinstance(task, QueueWrapper):
+            if not (result := await self.handle_queue_wrapper(task, task_uuid, writer)):
+                return False
         elif callable(task):
-            result = await self.run_function(task, self._loop)
+            result = await self.run_function(
+                task, self._loop
+            )
         else:
             # put work in Queue:
             try:
                 await self.queue.put(task)
                 await asyncio.sleep(.1)
-                logging.debug(f'Current QUEUE Size: {self.queue.qsize()}')
-                result = f'Task {task!s} with id {task_uuid} was queued.'.encode('utf-8')
+                self.logger.debug(
+                    f'Current QUEUE Size: {self.queue.qsize()}'
+                )
+                result = f'Task {task!s}:{task_uuid} was Queued.'.encode('utf-8')
             except asyncio.QueueFull:
-                logging.debug(
+                self.logger.error(
                     f"Worker Queue is Full, discarding Task {task!r}"
                 )
+                return await self.discard_task(
+                    message=f'Task {task!s} was discarded, queue full',
+                    writer=writer
+                )
         if result is None:
             # Not always a Task returns Value, sometimes returns None.
             result = [
                 {"uuid": task_uuid, "worker": self.name}
             ]
         try:
             if isinstance(result, BaseException):
+                try:
+                    msg = result.message
+                except Exception:
+                    msg = str(result)
                 result = {
                     "exception": result,
-                    "error": result.message
+                    "error": msg
                 }
-                # sending Task Exception
-                # result = jsonpickle.encode(result)
             elif inspect.isgeneratorfunction(result) or isinstance(result, list):
                 try:
                     result = json_encoder(list(result))
                 except (ValueError, TypeError):
-                    result = f"{result!r}" # cannot pickle a generator object
+                    result = f"{result!r}"  # cannot pickle a generator object
             result = cloudpickle.dumps(result)
-        except Exception as err: # pylint: disable=W0703
+        except Exception as err:  # pylint: disable=W0703
             error = {
                 "exception": err,
                 "error": str(err)
             }
             result = cloudpickle.dumps(error)
-            logging.error(f'Error dumping result: {err!s}')
+            self.logger.error(
+                f'Error dumping result: {err!s}'
+            )
         await self.closing_writer(writer, result)
-        return True
 
     async def closing_writer(self, writer: asyncio.StreamWriter, result):
         """Sending results and closing the streamer."""
-        writer.write(result)
-        await writer.drain()
-        if writer.can_write_eof():
-            writer.write_eof()
-        if self.debug is True:
-            cPrint(f"Closing client socket, pid: {self._pid}", level='DEBUG')
-        writer.close()
+        try:
+            writer.write(result)
+            await writer.drain()
+            if writer.can_write_eof():
+                writer.write_eof()
+            if self.debug is True:
+                cPrint(f"Closing client socket, pid: {self._pid}", level='DEBUG')
+            writer.close()
+        except Exception as e:
+            self.logger.error(f"Error while closing writer: {str(e)}")
+
+
+### Start Server ###
+def start_server(num_worker, host, port, debug: bool):
+    """thread worker function"""
+    loop = None
+    worker = None
+    try:
+        loop = asyncio.new_event_loop()
+        asyncio.set_event_loop(loop)
+    except RuntimeError as ex:
+        raise QWException(
+            f"Unable to set an event loop: {ex}"
+        ) from ex
+    try:
+        worker = QWorker(
+            host=host,
+            port=port,
+            event_loop=loop,
+            debug=debug,
+            worker_id=num_worker
+        )
+        loop.run_until_complete(
+            worker.start()
+        )
+    except (OSError, RuntimeError) as ex:
+        raise QWException(
+            f"Unable to Spawn a new Worker: {ex}"
+        )
+    except KeyboardInterrupt:
+        if loop and worker:
+            worker.logger.info(
+                f'Shutting down Worker {worker.name if worker else "unknown"}'
+            )
+            loop.run_until_complete(
+                worker.shutdown()
+            )
+    finally:
+        if loop:
+            loop.close()  # Close the event loop
```

## qw/protocols.py

```diff
@@ -1,52 +1,132 @@
+import time
 import asyncio
+from typing import Any
 import hashlib
-import time
+import socket
+import struct
+from json import JSONDecodeError
 from navconfig.logging import logging
+from qw.exceptions import QWException
+from qw.utils.json import json_encoder, json_decoder
+from .conf import (
+    WORKER_DISCOVERY_HOST,
+    WORKER_DEFAULT_MULTICAST,
+    expected_message
+)
+
+MULTICAST_ADDRESS = WORKER_DEFAULT_MULTICAST
+
+DEFAULT_HOST = WORKER_DISCOVERY_HOST
+if not DEFAULT_HOST:
+    DEFAULT_HOST = socket.gethostbyname(socket.gethostname())
 
-class QueueServer(asyncio.Protocol):
-    """Connection Protocol for QueueWorker Server."""
-    connections = {}
+
+class DiscoveryProtocol(asyncio.DatagramProtocol):
+    """Basic Discovery Protocol for Workers."""
+
+    workers: dict = {}
 
     def __init__(self):
-        self.logger = logging.getLogger('qworker.protocol')
+        try:
+            self._loop = asyncio.get_event_loop()
+            self._loop.set_debug(True)
+        except RuntimeError as ex:
+            raise QWException(
+                f"Error getting event loop: {ex}"
+            )
+        self.logger = logging.getLogger('QW:Discovery')
+        self.transport = None
+        super().__init__()
+
+    def connection_made(self, transport):
+        self.transport = transport
+        # Allow receiving multicast broadcasts
+        sock = self.transport.get_extra_info('socket')
+        group = socket.inet_aton(MULTICAST_ADDRESS)
+        mreq = struct.pack('4sL', group, socket.INADDR_ANY)
+        sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
+
+    def datagram_received(self, data: Any, addr: str):
+        data = data.decode('utf-8')
+        logging.debug("%s:%s > %s", *(addr + (data,)))
+        if data == 'list_workers':
+            data = json_encoder(self.workers).encode('utf-8')
+            self.transport.sendto(data, addr)
+        elif data == expected_message:
+            # send information Protocol:
+            data = expected_message.encode('utf-8')
+            self.transport.sendto(data, addr)
+        else:
+            # register a worker
+            try:
+                server, addr = zip(*json_decoder(data).items())
+                self.workers[server[0]] = tuple(addr[0])
+            except JSONDecodeError as ex:
+                self.logger.error(
+                    f"JSON decode error: {ex}"
+                )
+            except Exception as ex:
+                self.logger.error(ex)
+
+    def error_received(self, exc):
+        print('Error received:', exc)
+
+    def connection_lost(self, exc):
+        print("Socket closed, stop the event loop", exc)
+
+    def register_worker(self, server: str, addr: tuple):
+        self.workers[server] = addr
+
+    def remove_worker(self, server: str):
+        del self.workers[server]
+
+class QueueProtocol(asyncio.Protocol):
+    """Connection Protocol for QueueWorker Server."""
+
+    def __init__(self, queue: asyncio.Queue, name: str):
+        self.queue = queue
+        self.logger = logging.getLogger(
+            f'QW:QueueServer-{name}'
+        )
         self.transport = None
         self.loop = asyncio.get_event_loop()
         self._ready = asyncio.Event()
-        print('Starting Protocol ... ')
+        self._name = name
+        print(
+            f'Starting Protocol for {name}'
+        )
 
     def setup(self, peer):
         self.ip, self.port = peer[0], peer[1]
 
     @property
     def connection_id(self):
         if not hasattr(self, '_connection_id'):
-            self._connection_id = hashlib.md5('{}{}{}'.format(self.ip, self.port, time.time()).encode('utf-8')).hexdigest()
+            self._connection_id = hashlib.md5(
+                '{}{}{}'.format(self.ip, self.port, time.time()).encode('utf-8')
+            ).hexdigest()
         print('ID: ', self._connection_id)
         return self._connection_id
 
     def connection_lost(self, exc):
-        print('Connection Lost')
         if exc:
             self.logger.exception(exc)
-        self.logger.debug('Server closed the connection')
-        del QueueServer.connections[self.connection_id]
+        self.logger.debug(
+            'Server closed the Connection'
+        )
         super().connection_lost(exc=exc)
 
     def connection_made(self, transport):
         print('Connection Made')
         self.transport = transport
         self.peername = transport.get_extra_info('peername')
         self.setup(self.peername)
         print('Connection Accepted')
 
     def data_received(self, data):
-        print('Data Received')
-        print('Data received: {!r}',len(data))
-        self.logger.debug('received {!r}'.format(data))
-        self.transport.write(data)
-        self.logger.debug('sent {!r}'.format(data))
-
-    def eof_received(self):
-        self.logger.debug('received EOF')
-        if self.transport.can_write_eof():
-            self.transport.write_eof()
+        pass
+
+    # def eof_received(self):
+    #     self.logger.debug('received EOF')
+    #     if self.transport.can_write_eof():
+    #         self.transport.write_eof()
```

## qw/wrappers/di_task.py

```diff
@@ -81,22 +81,22 @@
         result = None
         try:
             # first: we create the task
             await self.create()
             result = await self.run()
             try:
                 stats = self._task.stats.stats
-            except Exception: # pylint: disable=W0703
+            except Exception:  # pylint: disable=W0703
                 stats = None
             result = {
                 "result": result,
                 "stats": stats
             }
             return result
-        except Exception as err: # pylint: disable=W0703
+        except Exception as err:  # pylint: disable=W0703
             return TaskFailed(str(err))
         finally:
             await self.close()
 
     async def run(self):
         """ Running the Task in the loop."""
         result = None
@@ -122,12 +122,12 @@
         return result
 
     async def close(self):
         try:
             if self._task:
                 await self._task.close()
                 self._task = None
-        except Exception as err: # pylint: disable=W0703
+        except Exception as err:  # pylint: disable=W0703
             logging.error(err)
 
     def __str__(self):
         return f"{self.program}.{self.task}"
```

## Comparing `qworker-1.6.6.dist-info/METADATA` & `qworker-1.7.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 Metadata-Version: 2.1
 Name: qworker
-Version: 1.6.6
-Summary: QueueWorker is asynchronous Task Queue implementation built on to of Asyncio.Can you spawn distributed workers to run functions inside workers.
+Version: 1.7.0
+Summary: QueueWorker is asynchronous Task Queue implementation built on top of Asyncio.Can you spawn distributed workers to run functions inside workers.
 Home-page: https://github.com/phenobarbital/qworker
 Author: Jesus Lara
 Author-email: jesuslara@phenobarbital.info
 License: MIT
 Project-URL: Source, https://github.com/phenobarbital/qworker
 Project-URL: Funding, https://paypal.me/phenobarbital
 Project-URL: Say Thanks!, https://saythanks.io/to/phenobarbital
```

## Comparing `qworker-1.6.6.dist-info/RECORD` & `qworker-1.7.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-qworker-1.6.6.dist-info/METADATA,sha256=PoGavd5xw5zFIT_bnv_hAj_E_fpeiCAB8HSSOfZZIro,3169
-qworker-1.6.6.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
-qworker-1.6.6.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
-qworker-1.6.6.dist-info/RECORD,,
-qworker-1.6.6.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
-qworker-1.6.6.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
-qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=31p1G11akVAHtoIku71L-pdOUKnmz0rZrK3glmOCQ9A,568544
-qw/__main__.py,sha256=99r7jORLEJXqQ0rACf7DMcB-TctLN1cYDCC4WNDV8f0,2028
-qw/discovery.py,sha256=Omo23fJXPwURwQ1xKZU74CAhb4CzB5-FzPt64wAVB7A,4064
-qw/conf.py,sha256=1T73K08uqrAQ9A-YpgPR4Sx9ePbfWYWUSX6a5sgLB34,2172
-qw/__init__.py,sha256=awMNjg7WGznbrNuJLoEJV6BbnREk4BPD8k9My8BD7Uo,137
+qworker-1.7.0.dist-info/RECORD,,
+qworker-1.7.0.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
+qworker-1.7.0.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
+qworker-1.7.0.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
+qworker-1.7.0.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
+qworker-1.7.0.dist-info/METADATA,sha256=rsm53FlETmXKQXhiBz80DoyxLZp9080HbuIYiI3Isq0,3170
+qw/discovery.py,sha256=l_Lb3Bmni6WTTu5fxzj4-9KquiRak1rq8k9I70f_hSI,2160
+qw/client.py,sha256=z36wNtSCVac5Q2G5luu1IlBPlm6lMLml1NPdRbLRHMw,15257
+qw/conf.py,sha256=Jbim5sPoYxQ2jv7xU4jxVfiGZTibxa5dINlvahaB7cA,2370
+qw/__main__.py,sha256=BVERFBdMVjDqDQaWPlOFM4cLy_5qm61kLKExJcsWmFk,2388
 qw/decorators.py,sha256=m53pMJcotaf6XLJXbaSO6ywIgN8TWvbk__DbbsLWvkE,379
-qw/process.py,sha256=ASp3JJHk4T11zV6XUwqaP3smtxKCMR-qfpzhJqdTK-8,5282
-qw/version.py,sha256=7gJEx1VuYCWyyKDEaRQt-XD9KonB1-4OLyzZq6g8V5Y,593
-qw/client.py,sha256=-3Hivp6t58v9nNS5xF-tbh7qmVG4fLLVS9xVJORZxxE,15504
-qw/server.py,sha256=EItLmestuH2OeXHVpBuFAs19LJBfjAXfAy0PyZzixFc,19834
-qw/protocols.py,sha256=NMBQ1X-yOKfHqj0PALilWbDRXVoN_gsCVoMoyeYkps0,1732
-qw/utils/versions.py,sha256=d8AdLmhM1bPc82vTAshCJBljGr-Ur3_hiZ_GtTbbORA,597
+qw/server.py,sha256=CEszx1Rfrld-1NJUHT6C2sZQ7sc7ncQa3Fs2lYVHMj4,21403
+qw/version.py,sha256=J4dxUsqcYCh5PG32NyVV3S6H6_hkOlJGRydkI4EFalA,618
+qw/protocols.py,sha256=FkBzIGCRjDUiYPzoUrvIXOHCq_LSplEJXVtLfsn8B04,4129
+qw/process.py,sha256=_2xZ1JoMmdqrFnRzLreNxN3FttSuabeJAWSXwBQfjFE,7462
+qw/__init__.py,sha256=awMNjg7WGznbrNuJLoEJV6BbnREk4BPD8k9My8BD7Uo,137
+qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=31p1G11akVAHtoIku71L-pdOUKnmz0rZrK3glmOCQ9A,568544
 qw/utils/functions.py,sha256=9iXVvYLtQzOK0tRecOV2Oqrl-2raxAHwBCNilLui1xQ,512
 qw/utils/__init__.py,sha256=bYf_I4ymTf8vsqMjK00NJi2-A-lVijPTwN6HDOVjcjQ,46
+qw/utils/versions.py,sha256=d8AdLmhM1bPc82vTAshCJBljGr-Ur3_hiZ_GtTbbORA,597
 qw/utils/json.cpython-39-x86_64-linux-gnu.so,sha256=cL7bP26M2Hib8DD0QMRY-AL6izzzBUWrJxxWHbIEkhA,434800
-qw/wrappers/di_task.py,sha256=sW0t-xBEQcuujI-dc1SLSqM8bqCb9VM_lSLusdOrh10,4089
 qw/wrappers/base.py,sha256=RUpBrj1Zb6Mu1xWdH8npaQBPZzFLOCjuqFN7Ejd870A,895
-qw/wrappers/func.py,sha256=TOSZWL6_eynS0FqdQwSYDu_oW88DLZuj_i6QfWUHEkI,853
 qw/wrappers/__init__.py,sha256=Ot_f0GTaDB50Za4Hxsz2FZSkZDn4zZoDHjXOvqj9T9k,320
+qw/wrappers/di_task.py,sha256=8OLXojOJJ5Y0b77rKm7jllI2lQTWsulgPm2yUkuqKOA,4092
+qw/wrappers/func.py,sha256=TOSZWL6_eynS0FqdQwSYDu_oW88DLZuj_i6QfWUHEkI,853
```

## Comparing `qworker-1.6.6.dist-info/LICENSE` & `qworker-1.7.0.dist-info/LICENSE`

 * *Files identical despite different names*

