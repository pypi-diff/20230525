# Comparing `tmp/k1lib-1.3.9.1-py3-none-any.whl.zip` & `tmp/k1lib-1.3.9.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 2546728 bytes, number of entries: 84
+Zip file size: 2545979 bytes, number of entries: 84
 -rw-rw-r--  2.0 unx     1435 b- defN 23-Feb-08 00:13 k1lib/__init__.py
 -rw-rw-r--  2.0 unx    53798 b- defN 23-May-23 10:44 k1lib/_baseClasses.py
 -rw-rw-r--  2.0 unx    13494 b- defN 23-May-05 13:41 k1lib/_basics.py
 -rw-rw-r--  2.0 unx     4908 b- defN 23-May-08 18:53 k1lib/_context.py
 -rw-rw-r--  2.0 unx     2866 b- defN 22-Jul-20 04:30 k1lib/_higher.py
 -rw-rw-r--  2.0 unx      948 b- defN 22-Sep-21 23:54 k1lib/_k1a.py
 -rw-rw-r--  2.0 unx    11646 b- defN 23-Jan-14 15:37 k1lib/_learner.py
 -rw-rw-r--  2.0 unx    21467 b- defN 23-May-16 05:55 k1lib/_monkey.py
 -rw-rw-r--  2.0 unx     3571 b- defN 21-Nov-04 18:35 k1lib/_perlin.py
 -rw-rw-r--  2.0 unx    13377 b- defN 21-Dec-29 02:58 k1lib/eqn.py
 -rw-rw-r--  2.0 unx     6317 b- defN 23-May-07 04:31 k1lib/fmt.py
 -rw-rw-r--  2.0 unx     6398 b- defN 22-Sep-29 08:23 k1lib/graphEqn.py
--rw-rw-r--  2.0 unx     2750 b- defN 23-May-17 03:31 k1lib/imports.py
+-rw-rw-r--  2.0 unx     2750 b- defN 23-May-24 23:19 k1lib/imports.py
 -rw-rw-r--  2.0 unx     3107 b- defN 22-Oct-26 20:43 k1lib/knn.py
 -rw-rw-r--  2.0 unx     3564 b- defN 23-Jan-25 02:48 k1lib/p5.py
 -rw-rw-r--  2.0 unx     8077 b- defN 22-May-06 21:09 k1lib/schedule.py
 -rw-rw-r--  2.0 unx    17767 b- defN 22-Sep-29 07:48 k1lib/selector.py
 -rw-rw-r--  2.0 unx    16718 b- defN 23-May-05 13:50 k1lib/viz.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Aug-11 18:19 k1lib/_hidden/__init__.py
 -rw-rw-r--  2.0 unx       79 b- defN 21-Aug-11 18:19 k1lib/_hidden/hiddenFile.py
@@ -43,27 +43,27 @@
 -rw-rw-r--  2.0 unx     3465 b- defN 22-Nov-27 08:16 k1lib/callbacks/lossFunctions/shorts.py
 -rw-rw-r--  2.0 unx       45 b- defN 21-Aug-11 18:19 k1lib/callbacks/profilers/__init__.py
 -rw-rw-r--  2.0 unx     5054 b- defN 22-May-15 09:01 k1lib/callbacks/profilers/computation.py
 -rw-rw-r--  2.0 unx     2319 b- defN 22-May-15 08:59 k1lib/callbacks/profilers/io.py
 -rw-rw-r--  2.0 unx     4419 b- defN 22-May-15 09:00 k1lib/callbacks/profilers/memory.py
 -rw-rw-r--  2.0 unx     4215 b- defN 22-May-15 09:03 k1lib/callbacks/profilers/time.py
 -rw-rw-r--  2.0 unx      925 b- defN 22-Nov-16 09:22 k1lib/cli/__init__.py
--rw-rw-r--  2.0 unx    20738 b- defN 23-May-24 14:41 k1lib/cli/_applyCl.py
+-rw-rw-r--  2.0 unx    18925 b- defN 23-May-24 23:16 k1lib/cli/_applyCl.py
 -rw-rw-r--  2.0 unx     8308 b- defN 22-Nov-27 07:16 k1lib/cli/bio.py
 -rw-rw-r--  2.0 unx     4033 b- defN 23-Jan-25 02:02 k1lib/cli/cif.py
 -rw-rw-r--  2.0 unx    19321 b- defN 23-May-17 19:01 k1lib/cli/conv.py
 -rw-rw-r--  2.0 unx    24071 b- defN 23-May-17 03:12 k1lib/cli/filt.py
 -rw-rw-r--  2.0 unx     6672 b- defN 23-Jan-25 02:02 k1lib/cli/gb.py
 -rw-rw-r--  2.0 unx     6348 b- defN 23-Apr-05 15:10 k1lib/cli/grep.py
 -rw-rw-r--  2.0 unx    18351 b- defN 23-May-12 23:10 k1lib/cli/init.py
--rw-rw-r--  2.0 unx    32683 b- defN 23-May-23 20:23 k1lib/cli/inp.py
+-rw-rw-r--  2.0 unx    32027 b- defN 23-May-24 23:12 k1lib/cli/inp.py
 -rw-rw-r--  2.0 unx      623 b- defN 22-Jun-22 10:43 k1lib/cli/kcsv.py
 -rw-rw-r--  2.0 unx     4819 b- defN 22-Aug-11 20:44 k1lib/cli/kxml.py
 -rw-rw-r--  2.0 unx     1915 b- defN 21-Nov-12 16:48 k1lib/cli/mgi.py
--rw-rw-r--  2.0 unx    64736 b- defN 23-May-24 13:52 k1lib/cli/modifier.py
+-rw-rw-r--  2.0 unx    64305 b- defN 23-May-24 23:18 k1lib/cli/modifier.py
 -rw-rw-r--  2.0 unx      694 b- defN 22-Nov-27 07:17 k1lib/cli/mol.py
 -rw-rw-r--  2.0 unx     4038 b- defN 23-May-09 15:53 k1lib/cli/nb.py
 -rw-rw-r--  2.0 unx     3530 b- defN 22-Aug-16 15:08 k1lib/cli/optimizations.py
 -rw-rw-r--  2.0 unx    12118 b- defN 23-May-22 17:09 k1lib/cli/output.py
 -rw-rw-r--  2.0 unx     2394 b- defN 23-Jan-25 02:03 k1lib/cli/sam.py
 -rw-rw-r--  2.0 unx    49390 b- defN 23-May-19 19:21 k1lib/cli/structural.py
 -rw-rw-r--  2.0 unx    10399 b- defN 22-Aug-05 01:15 k1lib/cli/trace.py
@@ -71,16 +71,16 @@
 -rw-rw-r--  2.0 unx    21101 b- defN 23-May-17 16:57 k1lib/cli/utils.py
 -rw-rw-r--  2.0 unx       20 b- defN 23-Jan-19 22:00 k1lib/k1ui/__init__.py
 -rw-rw-r--  2.0 unx    61803 b- defN 23-Feb-10 12:10 k1lib/k1ui/main.py
 -rw-rw-r--  2.0 unx       20 b- defN 22-Sep-16 01:12 k1lib/serve/__init__.py
 -rw-rw-r--  2.0 unx    10361 b- defN 23-May-05 16:49 k1lib/serve/main.py
 -rw-rw-r--  2.0 unx      153 b- defN 23-May-05 16:00 k1lib/serve/suffix-dash.py
 -rw-rw-r--  2.0 unx      642 b- defN 23-Feb-13 19:00 k1lib/serve/suffix.py
--rw-rw-r--  2.0 unx  2453826 b- defN 23-Jan-19 22:50 k1lib-1.3.9.1.data/data/k1lib/k1ui/256.model.state_dict.pth
--rw-rw-r--  2.0 unx   304735 b- defN 23-Jan-17 19:16 k1lib-1.3.9.1.data/data/k1lib/k1ui/mouseKey.pth
--rw-rw-r--  2.0 unx    20544 b- defN 23-Mar-19 11:14 k1lib-1.3.9.1.data/data/k1lib/serve/main.html
--rw-rw-r--  2.0 unx     1049 b- defN 23-May-24 14:42 k1lib-1.3.9.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3890 b- defN 23-May-24 14:42 k1lib-1.3.9.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-24 14:42 k1lib-1.3.9.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx        6 b- defN 23-May-24 14:42 k1lib-1.3.9.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6704 b- defN 23-May-24 14:42 k1lib-1.3.9.1.dist-info/RECORD
-84 files, 3527632 bytes uncompressed, 2536388 bytes compressed:  28.1%
+-rw-rw-r--  2.0 unx  2453826 b- defN 23-Jan-19 22:50 k1lib-1.3.9.2.data/data/k1lib/k1ui/256.model.state_dict.pth
+-rw-rw-r--  2.0 unx   304735 b- defN 23-Jan-17 19:16 k1lib-1.3.9.2.data/data/k1lib/k1ui/mouseKey.pth
+-rw-rw-r--  2.0 unx    20544 b- defN 23-Mar-19 11:14 k1lib-1.3.9.2.data/data/k1lib/serve/main.html
+-rw-rw-r--  2.0 unx     1049 b- defN 23-May-24 23:19 k1lib-1.3.9.2.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3890 b- defN 23-May-24 23:19 k1lib-1.3.9.2.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-24 23:19 k1lib-1.3.9.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        6 b- defN 23-May-24 23:19 k1lib-1.3.9.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6704 b- defN 23-May-24 23:19 k1lib-1.3.9.2.dist-info/RECORD
+84 files, 3524732 bytes uncompressed, 2535639 bytes compressed:  28.1%
```

## zipnote {}

```diff
@@ -222,32 +222,32 @@
 
 Filename: k1lib/serve/suffix-dash.py
 Comment: 
 
 Filename: k1lib/serve/suffix.py
 Comment: 
 
-Filename: k1lib-1.3.9.1.data/data/k1lib/k1ui/256.model.state_dict.pth
+Filename: k1lib-1.3.9.2.data/data/k1lib/k1ui/256.model.state_dict.pth
 Comment: 
 
-Filename: k1lib-1.3.9.1.data/data/k1lib/k1ui/mouseKey.pth
+Filename: k1lib-1.3.9.2.data/data/k1lib/k1ui/mouseKey.pth
 Comment: 
 
-Filename: k1lib-1.3.9.1.data/data/k1lib/serve/main.html
+Filename: k1lib-1.3.9.2.data/data/k1lib/serve/main.html
 Comment: 
 
-Filename: k1lib-1.3.9.1.dist-info/LICENSE
+Filename: k1lib-1.3.9.2.dist-info/LICENSE
 Comment: 
 
-Filename: k1lib-1.3.9.1.dist-info/METADATA
+Filename: k1lib-1.3.9.2.dist-info/METADATA
 Comment: 
 
-Filename: k1lib-1.3.9.1.dist-info/WHEEL
+Filename: k1lib-1.3.9.2.dist-info/WHEEL
 Comment: 
 
-Filename: k1lib-1.3.9.1.dist-info/top_level.txt
+Filename: k1lib-1.3.9.2.dist-info/top_level.txt
 Comment: 
 
-Filename: k1lib-1.3.9.1.dist-info/RECORD
+Filename: k1lib-1.3.9.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## k1lib/cli/_applyCl.py

```diff
@@ -87,36 +87,37 @@
 def collapse(it):
     a, b = it | rows(0, -1); c = [a[0], b[1], a[2], b[3]]
     return [] if c[0] == c[1] else [c]
 def traj2(ir, traj): # just looks up the file names really, no processing involved
     idx2FileName = ir | apply(lambda arr: [arr[0], arr[2]]) | toDict()
     a = traj | groupBy(2) | filt(lambda x: len(x) > 1).split() | (apply(collapse)) + iden() | joinStreams(2) | deref()
     return a | lookup(idx2FileName, 2) | deref()
-def moveFile(fileName:str, sourceNodeId:str, destNodeId:str, timeout=60):
+def moveFile(fileName:str, sourceNodeId:str, destNodeId:str, timeout=60): # old, slow, corrupted version
     """Moves file from the current node to the destination node. Usually executed on other nodes than the driver node"""
     fn = os.path.expanduser(fileName); dirname = os.path.dirname(fn)
     applyCl.cmd(f"mkdir -p {dirname}", nodeIds=[destNodeId]); applyCl.cmd(f"rm -f {fn}", nodeIds=[destNodeId])
     for chunk in cat(fn, False, True): [destNodeId] | applyCl.aS(lambda: chunk >> file(fn), timeout=timeout) | deref()
-    None | cmd(f"rm {fn}") | deref()
+    None | cmd(f"rm {fn}") | deref(); return "ok1"
 def moveFile(fn:str, sourceN:str, destN:str, timeout=60): # runs on dest node
     fn = os.path.expanduser(fn); dirname = os.path.dirname(fn)
     windows = [sourceN] | applyCl.aS(lambda: range(os.path.getsize(fn)) | batched(settings.cli.cat.chunkSize, True) | apply("[x.start, x.stop]") | deref()) | cut(1) | item() | deref()
-    for chunk in [[sourceN]*len(windows), windows] | transpose() | ~applyCl(lambda sB,eB: cat(fn, False, sB=sB, eB=eB), pre=True, prefetch=20): chunk >> file(fn)
+    for chunk in [[sourceN]*len(windows), windows] | transpose() | ~applyCl(lambda sB,eB: cat(fn, False, sB=sB, eB=eB), pre=True, prefetch=20) | cut(1): chunk >> file(fn)
     [sourceN] | applyCl.aS(lambda: None | cmd(f"rm {fn}") | deref()) | deref()
+    return "ok file"
 def moveFF(ff:str, sourceN:str, destN:str, timeout=60): # runs on dest node
     """Moves file or folder from the current node to the destination node"""
-    #print(f"current node: {applyCl.nodeId()}")
-    if os.path.isfile(ff): return moveFile(ff, sourceN, destN, timeout)
-    ff | getFilesInFolder | apply(aS(moveFile, sourceN, destN, timeout)) | deref()
-    None | cmd(f"rm -rf {ff}") | ignore()
+    ff = os.path.expanduser(ff); isfile = [sourceN] | applyCl.aS(lambda: os.path.isfile(ff)) | cut(1) | item()
+    if isfile: return moveFile(ff, sourceN, destN, timeout)
+    [sourceN] | applyCl.aS(lambda: ff | getFilesInFolder | deref()) | cut(1) | item() | apply(aS(moveFile, sourceN, destN, timeout)) | deref()
+    None | cmd(f"rm -rf {ff}") | ignore(); return f"ok folder: {ff}"
 def moveAll(tr, timeout=60):
     groups = tr | groupBy(1) | deref() # grouping by destination
     with ray.progress(len(groups), "Moving files around") as rp:
         def process(idx, arrs): # processing requests for dest node
-            for i, arr in enumerate(arrs):
+            for i, arr in enumerate(arrs): # moveFF(fn, a, b)
                 [[arr[1], arr]] | ~applyCl(lambda a,b,fn,sc: moveFF(fn, a, b), pre=True, timeout=timeout) | deref() # move 1 file on dest node
                 rp.update.remote(idx, (i+1)/len(arrs))
         groups | insertIdColumn() | ~applyTh(process, timeout=None) | deref()
     # tr | apply(lambda arr: [arr[0], arr]) | ~applyCl(lambda a,b,fn,sc: moveFF(fn, b), pre=True, timeout=timeout) | deref() # old version without progress bar
 def balanceFolder(base, audit=False, maxSteps=1000, timeout=60): # currently executing each move step serially, will change in the future if it's too slow
     loadTestGuard(); applyCl.cmd(f"mkdir -p {base}"); ir = getIr(base)
     tr = traj2(ir, traj(ir, maxSteps, tuple(applyCl.nodeIds()))[1]); return tr if audit else moveAll(tr)
@@ -127,45 +128,14 @@
     return irs, auxs
 def decommissionFolder(base:str, nAs:List[str], audit=False, maxSteps=1000, timeout=60):
     loadTestGuard(); irs, auxs = decommissionFolderTraj(base, nAs)
     irs2, auxs2 = traj(irs[-1] if len(irs) > 0 else getIr(base), maxSteps, applyCl.nodeIds() | ~inSet(nAs) | aS(tuple))
     irs = [*irs, *irs2]; auxs = [*auxs, *auxs2]
     if len(irs) == 0: return
     tr = traj2(irs[-1], auxs); return tr if audit else moveAll(tr)
-def getSize(url):
-    for i in range(10):
-        try: return requests.head(url, timeout=3).headers.items() | apply(op().lower(), 0) | toDict() | op()["content-length"].ab_int()
-        except Exception as e:
-            if i == 9: raise Exception(f"Can't get size of file")
-class NoPartialContent(Exception): pass
-def getChunk(url:str, sB:int, eB:int, timeout:float) -> bytes:
-    for i in range(10):
-        try: res = requests.get(url, headers={"Range": f"bytes={sB}-{eB}"}, timeout=timeout)
-        except Exception as e:
-            if i == 9: raise Exception(f"Can't get file chunk")
-            continue
-        if res.status_code != 206: raise NoPartialContent("Server doesn't allow partial downloads at this particular url")
-        return res.content
-def getChunks(url:str, sB:int, eB:int, chunkSize=None, chunkTimeout:float=10) -> List[bytes]:
-    """Grabs bytes from sB to eB in chunks"""
-    chunkSize = chunkSize or settings.cli.cat.chunkSize
-    return range(sB, eB+1) | batched(chunkSize, True) | apply(lambda r: getChunk(url, r.start, r.stop-1, chunkTimeout))
-def download(url:str, folder:str, merge:bool=False, timeout=120, chunkTimeout=5):
-    getChunk(url, 0, 1, 10) # try to see if server accepts partial downloads first
-    folder = os.path.expanduser(folder); dirname = os.path.dirname(folder)
-    if merge: destFile = folder; folder = b"" | file(); None | cmd(f"rm -rf {folder}") | ignore(); None | cmd(f"mkdir -p {folder}") | ignore()
-    applyCl.cmd(f"rm -rf {folder}"); applyCl.cmd(f"mkdir -p {folder}"); size = getSize(url)
-    cpus = loadTestGuard().items() | deref(); n = cpus | cut(1) | toSum()
-    tasks = [cpus | ~apply(lambda x,y: [x]*y) | joinStreams(), range(size) | splitW(*[1]*n)] | transpose() | insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[y,f"{folder}/{z}.bin"]]) | deref(1)
-    tasks | ~applyCl(lambda r, fn: getChunks(url, r.start, r.stop-1, None, chunkTimeout) | file(fn), pre=True, timeout=timeout) | deref()
-    if merge:
-        None | cmd(f"rm -rf {destFile}") | deref()
-        None | cmd(f"mkdir -p {dirname}") | deref()
-        None | applyCl.aS(lambda: ls(folder)) | ungroup() | deref() | sortF(op().split(".bin")[0].split("/")[-1].ab_int(), 1) | applyCl(cat(text=False), pre=True) | cut(1) | file(destFile)
-        None | cmd(f"rm -rf {folder}") | deref()
 def a_transfer(fn, nse, nodeB, rpF:callable=iden()):
     """Transfers a lot of blocks from a bunch of nodes to nodeB. Does not delete from those node though
 
 nse = List[nodeAId, [sB, eB]]
 
 Runs on driver process, blocks, so better use applyTh outside of this
 
@@ -173,15 +143,15 @@
     blockSize = settings.cli.cat.chunkSize
     def inner():
         totalBytes = nse | cut(1) | ~apply(lambda x,y:y-x) | toSum(); currentByte = 0
         for chunk in nse | ~apply(lambda x, y: range(x, y) | batched(blockSize, True) | apply("[x.start, x.stop]"), 1) | ungroup() | deref()\
             | ~applyCl(lambda sB, eB: cat(fn, False, sB=sB, eB=eB), pre=True, timeout=None, prefetch=20) | cut(1):
             chunk >> file(fn); currentByte += len(chunk); rpF(currentByte/totalBytes)
     [nodeB] | applyCl.aS(inner, timeout=None) | item()
-def decommission(fn:str, nodeAs:List[str], nodeBs:List[str], rS=iden()):
+def decommission(fn:str, nodeAs:List[str], nodeBs:List[str], rS):
     """Spreads out a particular file in nodeAs to all nodeBs, to prepare
 to decomission nodeAs. The 2 sets should be mutually exclusive
 
 :param rS: instance of refineSeek"""
     nodeAs, nodeBs = [nodeAs, nodeBs] | deref()
     if len(nodeAs) == 0: return
     if len(nodeBs) == 0: raise Exception("Unsupported configuration! Trying to move data from A+B to C+D. Has to have some shared nodes, like moving data from A+B+C to B+C+D. This is not a fundamental limitation, but just can't be done with the current architecture. Might be fixed in the future.")
@@ -192,38 +162,39 @@
     a = nodeAs | applyCl.aS(lambda: fn | splitSeek(ws=ws) | rS | window(2) | deref() | insertColumn(nodeBs) | insert(applyCl.nodeId()).all() | deref()) | cut(1) | joinStreams() | deref()
     # actually transferring chunks
     with ray.progress(a | groupBy(1) | shape(0), "Decommissioning") as rp:
         c = b = a | groupBy(1, True) | apply(iden() + apply(lambda arr: [arr[0], arr[1:]]) | reverse() | insert(fn)) | deref()
         enumerate(c) | applyTh(~aS(lambda idx, e: a_transfer(*e, rpF=aS(lambda p: ray.get(rp.update.remote(idx, p))))), timeout=None) | deref()
     # deleting files from nodeAs
     nodeAs | applyCl.aS(lambda: None | cmd(f"rm -rf {fn}") | ignore()) | deref()
-def spreadOut(fn:str, nAs:List[str], nBs:List[str], rS=iden()):
+ranges2Seeks = apply("[x.start, x.stop]") | joinStreams() | aS(set) | sort(None) | deref()
+def spreadOut(fn:str, nAs:List[str], nBs:List[str], rS):
     """Spreads out a file from nodes A to B, where B fully contains A (no decomissioning).
 A and B should be mutually exclusive. Initial nodes are A, final nodes are A + B"""
     nAs, nBs = [nAs, nBs] | deref(); rS.fn = fn
     if len(nBs) == 0: return # no need to spread out
     nBs | applyCl.aS(lambda: None | cmd(f"mkdir -p {os.path.dirname(fn)}") | deref(), timeout=None) | deref()
     nBs | applyCl.aS(lambda: None | cmd(f"rm -rf {fn}") | deref(), timeout=None) | deref()
     # some initial metadata
     nodeIds = applyCl.nodeIds(); nodeId_cpu = loadTestGuard(False).items() | deref(); nodeId2Cpu = nodeId_cpu | toDict()
     sizes = nAs | applyCl.aS(lambda: os.path.getsize(fn) if os.path.exists(fn) else 0) | deref(); totalSize = sizes | cut(1) | toSum()
     ns = [*nAs, *nBs]; totalCpu = ns | lookup(nodeId2Cpu) | toSum(); bytePerCpu = totalSize/totalCpu; wsB = nBs | lookup(nodeId2Cpu) | deref()
     # prepares segments and metadata, List[nodeId, [sB, eB]], where sB and eB are the ranges of nAs that they're willing to share
-    sizePost = sizes | ~apply(lambda idx, size: [idx, nodeId2Cpu[idx]/totalCpu*totalSize/size]) | deref()
+    sizePost = sizes | ~apply(lambda idx, size: [idx, nodeId2Cpu[idx]/totalCpu*totalSize/size]) | deref() # size fraction to retain
     invalidNodes = sizePost | ~filt(lambda x: 0 <= x <= 1, 1) | cut(0) | deref()
     if len(invalidNodes) > 0: raise Exception(f"Unsupported configuration! These nodes have too little data to share: {invalidNodes}. This couldn't have happen using applyCl alone. Data is not corrupted, but you'll have to combine data from all files into 1 and spread them back out again.")
-    inter = sizePost | ~apply(lambda idx, x: [idx, [x, 1-x]]) | applyCl(lambda ws: fn | splitSeek(ws=ws) | rS | ~head(1), pre=True) | deref() | filt(~aS(lambda x,y: y-x>0), 1) | deref()
+    inter = sizePost | ~apply(lambda idx, x: [idx, [x, 1-x]]) | applyCl(lambda ws: fn | splitSeek(ws=ws) | rS | ~head(1), pre=True) | deref() | filt(~aS(lambda x,y: y-x>0), 1) | deref() # filter at the end to eliminate files that don't want to share anything (x == y)
     # actually transferring data to new nodes
-    meta = inter | apply(~aS(range) | splitW(*wsB) | rS | apply(wrapList()) | insertColumn(nBs) | deref(1), 1) | ungroup(False) | apply("[x.start, x.stop]", 2) | groupBy(1, True) | deref()
+    meta = inter | apply(~aS(range) | splitW(*wsB) | ranges2Seeks | apply(lambda x: splitSeek.backward(fn, x)) | deref() | rS | window(2) | deref() | apply(wrapList()) | insertColumn(nBs), 1) | ungroup(False) | groupBy(1, True) | deref()
     with ray.progress(len(meta), "Transferring data to new nodes") as rp:
         meta | insertIdColumn(True) | applyTh(~aS(lambda idx, nB, nse: a_transfer(fn, nse, nB, rpF=aS(lambda p: ray.get(rp.update.remote(idx, p))))), timeout=None) | deref()
     # truncates the files in nAs nodes
     inter | ~apply(lambda idx,se: [idx,se[0]]) | applyCl(lambda sB: open(fn, 'a').truncate(sB), pre=True, timeout=None) | deref()
-def balanceFile(fn:str, nAs:List[str]=None, nBs:List[str]=None, rS=iden()):
-    loadTestGuard(); fn = os.path.expanduser(fn)
+def balanceFile(fn:str, nAs:List[str]=None, nBs:List[str]=None, rS=None):
+    fn = os.path.expanduser(fn); rS = rS or refineSeek(); rS.injectFn(fn); loadTestGuard()
     if nAs is None: nAs = None | applyCl.aS(lambda: os.path.exists(fn)) | filt(op(), 1) | cut(0) | deref()
     if nBs is None: nBs = applyCl.nodeIds()
     decommission(fn, *nAs | inSet(nBs).split() | reverse(), rS)
     spreadOut(fn, *nBs | inSet(nAs).split(), rS)
 def diskScan1(base:str) -> List[str]: # like ls(), but returns files and folders that appear at least on 2 nodes
     isdir, base = base.split("\ue000")
     if not isdir: return []
```

## k1lib/cli/inp.py

```diff
@@ -5,18 +5,18 @@
 from collections import deque
 from k1lib.cli import BaseCli; import k1lib.cli as cli
 from k1lib.cli.typehint import *
 from contextlib import contextmanager
 requests = k1lib.dep("requests")
 try: import minio; hasMinio = True
 except: hasMinio = False
-__all__ = ["cat", "splitSeek", "refineSeek", "curl", "wget", "ls", "cmd", "walk", "requireCli", "urlPath"]
+__all__ = ["cat", "splitSeek", "refineSeek", "wget", "ls", "cmd", "walk", "requireCli", "urlPath"]
 settings = k1lib.settings.cli
 class NoPartialContent(Exception): pass
-def getChunk(url:str, sB:int, eB:int, timeout:float) -> bytes: # start amd end inclusive
+def getChunk(url:str, sB:int, eB:int, timeout:float) -> bytes: # start and end inclusive!!! Normally for everything else, it's start inclusive and end exclusive
     for i in range(10):
         try: res = requests.get(url, headers={"Range": f"bytes={sB}-{eB}"}, timeout=timeout)
         except Exception as e:
             if i == 9: raise Exception(f"Can't get file chunk")
             continue
         if res.status_code != 206: raise NoPartialContent(f"Server doesn't allow partial downloads at this particular url. Status code: {res.status_code}")
         return res.content
@@ -28,35 +28,39 @@
 catSettings.add("every", k1lib.Settings().add("text", 1000, "for text mode, will print every n lines").add("binary", 10, "for binary mode, will print every n 100000-byte blocks"), "profiler print frequency")
 settings.add("cat", catSettings, "inp.cat() settings")
 
 rfS = k1lib.Settings()
 settings.add("RemoteFile", rfS, "inp.RemoteFile() settings, used in cat(), splitSeek() and the like")
 rfS.add("memoryLimit", 100_000_000, "if the internal cache exceeds this limit (in bytes), and randomAccess is False, then old downloaded chunks will be deleted")
 def noPartial(url):
-    try: return len(getChunk(url, 0, 10, 10)) != 10
+    try: return len(getChunk(url, 0, 10, 10)) != 11
     except NoPartialContent: return True
 class RemoteFile:
     def __init__(self, url, randomAccess=True, blockSize=None, noPartialConfirm=False):
         """
 :param url: url of the remote file
 :param randomAccess: is random accessing parts of the file expected? If
     True, then keeps all of the reads in ram internally, else free them
     as soon as possible
 :param blockSize: all reads will fetch roughly this amount of bytes"""
         self.url = url; self.randomAccess = randomAccess; self.blockSize = blockSize or settings.cat.chunkSize
         self.noPartialConfirm = noPartialConfirm; self.size = None; self.domain = k1lib.Domain()
         self.seekPos = 0; self.reads = deque() # List[sB, eB, content]
         self._totalReadSize = 0; self.noPartial = noPartial(url)
+        self._confirmMsgShown = False
     def _fetch(self, sB:int, eB:int): # fetches from start to end byte and dumps to internal memory. Inclusive start byte, exclusive end byte
-        if not noPartial:
+        if not self.noPartial:
             eB = max(eB, min(sB+self.blockSize, len(self)))
             chunk = getChunk(self.url, sB, eB, 10)
         else:
-            if self.noPartialConfirm:
-                ans = input(f"Remote file '{self.url}' don't support partial downloads. Therefore the entire file will be loaded into RAM, which is may be undesireable. Do you want to continue? Y/n: ")
+            if self.noPartialConfirm and not self._confirmMsgShown:
+                ans = input(f"""Remote file '{self.url}' don't support partial downloads.
+Therefore the entire file will be loaded into RAM, which
+could be undesireable. Do you want to continue? Y/n: """)
+                self._confirmMsgShown = True
                 if ans.lower()[0] != "y": self.reads.append([0, 0, b""]); return
             sB = 0; chunk = requests.get(self.url).content; eB = len(chunk)
         self.reads.append([sB, eB, chunk])
         self._totalReadSize += len(chunk); self.domain = self.domain + k1lib.Domain([sB, eB])
         if not self.randomAccess and self._totalReadSize > rfS.memoryLimit: # deletes old reads
             sB, eB, chunk = self.reads.popleft()
             self._totalReadSize -= len(chunk)
@@ -141,17 +145,16 @@
 def wrap(fn, b): return b if b >= 0 else b + fileLength(fn) + 1
 class _cat(BaseCli):
     def __init__(self, text, chunks, sB, eB): self.text = text; self.chunks = chunks; self.sB = sB; self.eB = eB
     def _typehint(self, ignored=None):
         if self.text: return tIter(str) if self.chunks else tList(str)
         else: return tIter(bytes) if self.chunks else bytes
     def __ror__(self, fn:str) -> Union[Iterator[str], bytes]:
-        text = self.text; chunks = self.chunks; sB = self.sB; eB = self.eB
-        fn = os.path.expanduser(fn)
-        if text and chunks and k1lib._settings.packages.k1a:
+        text = self.text; chunks = self.chunks; sB = self.sB; eB = self.eB; fn = os.path.expanduser(fn)
+        if text and chunks and k1lib._settings.packages.k1a and os.path.exists(fn):
             return k1lib._k1a.k1a.StrIterCat(fn, sB, eB)
         if chunks: return _catGenText(fn, sB, eB) if text else _catGenBin(fn, sB, eB)
         sB = wrap(fn, sB); eB = wrap(fn, eB)
         if text:
             with openFile(fn, True) as f: f.seek(sB); return f.read(eB-sB).splitlines()
         else:
             with openFile(fn, False) as f: f.seek(sB); return f.read(eB-sB)
@@ -379,31 +382,17 @@
         else: self.res = func(fn); return self
     def __or__(self, aft):
         if self.res is None: return super().__or__(aft)
         return aft.__ror__(self)
     def __getitem__(self, idx): return self.res[idx]
     def __len__(self): return len(self.res)
     def __iter__(self): return iter(self.res)
-    @staticmethod
-    def window():
-        """Converts input boundaries into segment windows.
-Example::
-
-    # returns [[0, 4], [5, 9], [10, 20]]
-    [0, 5, 10, 20] | splitSeek.window()
-    # example use case
-    "abc.txt" | splitSeek(10) | splitSeek.window()
-"""
-        def inner(it):
-            it = it | cli.aS(list); ranges = it | cli.window(2) | cli.apply(lambda x: x-1, 1) | cli.deref()
-            ranges[-1][1] += 1; return ranges
-        return cli.aS(inner)
     def __repr__(self): return self.res.__repr__()
 class refineSeek(BaseCli):
-    def __init__(self, f, window=1):
+    def __init__(self, f=None, window=1):
         """Refines seek positions.
 Example::
 
     # returns list of integers for seek positions
     "abc.txt" | splitSeek(30)
     # returns refined seek positions, such that the line starting at the seek positions starts with "@"
     "abc.txt" | splitSeek(30) | refineSeek(lambda x: x.startswith(b"@"))
@@ -415,14 +404,15 @@
     "abc.txt" | splitSeek(30) | refineSeek.fastq()
 
 :param f: function that returns True if the current line/lines is a valid block boundary
 :param window: by default (1), will fetch 1 line and check boundary using ``f(line)``.
     If a value greater than 1 is passed (for example, 3), will fetch 3 lines and check
     boundary using ``f([line1, line2, line3])``
 """
+        if f is None: f = lambda x: True
         self.f = cli.fastF(f); self.window = window; self.fn = None
     def __ror__(self, seeks):
         f = self.f; window = self.window
         def read(fio, sB:int):
             fio.seek(sB)
             if window == 1: return fio.readline()
             return list(cli.repeatF(lambda: fio.readline(), window))
@@ -434,35 +424,21 @@
                     line = read(fio, seek)
                     while not f(line): seek = splitSeek.forward(fio, seek); line = read(fio, seek)
                     newSeeks.append(seek)
             newSeeks.append(seeks[-1]); return newSeeks
         if isinstance(fn, str):
             with openFile(fn, False) as fio: return process(fio)
         else: return process(fn)
+    def injectFn(self, fn):
+        """Injects file name dependency, if this is not used right after :class:`splitSeek`"""
+        self.fn = fn; return self
     @classmethod
     def fastq(cls):
         """Refine fastq file's seek points"""
         return cls(lambda x: x[0][0] == b"@"[0] and x[2][0] == b"+"[0], 3)
-def curl(url:str, tries=3) -> Iterator[str]:
-    """Gets file from url. File can't be a binary blob.
-Example::
-
-    # prints out first 10 lines of the website
-    curl("https://k1lib.github.io/") | headOut()
-
-:param url: the url to get the contents of
-:param tries: how many times to retry the request"""
-    for i in range(tries):
-        try:
-            for line in urllib.request.urlopen(url):
-                line = line.decode()
-                if line[-1] == "\n": yield line[:-1]
-                else: yield line
-        except Exception as e:
-            if i == tries-1: raise e
 def wget(url:str, fileName:str=None, mkdir=True):
     """Downloads a file. Also returns the file name, in case you want to pipe it
 to something else.
 
 :param url: The url of the file
 :param fileName: if None, then tries to infer it from the url
 :param mkdir: whether to make the directory leading up to the file or not"""
```

## k1lib/cli/modifier.py

```diff
@@ -413,14 +413,18 @@
     k1lib.settings.startup.init_ray = False
     from k1lib.imports import *
 
 As with :class:`applyMp`, there are pitfalls and weird quirks to multiprocessing,
 on 1 or multiple nodes, so check out the docs over there to be aware of them,
 as those translates well to here.
 
+There're more extensive documentation on these notebooks: `27-multi-node <https://mlexps.com/other/27-multi-node/>`_,
+`30-applyCl-benchmarks <https://mlexps.com/other/30-applyCl-benchmarks/>`_, if you want to kinda get the feel of this
+tool more.
+
 .. admonition:: Advanced use case
 
     Not really advanced, but just a bit difficult to understand/follow. Let's say
     that you want to scan through the home directory of all nodes, grab all files,
     read them, and get the number of bytes they have. You can do something like this::
     
         a = None | applyCl.aS(lambda: None | cmd("ls ~") | filt(os.path.isfile) | deref()) | deref()
@@ -492,15 +496,16 @@
         @ray.remote
         def resolveFRemote(o): return 1
         def resolveF(e):
             if resolve:
                 if pre: return [e[0], ray.get(e[1], timeout=timeout)]
                 else: return ray.get(e, timeout=timeout)
             else: # don't resolve to this node, but still block execution until that object is resolvable
-                ray.get(resolveFRemote.remote(e[1] if pre else e), timeout=timeout); return e
+                f = specificNode(resolveFRemote, e[0]) if pre else resolveFRemote # if node ids are available (pre=True), then resolves to that specific node only, else do generic resolve
+                ray.get(f.remote(e[1] if pre else e), timeout=timeout); return e
         self.preprocessF = preprocessF; self.resolveF = resolveF
     def __ror__(self, it):
         f = self.f; timeout = self.timeout; bs = self.bs; ogF = self.ogF; preprocessF = self.preprocessF; resolveF = self.resolveF
         if bs > 1: return it | cli.batched(bs, True) | applyCl(lambda x: x | apply(ogF) | cli.aS(list), self.prefetch, timeout) | cli.joinStreams()
         def gen(it):
             futures = deque(); it = iter(it)
             for i, e in zip(range(self.prefetch), it): futures.append(preprocessF(f, e))
@@ -690,22 +695,22 @@
     what nodes the file exists in
 :param nBs: node ids that will store the file after balancing everything out. If not
     specified, will take all available nodes
 :param rS: :class:`~k1lib.cli.inp.refineSeek` instance, if you need more fine-grained
     control over section boundaries so as to not make everything corrupted
 """
         from k1lib.cli._applyCl import balanceFile
-        balanceFile(fn, nAs, nBs, rS or cli.iden())
+        balanceFile(fn, nAs, nBs, rS)
     @staticmethod
     def decommissionFile(fn, nAs:List[str], rS=None):
         """Convenience function for :meth:`balanceFile`. See docs over there."""
         from k1lib.cli._applyCl import balanceFile
-        balanceFile(fn, None, applyCl.nodeIds() | ~cli.inSet(nAs) | cli.deref(), rS or cli.iden())
+        balanceFile(fn, None, applyCl.nodeIds() | ~cli.inSet(nAs) | cli.deref(), rS)
     @staticmethod
-    def cat(fn:str=None, f:Callable=None, nodeIds=None, timeout:float=60, keepNodeIds:bool=False, multiplier:int=1, includeId:bool=False, resolve:bool=True):
+    def cat(fn:str=None, f:Callable=None, nodeIds=None, timeout:float=60, pre:bool=False, multiplier:int=1, includeId:bool=False, resolve:bool=True):
         """Reads a file distributedly, does some operation on them, collects and
 returns all of the data together. Example::
 
     fn = "~/repos/labs/k1lib/k1lib/cli/test/applyCl.cat.data"
     ("0123456789"*5 + "\\n") * 1000 | file(fn)
     applyCl.splitFile(fn)
     applyCl.cat(fn, shape(0), keepNodeIds=True) | deref()
@@ -760,32 +765,34 @@
     exploratory analysis on a single remote file. To be efficient at bulk processing,
     use the normal mode instead.
 
 :param fn: file name
 :param f: function to execute in every process
 :param nodeIds: only read file from these nodes
 :param timeout: kills the processes if it takes longer than this amount of seconds
-:param keepNodeIds: whether to keep the node id column or not
+:param pre: "preserve" mode, just like in :class:`applyCl`. Whether to keep the node id column or not
 :param multiplier: by default, each node will spawn as many process as there
     are cpus. Sometimes you want to spawn more process, change this to a higher number
 :param includeId: includes a unique id for this process (just normal integers from 0 to n)
 :param resolve: whether to resolve the remote objects or not
 """
+        fn = os.path.expanduser(fn)
         if f is None: # simple case
             def inner(nodeId_fn:Tuple[str, str]):
                 nodeId, fn = nodeId_fn; seeks = [nodeId] | applyCl.aS(lambda: fn | cli.splitSeek(round(os.path.getsize(fn)/settings.cat.chunkSize+1))) | cli.cut(1) | cli.item() | cli.deref()
                 inter = seeks | cli.window(2) | apply(cli.wrapList() | cli.insert(nodeId)) | cli.deref()
                 return inter | ~applyCl(lambda sB, eB: cli.cat(fn,sB=sB,eB=eB) | cli.deref(), pre=True) | cli.cut(1) | cli.joinStreams()
                 # return [nodeId_fn] | applyCl(cat() | deref(), pre=True) | cut(1) | item() # direct, no chunking method
             if fn is None: return aS(inner) # [nodeId, fn] | applyCl.cat()
             if isinstance(fn, str): return aS(lambda nodeId: inner([nodeId, fn])) # nodeId | applyCl.cat()
             else: return inner(fn) # applyCl.cat([nodeId, fn])
         postprocess = cli.insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[*y,z]])
+        nodeIds = nodeIds or (applyCl.nodeIds() | applyCl.aS(lambda: os.path.exists(fn)) | cli.filt(cli.op(), 1) | cli.cut(0) | cli.deref())
         checkpoints = nodeIds | applyCl.aS(lambda: fn | cli.splitSeek(int(applyCl.meta()["Resources"]["CPU"]*multiplier)) | cli.window(2) | cli.deref()) | cli.ungroup() | postprocess | cli.deref()
-        postprocess = cli.iden() if keepNodeIds else cli.cut(1)
+        postprocess = cli.iden() if pre else cli.cut(1)
         a = checkpoints | applyCl(~aS(lambda x,y,idx: cli.cat(fn, sB=x, eB=y) | ((cli.wrapList() | cli.insert(idx)) if includeId else cli.iden()) | f), pre=True, timeout=timeout, num_cpus=1, resolve=resolve)
         return a | postprocess
     @staticmethod
     def balanceFolder(folder:str, maxSteps:int=None, audit:bool=False):
         """Balances all files within a folder across all nodes.
 Example::
 
@@ -822,32 +829,14 @@
 
 :param nAs: list of node ids to migrate files away from
 :param maxSteps: limits the total number of optimization steps. Normally don't have to specify,
     but just here in case it runs for too long trying to optimize the folder structure
 :param audit: if True, just returns the file movements it's planning to do"""
         from k1lib.cli._applyCl import decommissionFolder
         return decommissionFolder(folder, nAs, audit=audit, maxSteps=maxSteps, timeout=timeout)
-    def download(url:str, folder:str, merge:bool=False, timeout=120, chunkTimeout=5):
-        """Downloads a file distributedly to a specified folder.
-Example::
-
-    url = "https://vim.kelvinho.org"
-    fn = "~/repos/labs/k1lib/k1lib/cli/test/applyCl.download" # file/folder name
-    applyCl.download(url, fn)       # will download distributedly and dump file fragments into the folder fn
-    applyCl.download(url, fn, True) # same as above, but collects all fragments together, places it in fn in the current node, then deletes the temporary file fragments
-
-This only works if the server allows partial downloads btw.
-
-:param url: url to download
-:param folder: which folder to download parts into
-:param merge: whether to merge all of the fragments together into a single file in the current node or not
-:param timeout: timeout for each process
-:param chunkTimeout: timeout for each file chunk inside each process"""
-        from k1lib.cli._applyCl import download
-        download(url, folder, merge, timeout, chunkTimeout)
     @staticmethod
     def diskScan(folder:str, raw=False):
         """Scans for files and folders in the specified folder for potential
 distributed files and folders. A distributed file is a file that exists on more
 than 1 node. A distributed folder is a folder that that exists on more than 1
 node and does not have any shared children. Example::
```

## Comparing `k1lib-1.3.9.1.data/data/k1lib/k1ui/256.model.state_dict.pth` & `k1lib-1.3.9.2.data/data/k1lib/k1ui/256.model.state_dict.pth`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.9.1.data/data/k1lib/k1ui/mouseKey.pth` & `k1lib-1.3.9.2.data/data/k1lib/k1ui/mouseKey.pth`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.9.1.data/data/k1lib/serve/main.html` & `k1lib-1.3.9.2.data/data/k1lib/serve/main.html`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.9.1.dist-info/LICENSE` & `k1lib-1.3.9.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.9.1.dist-info/METADATA` & `k1lib-1.3.9.2.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: k1lib
-Version: 1.3.9.1
+Version: 1.3.9.2
 Summary: Some nice ML overhaul
 Home-page: https://k1lib.com
 Author: Quang Ho
 Author-email: 157239q@gmail.com
 License: MIT
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
```

## Comparing `k1lib-1.3.9.1.dist-info/RECORD` & `k1lib-1.3.9.2.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -42,27 +42,27 @@
 k1lib/callbacks/lossFunctions/shorts.py,sha256=wXeUSgGDIdu_nsiAvn4pKs3fQrcO2FwpNXKcBCFvGzY,3465
 k1lib/callbacks/profilers/__init__.py,sha256=Gp5IvLRABYAg1J0ilTT2v72gfDbyTvUUHUEpvlSo1Lc,45
 k1lib/callbacks/profilers/computation.py,sha256=gPNsoioghh4PI5w8s2p8qYOrR7XObslqCLH5EkNOPSI,5054
 k1lib/callbacks/profilers/io.py,sha256=H8E0YzmLWRD9T2_RyDG2eXvbQQnJgzEnEyx8PqfQ4wY,2319
 k1lib/callbacks/profilers/memory.py,sha256=L0F5pc5LB0dtSnRot8ReR-amZn1uIXv0py0XmGS166U,4419
 k1lib/callbacks/profilers/time.py,sha256=R2-2ZooDwLQIeyonLp2Zz5E_uXzdy6mWUgw6uavQbpE,4215
 k1lib/cli/__init__.py,sha256=hF0ODhL20OSM9o1j68VhcIVflibgSpPuqeyYlsh8oow,925
-k1lib/cli/_applyCl.py,sha256=5OllclNWMrQUQOHywNmRXYfQ_zE21yoZf1NcypMmtuk,20738
+k1lib/cli/_applyCl.py,sha256=KaCVvwcZNGmUYJZD03w6wIUuv-ZlFq8emWUYMJb14CE,18925
 k1lib/cli/bio.py,sha256=PhGvy-fDA-wrUzzEDpuRe4x-Kbylx0sNmoXCEZfE_FA,8308
 k1lib/cli/cif.py,sha256=77FX83m1FRYEeZkdXJ8MiVapqCSzZ-1xOQ8ZLeHfhf8,4033
 k1lib/cli/conv.py,sha256=KcwSs9mD54XA5BC7ajxPF_1CZsu54i6dH6DtEv_fbv0,19321
 k1lib/cli/filt.py,sha256=c-bueTrH_5KAgRTtNiK4t4jfCy6xu-d0rYEUVF9KKGA,24071
 k1lib/cli/gb.py,sha256=xxjuNYgWrrElRckon3gP0sj-dShYnKs3jmHAb1U0kVI,6672
 k1lib/cli/grep.py,sha256=Lu4PFOe2pkaqd-UfJe_HhHCUFTUifE6Bh96_k80sQDA,6348
 k1lib/cli/init.py,sha256=2FaWeRqZnb--XWV4Xpo0z4ANDdyWpNQlHKkALtOGHKQ,18351
-k1lib/cli/inp.py,sha256=qE7BxeGqAem1sA8ohaHCdwgXKQLi7sf1Dbp_5wZgxxU,32683
+k1lib/cli/inp.py,sha256=FMpMQfnye0LV7woSfZIXA76NKG137JtskaxyeWNhDRw,32027
 k1lib/cli/kcsv.py,sha256=YGUVVLTZGGujokhxtj5MfjU9t1jRGqp23d58JK8lhq0,623
 k1lib/cli/kxml.py,sha256=YQGutvKNm0_xAi_NhCNtuGey7fx3zZSmSo33kS--54c,4819
 k1lib/cli/mgi.py,sha256=aLke90nG89tgWLPwyKmTj3kM8yJnIBCJSrPS1jT8mUk,1915
-k1lib/cli/modifier.py,sha256=5rTWXpwCRrz89s7m58OYowvbhpRfbIv1KxxVXM-QLnk,64736
+k1lib/cli/modifier.py,sha256=cqXnZha1kH3PY3F8GJOXMYQxOPXXgt8flYubE8WdRxE,64305
 k1lib/cli/mol.py,sha256=wNFuCPXtdEcH4DRBbmYaLAWxtDzjN2MOKFX7ynJhaJs,694
 k1lib/cli/nb.py,sha256=LsNN7OFJ6KzAYKvZpm4fj9WRpsX6Srx6D_xpSTCV328,4038
 k1lib/cli/optimizations.py,sha256=iZ73DwLqZCxRm0sECVZ7A2nDxf5D4rsoSGzrKTgzGaI,3530
 k1lib/cli/output.py,sha256=sGk7Z_kiJ0A_wwRtlJZZgvfqvbl1K-XmNsGih0PlqeA,12118
 k1lib/cli/sam.py,sha256=_ersEPP2ue0Oa3AyftNjQu2PABpH4L7iFBbRJDOkeug,2394
 k1lib/cli/structural.py,sha256=Ugn0Fb-0n0ardjjAvlfsfhRehLeUAszjvkW1odv6ZDU,49390
 k1lib/cli/trace.py,sha256=nzZgOyXqFJYkQfbpR0lpX0Nnp0bQHXPjk8sDUBIe2hk,10399
@@ -70,15 +70,15 @@
 k1lib/cli/utils.py,sha256=XF-H1Ce5ssOw6Dp3htiosHce7xxTgxptrlUsMYxI7uE,21101
 k1lib/k1ui/__init__.py,sha256=8D5a8oKgqd6WA1RUkiKCn4l_PVemtyuckxQut0vDHXM,20
 k1lib/k1ui/main.py,sha256=PnmdOhkjYgRSZnDyGYNMYtQ5Nvcb1NhQ9yjfP_3QORI,61803
 k1lib/serve/__init__.py,sha256=8D5a8oKgqd6WA1RUkiKCn4l_PVemtyuckxQut0vDHXM,20
 k1lib/serve/main.py,sha256=Xh2SzgABfsBp2dRLUJRMftsG_We8ReVHYqXLi3ntMVA,10361
 k1lib/serve/suffix-dash.py,sha256=HMNJvB4d-PTHXDRDQTdYUKtzgirJ0LVnqqAkXxO0B4w,153
 k1lib/serve/suffix.py,sha256=UH3ITN6O2vzoha2f6v4bcQG3_Boav7VA7EC8wf8r9f8,642
-k1lib-1.3.9.1.data/data/k1lib/k1ui/256.model.state_dict.pth,sha256=Ga-UXlAJfUPNOZsvP_c1-1cfB2Hp_-oQ4ghdouX1d7g,2453826
-k1lib-1.3.9.1.data/data/k1lib/k1ui/mouseKey.pth,sha256=KULhK_gdK2Ppju9gQnv1zV2kf_A0K-vX2W7trY6DIg8,304735
-k1lib-1.3.9.1.data/data/k1lib/serve/main.html,sha256=gHFNqzE9JKb4eCwCJN-Du45jj75lDRED4Ico91T-b4g,20544
-k1lib-1.3.9.1.dist-info/LICENSE,sha256=psuy2wnTg9zacuCQ0dXfxS44iaa89aTgsNzHDzx4UGM,1049
-k1lib-1.3.9.1.dist-info/METADATA,sha256=ssFnJkGJpSlKhr7_gLO8iHG0u9QaqFY53_ZkW00dRAI,3890
-k1lib-1.3.9.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-k1lib-1.3.9.1.dist-info/top_level.txt,sha256=xxWmqZzuThnLZn49Mse6A6j41-IVduPVnQW54imcOTA,6
-k1lib-1.3.9.1.dist-info/RECORD,,
+k1lib-1.3.9.2.data/data/k1lib/k1ui/256.model.state_dict.pth,sha256=Ga-UXlAJfUPNOZsvP_c1-1cfB2Hp_-oQ4ghdouX1d7g,2453826
+k1lib-1.3.9.2.data/data/k1lib/k1ui/mouseKey.pth,sha256=KULhK_gdK2Ppju9gQnv1zV2kf_A0K-vX2W7trY6DIg8,304735
+k1lib-1.3.9.2.data/data/k1lib/serve/main.html,sha256=gHFNqzE9JKb4eCwCJN-Du45jj75lDRED4Ico91T-b4g,20544
+k1lib-1.3.9.2.dist-info/LICENSE,sha256=psuy2wnTg9zacuCQ0dXfxS44iaa89aTgsNzHDzx4UGM,1049
+k1lib-1.3.9.2.dist-info/METADATA,sha256=PrMsEXRCToekfsW4OwAnHJ2Py5WGGiiAKsmUZppOSCA,3890
+k1lib-1.3.9.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+k1lib-1.3.9.2.dist-info/top_level.txt,sha256=xxWmqZzuThnLZn49Mse6A6j41-IVduPVnQW54imcOTA,6
+k1lib-1.3.9.2.dist-info/RECORD,,
```

