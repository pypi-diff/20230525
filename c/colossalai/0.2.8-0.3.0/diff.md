# Comparing `tmp/colossalai-0.2.8.tar.gz` & `tmp/colossalai-0.3.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "colossalai-0.2.8.tar", last modified: Wed Mar 29 02:16:57 2023, max compression
+gzip compressed data, was "colossalai-0.3.0.tar", last modified: Thu May 25 08:21:07 2023, max compression
```

## Comparing `colossalai-0.2.8.tar` & `colossalai-0.3.0.tar`

### file list

```diff
@@ -1,852 +1,902 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.747869 colossalai-0.2.8/
--rw-r--r--   0 runner    (1001) docker     (123)    22244 2023-03-29 02:16:46.000000 colossalai-0.2.8/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-29 02:16:46.000000 colossalai-0.2.8/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    24125 2023-03-29 02:16:57.747869 colossalai-0.2.8/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    19766 2023-03-29 02:16:46.000000 colossalai-0.2.8/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai/_C/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/_C/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      530 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai/amp/
--rw-r--r--   0 runner    (1001) docker     (123)     2311 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      153 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/amp_type.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai/amp/apex_amp/
--rw-r--r--   0 runner    (1001) docker     (123)     1636 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/apex_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1076 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/apex_amp/apex_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (123)     2422 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13454 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/_fp16_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/
--rw-r--r--   0 runner    (1001) docker     (123)      222 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2053 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (123)      748 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4755 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6098 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/naive_amp/naive_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.651869 colossalai-0.2.8/colossalai/amp/torch_amp/
--rw-r--r--   0 runner    (1001) docker     (123)     1607 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/torch_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26519 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/torch_amp/_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3344 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/amp/torch_amp/torch_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.651869 colossalai-0.2.8/colossalai/auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.651869 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (123)      155 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      359 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/build_c_ext.py
--rw-r--r--   0 runner    (1001) docker     (123)     7716 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3469 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
--rw-r--r--   0 runner    (1001) docker     (123)    18543 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4938 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/checkpoint/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.651869 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      336 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.651869 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/
--rw-r--r--   0 runner    (1001) docker     (123)      241 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3991 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2782 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
--rw-r--r--   0 runner    (1001) docker     (123)     7617 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
--rw-r--r--   0 runner    (1001) docker     (123)     2638 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)    25362 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     1167 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
--rw-r--r--   0 runner    (1001) docker     (123)     9601 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7345 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
--rw-r--r--   0 runner    (1001) docker     (123)     3327 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
--rw-r--r--   0 runner    (1001) docker     (123)     3995 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/metainfo.py
--rw-r--r--   0 runner    (1001) docker     (123)      763 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.655869 colossalai-0.2.8/colossalai/auto_parallel/offload/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7001 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/amp_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3650 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/base_offload_module.py
--rw-r--r--   0 runner    (1001) docker     (123)     2122 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/mem_optimize.py
--rw-r--r--   0 runner    (1001) docker     (123)     5177 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/region.py
--rw-r--r--   0 runner    (1001) docker     (123)    20381 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/region_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     9936 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/runtime.py
--rw-r--r--   0 runner    (1001) docker     (123)    18888 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/solver.py
--rw-r--r--   0 runner    (1001) docker     (123)    18548 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/training_simulator.py
--rw-r--r--   0 runner    (1001) docker     (123)     2723 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/offload/util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.655869 colossalai-0.2.8/colossalai/auto_parallel/passes/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5135 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/passes/comm_metainfo_pass.py
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/passes/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     6001 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (123)    11844 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/passes/runtime_apply_pass.py
--rw-r--r--   0 runner    (1001) docker     (123)    22492 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/passes/runtime_preparation_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.655869 colossalai-0.2.8/colossalai/auto_parallel/pipeline_shard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/pipeline_shard/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.655869 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)    18242 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.659869 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/
--rw-r--r--   0 runner    (1001) docker     (123)     1973 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4187 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3589 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5425 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5148 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6009 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    12450 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    14745 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    20533 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    16488 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1849 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2118 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2999 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)      840 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1979 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.663869 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
--rw-r--r--   0 runner    (1001) docker     (123)     2019 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14856 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5258 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    24314 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    12308 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3597 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7524 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     9295 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    43650 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5523 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4733 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3727 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    19134 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5041 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    13270 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5400 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     2509 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4201 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3052 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1156 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1859 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1963 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3835 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1518 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/options.py
--rw-r--r--   0 runner    (1001) docker     (123)    10911 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.663869 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/
--rw-r--r--   0 runner    (1001) docker     (123)      238 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9987 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     5783 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
--rw-r--r--   0 runner    (1001) docker     (123)    20519 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/solver.py
--rw-r--r--   0 runner    (1001) docker     (123)     9158 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.663869 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/
--rw-r--r--   0 runner    (1001) docker     (123)     1161 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6018 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
--rw-r--r--   0 runner    (1001) docker     (123)     8431 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     3837 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     9121 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/reshape.py
--rw-r--r--   0 runner    (1001) docker     (123)     4530 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/sharding.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.663869 colossalai-0.2.8/colossalai/booster/
--rw-r--r--   0 runner    (1001) docker     (123)       93 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1475 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/accelerator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7836 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/booster.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/booster/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)     1154 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      102 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/mixed_precision/bf16.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/mixed_precision/fp16_apex.py
--rw-r--r--   0 runner    (1001) docker     (123)     4998 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/mixed_precision/fp16_torch.py
--rw-r--r--   0 runner    (1001) docker     (123)      101 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/mixed_precision/fp8.py
--rw-r--r--   0 runner    (1001) docker     (123)      543 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/mixed_precision/mixed_precision_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/booster/plugin/
--rw-r--r--   0 runner    (1001) docker     (123)      117 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/plugin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1485 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/plugin/plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     8324 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/booster/plugin/torch_ddp_plugin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/builder/
--rw-r--r--   0 runner    (1001) docker     (123)      166 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3013 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/builder/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/checkpoint_io/
--rw-r--r--   0 runner    (1001) docker     (123)      205 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/checkpoint_io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19324 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/checkpoint_io/checkpoint_io_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/checkpoint_io/general_checkpoint_io.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/cli/
--rw-r--r--   0 runner    (1001) docker     (123)       40 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/cli/benchmark/
--rw-r--r--   0 runner    (1001) docker     (123)     1249 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/benchmark/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4210 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/benchmark/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)      385 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/benchmark/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     4864 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/benchmark/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/cli/check/
--rw-r--r--   0 runner    (1001) docker     (123)      395 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/check/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8494 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/check/check_installation.py
--rw-r--r--   0 runner    (1001) docker     (123)      373 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/cli.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.667869 colossalai-0.2.8/colossalai/cli/launcher/
--rw-r--r--   0 runner    (1001) docker     (123)     3763 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/launcher/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/launcher/hostinfo.py
--rw-r--r--   0 runner    (1001) docker     (123)     4294 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/launcher/multinode_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)    10170 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cli/launcher/run.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.671869 colossalai-0.2.8/colossalai/cluster/
--rw-r--r--   0 runner    (1001) docker     (123)      227 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4207 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cluster/device_mesh_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     7308 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cluster/dist_coordinator.py
--rw-r--r--   0 runner    (1001) docker     (123)     2356 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/cluster/process_group_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.671869 colossalai-0.2.8/colossalai/communication/
--rw-r--r--   0 runner    (1001) docker     (123)      865 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/communication/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11446 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/communication/collective.py
--rw-r--r--   0 runner    (1001) docker     (123)    19463 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/communication/p2p.py
--rw-r--r--   0 runner    (1001) docker     (123)     9013 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/communication/p2p_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     2054 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/communication/ring.py
--rw-r--r--   0 runner    (1001) docker     (123)     5095 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/communication/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      977 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.671869 colossalai-0.2.8/colossalai/context/
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3152 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4649 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/moe_context.py
--rw-r--r--   0 runner    (1001) docker     (123)    23921 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/parallel_context.py
--rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/parallel_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.671869 colossalai-0.2.8/colossalai/context/process_group_initializer/
--rw-r--r--   0 runner    (1001) docker     (123)      730 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2129 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_1d.py
--rw-r--r--   0 runner    (1001) docker     (123)     6239 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_2d.py
--rw-r--r--   0 runner    (1001) docker     (123)    12842 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (123)    13218 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2033 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     2165 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2361 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     4090 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_sequence.py
--rw-r--r--   0 runner    (1001) docker     (123)     2043 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1134 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/process_group_initializer/process_group_initializer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.671869 colossalai-0.2.8/colossalai/context/random/
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/random/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5169 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/random/_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     3373 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/random/seed_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)      786 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/context/singleton_meta.py
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/core.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.675869 colossalai-0.2.8/colossalai/device/
--rw-r--r--   0 runner    (1001) docker     (123)      139 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/device/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17420 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/device/alpha_beta_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5691 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/device/calc_pipeline_strategy.py
--rw-r--r--   0 runner    (1001) docker     (123)    11473 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/device/device_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.675869 colossalai-0.2.8/colossalai/engine/
--rw-r--r--   0 runner    (1001) docker     (123)       87 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7805 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/_base_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.675869 colossalai-0.2.8/colossalai/engine/gradient_accumulation/
--rw-r--r--   0 runner    (1001) docker     (123)     2618 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_accumulation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10274 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_accumulation/_gradient_accumulation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.675869 colossalai-0.2.8/colossalai/engine/gradient_handler/
--rw-r--r--   0 runner    (1001) docker     (123)      681 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      764 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/_base_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1130 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/_data_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2144 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/_moe_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2379 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1129 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/_sequence_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)      750 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/_zero_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/gradient_handler/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.675869 colossalai-0.2.8/colossalai/engine/schedule/
--rw-r--r--   0 runner    (1001) docker     (123)      315 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5875 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/schedule/_base_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     3892 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/schedule/_non_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)    41282 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/schedule/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     7523 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/engine/schedule/_pipeline_schedule_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.675869 colossalai-0.2.8/colossalai/fx/
--rw-r--r--   0 runner    (1001) docker     (123)      217 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1721 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (123)    19388 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/_meta_regist_12.py
--rw-r--r--   0 runner    (1001) docker     (123)     1906 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/_meta_regist_13.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.679869 colossalai-0.2.8/colossalai/fx/codegen/
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/codegen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    44750 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/codegen/activation_checkpoint_codegen.py
--rw-r--r--   0 runner    (1001) docker     (123)     7373 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/graph_module.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.679869 colossalai-0.2.8/colossalai/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (123)      266 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13584 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/adding_split_node_pass.py
--rw-r--r--   0 runner    (1001) docker     (123)    12179 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/concrete_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (123)    13968 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (123)    16310 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/passes_for_gpt2_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/shard_1d_pass.py
--rw-r--r--   0 runner    (1001) docker     (123)    14057 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/split_module.py
--rw-r--r--   0 runner    (1001) docker     (123)     6160 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/passes/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.679869 colossalai-0.2.8/colossalai/fx/profiler/
--rw-r--r--   0 runner    (1001) docker     (123)      695 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      871 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     6315 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/dataflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.679869 colossalai-0.2.8/colossalai/fx/profiler/experimental/
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      782 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     7047 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/
--rw-r--r--   0 runner    (1001) docker     (123)      211 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (123)     3475 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (123)      630 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)      436 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     2114 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     1193 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/pooling.py
--rw-r--r--   0 runner    (1001) docker     (123)      414 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (123)     2236 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/
--rw-r--r--   0 runner    (1001) docker     (123)      252 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (123)     2288 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     6632 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (123)      422 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/dropout.py
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)      493 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     1591 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     1011 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (123)     3034 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (123)      302 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
--rw-r--r--   0 runner    (1001) docker     (123)      605 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/experimental/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2244 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/memory_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    13155 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/opcount.py
--rw-r--r--   0 runner    (1001) docker     (123)    15387 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4235 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5011 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/profiler/tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4039 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/proxy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (123)      201 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4825 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/_meta_trace.py
--rw-r--r--   0 runner    (1001) docker     (123)     2197 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/_symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/_tracer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/
--rw-r--r--   0 runner    (1001) docker     (123)       90 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
--rw-r--r--   0 runner    (1001) docker     (123)      193 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2848 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
--rw-r--r--   0 runner    (1001) docker     (123)     2221 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
--rw-r--r--   0 runner    (1001) docker     (123)     4471 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
--rw-r--r--   0 runner    (1001) docker     (123)      776 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
--rw-r--r--   0 runner    (1001) docker     (123)       78 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4420 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
--rw-r--r--   0 runner    (1001) docker     (123)     2457 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
--rw-r--r--   0 runner    (1001) docker     (123)      536 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)    26968 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/experimental.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.683869 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/
--rw-r--r--   0 runner    (1001) docker     (123)       62 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.687869 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/
--rw-r--r--   0 runner    (1001) docker     (123)      167 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      217 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (123)     3202 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (123)     5762 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
--rw-r--r--   0 runner    (1001) docker     (123)      537 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)      707 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     2047 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (123)     5753 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.687869 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/
--rw-r--r--   0 runner    (1001) docker     (123)      179 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (123)     4674 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (123)      254 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)      384 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     1128 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     6769 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (123)      836 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)    24623 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.687869 colossalai-0.2.8/colossalai/gemini/
--rw-r--r--   0 runner    (1001) docker     (123)      407 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.687869 colossalai-0.2.8/colossalai/gemini/chunk/
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/chunk/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    22078 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/chunk/chunk.py
--rw-r--r--   0 runner    (1001) docker     (123)     9546 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/chunk/manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     5729 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/chunk/search_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/chunk/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1483 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/gemini_context.py
--rw-r--r--   0 runner    (1001) docker     (123)     6688 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/gemini_mgr.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.691869 colossalai-0.2.8/colossalai/gemini/memory_tracer/
--rw-r--r--   0 runner    (1001) docker     (123)      610 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1258 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/chunk_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     3978 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/memory_monitor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4121 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/memory_stats.py
--rw-r--r--   0 runner    (1001) docker     (123)     3674 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)      859 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/param_runtime_order.py
--rw-r--r--   0 runner    (1001) docker     (123)     3726 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/runtime_mem_tracer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4228 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/static_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/memory_tracer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.691869 colossalai-0.2.8/colossalai/gemini/ophooks/
--rw-r--r--   0 runner    (1001) docker     (123)      118 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/ophooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      768 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/ophooks/_shard_grad_ophook.py
--rw-r--r--   0 runner    (1001) docker     (123)     1356 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/ophooks/_shard_param_ophook.py
--rw-r--r--   0 runner    (1001) docker     (123)     5144 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/ophooks/runtime_mem_tracer_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     4815 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/ophooks/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.691869 colossalai-0.2.8/colossalai/gemini/paramhooks/
--rw-r--r--   0 runner    (1001) docker     (123)       77 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/paramhooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1253 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/paramhooks/_param_hookmgr.py
--rw-r--r--   0 runner    (1001) docker     (123)    10515 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (123)     6962 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/stateful_tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4301 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/stateful_tensor_mgr.py
--rw-r--r--   0 runner    (1001) docker     (123)     6494 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/tensor_placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (123)     3999 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/gemini/tensor_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2159 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/global_variables.py
--rw-r--r--   0 runner    (1001) docker     (123)    21066 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.691869 colossalai-0.2.8/colossalai/interface/
--rw-r--r--   0 runner    (1001) docker     (123)      120 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/interface/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      645 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/interface/model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3922 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/interface/optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.691869 colossalai-0.2.8/colossalai/kernel/
--rw-r--r--   0 runner    (1001) docker     (123)      165 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.691869 colossalai-0.2.8/colossalai/kernel/cuda_native/
--rw-r--r--   0 runner    (1001) docker     (123)      297 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.699869 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/
--rw-r--r--   0 runner    (1001) docker     (123)     2606 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/colossal_C_frontend.cpp
--rw-r--r--   0 runner    (1001) docker     (123)      213 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/compat.h
--rw-r--r--   0 runner    (1001) docker     (123)    16815 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (123)     5187 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/cpu_adam.h
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.699869 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/
--rw-r--r--   0 runner    (1001) docker     (123)     7526 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/cross_entropy.cu
--rw-r--r--   0 runner    (1001) docker     (123)     3767 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/cublas_wrappers.cu
--rw-r--r--   0 runner    (1001) docker     (123)     5441 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/cuda_util.cu
--rw-r--r--   0 runner    (1001) docker     (123)    37586 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/dropout_kernels.cu
--rw-r--r--   0 runner    (1001) docker     (123)     7653 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/general_kernels.cu
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.699869 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/
--rw-r--r--   0 runner    (1001) docker     (123)     8585 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/block_reduce.h
--rw-r--r--   0 runner    (1001) docker     (123)      647 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/context.h
--rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/cross_entropy_layer.h
--rw-r--r--   0 runner    (1001) docker     (123)     1803 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/cublas_wrappers.h
--rw-r--r--   0 runner    (1001) docker     (123)      966 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/cuda_util.h
--rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/dropout.h
--rw-r--r--   0 runner    (1001) docker     (123)     2232 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/feed_forward.h
--rw-r--r--   0 runner    (1001) docker     (123)     9260 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/kernels.h
--rw-r--r--   0 runner    (1001) docker     (123)      296 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/ls_cub.cuh
--rw-r--r--   0 runner    (1001) docker     (123)     1905 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/normalize_layer.h
--rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/softmax.h
--rw-r--r--   0 runner    (1001) docker     (123)     3043 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/strided_batch_gemm.h
--rw-r--r--   0 runner    (1001) docker     (123)    48403 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/normalize_kernels.cu
--rw-r--r--   0 runner    (1001) docker     (123)    13619 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/softmax_kernels.cu
--rw-r--r--   0 runner    (1001) docker     (123)    10953 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/transform_kernels.cu
--rw-r--r--   0 runner    (1001) docker     (123)     4877 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/layer_norm_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (123)    25828 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/layer_norm_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (123)     4003 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/moe_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (123)    26286 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/moe_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (123)     5046 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_adam.cu
--rw-r--r--   0 runner    (1001) docker     (123)     5200 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (123)    13278 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (123)    13114 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_lamb.cu
--rw-r--r--   0 runner    (1001) docker     (123)     4439 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (123)     6478 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (123)    16988 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.cpp
--rw-r--r--   0 runner    (1001) docker     (123)     4701 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.h
--rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (123)    21701 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (123)     3467 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (123)     2066 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (123)    22850 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (123)    13636 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/type_shim.h
--rw-r--r--   0 runner    (1001) docker     (123)    24865 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/flash_attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     2552 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/layer_norm.py
--rw-r--r--   0 runner    (1001) docker     (123)    10751 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/multihead_attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     6715 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/cuda_native/scaled_softmax.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.703869 colossalai-0.2.8/colossalai/kernel/jit/
--rw-r--r--   0 runner    (1001) docker     (123)      308 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/jit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      868 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/jit/bias_dropout_add.py
--rw-r--r--   0 runner    (1001) docker     (123)     1359 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/jit/bias_gelu.py
--rw-r--r--   0 runner    (1001) docker     (123)     3478 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/jit/option.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.703869 colossalai-0.2.8/colossalai/kernel/op_builder/
--rw-r--r--   0 runner    (1001) docker     (123)      999 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8815 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/cpu_adam.py
--rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/fused_optim.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/layernorm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/moe.py
--rw-r--r--   0 runner    (1001) docker     (123)     1459 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/multi_head_attn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/scaled_masked_softmax.py
--rw-r--r--   0 runner    (1001) docker     (123)     1286 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/scaled_upper_triangle_masked_softmax.py
--rw-r--r--   0 runner    (1001) docker     (123)     8186 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/kernel/op_builder/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.703869 colossalai-0.2.8/colossalai/logging/
--rw-r--r--   0 runner    (1001) docker     (123)     1597 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/logging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7511 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/logging/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.703869 colossalai-0.2.8/colossalai/nn/
--rw-r--r--   0 runner    (1001) docker     (123)      136 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.703869 colossalai-0.2.8/colossalai/nn/_ops/
--rw-r--r--   0 runner    (1001) docker     (123)      317 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8758 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3845 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/addmm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1152 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/batch_norm.py
--rw-r--r--   0 runner    (1001) docker     (123)     9289 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/element_wise.py
--rw-r--r--   0 runner    (1001) docker     (123)     6666 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     6606 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/embedding_bag.py
--rw-r--r--   0 runner    (1001) docker     (123)     1105 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/layernorm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7122 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     2641 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3143 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/_ops/view.py
--rw-r--r--   0 runner    (1001) docker     (123)     9552 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/init.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.703869 colossalai-0.2.8/colossalai/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2844 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/base_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.707869 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/
--rw-r--r--   0 runner    (1001) docker     (123)      307 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      992 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (123)     6304 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     5375 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/normalization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.707869 colossalai-0.2.8/colossalai/nn/layer/moe/
--rw-r--r--   0 runner    (1001) docker     (123)      434 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5557 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/moe/_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     7034 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/moe/experts.py
--rw-r--r--   0 runner    (1001) docker     (123)     9607 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/moe/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)    10066 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/moe/routers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2696 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/moe/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.707869 colossalai-0.2.8/colossalai/nn/layer/parallel_1d/
--rw-r--r--   0 runner    (1001) docker     (123)      404 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_1d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3796 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_1d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5048 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_1d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    49748 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_1d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.707869 colossalai-0.2.8/colossalai/nn/layer/parallel_2d/
--rw-r--r--   0 runner    (1001) docker     (123)      419 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    34900 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)      843 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    50524 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.707869 colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/
--rw-r--r--   0 runner    (1001) docker     (123)      455 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    37643 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    50769 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.707869 colossalai-0.2.8/colossalai/nn/layer/parallel_3d/
--rw-r--r--   0 runner    (1001) docker     (123)      455 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_3d/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    22769 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_3d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2967 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_3d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    51441 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_3d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/
--rw-r--r--   0 runner    (1001) docker     (123)      152 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6411 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/_operation.py
--rw-r--r--   0 runner    (1001) docker     (123)      483 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    10752 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/layer/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      413 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/layer/vanilla/
--rw-r--r--   0 runner    (1001) docker     (123)      324 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/vanilla/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14535 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/vanilla/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/layer/wrapper/
--rw-r--r--   0 runner    (1001) docker     (123)      101 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2133 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/layer/wrapper/pipeline_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (123)     1629 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4694 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/loss/loss_1d.py
--rw-r--r--   0 runner    (1001) docker     (123)     5728 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/loss/loss_2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     5536 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/loss/loss_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (123)     6362 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/loss/loss_3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3249 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/loss/loss_moe.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/lr_scheduler/
--rw-r--r--   0 runner    (1001) docker     (123)      628 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6171 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/cosine.py
--rw-r--r--   0 runner    (1001) docker     (123)     7800 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/delayed.py
--rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     2867 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/multistep.py
--rw-r--r--   0 runner    (1001) docker     (123)     5319 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/onecycle.py
--rw-r--r--   0 runner    (1001) docker     (123)     2686 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/poly.py
--rw-r--r--   0 runner    (1001) docker     (123)     3687 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/lr_scheduler/torch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.711869 colossalai-0.2.8/colossalai/nn/metric/
--rw-r--r--   0 runner    (1001) docker     (123)      704 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/metric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      155 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/metric/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      784 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/metric/accuracy_2d.py
--rw-r--r--   0 runner    (1001) docker     (123)      798 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/metric/accuracy_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/metric/accuracy_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.715869 colossalai-0.2.8/colossalai/nn/optimizer/
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/colossalai_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     7561 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/cpu_adam.py
--rw-r--r--   0 runner    (1001) docker     (123)     6363 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/fused_adam.py
--rw-r--r--   0 runner    (1001) docker     (123)     8926 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/fused_lamb.py
--rw-r--r--   0 runner    (1001) docker     (123)     6094 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/fused_sgd.py
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/gemini_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6889 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/hybrid_adam.py
--rw-r--r--   0 runner    (1001) docker     (123)     4494 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/lamb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3784 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/lars.py
--rw-r--r--   0 runner    (1001) docker     (123)     6535 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/nvme_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    13754 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/optimizer/zero_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.715869 colossalai-0.2.8/colossalai/nn/parallel/
--rw-r--r--   0 runner    (1001) docker     (123)      239 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    31628 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/data_parallel.py
--rw-r--r--   0 runner    (1001) docker     (123)     3103 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/gemini_parallel.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.715869 colossalai-0.2.8/colossalai/nn/parallel/layers/
--rw-r--r--   0 runner    (1001) docker     (123)      883 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.715869 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/
--rw-r--r--   0 runner    (1001) docker     (123)      721 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/base_embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)    28600 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/cache_mgr.py
--rw-r--r--   0 runner    (1001) docker     (123)     8826 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     2042 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/copyer.py
--rw-r--r--   0 runner    (1001) docker     (123)      855 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/embedding_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     5922 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)    10057 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
--rw-r--r--   0 runner    (1001) docker     (123)     7481 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
--rw-r--r--   0 runner    (1001) docker     (123)     2020 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/colo_module.py
--rw-r--r--   0 runner    (1001) docker     (123)     1153 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     1231 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     4816 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/layers/module_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3875 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/reducer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4172 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4946 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/nn/parallel/zero_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.719869 colossalai-0.2.8/colossalai/pipeline/
--rw-r--r--   0 runner    (1001) docker     (123)      162 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/layer_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.719869 colossalai-0.2.8/colossalai/pipeline/middleware/
--rw-r--r--   0 runner    (1001) docker     (123)      148 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/middleware/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.719869 colossalai-0.2.8/colossalai/pipeline/middleware/adaptor/
--rw-r--r--   0 runner    (1001) docker     (123)       78 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/middleware/adaptor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6154 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/middleware/adaptor/fx.py
--rw-r--r--   0 runner    (1001) docker     (123)     7051 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/middleware/topo.py
--rw-r--r--   0 runner    (1001) docker     (123)    11408 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/pipelinable.py
--rw-r--r--   0 runner    (1001) docker     (123)     5857 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/pipeline_process_group.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.719869 colossalai-0.2.8/colossalai/pipeline/rpc/
--rw-r--r--   0 runner    (1001) docker     (123)      236 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/rpc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    59163 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/rpc/_pipeline_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    14858 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/rpc/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     5460 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/rpc/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9011 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/pipeline/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.719869 colossalai-0.2.8/colossalai/registry/
--rw-r--r--   0 runner    (1001) docker     (123)      690 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3054 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/registry/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.723869 colossalai-0.2.8/colossalai/tensor/
--rw-r--r--   0 runner    (1001) docker     (123)      946 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4449 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/colo_parameter.py
--rw-r--r--   0 runner    (1001) docker     (123)    13182 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/colo_tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)    22268 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/comm_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)      779 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/compute_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)      103 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/const.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.723869 colossalai-0.2.8/colossalai/tensor/d_tensor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11875 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/comm_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)     4944 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/d_tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2865 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/layout.py
--rw-r--r--   0 runner    (1001) docker     (123)    25229 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/layout_converter.py
--rw-r--r--   0 runner    (1001) docker     (123)      231 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     9273 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)     3268 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/d_tensor/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8382 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/dist_spec_mgr.py
--rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/distspec.py
--rw-r--r--   0 runner    (1001) docker     (123)     1691 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/op_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     6126 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/param_op_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)    10365 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/process_group.py
--rw-r--r--   0 runner    (1001) docker     (123)    36491 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/shape_consistency.py
--rw-r--r--   0 runner    (1001) docker     (123)    11616 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)      683 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/tensor_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)     8439 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.723869 colossalai-0.2.8/colossalai/testing/
--rw-r--r--   0 runner    (1001) docker     (123)      433 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1128 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/testing/comparison.py
--rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/testing/pytest_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/testing/random.py
--rw-r--r--   0 runner    (1001) docker     (123)     6751 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/testing/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.723869 colossalai-0.2.8/colossalai/trainer/
--rw-r--r--   0 runner    (1001) docker     (123)       53 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14778 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.727869 colossalai-0.2.8/colossalai/trainer/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2980 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/_base_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     3247 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/_checkpoint_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)      231 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/_commons_.py
--rw-r--r--   0 runner    (1001) docker     (123)    13106 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/_log_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     2131 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/_lr_scheduler_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)    16131 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/trainer/hooks/_metric_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.727869 colossalai-0.2.8/colossalai/utils/
--rw-r--r--   0 runner    (1001) docker     (123)     2059 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9856 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/activation_checkpoint.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.727869 colossalai-0.2.8/colossalai/utils/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (123)      114 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5520 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint/module_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     2167 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.727869 colossalai-0.2.8/colossalai/utils/checkpoint_io/
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1967 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/constant.py
--rw-r--r--   0 runner    (1001) docker     (123)     9794 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/convertor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5971 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/distributed.py
--rw-r--r--   0 runner    (1001) docker     (123)     7008 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/io.py
--rw-r--r--   0 runner    (1001) docker     (123)     1998 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/meta.py
--rw-r--r--   0 runner    (1001) docker     (123)     4813 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     8908 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3966 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpoint_io/writer.py
--rw-r--r--   0 runner    (1001) docker     (123)    11400 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/checkpointing.py
--rw-r--r--   0 runner    (1001) docker     (123)    17885 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/common.py
--rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.727869 colossalai-0.2.8/colossalai/utils/data_sampler/
--rw-r--r--   0 runner    (1001) docker     (123)      177 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/data_sampler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/data_sampler/base_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     7128 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/data_sampler/data_parallel_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6425 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/utils/model/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7486 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/model/colo_init_context.py
--rw-r--r--   0 runner    (1001) docker     (123)    21939 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/model/experimental.py
--rw-r--r--   0 runner    (1001) docker     (123)     8911 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/model/lazy_init_context.py
--rw-r--r--   0 runner    (1001) docker     (123)     3911 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/model/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/moe.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/utils/multi_tensor_apply/
--rw-r--r--   0 runner    (1001) docker     (123)      101 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/multi_tensor_apply/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1112 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/utils/profiler/
--rw-r--r--   0 runner    (1001) docker     (123)       52 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/extention.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/utils/profiler/legacy/
--rw-r--r--   0 runner    (1001) docker     (123)      272 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10915 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/legacy/comm_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5050 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/legacy/pcie_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3894 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/legacy/prof_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8616 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3998 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/profiler/stateful_tensor_mem_extention.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/utils/rank_recorder/
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/rank_recorder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5528 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/rank_recorder/rank_recorder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/utils/tensor_detector/
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/tensor_detector/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8647 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/tensor_detector/tensor_detector.py
--rw-r--r--   0 runner    (1001) docker     (123)     4279 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/utils/timer.py
--rw-r--r--   0 runner    (1001) docker     (123)       47 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/zero/
--rw-r--r--   0 runner    (1001) docker     (123)     1441 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/zero/init_ctx/
--rw-r--r--   0 runner    (1001) docker     (123)      171 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/init_ctx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10942 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/init_ctx/init_context.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/zero/shard_utils/
--rw-r--r--   0 runner    (1001) docker     (123)      259 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/shard_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      637 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/shard_utils/base_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (123)     2212 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/shard_utils/bucket_tensor_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (123)      737 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/shard_utils/commons.py
--rw-r--r--   0 runner    (1001) docker     (123)     2609 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/shard_utils/tensor_shard_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.731869 colossalai-0.2.8/colossalai/zero/sharded_model/
--rw-r--r--   0 runner    (1001) docker     (123)       74 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2727 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8357 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_model/reduce_scatter.py
--rw-r--r--   0 runner    (1001) docker     (123)    28931 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_model/sharded_model_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)      800 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_model/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.735869 colossalai-0.2.8/colossalai/zero/sharded_optim/
--rw-r--r--   0 runner    (1001) docker     (123)      159 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9648 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.735869 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/
--rw-r--r--   0 runner    (1001) docker     (123)      242 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      410 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/base_store.py
--rw-r--r--   0 runner    (1001) docker     (123)     1446 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/bucket_store.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/gradient_store.py
--rw-r--r--   0 runner    (1001) docker     (123)     3446 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/parameter_store.py
--rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/tensor_bucket.py
--rw-r--r--   0 runner    (1001) docker     (123)    25467 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/low_level_optim.py
--rw-r--r--   0 runner    (1001) docker     (123)    18615 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_optim/sharded_optim_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.735869 colossalai-0.2.8/colossalai/zero/sharded_param/
--rw-r--r--   0 runner    (1001) docker     (123)      189 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_param/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3889 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_param/sharded_param.py
--rw-r--r--   0 runner    (1001) docker     (123)     1132 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/sharded_param/sharded_tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.735869 colossalai-0.2.8/colossalai/zero/utils/
--rw-r--r--   0 runner    (1001) docker     (123)       55 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2385 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/utils/gemini_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     4798 2023-03-29 02:16:46.000000 colossalai-0.2.8/colossalai/zero/utils/zero_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.647869 colossalai-0.2.8/colossalai.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    24125 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    35338 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)       69 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)       93 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-03-29 02:16:57.000000 colossalai-0.2.8/colossalai.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.735869 colossalai-0.2.8/op_builder/
--rw-r--r--   0 runner    (1001) docker     (123)      999 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8815 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/cpu_adam.py
--rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/fused_optim.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/layernorm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/moe.py
--rw-r--r--   0 runner    (1001) docker     (123)     1459 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/multi_head_attn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/scaled_masked_softmax.py
--rw-r--r--   0 runner    (1001) docker     (123)     1286 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/scaled_upper_triangle_masked_softmax.py
--rw-r--r--   0 runner    (1001) docker     (123)     8186 2023-03-29 02:16:46.000000 colossalai-0.2.8/op_builder/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.735869 colossalai-0.2.8/requirements/
--rw-r--r--   0 runner    (1001) docker     (123)      268 2023-03-29 02:16:46.000000 colossalai-0.2.8/requirements/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (123)       93 2023-03-29 02:16:46.000000 colossalai-0.2.8/requirements/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-03-29 02:16:57.747869 colossalai-0.2.8/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     6229 2023-03-29 02:16:46.000000 colossalai-0.2.8/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.643869 colossalai-0.2.8/tests/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/components_to_test/
--rw-r--r--   0 runner    (1001) docker     (123)      420 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2461 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/albert.py
--rw-r--r--   0 runner    (1001) docker     (123)     1394 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/beit.py
--rw-r--r--   0 runner    (1001) docker     (123)     3260 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/bert.py
--rw-r--r--   0 runner    (1001) docker     (123)     2857 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/hanging_param_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1310 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/inline_op_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/nested_model.py
--rw-r--r--   0 runner    (1001) docker     (123)      925 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1401 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/repeated_computed_layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/resnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/simple_net.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/components_to_test/utils/
--rw-r--r--   0 runner    (1001) docker     (123)       87 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      479 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/utils/dummy_data_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)      673 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/components_to_test/utils/executor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/model_zoo/
--rw-r--r--   0 runner    (1001) docker     (123)      136 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/model_zoo/diffusers/
--rw-r--r--   0 runner    (1001) docker     (123)       25 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/diffusers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2754 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/diffusers/diffusers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2287 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/model_zoo/timm/
--rw-r--r--   0 runner    (1001) docker     (123)       20 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/timm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7513 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/timm/timm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/model_zoo/torchaudio/
--rw-r--r--   0 runner    (1001) docker     (123)       26 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/torchaudio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/torchaudio/torchaudio.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/model_zoo/torchrec/
--rw-r--r--   0 runner    (1001) docker     (123)       24 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/torchrec/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4634 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/torchrec/torchrec.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.739869 colossalai-0.2.8/tests/kit/model_zoo/torchvision/
--rw-r--r--   0 runner    (1001) docker     (123)       27 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/torchvision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5939 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/torchvision/torchvision.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.743869 colossalai-0.2.8/tests/kit/model_zoo/transformers/
--rw-r--r--   0 runner    (1001) docker     (123)       98 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/transformers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4023 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/transformers/albert.py
--rw-r--r--   0 runner    (1001) docker     (123)     4335 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/transformers/bert.py
--rw-r--r--   0 runner    (1001) docker     (123)     2767 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/transformers/gpt.py
--rw-r--r--   0 runner    (1001) docker     (123)     1262 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/transformers/opt.py
--rw-r--r--   0 runner    (1001) docker     (123)     1766 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/kit/model_zoo/transformers/t5.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.743869 colossalai-0.2.8/tests/test_auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.743869 colossalai-0.2.8/tests/test_auto_parallel/test_pass/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_pass/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1726 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
--rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.743869 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3061 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
--rw-r--r--   0 runner    (1001) docker     (123)     2234 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
--rw-r--r--   0 runner    (1001) docker     (123)     2706 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     3653 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
--rw-r--r--   0 runner    (1001) docker     (123)     4414 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
--rw-r--r--   0 runner    (1001) docker     (123)     3483 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.743869 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11050 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)     7566 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)     3768 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
--rw-r--r--   0 runner    (1001) docker     (123)     1734 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:57.747869 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12069 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     7996 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4887 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6980 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     6466 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
--rw-r--r--   0 runner    (1001) docker     (123)    11321 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     9190 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    12975 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3547 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    11804 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2603 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     8255 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4439 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    13408 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     7887 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2360 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2340 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    18904 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2855 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
--rw-r--r--   0 runner    (1001) docker     (123)     9213 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    12824 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    12596 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2525 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
--rw-r--r--   0 runner    (1001) docker     (123)     3641 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    14422 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3339 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     8642 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5055 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_param_resharding_cost.py
--rw-r--r--   0 runner    (1001) docker     (123)     2836 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_shape_consistency_pass.py
--rw-r--r--   0 runner    (1001) docker     (123)     5096 2023-03-29 02:16:46.000000 colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-03-29 02:16:46.000000 colossalai-0.2.8/version.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.884923 colossalai-0.3.0/
+-rw-r--r--   0 runner    (1001) docker     (123)    22244 2023-05-25 08:20:55.000000 colossalai-0.3.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-25 08:20:55.000000 colossalai-0.3.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    25388 2023-05-25 08:21:07.884923 colossalai-0.3.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    20789 2023-05-25 08:20:55.000000 colossalai-0.3.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.756918 colossalai-0.3.0/colossalai/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai/_C/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_C/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      530 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai/_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai/_analyzer/_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20609 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/_subclasses/_meta_registration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2121 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/_subclasses/_monkey_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18694 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/_subclasses/flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7667 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/_subclasses/meta_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      114 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/envs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai/_analyzer/fx/
+-rw-r--r--   0 runner    (1001) docker     (123)      129 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18761 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/codegen.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9920 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/graph_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8526 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/node_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai/_analyzer/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12600 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/passes/graph_profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9892 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/passes/shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (123)      983 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/symbolic_profile.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (123)       63 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6512 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1107 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/proxy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6048 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15758 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/_analyzer/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/amp/
+-rw-r--r--   0 runner    (1001) docker     (123)     2313 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      153 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/amp_type.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/amp/apex_amp/
+-rw-r--r--   0 runner    (1001) docker     (123)     1636 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/apex_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1076 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/apex_amp/apex_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (123)     2422 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13454 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/_fp16_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/
+-rw-r--r--   0 runner    (1001) docker     (123)      222 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2053 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      748 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4755 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6098 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/naive_amp/naive_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/amp/torch_amp/
+-rw-r--r--   0 runner    (1001) docker     (123)     1607 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/torch_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26520 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/torch_amp/_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3344 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/amp/torch_amp/torch_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (123)      155 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      359 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/build_c_ext.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7716 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3469 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18543 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4938 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/checkpoint/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.764918 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/
+-rw-r--r--   0 runner    (1001) docker     (123)       95 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      336 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.768919 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/
+-rw-r--r--   0 runner    (1001) docker     (123)      241 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4050 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2824 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7900 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2668 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25633 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9721 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7399 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3363 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2968 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
+-rw-r--r--   0 runner    (1001) docker     (123)      763 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4847 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.768919 colossalai-0.3.0/colossalai/auto_parallel/offload/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7001 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/amp_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3618 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/base_offload_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/mem_optimize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5134 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/region.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20381 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/region_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10128 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/runtime.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18888 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/solver.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18549 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/training_simulator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2966 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/offload/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.772919 colossalai-0.3.0/colossalai/auto_parallel/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5214 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/passes/comm_metainfo_pass.py
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/passes/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6030 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12067 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/passes/runtime_apply_pass.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22686 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/passes/runtime_preparation_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.772919 colossalai-0.3.0/colossalai/auto_parallel/pipeline_shard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/pipeline_shard/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.772919 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18744 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.776919 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/
+-rw-r--r--   0 runner    (1001) docker     (123)     1973 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4187 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3519 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5425 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5270 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6009 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12452 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14746 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20532 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16519 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1849 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2118 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2999 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      840 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1979 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.780919 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
+-rw-r--r--   0 runner    (1001) docker     (123)     2019 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14859 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5258 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24314 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12308 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3597 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7524 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9295 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43503 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5524 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4733 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3727 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19134 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5041 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13270 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5400 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2509 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4201 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3052 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1156 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1859 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1963 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3835 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1518 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/options.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10911 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.780919 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/
+-rw-r--r--   0 runner    (1001) docker     (123)      238 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9989 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5784 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20520 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/solver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9158 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     1161 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6015 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8432 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3837 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9116 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/reshape.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4530 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/sharding.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/booster/
+-rw-r--r--   0 runner    (1001) docker     (123)       93 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1475 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11106 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/booster.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/booster/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (123)     1272 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      102 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/fp16_apex.py
+-rw-r--r--   0 runner    (1001) docker     (123)      918 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/fp16_naive.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4998 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/fp16_torch.py
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/fp8.py
+-rw-r--r--   0 runner    (1001) docker     (123)      543 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/mixed_precision/mixed_precision_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/booster/plugin/
+-rw-r--r--   0 runner    (1001) docker     (123)      447 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3177 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/dp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13628 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/gemini_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9086 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/low_level_zero_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2267 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6171 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/torch_ddp_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8714 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/booster/plugin/torch_fsdp_plugin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/builder/
+-rw-r--r--   0 runner    (1001) docker     (123)      166 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3013 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/builder/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/checkpoint_io/
+-rw-r--r--   0 runner    (1001) docker     (123)      218 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/checkpoint_io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14747 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/checkpoint_io/checkpoint_io_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5611 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/checkpoint_io/general_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5269 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/checkpoint_io/index_file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14463 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/checkpoint_io/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)       40 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.788919 colossalai-0.3.0/colossalai/cli/benchmark/
+-rw-r--r--   0 runner    (1001) docker     (123)     1249 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/benchmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4240 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/benchmark/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/benchmark/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4864 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/benchmark/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.792919 colossalai-0.3.0/colossalai/cli/check/
+-rw-r--r--   0 runner    (1001) docker     (123)      395 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/check/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8495 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/check/check_installation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      373 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/cli.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.792919 colossalai-0.3.0/colossalai/cli/launcher/
+-rw-r--r--   0 runner    (1001) docker     (123)     3763 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/launcher/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/launcher/hostinfo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4294 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/launcher/multinode_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10170 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cli/launcher/run.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.792919 colossalai-0.3.0/colossalai/cluster/
+-rw-r--r--   0 runner    (1001) docker     (123)      227 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cluster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4207 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cluster/device_mesh_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7307 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cluster/dist_coordinator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2356 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/cluster/process_group_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.792919 colossalai-0.3.0/colossalai/communication/
+-rw-r--r--   0 runner    (1001) docker     (123)      865 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/communication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11446 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/communication/collective.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19462 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/communication/p2p.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9013 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/communication/p2p_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2054 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/communication/ring.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5095 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/communication/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      977 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.792919 colossalai-0.3.0/colossalai/context/
+-rw-r--r--   0 runner    (1001) docker     (123)      230 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3152 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4649 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/moe_context.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23924 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/parallel_context.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/parallel_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.796920 colossalai-0.3.0/colossalai/context/process_group_initializer/
+-rw-r--r--   0 runner    (1001) docker     (123)      730 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2129 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_1d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6239 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12842 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13259 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2034 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2165 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2361 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4092 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_sequence.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2043 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1134 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/process_group_initializer/process_group_initializer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.796920 colossalai-0.3.0/colossalai/context/random/
+-rw-r--r--   0 runner    (1001) docker     (123)      383 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/random/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5169 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/random/_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3372 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/random/seed_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)      786 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/context/singleton_meta.py
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/core.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.796920 colossalai-0.3.0/colossalai/device/
+-rw-r--r--   0 runner    (1001) docker     (123)      139 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/device/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17419 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/device/alpha_beta_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5691 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/device/calc_pipeline_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11473 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/device/device_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.796920 colossalai-0.3.0/colossalai/engine/
+-rw-r--r--   0 runner    (1001) docker     (123)       87 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7809 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/_base_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.796920 colossalai-0.3.0/colossalai/engine/gradient_accumulation/
+-rw-r--r--   0 runner    (1001) docker     (123)     2624 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_accumulation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10256 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_accumulation/_gradient_accumulation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.796920 colossalai-0.3.0/colossalai/engine/gradient_handler/
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      763 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/_base_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1129 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/_data_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2100 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/_moe_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2378 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1128 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/_sequence_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      751 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/_zero_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/gradient_handler/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.800920 colossalai-0.3.0/colossalai/engine/schedule/
+-rw-r--r--   0 runner    (1001) docker     (123)      315 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5875 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/schedule/_base_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3876 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/schedule/_non_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41258 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/schedule/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7511 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/engine/schedule/_pipeline_schedule_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.800920 colossalai-0.3.0/colossalai/fx/
+-rw-r--r--   0 runner    (1001) docker     (123)      217 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1616 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19408 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/_meta_regist_12.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1906 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/_meta_regist_13.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.800920 colossalai-0.3.0/colossalai/fx/codegen/
+-rw-r--r--   0 runner    (1001) docker     (123)       45 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/codegen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44753 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/codegen/activation_checkpoint_codegen.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7373 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/graph_module.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.800920 colossalai-0.3.0/colossalai/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (123)      266 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13583 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/adding_split_node_pass.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12179 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/concrete_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13967 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16288 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/passes_for_gpt2_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/shard_1d_pass.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14022 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/split_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6160 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/passes/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.804920 colossalai-0.3.0/colossalai/fx/profiler/
+-rw-r--r--   0 runner    (1001) docker     (123)      695 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      871 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6315 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/dataflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.804920 colossalai-0.3.0/colossalai/fx/profiler/experimental/
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      782 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7047 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.804920 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3475 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (123)      630 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)      436 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2114 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1193 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)      414 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2236 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.808920 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/
+-rw-r--r--   0 runner    (1001) docker     (123)      252 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2288 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6632 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)      422 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (123)      431 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)      493 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1591 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1011 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3034 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      302 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
+-rw-r--r--   0 runner    (1001) docker     (123)      605 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/experimental/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2244 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/memory_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13226 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/opcount.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15387 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4235 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5011 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/profiler/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4041 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/proxy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.808920 colossalai-0.3.0/colossalai/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4825 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/_meta_trace.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2197 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/_symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1516 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/_tracer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.808920 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/
+-rw-r--r--   0 runner    (1001) docker     (123)       90 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.808920 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
+-rw-r--r--   0 runner    (1001) docker     (123)      193 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2848 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2221 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4471 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)      776 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.808920 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
+-rw-r--r--   0 runner    (1001) docker     (123)       78 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4420 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2457 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
+-rw-r--r--   0 runner    (1001) docker     (123)      536 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26968 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/experimental.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.808920 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/
+-rw-r--r--   0 runner    (1001) docker     (123)       62 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.812920 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/
+-rw-r--r--   0 runner    (1001) docker     (123)      167 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      217 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3202 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5762 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)      537 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)      707 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2047 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5753 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.812920 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/
+-rw-r--r--   0 runner    (1001) docker     (123)      179 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      428 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4674 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)      254 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)      384 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1128 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6769 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      836 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24623 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/fx/tracer/tracer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/global_variables.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21027 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.812920 colossalai-0.3.0/colossalai/interface/
+-rw-r--r--   0 runner    (1001) docker     (123)      120 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/interface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/interface/model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3922 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/interface/optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.812920 colossalai-0.3.0/colossalai/kernel/
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.816920 colossalai-0.3.0/colossalai/kernel/cuda_native/
+-rw-r--r--   0 runner    (1001) docker     (123)      297 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.820921 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/
+-rw-r--r--   0 runner    (1001) docker     (123)     2606 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/colossal_C_frontend.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)      213 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/compat.h
+-rw-r--r--   0 runner    (1001) docker     (123)    16815 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)     5187 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/cpu_adam.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.820921 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/
+-rw-r--r--   0 runner    (1001) docker     (123)     7526 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/cross_entropy.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     3767 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/cublas_wrappers.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     5441 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/cuda_util.cu
+-rw-r--r--   0 runner    (1001) docker     (123)    37586 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/dropout_kernels.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     7653 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/general_kernels.cu
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.820921 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/
+-rw-r--r--   0 runner    (1001) docker     (123)     8585 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/block_reduce.h
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/context.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/cross_entropy_layer.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1803 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/cublas_wrappers.h
+-rw-r--r--   0 runner    (1001) docker     (123)      966 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/cuda_util.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/dropout.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2232 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/feed_forward.h
+-rw-r--r--   0 runner    (1001) docker     (123)     9260 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/kernels.h
+-rw-r--r--   0 runner    (1001) docker     (123)      296 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/ls_cub.cuh
+-rw-r--r--   0 runner    (1001) docker     (123)     1905 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/normalize_layer.h
+-rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/softmax.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3043 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/strided_batch_gemm.h
+-rw-r--r--   0 runner    (1001) docker     (123)    48403 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/normalize_kernels.cu
+-rw-r--r--   0 runner    (1001) docker     (123)    13619 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/softmax_kernels.cu
+-rw-r--r--   0 runner    (1001) docker     (123)    10953 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/transform_kernels.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     4877 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/layer_norm_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)    25828 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/layer_norm_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     4003 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/moe_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)    26286 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/moe_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     5046 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_adam.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     5200 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (123)    13278 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (123)    13114 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_lamb.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     4439 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     6478 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (123)    16988 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)     4701 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)    21701 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3467 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     2066 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)    22850 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (123)    13636 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/type_shim.h
+-rw-r--r--   0 runner    (1001) docker     (123)    24865 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/flash_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2552 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10752 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6715 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/cuda_native/scaled_softmax.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.824921 colossalai-0.3.0/colossalai/kernel/jit/
+-rw-r--r--   0 runner    (1001) docker     (123)      308 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/jit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      868 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/jit/bias_dropout_add.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1359 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/jit/bias_gelu.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3478 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/jit/option.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.824921 colossalai-0.3.0/colossalai/kernel/op_builder/
+-rw-r--r--   0 runner    (1001) docker     (123)      999 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8943 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/cpu_adam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/fused_optim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/moe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1459 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/multi_head_attn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/scaled_masked_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1286 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/scaled_upper_triangle_masked_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8210 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/kernel/op_builder/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.824921 colossalai-0.3.0/colossalai/logging/
+-rw-r--r--   0 runner    (1001) docker     (123)     1597 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/logging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7511 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/logging/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.824921 colossalai-0.3.0/colossalai/nn/
+-rw-r--r--   0 runner    (1001) docker     (123)      136 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.824921 colossalai-0.3.0/colossalai/nn/_ops/
+-rw-r--r--   0 runner    (1001) docker     (123)      317 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8713 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4153 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/addmm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1152 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/batch_norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9289 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/element_wise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6666 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6607 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/embedding_bag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1105 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7122 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2641 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3143 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/_ops/view.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9552 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/init.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.828921 colossalai-0.3.0/colossalai/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2844 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/base_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.828921 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/
+-rw-r--r--   0 runner    (1001) docker     (123)      307 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      992 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6304 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5375 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/normalization.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.828921 colossalai-0.3.0/colossalai/nn/layer/moe/
+-rw-r--r--   0 runner    (1001) docker     (123)      516 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5557 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1454 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8241 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9431 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10066 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/routers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2696 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/moe/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.828921 colossalai-0.3.0/colossalai/nn/layer/parallel_1d/
+-rw-r--r--   0 runner    (1001) docker     (123)      404 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_1d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3796 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_1d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5048 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_1d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49753 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_1d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.828921 colossalai-0.3.0/colossalai/nn/layer/parallel_2d/
+-rw-r--r--   0 runner    (1001) docker     (123)      419 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34900 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      843 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    50524 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/
+-rw-r--r--   0 runner    (1001) docker     (123)      455 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37643 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    50769 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/layer/parallel_3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      455 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_3d/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    22769 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_3d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2967 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_3d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51441 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_3d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/
+-rw-r--r--   0 runner    (1001) docker     (123)      152 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6411 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      483 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10752 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/layer/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      413 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/layer/vanilla/
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/vanilla/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14535 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/vanilla/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/layer/wrapper/
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2133 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/layer/wrapper/pipeline_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.832921 colossalai-0.3.0/colossalai/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (123)     1629 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4694 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/loss/loss_1d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5728 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/loss/loss_2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5536 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/loss/loss_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6362 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/loss/loss_3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3249 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/loss/loss_moe.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.836921 colossalai-0.3.0/colossalai/nn/lr_scheduler/
+-rw-r--r--   0 runner    (1001) docker     (123)      628 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6171 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/cosine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7800 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/delayed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2867 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/multistep.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5319 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/onecycle.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2686 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/poly.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3687 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/lr_scheduler/torch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.836921 colossalai-0.3.0/colossalai/nn/metric/
+-rw-r--r--   0 runner    (1001) docker     (123)      704 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      155 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/metric/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      784 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/metric/accuracy_2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      798 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/metric/accuracy_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/metric/accuracy_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.836921 colossalai-0.3.0/colossalai/nn/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/colossalai_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7562 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/cpu_adam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6363 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/fused_adam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8926 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/fused_lamb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6094 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/fused_sgd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6889 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/hybrid_adam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4494 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/lamb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3784 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/lars.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6777 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/optimizer/nvme_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.836921 colossalai-0.3.0/colossalai/nn/parallel/
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6575 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/data_parallel.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.840921 colossalai-0.3.0/colossalai/nn/parallel/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)      883 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.840921 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/
+-rw-r--r--   0 runner    (1001) docker     (123)      721 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/base_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28600 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/cache_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8826 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2042 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/copyer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      855 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/embedding_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5922 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10057 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7481 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2020 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/colo_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1153 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1231 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4816 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/layers/module_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3875 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/nn/parallel/reducer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.840921 colossalai-0.3.0/colossalai/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)      162 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/layer_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.840921 colossalai-0.3.0/colossalai/pipeline/middleware/
+-rw-r--r--   0 runner    (1001) docker     (123)      148 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/middleware/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.840921 colossalai-0.3.0/colossalai/pipeline/middleware/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (123)       78 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/middleware/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6154 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/middleware/adaptor/fx.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7051 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/middleware/topo.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11408 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/pipelinable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5857 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/pipeline_process_group.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.844921 colossalai-0.3.0/colossalai/pipeline/rpc/
+-rw-r--r--   0 runner    (1001) docker     (123)      236 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/rpc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    59163 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/rpc/_pipeline_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14858 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/rpc/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5460 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/rpc/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9011 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/pipeline/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.844921 colossalai-0.3.0/colossalai/registry/
+-rw-r--r--   0 runner    (1001) docker     (123)      690 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3054 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/registry/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.844921 colossalai-0.3.0/colossalai/tensor/
+-rw-r--r--   0 runner    (1001) docker     (123)      946 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4449 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/colo_parameter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13185 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/colo_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22269 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/comm_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)      779 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/compute_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)      103 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/const.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.848922 colossalai-0.3.0/colossalai/tensor/d_tensor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11875 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/comm_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4944 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/d_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2863 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/layout.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25229 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/layout_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      231 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9275 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3268 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/d_tensor/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8688 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/dist_spec_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/distspec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1691 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/op_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6128 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/param_op_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10365 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/process_group.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36495 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/shape_consistency.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11617 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)      683 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/tensor_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8442 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.848922 colossalai-0.3.0/colossalai/testing/
+-rw-r--r--   0 runner    (1001) docker     (123)      718 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1974 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/testing/comparison.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/testing/pytest_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/testing/random.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8629 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/testing/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.848922 colossalai-0.3.0/colossalai/trainer/
+-rw-r--r--   0 runner    (1001) docker     (123)       53 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14778 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.848922 colossalai-0.3.0/colossalai/trainer/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2980 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/_base_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3247 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/_checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)      231 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/_commons_.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13106 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/_log_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2131 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/_lr_scheduler_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16131 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/trainer/hooks/_metric_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.848922 colossalai-0.3.0/colossalai/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     2027 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9856 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/activation_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.848922 colossalai-0.3.0/colossalai/utils/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (123)      114 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5521 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint/module_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2167 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.852922 colossalai-0.3.0/colossalai/utils/checkpoint_io/
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1967 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/constant.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9794 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/convertor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5971 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7008 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/io.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1998 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/meta.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4813 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8908 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3966 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpoint_io/writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11400 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17450 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.852922 colossalai-0.3.0/colossalai/utils/data_sampler/
+-rw-r--r--   0 runner    (1001) docker     (123)      177 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/data_sampler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/data_sampler/base_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7128 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/data_sampler/data_parallel_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6425 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.852922 colossalai-0.3.0/colossalai/utils/model/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23000 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/model/experimental.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8911 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/model/lazy_init_context.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3911 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/model/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/moe.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.852922 colossalai-0.3.0/colossalai/utils/multi_tensor_apply/
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/multi_tensor_apply/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1112 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.852922 colossalai-0.3.0/colossalai/utils/profiler/
+-rw-r--r--   0 runner    (1001) docker     (123)       52 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/extention.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.856922 colossalai-0.3.0/colossalai/utils/profiler/legacy/
+-rw-r--r--   0 runner    (1001) docker     (123)      272 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10915 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/legacy/comm_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5050 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/legacy/pcie_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3894 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/legacy/prof_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8616 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3998 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/profiler/stateful_tensor_mem_extention.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.856922 colossalai-0.3.0/colossalai/utils/rank_recorder/
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/rank_recorder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5528 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/rank_recorder/rank_recorder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.856922 colossalai-0.3.0/colossalai/utils/tensor_detector/
+-rw-r--r--   0 runner    (1001) docker     (123)       45 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/tensor_detector/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8651 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/tensor_detector/tensor_detector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4279 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/utils/timer.py
+-rw-r--r--   0 runner    (1001) docker     (123)       47 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.856922 colossalai-0.3.0/colossalai/zero/
+-rw-r--r--   0 runner    (1001) docker     (123)      512 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.856922 colossalai-0.3.0/colossalai/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (123)      601 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.856922 colossalai-0.3.0/colossalai/zero/gemini/chunk/
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/chunk/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22078 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/chunk/chunk.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9530 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/chunk/manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6762 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/chunk/search_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1534 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/chunk/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7515 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/colo_init_context.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36877 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/gemini_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2395 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/gemini_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6627 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/gemini_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14500 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/gemini_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/
+-rw-r--r--   0 runner    (1001) docker     (123)      610 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1245 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3978 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/memory_monitor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4111 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/memory_stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3582 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)      859 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3738 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4233 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10482 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4167 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/gemini/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/legacy/
+-rw-r--r--   0 runner    (1001) docker     (123)     1562 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/legacy/gemini/
+-rw-r--r--   0 runner    (1001) docker     (123)      466 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1483 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/gemini_context.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      118 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      767 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/_shard_grad_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1356 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/_shard_param_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5161 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/runtime_mem_tracer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4815 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/legacy/gemini/paramhooks/
+-rw-r--r--   0 runner    (1001) docker     (123)       77 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/paramhooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/paramhooks/_param_hookmgr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6927 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/stateful_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4252 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/stateful_tensor_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6449 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/tensor_placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3984 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/gemini/tensor_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/legacy/init_ctx/
+-rw-r--r--   0 runner    (1001) docker     (123)      171 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/init_ctx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10972 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/init_ctx/init_context.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.860922 colossalai-0.3.0/colossalai/zero/legacy/shard_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      259 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/shard_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/shard_utils/base_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2217 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/shard_utils/bucket_tensor_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)      706 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/shard_utils/commons.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2642 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/shard_utils/tensor_shard_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.864922 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/
+-rw-r--r--   0 runner    (1001) docker     (123)       75 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2722 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8357 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/reduce_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29015 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/sharded_model_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      808 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4846 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_model/zero_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.864922 colossalai-0.3.0/colossalai/zero/legacy/sharded_optim/
+-rw-r--r--   0 runner    (1001) docker     (123)       83 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18671 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_optim/sharded_optim_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.864922 colossalai-0.3.0/colossalai/zero/legacy/sharded_param/
+-rw-r--r--   0 runner    (1001) docker     (123)      131 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_param/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3869 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_param/sharded_param.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/legacy/sharded_param/sharded_tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.864922 colossalai-0.3.0/colossalai/zero/low_level/
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9957 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.864922 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      410 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/base_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1446 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/bucket_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/gradient_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3353 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/parameter_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25694 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/low_level/low_level_optim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5176 2023-05-25 08:20:55.000000 colossalai-0.3.0/colossalai/zero/wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.760918 colossalai-0.3.0/colossalai.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    25388 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    37361 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       69 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      100 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-25 08:21:07.000000 colossalai-0.3.0/colossalai.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.868922 colossalai-0.3.0/op_builder/
+-rw-r--r--   0 runner    (1001) docker     (123)      999 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8943 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/cpu_adam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/fused_optim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/moe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1459 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/multi_head_attn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/scaled_masked_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1286 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/scaled_upper_triangle_masked_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8210 2023-05-25 08:20:55.000000 colossalai-0.3.0/op_builder/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.868922 colossalai-0.3.0/requirements/
+-rw-r--r--   0 runner    (1001) docker     (123)      602 2023-05-25 08:20:55.000000 colossalai-0.3.0/requirements/requirements-test.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      100 2023-05-25 08:20:55.000000 colossalai-0.3.0/requirements/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-05-25 08:21:07.884923 colossalai-0.3.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     6229 2023-05-25 08:20:55.000000 colossalai-0.3.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.756918 colossalai-0.3.0/tests/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/components_to_test/
+-rw-r--r--   0 runner    (1001) docker     (123)      440 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2469 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/albert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1396 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/beit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3270 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/bert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2857 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/hanging_param_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1310 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/inline_op_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/nested_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1401 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/repeated_computed_layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/simple_net.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/components_to_test/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       96 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      479 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/utils/dummy_data_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/components_to_test/utils/executor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/
+-rw-r--r--   0 runner    (1001) docker     (123)      136 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/diffusers/
+-rw-r--r--   0 runner    (1001) docker     (123)       25 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/diffusers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2825 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/diffusers/diffusers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2287 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/timm/
+-rw-r--r--   0 runner    (1001) docker     (123)       20 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/timm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7513 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/timm/timm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/torchaudio/
+-rw-r--r--   0 runner    (1001) docker     (123)       26 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/torchaudio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5409 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/torchaudio/torchaudio.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/torchrec/
+-rw-r--r--   0 runner    (1001) docker     (123)       24 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/torchrec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4634 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/torchrec/torchrec.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/torchvision/
+-rw-r--r--   0 runner    (1001) docker     (123)       27 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/torchvision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5931 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/torchvision/torchvision.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/kit/model_zoo/transformers/
+-rw-r--r--   0 runner    (1001) docker     (123)       98 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4023 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/transformers/albert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4335 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/transformers/bert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2767 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/transformers/gpt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1262 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/transformers/opt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1766 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/kit/model_zoo/transformers/t5.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.872923 colossalai-0.3.0/tests/test_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.876923 colossalai-0.3.0/tests/test_analyzer/test_fx/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4330 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/test_bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2735 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/test_mod_dir.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1751 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/test_nested_ckpt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2176 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/test_shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1750 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/test_symbolic_profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1271 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_fx/zoo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.876923 colossalai-0.3.0/tests/test_analyzer/test_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3644 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_subclasses/test_aten.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1925 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_subclasses/test_flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1487 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_analyzer/test_subclasses/test_meta_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.876923 colossalai-0.3.0/tests/test_auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.876923 colossalai-0.3.0/tests/test_auto_parallel/test_pass/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_pass/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2283 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.876923 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2842 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2234 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2417 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3521 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4173 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3747 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.880923 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11050 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7786 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3932 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2053 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-25 08:21:07.884923 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11552 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7693 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4841 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6740 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6205 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10993 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9001 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12886 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3734 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11802 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3003 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8173 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4372 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13108 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8356 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2552 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2623 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18730 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3153 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4263 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9001 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12527 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12380 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2782 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3762 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14158 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3556 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8918 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5325 2023-05-25 08:20:55.000000 colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-05-25 08:20:55.000000 colossalai-0.3.0/version.txt
```

### Comparing `colossalai-0.2.8/LICENSE` & `colossalai-0.3.0/LICENSE`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/PKG-INFO` & `colossalai-0.3.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: colossalai
-Version: 0.2.8
+Version: 0.3.0
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
 Project-URL: Github, https://github.com/hpcaitech/ColossalAI
 Description: # Colossal-AI
         <div id="top" align="center">
         
            [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)
         
-           Colossal-AI: Making large AI models cheaper, faster and more accessible
+           Colossal-AI: Making large AI models cheaper, faster, and more accessible
         
            <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> |
            <a href="https://www.colossalai.org/"> Documentation </a> |
            <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> |
            <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> |
            <a href="https://medium.com/@hpcaitech"> Blog </a></h3>
         
@@ -33,26 +33,35 @@
         
            | [English](README.md) | [中文](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
         * [2023/03] [ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+        * [2023/03] [Intel and Colossal-AI Partner to Deliver Cost-Efficient Open-Source Solution for Protein Folding Structure Prediction](https://www.hpc-ai.tech/blog/intel-habana)
         * [2023/03] [AWS and Google Fund Colossal-AI with Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-google-fund-colossal-ai-with-startup-cloud-programs)
         * [2023/02] [Open Source Solution Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU Memory](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt)
         * [2023/01] [Hardware Savings Up to 46 Times for AIGC and  Automatic Parallelism](https://medium.com/pytorch/latest-colossal-ai-boasts-novel-automatic-parallelism-and-offers-savings-up-to-46x-for-stable-1453b48f3f02)
         * [2022/11] [Diffusion Pretraining and Hardware Fine-Tuning Can Be Almost 7X Cheaper](https://www.hpc-ai.tech/blog/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper)
         * [2022/10] [Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference Sequence Exceeding 10,000](https://www.hpc-ai.tech/blog/use-a-laptop-to-analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding)
         * [2022/09] [HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising](https://www.hpc-ai.tech/blog/hpc-ai-tech-completes-6-million-seed-and-angel-round-fundraising-led-by-bluerun-ventures-in-the)
         
         ## Table of Contents
         <ul>
          <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>
          <li><a href="#Features">Features</a> </li>
          <li>
+           <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
+           <ul>
+             <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
+             <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
+             <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
+           </ul>
+         </li>
+         <li>
            <a href="#Parallel-Training-Demo">Parallel Training Demo</a>
            <ul>
              <li><a href="#GPT-3">GPT-3</a></li>
              <li><a href="#GPT-2">GPT-2</a></li>
              <li><a href="#BERT">BERT</a></li>
              <li><a href="#PaLM">PaLM</a></li>
              <li><a href="#OPT">OPT</a></li>
@@ -71,22 +80,14 @@
            <a href="#Inference-Energon-AI-Demo">Inference (Energon-AI) Demo</a>
            <ul>
              <li><a href="#GPT-3-Inference">GPT-3</a></li>
              <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
              <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
-           <li>
-           <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
-           <ul>
-             <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
-             <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
-             <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
-           </ul>
-         </li>
          <li>
            <a href="#Installation">Installation</a>
            <ul>
              <li><a href="#PyPI">PyPI</a></li>
              <li><a href="#Install-From-Source">Install From Source</a></li>
            </ul>
          </li>
@@ -121,29 +122,121 @@
           - [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)
           - [Auto-Parallelism](https://arxiv.org/abs/2302.02599)
         
         - Heterogeneous Memory Management
           - [PatrickStar](https://arxiv.org/abs/2108.05818)
         
         - Friendly Usage
-          - Parallelism based on configuration file
+          - Parallelism based on the configuration file
         
         - Inference
           - [Energon-AI](https://github.com/hpcaitech/EnergonAI)
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
+        ## Colossal-AI in the Real World
+        
+        ### ColossalChat
+        
+        <div align="center">
+           <a href="https://www.youtube.com/watch?v=HcTiHzApHm0">
+           <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png" width="700" />
+           </a>
+        </div>
+        
+        [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)
+        [[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+        [[demo]](https://www.youtube.com/watch?v=HcTiHzApHm0)
+        [[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)
+        
+        <p id="ColossalChat-Speed" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg" width=450/>
+        </p>
+        
+        - Up to 10 times faster for RLHF PPO Stage3 Training
+        
+        <p id="ColossalChat_scaling" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>
+        </p>
+        
+        - Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference
+        
+        <p id="ColossalChat-1GPU" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>
+        </p>
+        
+        - Up to 10.3x growth in model capacity on one GPU
+        - A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)
+        
+        <p id="ColossalChat-LoRA" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>
+        </p>
+        
+        - Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU
+        - Keep at a sufficiently high running speed
+        
+        <p align="right">(<a href="#top">back to top</a>)</p>
+        
+        
+        ### AIGC
+        Acceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).
+        <p id="diffusion_train" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>
+        </p>
+        
+        - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).
+        
+        <p id="diffusion_demo" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>
+        </p>
+        
+        - [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.
+        
+        <p id="inference" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>
+        </p>
+        
+        - [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.
+        
+        
+        <p align="right">(<a href="#top">back to top</a>)</p>
+        
+        ### Biomedicine
+        Acceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)
+        
+        <p id="FastFold" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>
+        </p>
+        
+        - [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.
+        
+        <p id="FastFold-Intel" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>
+        </p>
+        
+        - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.
+        
+        <p id="xTrimoMultimer" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>
+        </p>
+        
+        - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
+        
+        
+        <p align="right">(<a href="#top">back to top</a>)</p>
+        
         ## Parallel Training Demo
         
         ### GPT-3
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png" width=700/>
         </p>
         
-        - Save 50% GPU resources, and 10.7% acceleration
+        - Save 50% GPU resources and 10.7% acceleration
         
         ### GPT-2
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png" width=800/>
         
         - 11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism
         
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png" width=800>
@@ -157,15 +250,15 @@
         
         ### PaLM
         - [PaLM-colossalai](https://github.com/hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)).
         
         ### OPT
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png" width=800/>
         
-        - [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because public pretrained model weights.
+        - [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.
         - 45% speedup fine-tuning OPT at low cost in lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/opt_service)
         
         Please visit our [documentation](https://www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more details.
         
         ### ViT
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png" width="450" />
@@ -221,104 +314,24 @@
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
         </p>
         
         - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
-        ## Colossal-AI in the Real World
-        
-        ### ColossalChat
-        
-        <div align="center">
-           <a href="https://chat.colossalai.org/">
-           <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Chat-demo.png" width="700" />
-           </a>
-        </div>
-        
-        [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline. [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) [[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://chat.colossalai.org)
-        
-        <p id="ColossalChat_scaling" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>
-        </p>
-        
-        - Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference
-        
-        <p id="ColossalChat-1GPU" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>
-        </p>
-        
-        - Up to 10.3x growth in model capacity on one GPU
-        - A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)
-        
-        <p id="ColossalChat-LoRA" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>
-        </p>
-        
-        - Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU
-        - Keep in a sufficiently high running speed
-        
-        <p align="right">(<a href="#top">back to top</a>)</p>
-        
-        
-        ### AIGC
-        Acceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).
-        <p id="diffusion_train" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>
-        </p>
-        
-        - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).
-        
-        <p id="diffusion_demo" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>
-        </p>
-        
-        - [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.
-        
-        <p id="inference" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>
-        </p>
-        
-        - [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.
-        
-        
-        <p align="right">(<a href="#top">back to top</a>)</p>
-        
-        ### Biomedicine
-        Acceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)
-        
-        <p id="FastFold" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>
-        </p>
-        
-        - [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.
-        
-        <p id="FastFold-Intel" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>
-        </p>
-        
-        - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.
-        
-        <p id="xTrimoMultimer" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>
-        </p>
-        
-        - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
-        
-        
-        <p align="right">(<a href="#top">back to top</a>)</p>
-        
         ## Installation
         
         Requirements:
         - PyTorch >= 1.11 (PyTorch 2.x in progress)
         - Python >= 3.7
         - CUDA >= 11.0
+        - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
+        - Linux OS
         
-        If you encounter any problem about installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+        If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
         
         ### Install from PyPI
         
         You can easily install Colossal-AI with the following command. **By default, we do not build PyTorch extensions during installation.**
         
         ```bash
         pip install colossalai
@@ -328,26 +341,26 @@
         
         However, if you want to build the PyTorch extensions during installation, you can set `CUDA_EXT=1`.
         
         ```bash
         CUDA_EXT=1 pip install colossalai
         ```
         
-        **Otherwise, CUDA kernels will be built during runtime when you actually need it.**
+        **Otherwise, CUDA kernels will be built during runtime when you actually need them.**
         
-        We also keep release the nightly version to PyPI on a weekly basis. This allows you to access the unreleased features and bug fixes in the main branch.
+        We also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch.
         Installation can be made via
         
         ```bash
         pip install colossalai-nightly
         ```
         
         ### Download From Source
         
-        > The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problem. :)
+        > The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)
         
         ```shell
         git clone https://github.com/hpcaitech/ColossalAI.git
         cd ColossalAI
         
         # install colossalai
         pip install .
@@ -356,14 +369,30 @@
         By default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
         If you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):
         
         ```shell
         CUDA_EXT=1 pip install .
         ```
         
+        For Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.
+        
+        ```bash
+        # clone the repository
+        git clone https://github.com/hpcaitech/ColossalAI.git
+        cd ColossalAI
+        
+        # download the cub library
+        wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip
+        unzip 1.8.0.zip
+        cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/
+        
+        # install
+        CUDA_EXT=1 pip install .
+        ```
+        
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Use Docker
         
         ### Pull from DockerHub
         
         You can directly pull the docker image from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The image is automatically uploaded upon release.
@@ -402,17 +431,18 @@
         You may contact us or participate in the following ways:
         1. [Leaving a Star ⭐](https://github.com/hpcaitech/ColossalAI/stargazers) to show your like and support. Thanks!
         2. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/CONTRIBUTING.md)
         3. Send your official proposal to email contact@hpcaitech.com
         
         Thanks so much to all of our amazing contributors!
         
-        <a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors"><img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/contributor_avatar.png" width="800px"></a>
+        <a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors">
+          <img src="https://contrib.rocks/image?repo=hpcaitech/ColossalAI"  width="800px"/>
+        </a>
         
-        *The order of contributor avatars is randomly shuffled.*
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## CI/CD
         
         We leverage the power of [GitHub Actions](https://github.com/features/actions) to automate our development, release and deployment workflows. Please check out this [documentation](.github/workflows/README.md) on how the automated workflows are operated.
@@ -429,15 +459,15 @@
           title={Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
           author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
           journal={arXiv preprint arXiv:2110.14883},
           year={2021}
         }
         ```
         
-        Colossal-AI has been accepted as official tutorials by top conference [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
+        Colossal-AI has been accepted as official tutorial by top conferences [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Environment :: GPU :: NVIDIA CUDA
```

#### html2text {}

```diff
@@ -1,18 +1,18 @@
-Metadata-Version: 2.1 Name: colossalai Version: 0.2.8 Summary: An integrated
+Metadata-Version: 2.1 Name: colossalai Version: 0.3.0 Summary: An integrated
 large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
    [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/
   colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/
-   ) Colossal-AI: Making large AI models cheaper, faster and more accessible
+  ) Colossal-AI: Making large AI models cheaper, faster, and more accessible
            **** Paper | Documentation | Examples | Forum | Blog ****
      [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/
 ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)
      [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/
   build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/
       actions/workflows/build_on_schedule.yml) [![Documentation](https://
      readthedocs.org/projects/colossalai/badge/?version=latest)](https://
@@ -25,32 +25,39 @@
   shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w) [![WeChat badge](https://
        img.shields.io/badge/å¾®ä¿¡-å å¥-green?logo=wechat&)](https://
     raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
     WeChat.png) | [English](README.md) | [ä¸­æ](docs/README-zh-Hans.md) |
 ## Latest News * [2023/03] [ColossalChat: An Open-Source Solution for Cloning
 ChatGPT With a Complete RLHF Pipeline](https://medium.com/@yangyou_berkeley/
 colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-
-pipeline-5edf08fb538b) * [2023/03] [AWS and Google Fund Colossal-AI with
-Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-google-fund-
-colossal-ai-with-startup-cloud-programs) * [2023/02] [Open Source Solution
-Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU Memory]
-(https://www.hpc-ai.tech/blog/colossal-ai-chatgpt) * [2023/01] [Hardware
+pipeline-5edf08fb538b) * [2023/03] [Intel and Colossal-AI Partner to Deliver
+Cost-Efficient Open-Source Solution for Protein Folding Structure Prediction]
+(https://www.hpc-ai.tech/blog/intel-habana) * [2023/03] [AWS and Google Fund
+Colossal-AI with Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-
+google-fund-colossal-ai-with-startup-cloud-programs) * [2023/02] [Open Source
+Solution Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU
+Memory](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt) * [2023/01] [Hardware
 Savings Up to 46 Times for AIGC and Automatic Parallelism](https://medium.com/
 pytorch/latest-colossal-ai-boasts-novel-automatic-parallelism-and-offers-
 savings-up-to-46x-for-stable-1453b48f3f02) * [2022/11] [Diffusion Pretraining
 and Hardware Fine-Tuning Can Be Almost 7X Cheaper](https://www.hpc-ai.tech/
 blog/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper) *
 [2022/10] [Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference
 Sequence Exceeding 10,000](https://www.hpc-ai.tech/blog/use-a-laptop-to-
 analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding) * [2022/
 09] [HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising](https:/
 /www.hpc-ai.tech/blog/hpc-ai-tech-completes-6-million-seed-and-angel-round-
 fundraising-led-by-bluerun-ventures-in-the) ## Table of Contents
     * Why_Colossal-AI
     * Features
+    * Colossal-AI_for_Real_World_Applications
+          o ColossalChat:_An_Open-Source_Solution_for_Cloning_ChatGPT_With_a
+            Complete_RLHF_Pipeline
+          o AIGC:_Acceleration_of_Stable_Diffusion
+          o Biomedicine:_Acceleration_of_AlphaFold_Protein_Structure
     * Parallel_Training_Demo
           o GPT-3
           o GPT-2
           o BERT
           o PaLM
           o OPT
           o ViT
@@ -58,19 +65,14 @@
     * Single_GPU_Training_Demo
           o GPT-2
           o PaLM
     * Inference_(Energon-AI)_Demo
           o GPT-3
           o OPT-175B_Online_Serving_for_Text_Generation
           o 176B_BLOOM
-    * Colossal-AI_for_Real_World_Applications
-          o ColossalChat:_An_Open-Source_Solution_for_Cloning_ChatGPT_With_a
-            Complete_RLHF_Pipeline
-          o AIGC:_Acceleration_of_Stable_Diffusion
-          o Biomedicine:_Acceleration_of_AlphaFold_Protein_Structure
     * Installation
           o PyPI
           o Install_From_Source
     * Use_Docker
     * Community
     * Contributing
     * Cite_Us
@@ -85,96 +87,43 @@
 kickstart distributed training and inference in a few lines. - Parallelism
 strategies - Data Parallelism - Pipeline Parallelism - 1D, [2D](https://
 arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D]
 (https://arxiv.org/abs/2105.14450) Tensor Parallelism - [Sequence Parallelism]
 (https://arxiv.org/abs/2105.13120) - [Zero Redundancy Optimizer (ZeRO)](https:/
 /arxiv.org/abs/1910.02054) - [Auto-Parallelism](https://arxiv.org/abs/
 2302.02599) - Heterogeneous Memory Management - [PatrickStar](https://
-arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on configuration
-file - Inference - [Energon-AI](https://github.com/hpcaitech/EnergonAI)
-                                                                  (back_to_top)
-## Parallel Training Demo ### GPT-3
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                 GPT3-v5.png]
-- Save 50% GPU resources, and 10.7% acceleration ### GPT-2 [https://
-raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png]
-- 11x lower GPU memory consumption, and superlinear scaling efficiency with
-Tensor Parallelism [https://raw.githubusercontent.com/hpcaitech/public_assets/
-main/colossalai/img/(updated)GPT-2.png] - 24x larger model size on the same
-hardware - over 3x acceleration ### BERT [https://raw.githubusercontent.com/
-hpcaitech/public_assets/main/colossalai/img/BERT.png] - 2x faster training, or
-50% longer sequence length ### PaLM - [PaLM-colossalai](https://github.com/
-hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways
-Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-
-model-palm-scaling-to.html)). ### OPT [https://raw.githubusercontent.com/
-hpcaitech/public_assets/main/colossalai/img/OPT_update.png] - [Open Pretrained
-Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion
-parameter AI language model released by Meta, which stimulates AI programmers
-to perform various downstream tasks and application deployments because public
-pretrained model weights. - 45% speedup fine-tuning OPT at low cost in lines. [
-[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/
-opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/
-opt_service) Please visit our [documentation](https://www.colossalai.org/) and
-[examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more
-details. ### ViT
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                   ViT.png]
-- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64 ###
-Recommendation System Models - [Cached Embedding](https://github.com/hpcaitech/
-CachedEmbedding), utilize software cache to train larger embedding tables with
-a smaller GPU memory budget.
-                                                                  (back_to_top)
-## Single GPU Training Demo ### GPT-2
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                GPT2-GPU1.png]
-- 20x larger model size on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                GPT2-NVME.png]
-- 120x larger model size on the same hardware (RTX 3080) ### PaLM
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                PaLM-GPU1.png]
-- 34x larger model size on the same hardware
-                                                                  (back_to_top)
-## Inference (Energon-AI) Demo
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             inference_GPT-3.jpg]
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
-acceleration on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             BLOOM%20serving.png]
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
-Try 175-billion-parameter OPT online services
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                            BLOOM%20Inference.PNG]
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
-Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
-times.
+arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on the
+configuration file - Inference - [Energon-AI](https://github.com/hpcaitech/
+EnergonAI)
                                                                   (back_to_top)
 ## Colossal-AI in the Real World ### ColossalChat
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                Chat-demo.png]
+ [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
+                       chat/ColossalChat%20YouTube.png]
 [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/
 Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/
 chatgpt/) with a complete RLHF pipeline. [[code]](https://github.com/hpcaitech/
 ColossalAI/tree/main/applications/Chat) [[blog]](https://medium.com/
 @yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-
-with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://
-chat.colossalai.org)
+with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://www.youtube.com/
+watch?v=HcTiHzApHm0) [[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)
+ [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
+                        chat/ColossalChat%20Speed.jpg]
+- Up to 10 times faster for RLHF PPO Stage3 Training
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                         chatgpt/ChatGPT%20scaling.png]
 - Up to 7.73 times faster for single server training and 1.42 times faster for
 single-GPU inference
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                            chatgpt/ChatGPT-1GPU.jpg]
 - Up to 10.3x growth in model capacity on one GPU - A mini demo training
 process requires only 1.62GB of GPU memory (any consumer-grade GPU)
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                            chatgpt/LoRA%20data.jpg]
 - Increase the capacity of the fine-tuning model by up to 3.7 times on a single
-GPU - Keep in a sufficiently high running speed
+GPU - Keep at a sufficiently high running speed
                                                                   (back_to_top)
 ### AIGC Acceleration of AIGC (AI-Generated Content) models such as [Stable
 Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable
 Diffusion v2](https://github.com/Stability-AI/stablediffusion).
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                          Stable%20Diffusion%20v2.png]
 - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/
@@ -202,34 +151,99 @@
 - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference
 acceleration and 39% cost reduce.
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                            xTrimoMultimer_Table.jpg]
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer):
 accelerating structure prediction of protein monomers and multimer by 11x.
                                                                   (back_to_top)
+## Parallel Training Demo ### GPT-3
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                 GPT3-v5.png]
+- Save 50% GPU resources and 10.7% acceleration ### GPT-2 [https://
+raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png]
+- 11x lower GPU memory consumption, and superlinear scaling efficiency with
+Tensor Parallelism [https://raw.githubusercontent.com/hpcaitech/public_assets/
+main/colossalai/img/(updated)GPT-2.png] - 24x larger model size on the same
+hardware - over 3x acceleration ### BERT [https://raw.githubusercontent.com/
+hpcaitech/public_assets/main/colossalai/img/BERT.png] - 2x faster training, or
+50% longer sequence length ### PaLM - [PaLM-colossalai](https://github.com/
+hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways
+Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-
+model-palm-scaling-to.html)). ### OPT [https://raw.githubusercontent.com/
+hpcaitech/public_assets/main/colossalai/img/OPT_update.png] - [Open Pretrained
+Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion
+parameter AI language model released by Meta, which stimulates AI programmers
+to perform various downstream tasks and application deployments because of
+public pre-trained model weights. - 45% speedup fine-tuning OPT at low cost in
+lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/
+language/opt) [[Online Serving]](https://colossalai.org/docs/
+advanced_tutorials/opt_service) Please visit our [documentation](https://
+www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/
+tree/main/examples) for more details. ### ViT
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                   ViT.png]
+- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64 ###
+Recommendation System Models - [Cached Embedding](https://github.com/hpcaitech/
+CachedEmbedding), utilize software cache to train larger embedding tables with
+a smaller GPU memory budget.
+                                                                  (back_to_top)
+## Single GPU Training Demo ### GPT-2
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                GPT2-GPU1.png]
+- 20x larger model size on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                GPT2-NVME.png]
+- 120x larger model size on the same hardware (RTX 3080) ### PaLM
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                PaLM-GPU1.png]
+- 34x larger model size on the same hardware
+                                                                  (back_to_top)
+## Inference (Energon-AI) Demo
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             inference_GPT-3.jpg]
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
+acceleration on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             BLOOM%20serving.png]
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
+Try 175-billion-parameter OPT online services
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                            BLOOM%20Inference.PNG]
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
+Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
+times.
+                                                                  (back_to_top)
 ## Installation Requirements: - PyTorch >= 1.11 (PyTorch 2.x in progress) -
-Python >= 3.7 - CUDA >= 11.0 If you encounter any problem about installation,
-you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/
-issues/new/choose) in this repository. ### Install from PyPI You can easily
-install Colossal-AI with the following command. **By default, we do not build
-PyTorch extensions during installation.** ```bash pip install colossalai ```
-**Note: only Linux is supported for now.** However, if you want to build the
-PyTorch extensions during installation, you can set `CUDA_EXT=1`. ```bash
-CUDA_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
-during runtime when you actually need it.** We also keep release the nightly
-version to PyPI on a weekly basis. This allows you to access the unreleased
-features and bug fixes in the main branch. Installation can be made via ```bash
-pip install colossalai-nightly ``` ### Download From Source > The version of
-Colossal-AI will be in line with the main branch of the repository. Feel free
-to raise an issue if you encounter any problem. :) ```shell git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
-install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
-build them during runtime. If you want to install and enable CUDA kernel fusion
-(compulsory installation when using fused optimizer): ```shell CUDA_EXT=1 pip
-install . ```
+Python >= 3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
+developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
+you encounter any problem with installation, you may want to raise an [issue]
+(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+### Install from PyPI You can easily install Colossal-AI with the following
+command. **By default, we do not build PyTorch extensions during
+installation.** ```bash pip install colossalai ``` **Note: only Linux is
+supported for now.** However, if you want to build the PyTorch extensions
+during installation, you can set `CUDA_EXT=1`. ```bash CUDA_EXT=1 pip install
+colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
+actually need them.** We also keep releasing the nightly version to PyPI every
+week. This allows you to access the unreleased features and bug fixes in the
+main branch. Installation can be made via ```bash pip install colossalai-
+nightly ``` ### Download From Source > The version of Colossal-AI will be in
+line with the main branch of the repository. Feel free to raise an issue if you
+encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
+ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
+we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
+If you want to install and enable CUDA kernel fusion (compulsory installation
+when using fused optimizer): ```shell CUDA_EXT=1 pip install . ``` For Users
+with CUDA 10.2, you can still build ColossalAI from source. However, you need
+to manually download the cub library and copy it to the corresponding
+directory. ```bash # clone the repository git clone https://github.com/
+hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
+github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
+1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
+CUDA_EXT=1 pip install . ```
                                                                   (back_to_top)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
@@ -252,32 +266,30 @@
 join and build the Colossal-AI community, making efforts towards the era of big
 AI models! You may contact us or participate in the following ways: 1. [Leaving
 a Star â­](https://github.com/hpcaitech/ColossalAI/stargazers) to show your
 like and support. Thanks! 2. Posting an [issue](https://github.com/hpcaitech/
 ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the
 guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/
 CONTRIBUTING.md) 3. Send your official proposal to email contact@hpcaitech.com
-Thanks so much to all of our amazing contributors! [https://
-raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-contributor_avatar.png] *The order of contributor avatars is randomly
-shuffled.*
+Thanks so much to all of our amazing contributors! [https://contrib.rocks/
+image?repo=hpcaitech/ColossalAI]
                                                                   (back_to_top)
 ## CI/CD We leverage the power of [GitHub Actions](https://github.com/features/
 actions) to automate our development, release and deployment workflows. Please
 check out this [documentation](.github/workflows/README.md) on how the
 automated workflows are operated. ## Cite Us This project is inspired by some
 related projects (some by our team and some by other organizations). We would
 like to credit these amazing projects as listed in the [Reference List](./docs/
 REFERENCE.md). To cite this project, you can use the following BibTeX citation.
 ``` @article{bian2021colossal, title={Colossal-AI: A Unified Deep Learning
 System For Large-Scale Parallel Training}, author={Bian, Zhengda and Liu,
 Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui
 and Cui, Fan and You, Yang}, journal={arXiv preprint arXiv:2110.14883}, year=
-{2021} } ``` Colossal-AI has been accepted as official tutorials by top
-conference [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/
+{2021} } ``` Colossal-AI has been accepted as official tutorial by top
+conferences [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/
 Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://
 cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
                                                                   (back_to_top)
 Platform: UNKNOWN Classifier: Programming Language :: Python :: 3 Classifier:
 License :: OSI Approved :: Apache Software License Classifier: Environment ::
 GPU :: NVIDIA CUDA Classifier: Topic :: Scientific/Engineering :: Artificial
 Intelligence Classifier: Topic :: System :: Distributed Computing Requires-
```

### Comparing `colossalai-0.2.8/README.md` & `colossalai-0.3.0/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Colossal-AI
 <div id="top" align="center">
 
    [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)
 
-   Colossal-AI: Making large AI models cheaper, faster and more accessible
+   Colossal-AI: Making large AI models cheaper, faster, and more accessible
 
    <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> |
    <a href="https://www.colossalai.org/"> Documentation </a> |
    <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> |
    <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> |
    <a href="https://medium.com/@hpcaitech"> Blog </a></h3>
 
@@ -22,26 +22,35 @@
 
    | [English](README.md) | [中文](docs/README-zh-Hans.md) |
 
 </div>
 
 ## Latest News
 * [2023/03] [ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+* [2023/03] [Intel and Colossal-AI Partner to Deliver Cost-Efficient Open-Source Solution for Protein Folding Structure Prediction](https://www.hpc-ai.tech/blog/intel-habana)
 * [2023/03] [AWS and Google Fund Colossal-AI with Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-google-fund-colossal-ai-with-startup-cloud-programs)
 * [2023/02] [Open Source Solution Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU Memory](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt)
 * [2023/01] [Hardware Savings Up to 46 Times for AIGC and  Automatic Parallelism](https://medium.com/pytorch/latest-colossal-ai-boasts-novel-automatic-parallelism-and-offers-savings-up-to-46x-for-stable-1453b48f3f02)
 * [2022/11] [Diffusion Pretraining and Hardware Fine-Tuning Can Be Almost 7X Cheaper](https://www.hpc-ai.tech/blog/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper)
 * [2022/10] [Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference Sequence Exceeding 10,000](https://www.hpc-ai.tech/blog/use-a-laptop-to-analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding)
 * [2022/09] [HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising](https://www.hpc-ai.tech/blog/hpc-ai-tech-completes-6-million-seed-and-angel-round-fundraising-led-by-bluerun-ventures-in-the)
 
 ## Table of Contents
 <ul>
  <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>
  <li><a href="#Features">Features</a> </li>
  <li>
+   <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
+   <ul>
+     <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
+     <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
+     <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
+   </ul>
+ </li>
+ <li>
    <a href="#Parallel-Training-Demo">Parallel Training Demo</a>
    <ul>
      <li><a href="#GPT-3">GPT-3</a></li>
      <li><a href="#GPT-2">GPT-2</a></li>
      <li><a href="#BERT">BERT</a></li>
      <li><a href="#PaLM">PaLM</a></li>
      <li><a href="#OPT">OPT</a></li>
@@ -60,22 +69,14 @@
    <a href="#Inference-Energon-AI-Demo">Inference (Energon-AI) Demo</a>
    <ul>
      <li><a href="#GPT-3-Inference">GPT-3</a></li>
      <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
      <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
    </ul>
  </li>
-   <li>
-   <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
-   <ul>
-     <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
-     <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
-     <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
-   </ul>
- </li>
  <li>
    <a href="#Installation">Installation</a>
    <ul>
      <li><a href="#PyPI">PyPI</a></li>
      <li><a href="#Install-From-Source">Install From Source</a></li>
    </ul>
  </li>
@@ -110,29 +111,121 @@
   - [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)
   - [Auto-Parallelism](https://arxiv.org/abs/2302.02599)
 
 - Heterogeneous Memory Management
   - [PatrickStar](https://arxiv.org/abs/2108.05818)
 
 - Friendly Usage
-  - Parallelism based on configuration file
+  - Parallelism based on the configuration file
 
 - Inference
   - [Energon-AI](https://github.com/hpcaitech/EnergonAI)
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
+## Colossal-AI in the Real World
+
+### ColossalChat
+
+<div align="center">
+   <a href="https://www.youtube.com/watch?v=HcTiHzApHm0">
+   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png" width="700" />
+   </a>
+</div>
+
+[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.
+[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)
+[[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+[[demo]](https://www.youtube.com/watch?v=HcTiHzApHm0)
+[[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)
+
+<p id="ColossalChat-Speed" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg" width=450/>
+</p>
+
+- Up to 10 times faster for RLHF PPO Stage3 Training
+
+<p id="ColossalChat_scaling" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>
+</p>
+
+- Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference
+
+<p id="ColossalChat-1GPU" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>
+</p>
+
+- Up to 10.3x growth in model capacity on one GPU
+- A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)
+
+<p id="ColossalChat-LoRA" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>
+</p>
+
+- Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU
+- Keep at a sufficiently high running speed
+
+<p align="right">(<a href="#top">back to top</a>)</p>
+
+
+### AIGC
+Acceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).
+<p id="diffusion_train" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>
+</p>
+
+- [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).
+
+<p id="diffusion_demo" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>
+</p>
+
+- [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.
+
+<p id="inference" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>
+</p>
+
+- [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.
+
+
+<p align="right">(<a href="#top">back to top</a>)</p>
+
+### Biomedicine
+Acceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)
+
+<p id="FastFold" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>
+</p>
+
+- [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.
+
+<p id="FastFold-Intel" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>
+</p>
+
+- [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.
+
+<p id="xTrimoMultimer" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>
+</p>
+
+- [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
+
+
+<p align="right">(<a href="#top">back to top</a>)</p>
+
 ## Parallel Training Demo
 
 ### GPT-3
 <p align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png" width=700/>
 </p>
 
-- Save 50% GPU resources, and 10.7% acceleration
+- Save 50% GPU resources and 10.7% acceleration
 
 ### GPT-2
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png" width=800/>
 
 - 11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism
 
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png" width=800>
@@ -146,15 +239,15 @@
 
 ### PaLM
 - [PaLM-colossalai](https://github.com/hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)).
 
 ### OPT
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png" width=800/>
 
-- [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because public pretrained model weights.
+- [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.
 - 45% speedup fine-tuning OPT at low cost in lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/opt_service)
 
 Please visit our [documentation](https://www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more details.
 
 ### ViT
 <p align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png" width="450" />
@@ -210,104 +303,24 @@
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
 </p>
 
 - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
-## Colossal-AI in the Real World
-
-### ColossalChat
-
-<div align="center">
-   <a href="https://chat.colossalai.org/">
-   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Chat-demo.png" width="700" />
-   </a>
-</div>
-
-[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline. [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) [[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://chat.colossalai.org)
-
-<p id="ColossalChat_scaling" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>
-</p>
-
-- Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference
-
-<p id="ColossalChat-1GPU" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>
-</p>
-
-- Up to 10.3x growth in model capacity on one GPU
-- A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)
-
-<p id="ColossalChat-LoRA" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>
-</p>
-
-- Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU
-- Keep in a sufficiently high running speed
-
-<p align="right">(<a href="#top">back to top</a>)</p>
-
-
-### AIGC
-Acceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).
-<p id="diffusion_train" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>
-</p>
-
-- [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).
-
-<p id="diffusion_demo" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>
-</p>
-
-- [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.
-
-<p id="inference" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>
-</p>
-
-- [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.
-
-
-<p align="right">(<a href="#top">back to top</a>)</p>
-
-### Biomedicine
-Acceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)
-
-<p id="FastFold" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>
-</p>
-
-- [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.
-
-<p id="FastFold-Intel" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>
-</p>
-
-- [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.
-
-<p id="xTrimoMultimer" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>
-</p>
-
-- [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
-
-
-<p align="right">(<a href="#top">back to top</a>)</p>
-
 ## Installation
 
 Requirements:
 - PyTorch >= 1.11 (PyTorch 2.x in progress)
 - Python >= 3.7
 - CUDA >= 11.0
+- [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
+- Linux OS
 
-If you encounter any problem about installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
 
 ### Install from PyPI
 
 You can easily install Colossal-AI with the following command. **By default, we do not build PyTorch extensions during installation.**
 
 ```bash
 pip install colossalai
@@ -317,26 +330,26 @@
 
 However, if you want to build the PyTorch extensions during installation, you can set `CUDA_EXT=1`.
 
 ```bash
 CUDA_EXT=1 pip install colossalai
 ```
 
-**Otherwise, CUDA kernels will be built during runtime when you actually need it.**
+**Otherwise, CUDA kernels will be built during runtime when you actually need them.**
 
-We also keep release the nightly version to PyPI on a weekly basis. This allows you to access the unreleased features and bug fixes in the main branch.
+We also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch.
 Installation can be made via
 
 ```bash
 pip install colossalai-nightly
 ```
 
 ### Download From Source
 
-> The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problem. :)
+> The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)
 
 ```shell
 git clone https://github.com/hpcaitech/ColossalAI.git
 cd ColossalAI
 
 # install colossalai
 pip install .
@@ -345,14 +358,30 @@
 By default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
 If you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):
 
 ```shell
 CUDA_EXT=1 pip install .
 ```
 
+For Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.
+
+```bash
+# clone the repository
+git clone https://github.com/hpcaitech/ColossalAI.git
+cd ColossalAI
+
+# download the cub library
+wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip
+unzip 1.8.0.zip
+cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/
+
+# install
+CUDA_EXT=1 pip install .
+```
+
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 ## Use Docker
 
 ### Pull from DockerHub
 
 You can directly pull the docker image from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The image is automatically uploaded upon release.
@@ -391,17 +420,18 @@
 You may contact us or participate in the following ways:
 1. [Leaving a Star ⭐](https://github.com/hpcaitech/ColossalAI/stargazers) to show your like and support. Thanks!
 2. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/CONTRIBUTING.md)
 3. Send your official proposal to email contact@hpcaitech.com
 
 Thanks so much to all of our amazing contributors!
 
-<a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors"><img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/contributor_avatar.png" width="800px"></a>
+<a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors">
+  <img src="https://contrib.rocks/image?repo=hpcaitech/ColossalAI"  width="800px"/>
+</a>
 
-*The order of contributor avatars is randomly shuffled.*
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 
 ## CI/CD
 
 We leverage the power of [GitHub Actions](https://github.com/features/actions) to automate our development, release and deployment workflows. Please check out this [documentation](.github/workflows/README.md) on how the automated workflows are operated.
@@ -418,10 +448,10 @@
   title={Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
   author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
   journal={arXiv preprint arXiv:2110.14883},
   year={2021}
 }
 ```
 
-Colossal-AI has been accepted as official tutorials by top conference [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
+Colossal-AI has been accepted as official tutorial by top conferences [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
 
 <p align="right">(<a href="#top">back to top</a>)</p>
```

#### html2text {}

```diff
@@ -1,11 +1,11 @@
 # Colossal-AI
    [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/
   colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/
-   ) Colossal-AI: Making large AI models cheaper, faster and more accessible
+  ) Colossal-AI: Making large AI models cheaper, faster, and more accessible
            **** Paper | Documentation | Examples | Forum | Blog ****
      [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/
 ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)
      [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/
   build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/
       actions/workflows/build_on_schedule.yml) [![Documentation](https://
      readthedocs.org/projects/colossalai/badge/?version=latest)](https://
@@ -18,32 +18,39 @@
   shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w) [![WeChat badge](https://
        img.shields.io/badge/å¾®ä¿¡-å å¥-green?logo=wechat&)](https://
     raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
     WeChat.png) | [English](README.md) | [ä¸­æ](docs/README-zh-Hans.md) |
 ## Latest News * [2023/03] [ColossalChat: An Open-Source Solution for Cloning
 ChatGPT With a Complete RLHF Pipeline](https://medium.com/@yangyou_berkeley/
 colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-
-pipeline-5edf08fb538b) * [2023/03] [AWS and Google Fund Colossal-AI with
-Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-google-fund-
-colossal-ai-with-startup-cloud-programs) * [2023/02] [Open Source Solution
-Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU Memory]
-(https://www.hpc-ai.tech/blog/colossal-ai-chatgpt) * [2023/01] [Hardware
+pipeline-5edf08fb538b) * [2023/03] [Intel and Colossal-AI Partner to Deliver
+Cost-Efficient Open-Source Solution for Protein Folding Structure Prediction]
+(https://www.hpc-ai.tech/blog/intel-habana) * [2023/03] [AWS and Google Fund
+Colossal-AI with Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-
+google-fund-colossal-ai-with-startup-cloud-programs) * [2023/02] [Open Source
+Solution Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU
+Memory](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt) * [2023/01] [Hardware
 Savings Up to 46 Times for AIGC and Automatic Parallelism](https://medium.com/
 pytorch/latest-colossal-ai-boasts-novel-automatic-parallelism-and-offers-
 savings-up-to-46x-for-stable-1453b48f3f02) * [2022/11] [Diffusion Pretraining
 and Hardware Fine-Tuning Can Be Almost 7X Cheaper](https://www.hpc-ai.tech/
 blog/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper) *
 [2022/10] [Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference
 Sequence Exceeding 10,000](https://www.hpc-ai.tech/blog/use-a-laptop-to-
 analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding) * [2022/
 09] [HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising](https:/
 /www.hpc-ai.tech/blog/hpc-ai-tech-completes-6-million-seed-and-angel-round-
 fundraising-led-by-bluerun-ventures-in-the) ## Table of Contents
     * Why_Colossal-AI
     * Features
+    * Colossal-AI_for_Real_World_Applications
+          o ColossalChat:_An_Open-Source_Solution_for_Cloning_ChatGPT_With_a
+            Complete_RLHF_Pipeline
+          o AIGC:_Acceleration_of_Stable_Diffusion
+          o Biomedicine:_Acceleration_of_AlphaFold_Protein_Structure
     * Parallel_Training_Demo
           o GPT-3
           o GPT-2
           o BERT
           o PaLM
           o OPT
           o ViT
@@ -51,19 +58,14 @@
     * Single_GPU_Training_Demo
           o GPT-2
           o PaLM
     * Inference_(Energon-AI)_Demo
           o GPT-3
           o OPT-175B_Online_Serving_for_Text_Generation
           o 176B_BLOOM
-    * Colossal-AI_for_Real_World_Applications
-          o ColossalChat:_An_Open-Source_Solution_for_Cloning_ChatGPT_With_a
-            Complete_RLHF_Pipeline
-          o AIGC:_Acceleration_of_Stable_Diffusion
-          o Biomedicine:_Acceleration_of_AlphaFold_Protein_Structure
     * Installation
           o PyPI
           o Install_From_Source
     * Use_Docker
     * Community
     * Contributing
     * Cite_Us
@@ -78,96 +80,43 @@
 kickstart distributed training and inference in a few lines. - Parallelism
 strategies - Data Parallelism - Pipeline Parallelism - 1D, [2D](https://
 arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D]
 (https://arxiv.org/abs/2105.14450) Tensor Parallelism - [Sequence Parallelism]
 (https://arxiv.org/abs/2105.13120) - [Zero Redundancy Optimizer (ZeRO)](https:/
 /arxiv.org/abs/1910.02054) - [Auto-Parallelism](https://arxiv.org/abs/
 2302.02599) - Heterogeneous Memory Management - [PatrickStar](https://
-arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on configuration
-file - Inference - [Energon-AI](https://github.com/hpcaitech/EnergonAI)
-                                                                  (back_to_top)
-## Parallel Training Demo ### GPT-3
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                 GPT3-v5.png]
-- Save 50% GPU resources, and 10.7% acceleration ### GPT-2 [https://
-raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png]
-- 11x lower GPU memory consumption, and superlinear scaling efficiency with
-Tensor Parallelism [https://raw.githubusercontent.com/hpcaitech/public_assets/
-main/colossalai/img/(updated)GPT-2.png] - 24x larger model size on the same
-hardware - over 3x acceleration ### BERT [https://raw.githubusercontent.com/
-hpcaitech/public_assets/main/colossalai/img/BERT.png] - 2x faster training, or
-50% longer sequence length ### PaLM - [PaLM-colossalai](https://github.com/
-hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways
-Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-
-model-palm-scaling-to.html)). ### OPT [https://raw.githubusercontent.com/
-hpcaitech/public_assets/main/colossalai/img/OPT_update.png] - [Open Pretrained
-Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion
-parameter AI language model released by Meta, which stimulates AI programmers
-to perform various downstream tasks and application deployments because public
-pretrained model weights. - 45% speedup fine-tuning OPT at low cost in lines. [
-[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/
-opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/
-opt_service) Please visit our [documentation](https://www.colossalai.org/) and
-[examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more
-details. ### ViT
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                   ViT.png]
-- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64 ###
-Recommendation System Models - [Cached Embedding](https://github.com/hpcaitech/
-CachedEmbedding), utilize software cache to train larger embedding tables with
-a smaller GPU memory budget.
-                                                                  (back_to_top)
-## Single GPU Training Demo ### GPT-2
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                GPT2-GPU1.png]
-- 20x larger model size on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                GPT2-NVME.png]
-- 120x larger model size on the same hardware (RTX 3080) ### PaLM
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                PaLM-GPU1.png]
-- 34x larger model size on the same hardware
-                                                                  (back_to_top)
-## Inference (Energon-AI) Demo
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             inference_GPT-3.jpg]
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
-acceleration on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             BLOOM%20serving.png]
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
-Try 175-billion-parameter OPT online services
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                            BLOOM%20Inference.PNG]
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
-Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
-times.
+arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on the
+configuration file - Inference - [Energon-AI](https://github.com/hpcaitech/
+EnergonAI)
                                                                   (back_to_top)
 ## Colossal-AI in the Real World ### ColossalChat
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                Chat-demo.png]
+ [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
+                       chat/ColossalChat%20YouTube.png]
 [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/
 Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/
 chatgpt/) with a complete RLHF pipeline. [[code]](https://github.com/hpcaitech/
 ColossalAI/tree/main/applications/Chat) [[blog]](https://medium.com/
 @yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-
-with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://
-chat.colossalai.org)
+with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://www.youtube.com/
+watch?v=HcTiHzApHm0) [[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)
+ [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
+                        chat/ColossalChat%20Speed.jpg]
+- Up to 10 times faster for RLHF PPO Stage3 Training
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                         chatgpt/ChatGPT%20scaling.png]
 - Up to 7.73 times faster for single server training and 1.42 times faster for
 single-GPU inference
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                            chatgpt/ChatGPT-1GPU.jpg]
 - Up to 10.3x growth in model capacity on one GPU - A mini demo training
 process requires only 1.62GB of GPU memory (any consumer-grade GPU)
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                            chatgpt/LoRA%20data.jpg]
 - Increase the capacity of the fine-tuning model by up to 3.7 times on a single
-GPU - Keep in a sufficiently high running speed
+GPU - Keep at a sufficiently high running speed
                                                                   (back_to_top)
 ### AIGC Acceleration of AIGC (AI-Generated Content) models such as [Stable
 Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable
 Diffusion v2](https://github.com/Stability-AI/stablediffusion).
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                          Stable%20Diffusion%20v2.png]
 - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/
@@ -195,34 +144,99 @@
 - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference
 acceleration and 39% cost reduce.
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                            xTrimoMultimer_Table.jpg]
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer):
 accelerating structure prediction of protein monomers and multimer by 11x.
                                                                   (back_to_top)
+## Parallel Training Demo ### GPT-3
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                 GPT3-v5.png]
+- Save 50% GPU resources and 10.7% acceleration ### GPT-2 [https://
+raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png]
+- 11x lower GPU memory consumption, and superlinear scaling efficiency with
+Tensor Parallelism [https://raw.githubusercontent.com/hpcaitech/public_assets/
+main/colossalai/img/(updated)GPT-2.png] - 24x larger model size on the same
+hardware - over 3x acceleration ### BERT [https://raw.githubusercontent.com/
+hpcaitech/public_assets/main/colossalai/img/BERT.png] - 2x faster training, or
+50% longer sequence length ### PaLM - [PaLM-colossalai](https://github.com/
+hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways
+Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-
+model-palm-scaling-to.html)). ### OPT [https://raw.githubusercontent.com/
+hpcaitech/public_assets/main/colossalai/img/OPT_update.png] - [Open Pretrained
+Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion
+parameter AI language model released by Meta, which stimulates AI programmers
+to perform various downstream tasks and application deployments because of
+public pre-trained model weights. - 45% speedup fine-tuning OPT at low cost in
+lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/
+language/opt) [[Online Serving]](https://colossalai.org/docs/
+advanced_tutorials/opt_service) Please visit our [documentation](https://
+www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/
+tree/main/examples) for more details. ### ViT
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                   ViT.png]
+- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64 ###
+Recommendation System Models - [Cached Embedding](https://github.com/hpcaitech/
+CachedEmbedding), utilize software cache to train larger embedding tables with
+a smaller GPU memory budget.
+                                                                  (back_to_top)
+## Single GPU Training Demo ### GPT-2
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                GPT2-GPU1.png]
+- 20x larger model size on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                GPT2-NVME.png]
+- 120x larger model size on the same hardware (RTX 3080) ### PaLM
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                PaLM-GPU1.png]
+- 34x larger model size on the same hardware
+                                                                  (back_to_top)
+## Inference (Energon-AI) Demo
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             inference_GPT-3.jpg]
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
+acceleration on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             BLOOM%20serving.png]
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
+Try 175-billion-parameter OPT online services
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                            BLOOM%20Inference.PNG]
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
+Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
+times.
+                                                                  (back_to_top)
 ## Installation Requirements: - PyTorch >= 1.11 (PyTorch 2.x in progress) -
-Python >= 3.7 - CUDA >= 11.0 If you encounter any problem about installation,
-you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/
-issues/new/choose) in this repository. ### Install from PyPI You can easily
-install Colossal-AI with the following command. **By default, we do not build
-PyTorch extensions during installation.** ```bash pip install colossalai ```
-**Note: only Linux is supported for now.** However, if you want to build the
-PyTorch extensions during installation, you can set `CUDA_EXT=1`. ```bash
-CUDA_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
-during runtime when you actually need it.** We also keep release the nightly
-version to PyPI on a weekly basis. This allows you to access the unreleased
-features and bug fixes in the main branch. Installation can be made via ```bash
-pip install colossalai-nightly ``` ### Download From Source > The version of
-Colossal-AI will be in line with the main branch of the repository. Feel free
-to raise an issue if you encounter any problem. :) ```shell git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
-install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
-build them during runtime. If you want to install and enable CUDA kernel fusion
-(compulsory installation when using fused optimizer): ```shell CUDA_EXT=1 pip
-install . ```
+Python >= 3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
+developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
+you encounter any problem with installation, you may want to raise an [issue]
+(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+### Install from PyPI You can easily install Colossal-AI with the following
+command. **By default, we do not build PyTorch extensions during
+installation.** ```bash pip install colossalai ``` **Note: only Linux is
+supported for now.** However, if you want to build the PyTorch extensions
+during installation, you can set `CUDA_EXT=1`. ```bash CUDA_EXT=1 pip install
+colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
+actually need them.** We also keep releasing the nightly version to PyPI every
+week. This allows you to access the unreleased features and bug fixes in the
+main branch. Installation can be made via ```bash pip install colossalai-
+nightly ``` ### Download From Source > The version of Colossal-AI will be in
+line with the main branch of the repository. Feel free to raise an issue if you
+encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
+ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
+we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
+If you want to install and enable CUDA kernel fusion (compulsory installation
+when using fused optimizer): ```shell CUDA_EXT=1 pip install . ``` For Users
+with CUDA 10.2, you can still build ColossalAI from source. However, you need
+to manually download the cub library and copy it to the corresponding
+directory. ```bash # clone the repository git clone https://github.com/
+hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
+github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
+1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
+CUDA_EXT=1 pip install . ```
                                                                   (back_to_top)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
@@ -245,28 +259,26 @@
 join and build the Colossal-AI community, making efforts towards the era of big
 AI models! You may contact us or participate in the following ways: 1. [Leaving
 a Star â­](https://github.com/hpcaitech/ColossalAI/stargazers) to show your
 like and support. Thanks! 2. Posting an [issue](https://github.com/hpcaitech/
 ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the
 guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/
 CONTRIBUTING.md) 3. Send your official proposal to email contact@hpcaitech.com
-Thanks so much to all of our amazing contributors! [https://
-raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-contributor_avatar.png] *The order of contributor avatars is randomly
-shuffled.*
+Thanks so much to all of our amazing contributors! [https://contrib.rocks/
+image?repo=hpcaitech/ColossalAI]
                                                                   (back_to_top)
 ## CI/CD We leverage the power of [GitHub Actions](https://github.com/features/
 actions) to automate our development, release and deployment workflows. Please
 check out this [documentation](.github/workflows/README.md) on how the
 automated workflows are operated. ## Cite Us This project is inspired by some
 related projects (some by our team and some by other organizations). We would
 like to credit these amazing projects as listed in the [Reference List](./docs/
 REFERENCE.md). To cite this project, you can use the following BibTeX citation.
 ``` @article{bian2021colossal, title={Colossal-AI: A Unified Deep Learning
 System For Large-Scale Parallel Training}, author={Bian, Zhengda and Liu,
 Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui
 and Cui, Fan and You, Yang}, journal={arXiv preprint arXiv:2110.14883}, year=
-{2021} } ``` Colossal-AI has been accepted as official tutorials by top
-conference [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/
+{2021} } ``` Colossal-AI has been accepted as official tutorial by top
+conferences [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/
 Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://
 cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
                                                                   (back_to_top)
```

### Comparing `colossalai-0.2.8/colossalai/__init__.py` & `colossalai-0.3.0/colossalai/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/__init__.py` & `colossalai-0.3.0/colossalai/amp/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
-from .amp_type import AMP_TYPE
-from colossalai.context import Config
 import torch.nn as nn
-from torch.optim import Optimizer
 from torch.nn.modules.loss import _Loss
-from .torch_amp import convert_to_torch_amp
+from torch.optim import Optimizer
+
+from colossalai.context import Config
+
+from .amp_type import AMP_TYPE
 from .apex_amp import convert_to_apex_amp
 from .naive_amp import convert_to_naive_amp
+from .torch_amp import convert_to_torch_amp
 
 __all__ = ['convert_to_amp', 'convert_to_naive_amp', 'convert_to_apex_amp', 'convert_to_torch_amp', 'AMP_TYPE']
 
 
 def convert_to_amp(model: nn.Module, optimizer: Optimizer, criterion: _Loss, mode: AMP_TYPE, amp_config: Config = None):
     """A helper function to wrap training components with Torch AMP modules.
```

### Comparing `colossalai-0.2.8/colossalai/amp/apex_amp/__init__.py` & `colossalai-0.3.0/colossalai/amp/apex_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/apex_amp/apex_amp.py` & `colossalai-0.3.0/colossalai/amp/apex_amp/apex_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/__init__.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/_fp16_optimizer.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/_fp16_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/_utils.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/naive_amp/naive_amp.py` & `colossalai-0.3.0/colossalai/amp/naive_amp/naive_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/torch_amp/__init__.py` & `colossalai-0.3.0/colossalai/amp/torch_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/amp/torch_amp/_grad_scaler.py` & `colossalai-0.3.0/colossalai/amp/torch_amp/_grad_scaler.py`

 * *Files 1% similar despite different names*

```diff
@@ -236,15 +236,15 @@
                     # TODO: is there a way to split by device and dtype without appending in the inner loop?
                     per_device_and_dtype_grads[to_unscale.device][to_unscale.dtype].append(to_unscale)
 
             for device, per_dtype_grads in per_device_and_dtype_grads.items():
                 for grads in per_dtype_grads.values():
                     torch._amp_foreach_non_finite_check_and_unscale_(grads, per_device_found_inf.get(device),
                                                                      per_device_inv_scale.get(device))
-        # For tensor parallel paramters it should be all-reduced over tensor parallel process group
+        # For tensor parallel parameters it should be all-reduced over tensor parallel process group
         if gpc.is_initialized(ParallelMode.MODEL) and gpc.get_world_size(ParallelMode.MODEL) > 1:
             vals = [val for val in per_device_found_inf._per_device_tensors.values()]
             coalesced = _flatten_dense_tensors(vals)
             dist.all_reduce(coalesced, op=dist.ReduceOp.MAX, group=gpc.get_group(ParallelMode.MODEL))
             for buf, synced in zip(vals, _unflatten_dense_tensors(coalesced, vals)):
                 buf.copy_(synced)
         return per_device_found_inf._per_device_tensors
```

### Comparing `colossalai-0.2.8/colossalai/amp/torch_amp/torch_amp.py` & `colossalai-0.3.0/colossalai/amp/torch_amp/torch_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py` & `colossalai-0.3.0/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py` & `colossalai-0.3.0/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py` & `colossalai-0.3.0/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/checkpoint/operation.py` & `colossalai-0.3.0/colossalai/auto_parallel/checkpoint/operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import Callable, List, Tuple
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import ewise_flop_counter as elementwise_flop_counter
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes as activation_size
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import elementwise_flop_counter
 
 from ..registry import meta_register
 
 __all__ = ["elementwise_meta_info"]
 
 
 def elementwise_meta_info(temp_mem_scale: float = 0, buffer_mem_scale: float = 0) -> Callable:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 from typing import List, Tuple
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes as activation_size
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 
 from ..constants import BCAST_FUNC_OP, NO_SAVE_ACTIVATION
 from ..registry import meta_register
 
 __all__ = ['binary_elementwise_meta_info']
 
 
 @meta_register.register(BCAST_FUNC_OP)
 def binary_elementwise_meta_info(*args, **kwargs) -> Tuple[TrainCycleItem, TrainCycleItem, List[torch.Tensor]]:
     """Meta information generator for binary elementwise operations
     NOTE: Some of the binary elementwise operations will discard the input activation after computation, as they
     don't need those tensors for back propagation, for example, if there are two tensors being sent for `torch.add`,
-    they will be discarded right after add operation is done. We create a simple API in `MetaInfo` class to identify
+    they will be discarded right after add operation is done. We create a simple API in `ShardMetaInfo` class to identify
     this behavior, it is critical for better memory estimation.
 
     Returns:
         Tuple[TrainCycleItem, TrainCycleItem, List[torch.Tensor]]: compute cost, memory cost and forward inputs
     """
 
     input_op_data = [arg for arg in args if arg.type != OperationDataType.OUTPUT]
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 from typing import Callable, Dict, List, Tuple, Union
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     MemoryCost,
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
     TrainCycleItem,
 )
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 from colossalai.tensor.sharding_spec import ShardingSpec
 
 from ..registry import meta_register
 
 __all__ = ['convnd_meta_info']
 
 
@@ -106,26 +106,26 @@
     bwd_compute_cost = flop_mapping[torch.ops.aten.convolution_backward.default](bwd_args, (input_tensor, weight_tensor, bias_tensor)) if has_bias else \
                        flop_mapping[torch.ops.aten.convolution_backward.default](bwd_args, (input_tensor, weight_tensor))
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # calculate memory cost
     # TODO: use profiler to check conv temp memory
     # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-    fwd_memory_cost = MemoryCost(
-        activation=activation_size([input_tensor, output_tensor]),
-        parameter=activation_size([weight_tensor, bias_tensor]) if has_bias else activation_size(weight_tensor),
-        temp=0,
-        buffer=0)
-
-    bwd_memory_cost = MemoryCost(
-        activation=activation_size([input_tensor, weight_tensor, bias_tensor])
-        if has_bias else activation_size([input_tensor, weight_tensor]),
-        parameter=activation_size([weight_tensor, bias_tensor]) if has_bias else activation_size(weight_tensor),
-        temp=0,
-        buffer=0)
+    fwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, output_tensor]),
+                                 parameter=compute_size_in_bytes([weight_tensor, bias_tensor])
+                                 if has_bias else compute_size_in_bytes(weight_tensor),
+                                 temp=0,
+                                 buffer=0)
+
+    bwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, weight_tensor, bias_tensor])
+                                 if has_bias else compute_size_in_bytes([input_tensor, weight_tensor]),
+                                 parameter=compute_size_in_bytes([weight_tensor, bias_tensor])
+                                 if has_bias else compute_size_in_bytes(weight_tensor),
+                                 temp=0,
+                                 buffer=0)
 
     # total cost is the sum of forward and backward cost
     total_cost = MemoryCost(activation=fwd_memory_cost.activation + bwd_memory_cost.activation,
                             parameter=fwd_memory_cost.parameter + bwd_memory_cost.parameter)
 
     memory_cost = TrainCycleItem(fwd=fwd_memory_cost, bwd=bwd_memory_cost, total=total_cost)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import List, Tuple
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 
 from ..registry import meta_register
 
 __all__ = ["embedding_meta_info"]
 
 
 @meta_register.register(torch.nn.Embedding)
@@ -30,19 +30,19 @@
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # memory cost
     # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
     # NOTE: during the backward phase of torch.nn.Embedding, it seems when the input is large enough, it will
     # have a temp memory which is kind of weird and we don't know the reason yet, so currently we just assume
     # that there will be no temp memory, as the temp memory is significantly smaller than the gradient memory
-    fwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, output_tensor]),
+    fwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, output_tensor]),
                                  parameter=0,
                                  temp=0,
                                  buffer=0)
-    bwd_memory_cost = MemoryCost(activation=activation_size([weight_tensor]), parameter=0, temp=0, buffer=0)
+    bwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([weight_tensor]), parameter=0, temp=0, buffer=0)
 
     total_memory_cost = MemoryCost(activation=fwd_memory_cost.activation + bwd_memory_cost.activation)
 
     memory_cost = TrainCycleItem(fwd=fwd_memory_cost, bwd=bwd_memory_cost, total=total_memory_cost)
 
     # store fwd_in, fwd_buffer, fwd_out
     fwd_in = [torch.zeros_like(input_tensor)]
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 from functools import reduce
 from typing import Callable, Dict, List, Tuple, Union
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     MemoryCost,
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
     TrainCycleItem,
 )
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 from colossalai.tensor.sharding_spec import ShardingSpec
 
 from ..registry import meta_register
 
 __all__ = ['linear_meta_info', 'matmul_meta_info']
 
 
@@ -108,22 +108,22 @@
                                       bwd=bwd_compute_cost,
                                       total=fwd_compute_cost + bwd_compute_cost)
 
         # calculate memory cost
         # NOTE: Linear don't have buffer and temp in forward and backward phase
         # the forward activation cost is the size of output_tensor, parameter cost is the size of weight_tensor and bias_tensor
         # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-        fwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, output_tensor]),
-                                     parameter=activation_size([weight_tensor, bias_tensor]),
+        fwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, output_tensor]),
+                                     parameter=compute_size_in_bytes([weight_tensor, bias_tensor]),
                                      temp=0,
                                      buffer=0)
 
         # the backward activation cost is the size of input_tensor, weight_tensor and bias_tensor, parameter cost is 0
-        bwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, weight_tensor, bias_tensor]),
-                                     parameter=activation_size([weight_tensor, bias_tensor]),
+        bwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, weight_tensor, bias_tensor]),
+                                     parameter=compute_size_in_bytes([weight_tensor, bias_tensor]),
                                      temp=0,
                                      buffer=0)
 
         # total cost is to sum the forward and backward cost
         total_cost = MemoryCost(activation=fwd_memory_cost.activation + bwd_memory_cost.activation,
                                 parameter=fwd_memory_cost.parameter + bwd_memory_cost.parameter)
 
@@ -144,22 +144,22 @@
                                       bwd=bwd_compute_cost,
                                       total=fwd_compute_cost + bwd_compute_cost)
 
         # calculate memory cost
         # NOTE: Linear don't have buffer and temp in forward and backward phase
         # the forward activation cost is the size of output_tensor, parameter cost is the size of weight_tensor
         # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-        fwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, output_tensor]),
-                                     parameter=activation_size(weight_tensor),
+        fwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, output_tensor]),
+                                     parameter=compute_size_in_bytes(weight_tensor),
                                      temp=0,
                                      buffer=0)
 
         # the backward activation cost is the size of input_tensor and weight_tensor, parameter cost is 0
-        bwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, weight_tensor]),
-                                     parameter=activation_size(weight_tensor),
+        bwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, weight_tensor]),
+                                     parameter=compute_size_in_bytes(weight_tensor),
                                      temp=0,
                                      buffer=0)
 
         # total cost is to sum the forward and backward cost
         total_cost = MemoryCost(activation=fwd_memory_cost.activation + bwd_memory_cost.activation,
                                 parameter=fwd_memory_cost.parameter + bwd_memory_cost.parameter)
 
@@ -206,73 +206,73 @@
     # Get input and output tensors
     input_tensors = [args[0].data, args[1].data]
     output_tensors = [args[-1].data]
 
     # Check dimension
     if all(len(tensor.shape) == 1 for tensor in input_tensors):
         # Dot
-        fwd_compute_cost = flop_mapping[torch.ops.aten.dot.default](input_tensors, output_tensors)
+        fwd_compute_cost = flop_mapping[torch.ops.aten.matmul.default](input_tensors, output_tensors)
         bwd_compute_cost = flop_mapping[torch.ops.aten.mul.Tensor](input_tensors[0], output_tensors) * 2
 
-        fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors), parameter=0, temp=0, buffer=0)
-        bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors), parameter=0, temp=0, buffer=0)
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors), parameter=0, temp=0, buffer=0)
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors), parameter=0, temp=0, buffer=0)
 
     elif len(input_tensors[0].shape) >= 2 and len(input_tensors[1].shape) == 1:
         # gemv case 1: matrix-vector multiplication
         # &
         # batched gemv case 1: batched matrix-vector multiplication
 
-        fwd_compute_cost = flop_mapping[torch.ops.aten.mv.default](
+        fwd_compute_cost = flop_mapping[torch.ops.aten.matmul.default](
             [input_tensors[0].reshape(-1, input_tensors[0].shape[-1]), input_tensors[1]], output_tensors)
 
         # combine the dimensions of output
         bwd_compute_cost = flop_mapping[torch.ops.aten.mul.Tensor](
                            [output_tensors[0].reshape(-1), input_tensors[1]],
                            output_tensors) + \
-                           flop_mapping[torch.ops.aten.mv.default](
+                           flop_mapping[torch.ops.aten.matmul.default](
                            [input_tensors[0].reshape(-1, input_tensors[0].shape[-1]).transpose(0, 1), output_tensors[0].reshape(-1)],
                            output_tensors)
 
-        fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors), parameter=0, temp=0, buffer=0)
-        bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors), parameter=0, temp=0, buffer=0)
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors), parameter=0, temp=0, buffer=0)
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors), parameter=0, temp=0, buffer=0)
 
     elif len(input_tensors[0].shape) == 1 and len(input_tensors[1].shape) == 2:
         # gemv case 2: vector-matrix multiplication
-        fwd_compute_cost = flop_mapping[torch.ops.aten.mv.default](input_tensors, output_tensors)
+        fwd_compute_cost = flop_mapping[torch.ops.aten.matmul.default](input_tensors, output_tensors)
 
         bwd_compute_cost = flop_mapping[torch.ops.aten.mul.Tensor]([output_tensors[0], input_tensors[0]], output_tensors) + \
-                           flop_mapping[torch.ops.aten.mv.default]([input_tensors[1], output_tensors[0]], output_tensors)
+                           flop_mapping[torch.ops.aten.matmul.default]([input_tensors[1], output_tensors[0]], output_tensors)
 
-        fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors), parameter=0, temp=0, buffer=0)
-        bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors),
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors), parameter=0, temp=0, buffer=0)
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors),
                                   parameter=0,
-                                  temp=activation_size(input_tensors[1]),
+                                  temp=compute_size_in_bytes(input_tensors[1]),
                                   buffer=0)
 
     elif len(input_tensors[0].shape) == 1 and len(input_tensors[1].shape) >= 3:
         # batched gemv case 2: vector-batched matrix multiplication
 
-        fwd_compute_cost = flop_mapping[torch.ops.aten.mv.default](
+        fwd_compute_cost = flop_mapping[torch.ops.aten.matmul.default](
             [input_tensors[1].transpose(-2, -1).reshape(-1, input_tensors[1].shape[-2]), input_tensors[0]],
             [output_tensors[0].reshape(-1)])
 
         # combine the dimensions of output
         bwd_compute_cost = flop_mapping[torch.ops.aten.mul.Tensor](
                            [output_tensors[0].reshape(-1), input_tensors[0]],
                            output_tensors
                            ) + \
-                           flop_mapping[torch.ops.aten.mv.default](
+                           flop_mapping[torch.ops.aten.matmul.default](
                            [input_tensors[1].transpose(-2, -1).reshape(-1, input_tensors[1].shape[-2]).transpose(0, 1), output_tensors[0].reshape(-1)],
                            output_tensors
                            )
 
-        fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors + [input_tensors[1]]))
-        bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors[0]),
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors + [input_tensors[1]]))
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors[0]),
                                   parameter=0,
-                                  temp=activation_size(input_tensors[1]),
+                                  temp=compute_size_in_bytes(input_tensors[1]),
                                   buffer=0)
 
     elif len(input_tensors[0].shape) >= 2 and len(input_tensors[1].shape) == 2:
         # gemm & batched gemm case 1: batched matrix-matrix multiplication
 
         fwd_compute_cost = flop_mapping[torch.ops.aten.mm.default](
             [input_tensors[0].reshape(-1, input_tensors[0].shape[-1]), input_tensors[1]],
@@ -283,16 +283,16 @@
                            [input_tensors[1]]
                            ) + \
                            flop_mapping[torch.ops.aten.mm.default](
                            [output_tensors[0].reshape(-1, output_tensors[0].shape[-1]), input_tensors[1].transpose(0, 1)],
                            [input_tensors[0].reshape(-1, input_tensors[0].shape[-1])]
                            )
 
-        fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors), parameter=0, temp=0, buffer=0)
-        bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors), parameter=0, temp=0, buffer=0)
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors), parameter=0, temp=0, buffer=0)
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors), parameter=0, temp=0, buffer=0)
 
     elif len(input_tensors[0].shape) == 2 and len(input_tensors[1].shape) >= 3:
         # batched gemm case 2: matrix-batched matrix multiplication
         fwd_compute_cost = flop_mapping[torch.ops.aten.mm.default]([
             input_tensors[1].transpose(-2, -1).reshape(-1, input_tensors[1].shape[-2]), input_tensors[0].transpose(
                 0, 1)
         ], [output_tensors[0].transpose(-2, -1)])
@@ -302,33 +302,34 @@
                            [input_tensors[0]]
                            ) + \
                            flop_mapping[torch.ops.aten.mm.default](
                            [output_tensors[0].transpose(-2, -1).reshape(-1, output_tensors[0].shape[-2]), input_tensors[0]],
                            [input_tensors[1].transpose(-2, -1).reshape(-1, input_tensors[1].shape[-2])]
                            )
 
-        fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors) + activation_size(input_tensors[1]),
-                                  temp=activation_size(output_tensors))
-        bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors[0]),
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors) +
+                                  compute_size_in_bytes(input_tensors[1]),
+                                  temp=compute_size_in_bytes(output_tensors))
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors[0]),
                                   parameter=0,
-                                  temp=activation_size(input_tensors[1]) + activation_size(output_tensors))
+                                  temp=compute_size_in_bytes(input_tensors[1]) + compute_size_in_bytes(output_tensors))
 
     elif all(len(tensor.shape) >= 3 for tensor in input_tensors):
         # Batched matrix-batched matrix multiplication
         # Fetch shape of the two inputs and see if the batch dimensions are the same
         _is_batch_dims_same = True
         if len(input_tensors[0].shape) == len(input_tensors[1].shape):
             for (shape_0, shape_1) in zip(input_tensors[0].shape[:-2], input_tensors[1].shape[:-2]):
                 if shape_0 != shape_1:
                     _is_batch_dims_same = False
                     break
         else:
             _is_batch_dims_same = False
 
-        # retireve dimensions
+        # retrieve dimensions
         input_dim_00 = input_tensors[0].shape[-2]
         input_dim_01 = input_tensors[0].shape[-1]
         input_dim_10 = input_tensors[1].shape[-2]
         input_dim_11 = input_tensors[1].shape[-1]
         output_dim_0 = output_tensors[0].shape[-2]
         output_dim_1 = output_tensors[0].shape[-1]
 
@@ -347,16 +348,16 @@
                                [input_tensors[1].reshape(-1, input_dim_11, input_dim_10)]
                                ) + \
                                flop_mapping[torch.ops.aten.bmm.default](
                                [output_tensors[0].reshape(-1, output_dim_0, output_dim_1), input_tensors[1].transpose(-2, -1).reshape(-1, input_dim_11, input_dim_10)],
                                [input_tensors[0].reshape(-1, input_dim_00, input_dim_01)]
                                )
 
-            fwd_mem_cost = MemoryCost(activation=activation_size(output_tensors))
-            bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors))
+            fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(output_tensors))
+            bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors))
 
         else:
             # Case 2: batch dimensions are different
             batch_dims = output_tensors[0].shape[:-2]
             extended_input_0 = torch.rand(reduce(lambda x, y: x * y, batch_dims),
                                           input_dim_00,
                                           input_dim_01,
@@ -377,18 +378,18 @@
                                ) + \
                                flop_mapping[torch.ops.aten.bmm.default](
                                [output_tensors[0].reshape(-1, output_dim_0, output_dim_1), extended_input_1.transpose(-2, -1)],
                                [extended_input_0]
                                )
 
             fwd_mem_cost = MemoryCost(
-                activation=activation_size([output_tensors[0], extended_input_0, extended_input_1]))
-            bwd_mem_cost = MemoryCost(activation=activation_size(input_tensors) -
-                                      activation_size([extended_input_0, extended_input_1]),
-                                      temp=activation_size([extended_input_0, extended_input_1]))
+                activation=compute_size_in_bytes([output_tensors[0], extended_input_0, extended_input_1]))
+            bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensors) -
+                                      compute_size_in_bytes([extended_input_0, extended_input_1]),
+                                      temp=compute_size_in_bytes([extended_input_0, extended_input_1]))
 
     # compute cost
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # memory cost
     total_cost = MemoryCost(activation=fwd_mem_cost.activation + bwd_mem_cost.activation,
                             parameter=fwd_mem_cost.parameter + bwd_mem_cost.parameter,
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,13 @@
 import operator
 from typing import List, Tuple
 
 import torch
 
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 
 from ..registry import meta_register
 
 __all__ = ["non_spmd_meta_info"]
 
 
 @meta_register.register(torch.Size)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 from typing import Callable, Dict, List, Tuple, Union
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     MemoryCost,
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
     TrainCycleItem,
 )
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 from colossalai.tensor.sharding_spec import ShardingSpec
 
 from ..registry import meta_register
 
 __all__ = ['batchnormnd_meta_info', 'layernorm_meta_info']
 
 
@@ -73,25 +73,26 @@
     fwd_compute_cost = flop_mapping[torch.ops.aten.cudnn_batch_norm.default](fwd_in_args, fwd_out_args)
     bwd_compute_cost = flop_mapping[torch.ops.aten.cudnn_batch_norm_backward.default](bwd_in_args, bwd_out_args)
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # calculate memory cost
     # the fwd activation cost is output plus saved mean and saved inv std
     # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-    fwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, output_tensor, mean_tensor, var_tensor]),
-                                 parameter=activation_size([weight_tensor, bias_tensor]),
+    fwd_memory_cost = MemoryCost(activation=compute_size_in_bytes(
+        [input_tensor, output_tensor, mean_tensor, var_tensor]),
+                                 parameter=compute_size_in_bytes([weight_tensor, bias_tensor]),
                                  temp=0,
-                                 buffer=activation_size([mean_tensor, var_tensor]))
+                                 buffer=compute_size_in_bytes([mean_tensor, var_tensor]))
 
     # the bwd memory cost is quite tricky here, BatchNorm will remove saved mean
     # and saved inv std during backward phase
-    bwd_memory_cost = MemoryCost(activation=activation_size([input_tensor]),
-                                 parameter=activation_size([weight_tensor, bias_tensor]),
-                                 temp=activation_size([mean_tensor, var_tensor]),
-                                 buffer=activation_size([mean_tensor, var_tensor]))
+    bwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor]),
+                                 parameter=compute_size_in_bytes([weight_tensor, bias_tensor]),
+                                 temp=compute_size_in_bytes([mean_tensor, var_tensor]),
+                                 buffer=compute_size_in_bytes([mean_tensor, var_tensor]))
 
     # total cost is the sum of forward and backward cost
     total_cost = MemoryCost(activation=fwd_memory_cost.activation + bwd_memory_cost.activation,
                             parameter=fwd_memory_cost.parameter + bwd_memory_cost.parameter)
 
     memory_cost = TrainCycleItem(fwd=fwd_memory_cost, bwd=bwd_memory_cost, total=total_cost)
 
@@ -127,23 +128,24 @@
     # compute cost
     fwd_compute_cost = flop_mapping[torch.ops.aten.native_layer_norm.default](fwd_in_args, fwd_out_args)
     bwd_compute_cost = flop_mapping[torch.ops.aten.native_layer_norm_backward.default](bwd_in_args, bwd_out_args)
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # memory cost
     # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-    fwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, output_tensor, weight_tensor, bias_tensor]),
-                                 parameter=activation_size([weight_tensor, bias_tensor]),
+    fwd_memory_cost = MemoryCost(activation=compute_size_in_bytes(
+        [input_tensor, output_tensor, weight_tensor, bias_tensor]),
+                                 parameter=compute_size_in_bytes([weight_tensor, bias_tensor]),
                                  temp=0,
-                                 buffer=activation_size([running_mean, running_var]))
+                                 buffer=compute_size_in_bytes([running_mean, running_var]))
 
-    bwd_memory_cost = MemoryCost(activation=activation_size([input_tensor, weight_tensor, bias_tensor]),
-                                 parameter=activation_size([weight_tensor, bias_tensor]),
-                                 temp=activation_size([running_mean, running_var]),
-                                 buffer=activation_size([running_mean, running_var]))
+    bwd_memory_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, weight_tensor, bias_tensor]),
+                                 parameter=compute_size_in_bytes([weight_tensor, bias_tensor]),
+                                 temp=compute_size_in_bytes([running_mean, running_var]),
+                                 buffer=compute_size_in_bytes([running_mean, running_var]))
 
     total_cost = MemoryCost(activation=fwd_memory_cost.activation + bwd_memory_cost.activation,
                             parameter=fwd_memory_cost.parameter + bwd_memory_cost.parameter,
                             temp=fwd_memory_cost.temp + bwd_memory_cost.temp,
                             buffer=fwd_memory_cost.buffer + bwd_memory_cost.buffer)
 
     memory_cost = TrainCycleItem(fwd=fwd_memory_cost, bwd=bwd_memory_cost, total=total_cost)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import List, Tuple
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 
 from ..registry import meta_register
 
 __all__ = ["avgpool_meta_info", "maxpool_meta_info"]
 
 
 @meta_register.register(torch.nn.AdaptiveAvgPool1d)
@@ -48,16 +48,16 @@
 
     # calculate compute cost
     fwd_compute_cost = flop_mapping[torch.ops.aten._adaptive_avg_pool2d.default](fwd_in_args, fwd_out_args)
     bwd_compute_cost = flop_mapping[torch.ops.aten._adaptive_avg_pool2d_backward.default](bwd_in_args, bwd_out_args)
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # calculate memory cost
-    fwd_mem_cost = MemoryCost() if is_inplace else MemoryCost(activation=activation_size(output_tensor))
-    bwd_mem_cost = MemoryCost() if is_inplace else MemoryCost(activation=activation_size(input_tensor))
+    fwd_mem_cost = MemoryCost() if is_inplace else MemoryCost(activation=compute_size_in_bytes(output_tensor))
+    bwd_mem_cost = MemoryCost() if is_inplace else MemoryCost(activation=compute_size_in_bytes(input_tensor))
 
     # total cost
     total_mem_cost = MemoryCost(activation=fwd_mem_cost.activation + bwd_mem_cost.activation)
 
     mem_cost = TrainCycleItem(fwd=fwd_mem_cost, bwd=bwd_mem_cost, total=total_mem_cost)
 
     # store fwd_in, fwd_buffer, fwd_out
@@ -110,19 +110,19 @@
     fwd_compute_cost = flop_mapping[torch.ops.aten.max_pool2d_with_indices.default](fwd_in_args, fwd_out_args)
     bwd_compute_cost = flop_mapping[torch.ops.aten.max_pool2d_with_indices_backward.default](bwd_in_args, bwd_out_args)
     compute_cost = TrainCycleItem(fwd=fwd_compute_cost, bwd=bwd_compute_cost, total=fwd_compute_cost + bwd_compute_cost)
 
     # calculate memory cost
     # NOTE: the index matrix will be discarded in backward phase
     # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-    fwd_mem_cost = MemoryCost(activation=activation_size([input_tensor, output_tensor, index_matrix]))
+    fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes([input_tensor, output_tensor, index_matrix]))
 
     # temp memory for backward is the index matrix to be discarded
-    bwd_mem_cost = MemoryCost(activation=activation_size(input_tensor) - activation_size(index_matrix),
-                              temp=activation_size(index_matrix))
+    bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(input_tensor) - compute_size_in_bytes(index_matrix),
+                              temp=compute_size_in_bytes(index_matrix))
 
     # total cost
     total_mem_cost = MemoryCost(activation=fwd_mem_cost.activation + bwd_mem_cost.activation, temp=bwd_mem_cost.temp)
 
     mem_cost = TrainCycleItem(fwd=fwd_mem_cost, bwd=bwd_mem_cost, total=total_mem_cost)
 
     # store fwd_in, fwd_buffer, fwd_out
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import Callable, List, Tuple
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 
 from ..registry import meta_register
 
 __all__ = ["tensor_related_metainfo"]
 
 
 def tensor_related_metainfo(bwd_mem_out_factor: float = 1, bwd_mem_tmp_factor: float = 0) -> Callable:
@@ -31,19 +31,19 @@
         outputs = next(filter(lambda x: x.type == OperationDataType.OUTPUT, args)).data
 
         # compute costs are all zero
         compute_cost = TrainCycleItem(fwd=0, bwd=0, total=0)
 
         # memory costs
         # NOTE: currently in SPMD solver we always believe that there will be a new tensor created in forward
-        fwd_mem_cost = MemoryCost(activation=activation_size(outputs) * 2, parameter=0, temp=0, buffer=0)
+        fwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(outputs) * 2, parameter=0, temp=0, buffer=0)
 
-        bwd_mem_cost = MemoryCost(activation=activation_size(outputs) * bwd_mem_out_factor,
+        bwd_mem_cost = MemoryCost(activation=compute_size_in_bytes(outputs) * bwd_mem_out_factor,
                                   parameter=0,
-                                  temp=activation_size(outputs) * bwd_mem_tmp_factor,
+                                  temp=compute_size_in_bytes(outputs) * bwd_mem_tmp_factor,
                                   buffer=0)
 
         total_mem_cost = MemoryCost(activation=fwd_mem_cost.activation + bwd_mem_cost.activation,
                                     parameter=fwd_mem_cost.parameter + bwd_mem_cost.parameter,
                                     temp=fwd_mem_cost.temp + bwd_mem_cost.temp,
                                     buffer=fwd_mem_cost.buffer + bwd_mem_cost.buffer)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/meta_registry/where.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/meta_registry/where.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import List, Tuple
 
 import torch
 
+from colossalai._analyzer._subclasses.flop_tensor import flop_mapping
+from colossalai._analyzer.fx.node_util import compute_size_in_bytes as activation_size
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, OperationDataType, TrainCycleItem
-from colossalai.fx.profiler.memory_utils import activation_size
-from colossalai.fx.profiler.opcount import flop_mapping
 
 from ..registry import meta_register
 
 __all__ = ["where_meta_info"]
 
 
 @meta_register.register(torch.where)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/metainfo.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/shard_metainfo.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,19 +11,19 @@
     TrainCycleItem,
 )
 from colossalai.tensor.sharding_spec import ShardingSpec
 
 from .constants import INPLACE_MODULE, INPLACE_OPS, NO_SAVE_ACTIVATION
 from .registry import meta_register
 
-__all__ = ['MetaInfo']
+__all__ = ['ShardMetaInfo']
 
 
-class MetaInfo:
-    """MetaInfo class
+class ShardMetaInfo:
+    """ShardMetaInfo class
     This class is used to store meta info based on sharding strategy and the given
     target function.
     """
 
     def __init__(self, strategy: ShardingStrategy = None, target: Callable = None) -> None:
         # compute cost of forward and backward computation
         self.compute_cost: TrainCycleItem
@@ -42,48 +42,62 @@
 
         # sharding strategy
         self._strategy = strategy
 
         # target function
         self._target = target
 
-        # compute metainfo if possible
+        # compute shard_metainfo if possible
         if self._strategy is not None and self._target is not None:
-            self.compute_metainfo()
+            self.compute_shard_metainfo()
 
     @property
     def strategy(self) -> ShardingStrategy:
         return self._strategy
 
     @property
     def target(self) -> Callable:
         return self._target
 
     @strategy.setter
     def strategy(self, strategy: ShardingStrategy) -> None:
         self._strategy = strategy
         if self._strategy is not None and self._target is not None:
-            self.compute_metainfo()
+            self.compute_shard_metainfo()
 
     @target.setter
     def target(self, target: Callable) -> None:
         self._target = target
         if self._strategy is not None and self._target is not None:
-            self.compute_metainfo()
+            self.compute_shard_metainfo()
 
-    def compute_sharded_opdata(self, operation_data: OperationData, sharding_spec: ShardingSpec) -> torch.Tensor:
+    def compute_sharded_opdata(self, operation_data: OperationData, sharding_spec: ShardingSpec):
         """
         Compute sharded opdata based on the given data and sharding spec.
         """
-        return OperationData(name=operation_data.name,
-                             data=torch.zeros(sharding_spec.get_sharded_shape_per_device(), device="meta"),
-                             type=operation_data.type,
-                             logical_shape=operation_data.logical_shape)
 
-    def compute_metainfo(self):
+        if isinstance(sharding_spec, ShardingSpec):
+            op_data = OperationData(name=operation_data.name,
+                                    data=torch.zeros(sharding_spec.get_sharded_shape_per_device(), device="meta"),
+                                    type=operation_data.type,
+                                    logical_shape=operation_data.logical_shape)
+        elif isinstance(sharding_spec, (list, tuple)):
+            data = operation_data.data
+            assert isinstance(data, (list, tuple)), f"Data Should be list or tuple, but got {type(data)}."
+            assert len(data) == len(sharding_spec), f"Length of data and sharding spec should be the same."
+            sharded_data = []
+            for d, s in zip(data, sharding_spec):
+                sharded_data.append(torch.zeros(s.get_sharded_shape_per_device(), device="meta"))
+            op_data = OperationData(name=operation_data.name, data=sharded_data, type=operation_data.type)
+        else:
+            raise ValueError(f"Sharding spec should be ShardingSpec or list, but got {type(sharding_spec)}.")
+
+        return op_data
+
+    def compute_shard_metainfo(self):
         """
         Compute meta info based on sharding strategy and the given target function.
         """
         assert meta_register.has(self._target.__class__) or meta_register.has(self._target), \
             f"Meta info for {self._target} is not registered."
         if meta_register.has(self._target.__class__):
             # module
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/meta_profiler/registry.py` & `colossalai-0.3.0/colossalai/auto_parallel/meta_profiler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/amp_optimizer.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/amp_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/base_offload_module.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/base_offload_module.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
-from typing import Optional, Set
 from functools import partial
+from typing import Optional, Set
+
 import torch
 import torch.nn as nn
 
 from colossalai.nn.parallel.data_parallel import _cast_float
-from colossalai.gemini.tensor_utils import free_storage
+from colossalai.zero.legacy.gemini.tensor_utils import free_storage
 
 from .region_manager import RegionManager
 from .util import GlobalRuntimeInfo
 
 
 class BaseOffloadModule:
     """
@@ -16,18 +17,15 @@
 
     Args:
         model (nn.Module): model to apply offloading.
         region_manager (RegionManager): a ``RegionManager`` instance.
         is_sync (bool): synchronous mode or not.
     """
 
-    def __init__(self,
-                 model: nn.Module,
-                 region_manager: RegionManager,
-                 is_sync=True):
+    def __init__(self, model: nn.Module, region_manager: RegionManager, is_sync=True):
 
         self.model = model
         self.region_manager = region_manager
         self.grad_hook_list = []
         self.overflow_counter = torch.cuda.IntTensor([0])
 
         self.grad_offload_stream = torch.cuda.current_stream() if is_sync else GlobalRuntimeInfo.d2h_stream
@@ -65,28 +63,28 @@
     def _post_backward(self):
         torch.cuda.synchronize()
         self.remove_grad_hook()
 
         for p in self.model.parameters():
             p.grad = None
 
-        GlobalRuntimeInfo.fwd_prefetch_event_map.clear()
-        GlobalRuntimeInfo.bwd_prefetch_event_map.clear()
+        GlobalRuntimeInfo().fwd_prefetch_event_map.clear()
+        GlobalRuntimeInfo().bwd_prefetch_event_map.clear()
 
     def grad_handle(self, p, grad):
         empty_grad = torch.empty_like(grad)
         free_storage(empty_grad)
         with torch._C.DisableTorchFunction():
             region = self.region_manager.get_region(p)
             region.copy_grad_to_region_slice(p, grad)
             if region.can_release:
                 self.overflow_counter += region.has_inf_or_nan
                 master_stream = torch.cuda.current_stream()
                 with torch.cuda.stream(self.grad_offload_stream):
-                    GlobalRuntimeInfo.d2h_stream.wait_stream(master_stream)
+                    GlobalRuntimeInfo().d2h_stream.wait_stream(master_stream)
                     region.move_grad_to_cpu()
         return empty_grad
 
     def _cast_buffers(self):
         for buffer in self.model.buffers():
             buffer.data = buffer.cuda()
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/mem_optimize.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/mem_optimize.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 from typing import Dict
+
 import torch
 import torch.fx
 from torch.fx import GraphModule
 from torch.utils._pytree import tree_map
 
 from colossalai.fx import ColoTracer, is_compatible_with_meta
 from colossalai.fx.passes.meta_info_prop import MetaInfoProp
 
-from .region_manager import RegionManager
-from .runtime import runtime_syn_offload_apply_pass, runtime_asyn_offload_apply_pass
 from .base_offload_module import BaseOffloadModule
-from .util import compute_max_param_mem, compute_total_param_mem, compute_act_peak_mem, GlobalRuntimeInfo
+from .region_manager import RegionManager
+from .runtime import runtime_asyn_offload_apply_pass, runtime_syn_offload_apply_pass
+from .util import GlobalRuntimeInfo, compute_act_peak_mem, compute_max_param_mem, compute_total_param_mem
+
 
 def memory_optimize(model: torch.nn.Module,
                     inps: Dict[str, torch.Tensor],
                     memory_budget: float = -1.0,
                     solver_name: str = 'asyn'):
 
     model = model.cpu().half()
@@ -25,25 +27,26 @@
     graph = tracer.trace(model, meta_args=meta_args)
     gm = GraphModule(model, graph, model.__class__.__name__)
     interp = MetaInfoProp(gm)
     interp.propagate(*meta_args.values())
 
     region_manager = RegionManager(graph, solver_name=solver_name, memory_budget=memory_budget)
     region_manager._build_regions()
-    GlobalRuntimeInfo.region_list = region_manager.region_list
+    GlobalRuntimeInfo().region_list = region_manager.region_list
 
-    act_peak_mem = compute_act_peak_mem(region_manager.region_list) / 1024 ** 2
-    max_param_mem = compute_max_param_mem(region_manager.region_list) / 1024 ** 2
-    total_param_mem = compute_total_param_mem(region_manager.region_list) / 1024 ** 2
+    act_peak_mem = compute_act_peak_mem(region_manager.region_list) / 1024**2
+    max_param_mem = compute_max_param_mem(region_manager.region_list) / 1024**2
+    total_param_mem = compute_total_param_mem(region_manager.region_list) / 1024**2
     print(
-        f"act_peak_mem={act_peak_mem:.3f} MB | max_param_mem={max_param_mem:.3f} MB | total_param_mem={total_param_mem:.3f}")
+        f"act_peak_mem={act_peak_mem:.3f} MB | max_param_mem={max_param_mem:.3f} MB | total_param_mem={total_param_mem:.3f}"
+    )
 
     if solver_name == 'syn':
         gm = runtime_syn_offload_apply_pass(gm, region_manager.region_list)
     elif solver_name == 'asyn':
         gm = runtime_asyn_offload_apply_pass(gm, region_manager.region_list)
     else:
         raise TypeError(f"Unknown solver name {solver_name}!")
 
     gm.recompile()
-    optimized_model = BaseOffloadModule(gm, region_manager, solver_name=='syn')
+    optimized_model = BaseOffloadModule(gm, region_manager, solver_name == 'syn')
     return optimized_model
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/region.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/region.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,14 @@
-from typing import List, Dict, Tuple
+from typing import Dict, List, Tuple
+
 import torch
 from torch.fx import Node
-from colossalai.gemini.tensor_utils import alloc_storage, free_storage
+
+from colossalai.zero.legacy.gemini.tensor_utils import alloc_storage, free_storage
+
 
 class Region:
     """
     Region: A container owning a piece of contiguous nodes in the DNN computing graph.
 
     Args:
         r_id (int): the index of the region in the computing graph.
@@ -48,23 +51,21 @@
         return torch.isinf(self.fp16_data).any() | torch.isnan(self.fp16_data).any()
 
     def init_param_data(self, pre_alloc_tensor: torch.Tensor = None):
         """
         Map the parameters in the region to a contiguous memory space.
         """
 
-        self.fp16_data = torch.zeros(
-            self.param_num, dtype=torch.half, device='cuda')
+        self.fp16_data = torch.zeros(self.param_num, dtype=torch.half, device='cuda')
         offset = 0
         for param in self.fp16_params:
             param.data = param.data.cuda()
             p_num = param.data.numel()
             self.fp16_data[offset:offset + p_num].copy_(param.data.flatten())
-            param.data = self.fp16_data[offset:offset +
-                                               p_num].view(param.data.shape)
+            param.data = self.fp16_data[offset:offset + p_num].view(param.data.shape)
             self.param_to_range[param] = (offset, offset + p_num)
             offset += p_num
 
         self.fp32_data = self.fp16_data.float().cpu().pin_memory()
         free_storage(self.fp16_data)
         if self.in_mem_pool_flag and pre_alloc_tensor is not None:
             self.fp16_data = pre_alloc_tensor
@@ -106,15 +107,15 @@
         # torch.cuda.empty_cache()
 
     def copy_grad_to_region_slice(self, param: torch.nn.Parameter, data_slice: torch.Tensor) -> None:
         """
         Copy data slice to the memory space indexed by the input tensor in the region.
 
         Args:
-            param (torch.nn.Parameter): the param used to retrive meta information
+            param (torch.nn.Parameter): the param used to retrieve meta information
             data_slice (torch.Tensor): the tensor to be copied to the region
         """
 
         begin, end = self.param_to_range[param]
         self.fp16_data[begin:end].copy_(data_slice.data.flatten())
         param.data = self.fp16_data[begin:end].view(param.data.shape)
 
@@ -137,8 +138,8 @@
         self.param_num -= new_reg.param_num
 
         return new_reg
 
     def __update_params_ptr(self) -> None:
         for param in self.fp16_params:
             begin, end = self.param_to_range[param]
-            param.data = self.fp16_data[begin:end].view(param.data.shape)
+            param.data = self.fp16_data[begin:end].view(param.data.shape)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/region_manager.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/region_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/runtime.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/runtime.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from typing import List
+
 import torch
 from torch.fx.node import Node
 
 from .region import Region
 from .util import GlobalRuntimeInfo, requires_upload_p_in_fwd
 
 
@@ -19,32 +20,32 @@
     """
 
     @staticmethod
     def forward(ctx, input_, fwd_info, bwd_info):
         ctx.bwd_info = bwd_info
         d2h_rid = fwd_info.get('d2h_rid', None)
         if d2h_rid is not None:
-            free_region = GlobalRuntimeInfo.region_list[d2h_rid]
+            free_region = GlobalRuntimeInfo().region_list[d2h_rid]
             assert isinstance(free_region, Region)
             free_region.free_cuda_data()
 
         h2d_rid = fwd_info.get('h2d_rid', None)
         if h2d_rid is not None:
-            h2d_region = GlobalRuntimeInfo.region_list[h2d_rid]
+            h2d_region = GlobalRuntimeInfo().region_list[h2d_rid]
             assert isinstance(h2d_region, Region)
             h2d_region.move_param_to_cuda()
 
         return input_
 
     @staticmethod
     def backward(ctx, grad_output):
 
         h2d_rid = ctx.bwd_info.get('h2d_rid', None)
         if h2d_rid is not None:
-            pref_region = GlobalRuntimeInfo.region_list[h2d_rid]
+            pref_region = GlobalRuntimeInfo().region_list[h2d_rid]
             assert isinstance(pref_region, Region)
             pref_region.move_param_to_cuda()
 
         return grad_output, None, None
 
 
 class AsynPreFwdPostBwdOP(torch.autograd.Function):
@@ -61,60 +62,58 @@
 
     @staticmethod
     def forward(ctx, input_, fwd_info, bwd_info):
         ctx.bwd_info = bwd_info
 
         sync_rid = fwd_info.get('sync_rid', None)
         if sync_rid is not None:
-            prefetch_event = GlobalRuntimeInfo.fwd_prefetch_event_map.get(
-                sync_rid, None)
+            prefetch_event = GlobalRuntimeInfo().fwd_prefetch_event_map.get(sync_rid, None)
             if prefetch_event:
                 prefetch_event.wait()
 
         h2d_rid = fwd_info.get('h2d_rid', None)
         if h2d_rid is not None:
-            pref_region = GlobalRuntimeInfo.region_list[h2d_rid]
+            pref_region = GlobalRuntimeInfo().region_list[h2d_rid]
             assert isinstance(pref_region, Region)
             master_stream = torch.cuda.current_stream()
-            with torch.cuda.stream(GlobalRuntimeInfo.h2d_stream):
-                GlobalRuntimeInfo.h2d_stream.wait_stream(master_stream)
+            with torch.cuda.stream(GlobalRuntimeInfo().h2d_stream):
+                GlobalRuntimeInfo().h2d_stream.wait_stream(master_stream)
                 pref_region.move_param_to_cuda()
 
             prefetch_event = torch.cuda.Event()
-            prefetch_event.record(GlobalRuntimeInfo.h2d_stream)
-            GlobalRuntimeInfo.fwd_prefetch_event_map[h2d_rid] = prefetch_event
+            prefetch_event.record(GlobalRuntimeInfo().h2d_stream)
+            GlobalRuntimeInfo().fwd_prefetch_event_map[h2d_rid] = prefetch_event
 
         return input_
 
     @staticmethod
     def backward(ctx, grad_output):
 
         sync_rid = ctx.bwd_info.get('sync_rid', None)
         if sync_rid is not None:
-            wait_region = GlobalRuntimeInfo.region_list[sync_rid]
+            wait_region = GlobalRuntimeInfo().region_list[sync_rid]
             assert isinstance(wait_region, Region)
-            prefetch_event = GlobalRuntimeInfo.bwd_prefetch_event_map.get(
-                sync_rid, None)
+            prefetch_event = GlobalRuntimeInfo().bwd_prefetch_event_map.get(sync_rid, None)
             if prefetch_event:
                 prefetch_event.wait()
             else:
                 wait_region.move_param_to_cuda()
 
         h2d_rid = ctx.bwd_info.get('h2d_rid', None)
         if h2d_rid is not None:
-            pref_region = GlobalRuntimeInfo.region_list[h2d_rid]
+            pref_region = GlobalRuntimeInfo().region_list[h2d_rid]
             assert isinstance(pref_region, Region)
             master_stream = torch.cuda.current_stream()
-            with torch.cuda.stream(GlobalRuntimeInfo.h2d_stream):
-                GlobalRuntimeInfo.h2d_stream.wait_stream(master_stream)
+            with torch.cuda.stream(GlobalRuntimeInfo().h2d_stream):
+                GlobalRuntimeInfo().h2d_stream.wait_stream(master_stream)
                 pref_region.move_param_to_cuda()
 
             prefetch_event = torch.cuda.Event()
-            prefetch_event.record(GlobalRuntimeInfo.h2d_stream)
-            GlobalRuntimeInfo.bwd_prefetch_event_map[h2d_rid] = prefetch_event
+            prefetch_event.record(GlobalRuntimeInfo().h2d_stream)
+            GlobalRuntimeInfo().bwd_prefetch_event_map[h2d_rid] = prefetch_event
         return grad_output, None, None
 
 
 def convert_fwd_upload_bwd_offload_to_action(tensor, fwd_info, bwd_info):
     '''
     Convert Upload and Offload operation into runtime action.
 
@@ -125,14 +124,15 @@
         bwd_info(dict): information dict, which contains region indices
             that need to be uploaded during backward pass.
     '''
     with torch._C.DisableTorchFunction():
         ret = SynPreFwdPostBwdOP.apply(tensor, fwd_info, bwd_info)
     return ret
 
+
 def convert_fwd_prefetch_bwd_offload_to_action(tensor, fwd_info, bwd_info):
     '''
     Convert Prefetch and Offload operation into runtime action.
 
     Argument:
         tensor(torch.Tensor): input tensor.
         fwd_info(dict): information dict, which contains region indices
@@ -185,15 +185,16 @@
         bwd_info = {}
         # backward upload
         if r_idx > 0 and region_list[r_idx - 1].need_offload:
             bwd_info['h2d_rid'] = region_list[r_idx - 1].r_id
 
         if fwd_info or bwd_info:
             with mod_graph.inserting_after(last_inp_node):
-                new_node = mod_graph.create_node('call_function', convert_fwd_upload_bwd_offload_to_action,
+                new_node = mod_graph.create_node('call_function',
+                                                 convert_fwd_upload_bwd_offload_to_action,
                                                  args=(last_inp_node, fwd_info, bwd_info))
             replace_node_users(last_inp_node, new_node)
 
         last_inp_node = region.nodes[-1]
 
     return gm
 
@@ -202,52 +203,54 @@
     """
     This pass is used to add the asynchronous prefetch and offload spec apply node to the origin graph.
     """
     mod_graph = gm.graph
 
     # upload parameters of the first region
     last_inp_node = tuple(mod_graph.nodes)[0]
-    first_region_with_p = [
-        region for region in region_list if region.param_size][0]
+    first_region_with_p = [region for region in region_list if region.param_size][0]
     fwd_info = {"h2d_rid": first_region_with_p.r_id}
     with mod_graph.inserting_after(last_inp_node):
-        upload_apply_node = mod_graph.create_node('call_function', convert_fwd_upload_bwd_offload_to_action,
+        upload_apply_node = mod_graph.create_node('call_function',
+                                                  convert_fwd_upload_bwd_offload_to_action,
                                                   args=(last_inp_node, fwd_info, {}))
     replace_node_users(last_inp_node, upload_apply_node)
     last_inp_node = upload_apply_node
 
     for r_idx, region in enumerate(region_list):
         # forward prefetch
         fwd_info = {}
         if region.param_size:
             fwd_info['sync_rid'] = region.r_id
         fwd_prefetch_region = region.fwd_prefetch_region
         if fwd_prefetch_region and requires_upload_p_in_fwd(region_list[fwd_prefetch_region.shared_rid]):
             fwd_info['h2d_rid'] = fwd_prefetch_region.r_id
 
         # forward offload
-        if r_idx > 0 and region_list[r_idx-1].need_offload:
+        if r_idx > 0 and region_list[r_idx - 1].need_offload:
             fwd_info['d2h_rid'] = r_idx - 1
 
         bwd_info = {}
         # backward prefetch
-        if r_idx > 0 and region_list[r_idx-1].need_offload:
+        if r_idx > 0 and region_list[r_idx - 1].need_offload:
             bwd_info['sync_rid'] = r_idx - 1
-        if r_idx > 0 and region_list[r_idx-1].bwd_prefetch_region:
-            bwd_info['h2d_rid'] = region_list[r_idx-1].bwd_prefetch_region.r_id
+        if r_idx > 0 and region_list[r_idx - 1].bwd_prefetch_region:
+            bwd_info['h2d_rid'] = region_list[r_idx - 1].bwd_prefetch_region.r_id
 
         if fwd_info or bwd_info:
             with mod_graph.inserting_after(last_inp_node):
-                new_node = mod_graph.create_node('call_function', convert_fwd_prefetch_bwd_offload_to_action,
+                new_node = mod_graph.create_node('call_function',
+                                                 convert_fwd_prefetch_bwd_offload_to_action,
                                                  args=(last_inp_node, fwd_info, bwd_info))
             replace_node_users(last_inp_node, new_node)
 
         last_inp_node = region.nodes[-1]
 
     if region.bwd_prefetch_region:
         bwd_info = {'h2d_rid': region.bwd_prefetch_region.r_id}
         with mod_graph.inserting_after(last_inp_node):
-            new_node = mod_graph.create_node('call_function', convert_fwd_prefetch_bwd_offload_to_action,
+            new_node = mod_graph.create_node('call_function',
+                                             convert_fwd_prefetch_bwd_offload_to_action,
                                              args=(last_inp_node, {}, bwd_info))
         replace_node_users(last_inp_node, new_node)
     # gm.graph.print_tabular()
     return gm
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/solver.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/training_simulator.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/training_simulator.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 class TrainingSimulator(ABC):
     """
     The Training Simulator is used to simulate the training process.
     It records computation, communication, and runtime memory during forward and backward passes.
 
     Args:
         region_list (List[Region]): represents the linearized DNN computing graph.
-        comp_power (float): the NVIDIA GPU FP16 compuing power.
+        comp_power (float): the NVIDIA GPU FP16 computing power.
         link_to_bw (Dict[str, Dict[float, float]]): communication links and the corresponding bandwidth.
     """
 
     def __init__(self,
                  region_list: List[Region],
                  comp_power: float,
                  link_to_bw: Dict[str, Dict[float, float]]) -> None:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/offload/util.py` & `colossalai-0.3.0/colossalai/auto_parallel/offload/util.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,25 @@
 from dataclasses import dataclass
 from typing import List
+
 import torch
+
+from colossalai.context.singleton_meta import SingletonMeta
 from colossalai.fx.profiler import calculate_fwd_out, calculate_fwd_tmp
 
 from .region import Region
 
 
 @dataclass
 class NodeInfo:
     node_id: int = 0
     runtime_fwd_mem: float = 0
     runtime_bwd_mem: float = 0
 
+
 class NvDevicePower:
     """
     NVIDIA GPU computing performance (TFLOPs).
     """
 
     RTX3080_FP16 = 70
     RTX3080_FP32 = 34.1
@@ -26,20 +30,22 @@
     V100_FP16 = 31.4
     V100_FP32 = 15.7
 
     A100_FP16 = 78
     A100_FP32 = 19.5
 
 
-class GlobalRuntimeInfo:
-    h2d_stream = torch.cuda.Stream()
-    d2h_stream = torch.cuda.Stream()
-    fwd_prefetch_event_map = {}
-    bwd_prefetch_event_map = {}
-    region_list = []
+class GlobalRuntimeInfo(metaclass=SingletonMeta):
+
+    def __init__(self):
+        self.h2d_stream = torch.cuda.Stream()
+        self.d2h_stream = torch.cuda.Stream()
+        self.fwd_prefetch_event_map = {}
+        self.bwd_prefetch_event_map = {}
+        self.region_list = []
 
 
 def compute_act_peak_mem(region_list: List[Region]) -> float:
     act_peak_mem = 0
     runtime_mem = 0
     # forward
     for region in region_list:
@@ -66,25 +72,28 @@
                 if user_node in bwd_deps:
                     bwd_deps[user_node] -= 1
                     if bwd_deps[user_node] <= 0:
                         runtime_mem -= user_node.meta['bwd_mem_out']
 
     return act_peak_mem
 
+
 def compute_max_param_mem(region_list: List[Region]) -> float:
     return max(region.param_size for region in region_list)
 
+
 def compute_total_param_mem(region_list: List[Region]) -> float:
     return sum(region.param_size for region in region_list if region.r_id <= region.shared_rid)
 
+
 def requires_upload_p_in_fwd(shared_reg: Region):
-    return (shared_reg.r_id >= shared_reg.shared_rid) or (
-                shared_reg.r_id < shared_reg.shared_rid and shared_reg.need_offload)
+    return (shared_reg.r_id >= shared_reg.shared_rid) or (shared_reg.r_id < shared_reg.shared_rid
+                                                          and shared_reg.need_offload)
+
 
 def requires_release_p_in_bwd(shared_reg: Region):
-    return (shared_reg.r_id >= shared_reg.shared_rid) or (
-                shared_reg.r_id < shared_reg.shared_rid and shared_reg.need_offload)
+    return (shared_reg.r_id >= shared_reg.shared_rid) or (shared_reg.r_id < shared_reg.shared_rid
+                                                          and shared_reg.need_offload)
+
 
 def requires_offload_g_in_bwd(region: Region):
     return region.param_size and (region.r_id <= region.shared_rid)
-
-
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/passes/comm_metainfo_pass.py` & `colossalai-0.3.0/colossalai/auto_parallel/passes/comm_metainfo_pass.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,85 +1,85 @@
 from typing import Dict
 
 import torch
 from torch.fx import GraphModule
 from torch.fx.node import Node
 
-from colossalai.auto_parallel.meta_profiler import MetaInfo
+from colossalai.auto_parallel.meta_profiler import ShardMetaInfo
 from colossalai.auto_parallel.passes.runtime_apply_pass import runtime_apply, runtime_comm_spec_apply
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import MemoryCost, TrainCycleItem
 from colossalai.tensor.comm_spec import CommSpec
 from colossalai.tensor.shape_consistency import ShapeConsistencyManager
 from colossalai.tensor.sharding_spec import ShardingSpec
 
 shape_consistency_manager = ShapeConsistencyManager()
 
 
-def _construct_meta_info(node: Node, origin_sharding_spec: ShardingSpec,
-                         target_sharding_spec: ShardingSpec) -> MetaInfo:
+def _construct_shard_meta_info(node: Node, origin_sharding_spec: ShardingSpec,
+                               target_sharding_spec: ShardingSpec) -> ShardMetaInfo:
     # get comm_action_sequence and total_cost from shape_consistency_manager
     _, comm_action_sequence, total_cost = shape_consistency_manager.shape_consistency(
         origin_sharding_spec, target_sharding_spec)
 
-    meta_info = MetaInfo()
+    meta_info = ShardMetaInfo()
     # NOTE: the cost in shape_consistency_manager.mem_cost is the count in number of numel
-    # get mem cost for MetaInfo
+    # get mem cost for ShardMetaInfo
     mem_cost = shape_consistency_manager.mem_cost(comm_action_sequence)
     # extract user that has _meta_data and extract element length
     input_node = next(n for n in node._input_nodes if hasattr(n, '_meta_data'))
     element_length = input_node._meta_data.element_size()
 
     mem_cost.fwd.activation *= element_length
     mem_cost.fwd.temp *= element_length
     mem_cost.bwd.activation *= element_length
     mem_cost.bwd.temp *= element_length
     mem_cost.total.activation *= element_length
 
     meta_info.memory_cost = mem_cost
 
-    # get computation cost for MetaInfo
+    # get computation cost for ShardMetaInfo
     meta_info.compute_cost = TrainCycleItem(total_cost['forward'] * element_length,
                                             total_cost['backward'] * element_length,
                                             total_cost['total'] * element_length)
 
-    # get tensor shape for MetaInfo
+    # get tensor shape for ShardMetaInfo
     origin_sharding_spec: ShardingSpec
     target_sharding_spec: ShardingSpec
     input_shape = origin_sharding_spec.get_sharded_shape_per_device()
     output_shape = target_sharding_spec.get_sharded_shape_per_device()
 
     meta_info.fwd_in = [torch.rand(input_shape, device='meta')]
     meta_info.fwd_buffer = []
     meta_info.fwd_out = [torch.rand(output_shape, device='meta')]
 
     return meta_info
 
 
-def _runtime_apply_meta_info(node: Node, origin_spec_dict, sharding_spec_dict) -> MetaInfo:
+def _runtime_apply_meta_info(node: Node, origin_spec_dict, sharding_spec_dict) -> ShardMetaInfo:
     """
     This method is used to construct `MetaInto` for shape consistency node
     """
 
     # extract node index and user node index
     args = node.args
     node_index, user_node_index = args[3], args[4]
     origin_sharding_spec, target_sharding_spec = origin_spec_dict[node_index], sharding_spec_dict[node_index][
         user_node_index]
 
-    return _construct_meta_info(node, origin_sharding_spec, target_sharding_spec)
+    return _construct_shard_meta_info(node, origin_sharding_spec, target_sharding_spec)
 
 
-def _runtime_comm_spec_apply_meta_info(node: Node, comm_actions_dict: Dict) -> MetaInfo:
+def _runtime_comm_spec_apply_meta_info(node: Node, comm_actions_dict: Dict) -> ShardMetaInfo:
     # extract node_index and op_data_name
     node_index, op_data_name = node.args[2], node.args[3]
 
     comm_action = comm_actions_dict[node_index][op_data_name]
     if isinstance(comm_action.comm_spec, CommSpec):
         # this case is for all_reduce, there will be no memory cost
-        meta_info = MetaInfo()
+        meta_info = ShardMetaInfo()
         meta_info.memory_cost = TrainCycleItem(MemoryCost(), MemoryCost(), MemoryCost)
         output_node = next(n for n in node.users if hasattr(n, '_meta_data'))
         element_length = output_node._meta_data.element_size()
 
         total_cost = comm_action.comm_spec.get_comm_cost()
         meta_info.compute_cost = TrainCycleItem(total_cost['forward'] * element_length,
                                                 total_cost['backward'] * element_length,
@@ -89,25 +89,25 @@
         meta_info.fwd_in = [torch.rand(input_shape, device='meta')]
         meta_info.fwd_buffer = []
         meta_info.fwd_out = [torch.rand(output_shape, device='meta')]
     else:
         # this case will be handled by shape consistency manager
         origin_sharding_spec, target_sharding_spec = comm_action.comm_spec['src_spec'], comm_action.comm_spec[
             'tgt_spec']
-        meta_info = _construct_meta_info(node, origin_sharding_spec, target_sharding_spec)
+        meta_info = _construct_shard_meta_info(node, origin_sharding_spec, target_sharding_spec)
 
     return meta_info
 
 
 def comm_metainfo_pass(gm: GraphModule, sharding_spec_dict: Dict, origin_spec_dict: Dict,
                        comm_actions_dict: Dict) -> GraphModule:
     """
     The method manages all the metainfo of the communication node (run_time_apply, runtime_comm_spec_apply) in the graph.
     """
     for node in gm.graph.nodes:
         if node.target == runtime_apply:
-            setattr(node, 'best_metainfo', _runtime_apply_meta_info(node, origin_spec_dict, sharding_spec_dict))
+            setattr(node, 'best_strategy_info', _runtime_apply_meta_info(node, origin_spec_dict, sharding_spec_dict))
         elif node.target == runtime_comm_spec_apply:
-            setattr(node, 'best_metainfo', _runtime_comm_spec_apply_meta_info(node, comm_actions_dict))
+            setattr(node, 'best_strategy_info', _runtime_comm_spec_apply_meta_info(node, comm_actions_dict))
         else:
             pass
     return gm
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/passes/meta_info_prop.py` & `colossalai-0.3.0/colossalai/auto_parallel/passes/meta_info_prop.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from typing import List
 
 import torch
 import torch.fx
 from torch.fx import GraphModule
 from torch.fx.node import Node
 
-from colossalai.auto_parallel.meta_profiler import MetaInfo
+from colossalai.auto_parallel.meta_profiler import ShardMetaInfo
 from colossalai.auto_parallel.passes.constants import OUTPUT_SAVED_MOD, OUTPUT_SAVED_OPS
 from colossalai.fx._compatibility import compatibility
 from colossalai.fx.profiler import GraphInfo
 
 
 def _normalize_tuple(x):
     if not isinstance(x, tuple):
@@ -92,20 +92,20 @@
         node.meta = {**asdict(graph_info)}
 
     @compatibility(is_backward_compatible=False)
     def node_handler(self, node: Node) -> None:
         """
         Handle other kind of nodes
         """
-        assert hasattr(node, 'best_metainfo'), f"Cannot find best_metainfo in node {node}, {node.op}"
+        assert hasattr(node, 'best_strategy_info'), f"Cannot find best_strategy_info in node {node}, {node.op}"
         graph_info = GraphInfo()
-        meta_info = node.best_metainfo
-        meta_info: MetaInfo
+        meta_info = node.best_strategy_info
+        meta_info: ShardMetaInfo
 
-        # set data_ptr for input_tensor in MetaInfo class
+        # set data_ptr for input_tensor in ShardMetaInfo class
         input_tensors: List[torch.Tensor] = meta_info.fwd_in
         buffer_tensors: List[torch.Tensor] = meta_info.fwd_buffer
         output_tensors: List[torch.Tensor] = meta_info.fwd_out
 
         if self._is_inplace(node):
             # inplace operation will not create new tensor, and it only has one parent node
             # TODO: Verify this observation
@@ -144,15 +144,15 @@
                 self._set_data_ptr(tensor)
 
         # attach them to graph_info
         graph_info.fwd_in = input_tensors
         graph_info.fwd_tmp = buffer_tensors
         graph_info.fwd_out = output_tensors
 
-        # fetch other memory informations
+        # fetch other memory information
         memory_cost = meta_info.memory_cost
         graph_info.fwd_mem_tmp = memory_cost.fwd.temp
         graph_info.fwd_mem_out = memory_cost.fwd.activation
         graph_info.bwd_mem_tmp = memory_cost.bwd.temp
         graph_info.bwd_mem_out = memory_cost.bwd.activation
 
         # fetch flop information
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/passes/runtime_apply_pass.py` & `colossalai-0.3.0/colossalai/auto_parallel/passes/runtime_apply_pass.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from copy import deepcopy
 from typing import Dict, List
 
 import torch
 from torch.fx.node import Node
 
-from colossalai.auto_parallel.meta_profiler import MetaInfo
+from colossalai._analyzer.fx.node_util import MetaInfo
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     CommAction,
     CommType,
     OperationData,
     OperationDataType,
     TrainCycleItem,
 )
@@ -124,17 +124,18 @@
                 if node.sharding_spec.sharding_sequence_difference(node.target_sharding_specs[user_node_index]) == 0:
                     continue
                 with mod_graph.inserting_before(user_node):
                     shape_consistency_node = mod_graph.create_node('call_function',
                                                                    runtime_apply,
                                                                    args=(node, origin_dict_node, input_dict_node,
                                                                          node_to_index_dict[node], user_node_index))
-            if 'activation_checkpoint' in user_node.meta:
-                shape_consistency_node.meta['activation_checkpoint'] = user_node.meta['activation_checkpoint']
-
+            if hasattr(user_node.meta['info'], 'activation_checkpoint'):
+                MetaInfo(shape_consistency_node,
+                         mod_dir=user_node.meta['info'].mod_dir,
+                         activation_checkpoint=tuple(user_node.meta['info'].activation_checkpoint))
             new_args = list(user_node.args)
             new_kwargs = dict(user_node.kwargs)
             # the origin node may be a positional argument or key word argument of user node
             if node in new_args:
                 # substitute the origin node with shape_consistency_node
                 origin_index_args = new_args.index(node)
                 new_args[origin_index_args] = shape_consistency_node
@@ -206,22 +207,23 @@
                         # substitute the origin node with comm_spec_apply_node
                         new_args[new_args.index(node)] = comm_spec_apply_node
                         user.args = tuple(new_args)
                     elif str(node) in new_kwargs:
                         # substitute the origin node with comm_spec_apply_node
                         new_kwargs[str(node)] = comm_spec_apply_node
                         user.kwargs = new_kwargs
-
-            if 'activation_checkpoint' in node.meta:
-                comm_spec_apply_node.meta['activation_checkpoint'] = node.meta['activation_checkpoint']
+            if hasattr(node.meta['info'], 'activation_checkpoint'):
+                MetaInfo(comm_spec_apply_node,
+                         mod_dir=node.meta['info'].mod_dir,
+                         activation_checkpoint=tuple(node.meta['info'].activation_checkpoint))
 
     return gm
 
 
-def _act_annotataion_pass(gm: torch.fx.GraphModule):
+def _act_annotation_pass(gm: torch.fx.GraphModule):
     """
     This pass is used to add the act annotation to the new inserted nodes.
     """
     mod_graph = gm.graph
     nodes = tuple(mod_graph.nodes)
 
     for node in nodes:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/passes/runtime_preparation_pass.py` & `colossalai-0.3.0/colossalai/auto_parallel/passes/runtime_preparation_pass.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 from copy import deepcopy
 from typing import Dict, List, Union
 
 import torch
 from torch.fx import symbolic_trace
 from torch.fx.node import Node
 
+from colossalai._analyzer.fx.node_util import MetaInfo
 from colossalai.auto_parallel.tensor_shard.constants import RESHAPE_FUNC_OP
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     CommAction,
     CommType,
     OperationDataType,
     ShardingStrategy,
 )
@@ -49,15 +50,15 @@
                     total_shard_size *= device_mesh_info[shard_dim]
                 size[dim] = dim_size * total_shard_size
         size = torch.Size(size)
 
     return size
 
 
-def solution_annotatation_pass(gm: torch.fx.GraphModule, solution: List[int],
+def solution_annotation_pass(gm: torch.fx.GraphModule, solution: List[int],
                                strategies_constructor: StrategiesConstructor):
     """
     This method is used to stick the solution strategy to the nodes and add the information
     required in runtime into graph as placeholder nodes.
     """
     mod_graph = gm.graph
 
@@ -70,17 +71,17 @@
         strategies_vector = node.strategies_vector
         # stick the solution strategy to the corresponding node
         setattr(node, 'best_strategy', strategies_vector[strategy_index])
         setattr(node, 'sharding_spec', strategies_vector[strategy_index].get_sharding_spec_by_name(str(node)))
         origin_node_sharding_spec_dict[node_index] = strategies_vector[strategy_index].get_sharding_spec_by_name(
             str(node))
 
-        # attach the corresponding metainfo if node has the attribute `metainfo_vector`
-        if hasattr(node, 'metainfo_vector'):
-            setattr(node, 'best_metainfo', node.metainfo_vector[strategy_index])
+        # attach the corresponding metainfo if node has the attribute `strategies_info`
+        if hasattr(node, 'strategies_info'):
+            setattr(node, 'best_strategy_info', node.strategies_info[strategy_index])
 
     # the dict to get input sharding specs of user node
     sharding_spec_convert_dict = {}
     # the dict to record comm actions of nodes
     comm_actions_dict = {}
     for index, node in enumerate(nodes):
         target_sharding_specs = []
@@ -144,15 +145,15 @@
     # DeviceMesh information instructs the scaling of the size value
     device_mesh_info = {}
     for dim, dim_size in enumerate(device_mesh.mesh_shape):
         device_mesh_info[dim] = dim_size
 
     def _extract_target_dim(node):
         '''
-        A helper function to etract the target dimension from size node.
+        A helper function to extract the target dimension from size node.
         There are two usages of torch.Tensor.size:
         1. tensor.size()
         2. tensor.size(dim)
 
         If a target_dim is assigned, then the output will be in type of int, instead of torch.Size.
         Otherwise, the output will be in type of torch.Size and this function will return None.
         '''
@@ -164,20 +165,23 @@
         return target_dim
 
     def _post_processing(node, size_processing_node):
         '''
         This function is used to process the dependency between the size node and its users after
         inserting the size_process_node.
         '''
-        # store original node and processing node pair in node_pairs dictioanry
+        # store original node and processing node pair in node_pairs dictionary
         # It will be used to replace the original node with processing node in slice object
         node_pairs[node] = size_processing_node
         size_processing_node._meta_data = node._meta_data
-        if 'activation_checkpoint' in node.meta:
-            size_processing_node.meta['activation_checkpoint'] = node.meta['activation_checkpoint']
+
+        if hasattr(node.meta['info'], 'activation_checkpoint'):
+            MetaInfo(size_processing_node,
+                     mod_dir=node.meta['info'].mod_dir,
+                     activation_checkpoint=tuple(node.meta['info'].activation_checkpoint))
 
         user_list = list(node.users.keys())
         for user in user_list:
             if user == size_processing_node:
                 continue
             new_args = list(user.args)
             new_kwargs = dict(user.kwargs)
@@ -380,33 +384,34 @@
 def module_params_sharding_pass(gm: torch.fx.GraphModule, device_mesh: DeviceMesh, overlap=False):
     """
     Apply the sharding action to the module parameters and buffers following the
     instructions of solver solution.
     """
     mod_graph = gm.graph
     nodes = tuple(mod_graph.nodes)
-    # This stream is created for overlaping the communication and computation.
+    # This stream is created for overlapping the communication and computation.
     reduction_stream = torch.cuda.Stream()
 
-    def _add_hook_for_grad_communication(node, param):
+    def _add_hook_for_grad_communication(node, param, name=None):
 
         comm_actions = node.best_strategy.communication_actions
 
-        def _filter_param_to_hook(node, op_data, comm_action):
-            if node.op == 'call_module' and op_data.type == OperationDataType.PARAM and op_data.name == param.name and comm_action.comm_type == CommType.HOOK:
+        def _filter_param_to_hook(node, op_data, comm_action, name):
+
+            if node.op == 'call_module' and op_data.type == OperationDataType.PARAM and op_data.name == name and comm_action.comm_type == CommType.HOOK:
                 return True
             if node.op == 'get_attr' and isinstance(
                     node._meta_data, torch.nn.parameter.Parameter) and comm_action.comm_type == CommType.HOOK:
                 return True
             return False
 
         for operation_data, comm_action in comm_actions.items():
             comm_spec_to_use = comm_action.comm_spec
             # register hook to the parameters
-            if _filter_param_to_hook(node, operation_data, comm_action):
+            if _filter_param_to_hook(node, operation_data, comm_action, name=name):
 
                 def wrapper(param, comm_spec, stream, overlap):
 
                     def hook_fn(grad):
                         if overlap:
                             with torch.cuda.stream(stream):
                                 _all_reduce(grad, comm_spec, async_op=True)
@@ -418,15 +423,15 @@
                 wrapper(param, comm_spec_to_use, reduction_stream, overlap=overlap)
 
     def _shard_param(param, target_sharding_spec):
         # apply the sharding spec of parameters
         if target_sharding_spec.dim_partition_dict != {}:
             origin_sharding_spec = ShardingSpec(device_mesh, param.shape, {})
             setattr(param, 'sharding_spec', origin_sharding_spec)
-            # TODO: build a ColoParamter class to manager the distributed parameters
+            # TODO: build a ColoParameter class to manager the distributed parameters
             # we could use .data here, because all the operations just happen before the real training
             # loop, so we don't need to track these operations in the autograd graph.
             param = torch.nn.Parameter(
                 shape_consistency_manager.apply_for_autoparallel_runtime(param.data, param.sharding_spec,
                                                                          target_sharding_spec).detach().clone())
         return param
 
@@ -438,15 +443,15 @@
                 continue
             setattr(target_module, 'processed', True)
             for name, param in target_module.named_parameters():
                 target_sharding_spec = node.best_strategy.get_sharding_spec_by_name(name)
                 param = _shard_param(param, target_sharding_spec)
 
                 setattr(target_module, name, param)
-                _add_hook_for_grad_communication(node, param)
+                _add_hook_for_grad_communication(node, param, name)
 
             sharded_buffer_dict = {}
             # apply the sharding spec of buffers
             for name, buffer in target_module.named_buffers():
                 origin_sharding_spec = ShardingSpec(device_mesh, buffer.shape, {})
                 setattr(buffer, 'sharding_spec', origin_sharding_spec)
                 target_sharding_spec = node.best_strategy.get_sharding_spec_by_name(name)
@@ -487,15 +492,15 @@
 
 
 def runtime_preparation_pass(gm: torch.fx.GraphModule,
                              solution: List[int],
                              device_mesh: DeviceMesh,
                              strategies_constructor: StrategiesConstructor,
                              overlap=False):
-    gm, sharding_spec_convert_dict, origin_node_sharding_spec_dict, comm_actions_dict = solution_annotatation_pass(
+    gm, sharding_spec_convert_dict, origin_node_sharding_spec_dict, comm_actions_dict = solution_annotation_pass(
         gm, solution, strategies_constructor)
     gm = size_value_converting_pass(gm, device_mesh)
     gm = node_args_converting_pass(gm, device_mesh)
     # TODO: the pass below should be uncommented after the implementation of implicit_comm_action_apply_pass completed.
     # gm = implicit_comm_action_apply(gm)
     gm = module_params_sharding_pass(gm, device_mesh, overlap=overlap)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/constants.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/initialize.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/initialize.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,23 +2,25 @@
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 from torch.fx import GraphModule
 from torch.fx.graph import Graph
 
+from colossalai._analyzer.fx.codegen import ActivationCheckpointCodeGen
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.passes.runtime_apply_pass import runtime_apply_pass
 from colossalai.auto_parallel.passes.runtime_preparation_pass import runtime_preparation_pass
 from colossalai.auto_parallel.tensor_shard.options import DataloaderOption, ShardOption, SolverOptions, SolverPerference
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import CommAction
 from colossalai.auto_parallel.tensor_shard.solver import CostGraph, GraphAnalyser, Solver, StrategiesConstructor
 from colossalai.device.alpha_beta_profiler import AlphaBetaProfiler
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.graph_module import ColoGraphModule
-from colossalai.fx.tracer import ColoTracer
 from colossalai.tensor.sharding_spec import ShardingSpec
 
 
 class ModuleWrapper(nn.Module):
     '''
     This class is used to wrap the original module, and add the sharding_spec_dict, origin_spec_dict, comm_actions_dict
     into the forward function.
@@ -122,14 +124,15 @@
     ret = solver.call_solver_serialized_args()
     solution = list(ret[0])
 
     return solution
 
 
 def transform_to_sharded_model(gm: ColoGraphModule,
+                               meta_args: Dict,
                                solution: List[int],
                                device_mesh: DeviceMesh,
                                strategies_constructor: StrategiesConstructor,
                                overlap: bool = False):
     '''
     This method is used to transform the original graph to the sharded graph.
     The model parameters will be sharded according to the solution and the grad hooks
@@ -138,14 +141,15 @@
     '''
     gm, sharding_spec_dict, origin_spec_dict, comm_actions_dict = runtime_preparation_pass(gm,
                                                                                            solution,
                                                                                            device_mesh,
                                                                                            strategies_constructor,
                                                                                            overlap=overlap)
     gm = runtime_apply_pass(gm)
+    shape_prop_pass(gm, *meta_args.values(), sharding_spec_dict, origin_spec_dict, comm_actions_dict)
     gm.recompile()
     sharding_spec_dicts = (sharding_spec_dict, origin_spec_dict, comm_actions_dict)
 
     return gm, sharding_spec_dicts
 
 
 def initialize_device_mesh(world_size: int = -1,
@@ -239,33 +243,38 @@
         load_solver_solution(optional): if the load_solver_solution is True, the solution will be loaded
             from the solution_path.
         solution_path(optional): the path to save or load the solution.
         return_solution(optional): if the return_solution is True, the solution will be returned. The returned
             solution will be used to debug or help to analyze the sharding result. Therefore, we will not just
             return a series of integers, but return the best strategies.
     '''
-    tracer = ColoTracer(trace_act_ckpt=True)
+    tracer = ColoTracer(trace_act_ckpt=True, bias_addition_split=True)
 
     graph = tracer.trace(root=model, meta_args=meta_args)
+    graph.set_codegen(ActivationCheckpointCodeGen())
     gm = ColoGraphModule(model, graph, model.__class__.__name__)
+
+    shape_prop_pass(gm, *meta_args.values())
     gm.recompile()
 
     strategies_constructor = build_strategy_constructor(graph,
                                                         device_mesh,
                                                         solver_preference=solver_preference,
                                                         dataloader_option=dataloader_option,
                                                         shard_option=shard_option)
     if load_solver_solution:
         solution = torch.load(solution_path)
     else:
         solution = solve_solution(gm, strategies_constructor, memory_budget)
         if save_solver_solution:
             torch.save(solution, solution_path)
 
-    gm, sharding_spec_dicts = transform_to_sharded_model(gm, solution, device_mesh, strategies_constructor, overlap)
+    gm, sharding_spec_dicts = transform_to_sharded_model(gm, meta_args, solution, device_mesh, strategies_constructor,
+                                                         overlap)
+
     model_to_return = ModuleWrapper(gm, *sharding_spec_dicts)
 
     if return_solution:
         solution_to_return = []
         nodes = [strategies_vector.node for strategies_vector in strategies_constructor.leaf_strategies]
         for index, node in enumerate(nodes):
             solution_to_return.append(f'{node.name} {node.strategies_vector[solution[index]].name}')
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,11 @@
 from typing import Dict, List
 
 import torch
 
-from colossalai.auto_parallel.meta_profiler.metainfo import MetaInfo
-
 from ..sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from .node_handler import MetaInfoModuleHandler, ModuleHandler
 from .registry import operator_registry
 from .strategy import BatchNormStrategyGenerator, StrategyGenerator
 
 __all__ = ['BatchNormModuleHandler']
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -77,15 +77,18 @@
     def get_operation_data_mapping(self) -> Dict[str, OperationData]:
         mapping = _get_data_mapping_for_bmm_op(node=self.node, input_idx=1, other_idx=2, bias_idx=0)
         return mapping
 
     def get_strategy_generator(self) -> List[StrategyGenerator]:
         op_data_mapping = self.get_operation_data_mapping()
         generators = []
-        generators.append(BatchedMatMulStrategyGenerator(op_data_mapping, self.device_mesh))
+        generator = BatchedMatMulStrategyGenerator(op_data_mapping, self.device_mesh)
+        # addbmm will shrink the first batch dim
+        generator.squeeze_batch_dim = True
+        generators.append(generator)
         return generators
 
     def post_process(self, strategy: ShardingStrategy) -> Union[ShardingStrategy, List[ShardingStrategy]]:
         # convert bias from its logical sharding spec to its physical sharding spec
         op_data_mapping = self.get_operation_data_mapping()
 
         if 'bias' in op_data_mapping:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -151,15 +151,15 @@
         return mapping
 
     def post_process(self, strategy: ShardingStrategy) -> Union[ShardingStrategy, List[ShardingStrategy]]:
         """
         Convert the sharding spec from the logical shape to the physical shape.
         """
         # create multiple sharding strategies for the inputs
-        # as input can be multi-dimensinal and the partition dim is only 2D,
+        # as input can be multi-dimensional and the partition dim is only 2D,
         # we need to map the partition at logical dim 0 to one of the first few dimensions of the input and output
         strategies = _convert_logical_sharding_to_physical_sharding_spec_for_embedding(strategy=strategy,
                                                                                        input_name=str(
                                                                                            self.node.args[0]),
                                                                                        output_name=str(self.node))
         return strategies
 
@@ -217,14 +217,14 @@
         return mapping
 
     def post_process(self, strategy: ShardingStrategy):
         """
         Convert the sharding spec from the logical shape to the physical shape.
         """
         # create multiple sharding strategies for the inputs
-        # as input can be multi-dimensinal and the partition dim is only 2D,
+        # as input can be multi-dimensional and the partition dim is only 2D,
         # we need to map the partition at logical dim 0 to one of the first few dimensions of the input and output
         strategies = _convert_logical_sharding_to_physical_sharding_spec_for_embedding(strategy=strategy,
                                                                                        input_name=str(
                                                                                            self.node.args[0]),
                                                                                        output_name=str(self.node))
         return strategies
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 __all__ = ['LinearModuleHandler', 'LinearFunctionHandler']
 
 
 def _update_sharding_spec_for_transposed_weight_for_linear(strategy: ShardingStrategy,
                                                            weight_name: str) -> ShardingStrategy:
     """
     This function is a helper function used by both module node handler and function node handler. This function will
-    convert the sharding spec for the transposed weight to the correct partititon spec.
+    convert the sharding spec for the transposed weight to the correct partition spec.
 
     Args:
         strategy (ShardingStrategy): the strategy generated by the strategy generator.
         weight_name (str): the name of the OperationData object for the weight.
     """
     # switch the dimensions of the transposed weight
     sharding_spec = strategy.get_sharding_spec_by_name(weight_name)
@@ -193,15 +193,15 @@
         1. the sharding spec is updated for the transposed weight
         2. the input and output sharding specs are updated to physical shape.
         """
         # switch the dimensions of the transposed weight
         strategy = _update_sharding_spec_for_transposed_weight_for_linear(strategy=strategy, weight_name='weight')
 
         # create multiple sharding strategies for the inputs
-        # as input can be multi-dimensinal and the partition dim is only 2D,
+        # as input can be multi-dimensional and the partition dim is only 2D,
         # we need to map the partition at dim 0 to one of the first few dimensions of the input
         strategies = _convert_logical_sharding_to_physical_sharding_spec_for_linear(strategy=strategy,
                                                                                     input_name=str(self.node.args[0]),
                                                                                     output_name=str(self.node))
         return strategies
 
 
@@ -263,13 +263,13 @@
         return mapping
 
     def post_process(self, strategy: ShardingStrategy):
         # switch the dimensions of the transposed weight
         strategy = _update_sharding_spec_for_transposed_weight_for_linear(strategy=strategy,
                                                                           weight_name=str(self.node.args[1]))
         # create multiple sharding strategies for the inputs
-        # as input can be multi-dimensinal and the partition dim is only 2D,
+        # as input can be multi-dimensional and the partition dim is only 2D,
         # we need to map the partition at dim 0 to one of the first few dimensions of the input
         strategies = _convert_logical_sharding_to_physical_sharding_spec_for_linear(strategy=strategy,
                                                                                     input_name=str(self.node.args[0]),
                                                                                     output_name=str(self.node))
         return strategies
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,16 +44,16 @@
 
 
 def get_matmul_type(input_dim: int, other_dim: int):
     """
     Determine which type of matmul operation should be executed for the given tensor dimensions.
 
     Args:
-        input_dim (int): the number of dimensions for the input tenosr
-        other_dim (int): the number of dimensions for the other tenosr
+        input_dim (int): the number of dimensions for the input tensor
+        other_dim (int): the number of dimensions for the other tensor
     """
     if input_dim == 1 and other_dim == 1:
         matmul_type = MatMulType.DOT
     elif input_dim in [1, 2] and other_dim == 2:
         matmul_type = MatMulType.MM
     elif input_dim == 2 and other_dim == 1:
         matmul_type = MatMulType.MV
@@ -202,15 +202,15 @@
             for dim_idx, broadcast_type in self.broadcast_dim_info[key].items():
                 if broadcast_type == BroadcastType.MULTIPLE:
                     # if the dim is originally 1 and multiplied during broadcast
                     # we set its sharding to R
                     # e.g. [1, 2, 4] x [4, 4, 8] -> [4, 2, 8]
                     # the dim 0 of [1, 2, 4] is multiplied to 4
                     tensor_shape[dim_idx] = 1
-                elif broadcast_type == BroadcastType.PADDDING:
+                elif broadcast_type == BroadcastType.PADDING:
                     # if the dim is padded
                     # we remove its sharding
                     tensor_shape[dim_idx] = None
 
             tensor_shape_before_broadcast = [dim for dim in tensor_shape if dim is not None]
 
             physical_sharding_spec, removed_dims = recover_sharding_spec_for_broadcast_shape(
@@ -264,21 +264,21 @@
             Map the logical batch dim to the physical batch dim
             """
             op_data = op_data_mapping[key]
             sharding_spec = strategy.get_sharding_spec_by_name(op_data.name)
             dim_partition_dict = sharding_spec.dim_partition_dict
             entire_shape = sharding_spec.entire_shape
 
-            # upddate the dimension index for the matrix dimensions
+            # update the dimension index for the matrix dimensions
             if 2 in dim_partition_dict:
                 dim_partition_dict[len(self.batch_dims_before_view) + 1] = dim_partition_dict.pop(2)
             if 1 in dim_partition_dict:
                 dim_partition_dict[len(self.batch_dims_before_view)] = dim_partition_dict.pop(1)
 
-            # map the logical batch dim to phyiscal batch dim
+            # map the logical batch dim to physical batch dim
             if 0 in dim_partition_dict:
                 batch_dim_shard = dim_partition_dict.pop(0)
                 dim_partition_dict[physical_batch_dim] = batch_dim_shard
 
             # the new shape will be the batch dims + the last 2 matrix dims
             shape_before_view = self.batch_dims_before_view + list(entire_shape[-2:])
             sharding_spec.__init__(sharding_spec.device_mesh, shape_before_view, dim_partition_dict)
@@ -410,15 +410,15 @@
         """
         The operands for the dot operation have the same logical shape as the physical shape
         """
         return None, None, None
 
     def _get_logical_shape_for_mm(self):
         """
-        We need to handle the input tensor for a matrix-matrix multiplcation as the input
+        We need to handle the input tensor for a matrix-matrix multiplication as the input
         tensor can be a 1D or 2D tensor. If it is a 1D tensor, 1 will be prepended to its shape
         (e.g. [4] -> [1, 4]).
         """
         if self.input_meta_data.dim() == 1:
             input_logical_shape = [1] + list(self.input_meta_data.shape)
             input_logical_shape = torch.Size(input_logical_shape)
         else:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from abc import ABC, abstractmethod
 from typing import Dict, List, Tuple, Union
 
 import torch
 from torch.fx.node import Node
 
-from colossalai.auto_parallel.meta_profiler.metainfo import MetaInfo, meta_register
+from colossalai.auto_parallel.meta_profiler.shard_metainfo import ShardMetaInfo, meta_register
 from colossalai.auto_parallel.tensor_shard.options import ShardOption, SolverPerference
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     OperationData,
     OperationDataType,
     ShardingSpec,
     ShardingStrategy,
     StrategiesVector,
@@ -71,15 +71,15 @@
             assert hasattr(node, 'strategies_vector'), \
                 f'The predecessor node {node_name} has no strategy vector to compute the resharding cost.'
             prev_strategy_vector = node.strategies_vector
             prev_sharding_specs = [
                 prev_strategy.get_sharding_spec_by_name(node_name) for prev_strategy in prev_strategy_vector
             ]
 
-            # create data structrure to store costs
+            # create data structure to store costs
             if node not in resharding_costs:
                 resharding_costs[node] = []
 
             def _compute_resharding_cost(
                     prev_sharding_spec: Union[ShardingSpec,
                                               List[ShardingSpec]], current_sharding_spec: Union[ShardingSpec,
                                                                                                 List[ShardingSpec]],
@@ -208,15 +208,15 @@
 
         for strategy in remove_strategy_list:
             self.strategies_vector.remove(strategy)
 
         return self.strategies_vector
 
     def post_process(self, strategy: ShardingStrategy) -> Union[ShardingStrategy, List[ShardingStrategy]]:
-        # tranform the strategy generated
+        # transform the strategy generated
         # e.g. to process the sharding strategy for the transposed weights
         return strategy
 
     @abstractmethod
     def get_strategy_generator(self) -> List[StrategyGenerator]:
         """
         Define which generators should be used by this NodeHandler object.
@@ -254,31 +254,31 @@
     Note: this class will be integrated into the NodeHandler class in the future, after
     all the functions are patched.
     """
 
     def register_strategy(self, compute_resharding_cost: bool = True) -> StrategiesVector:
         """
         This method is inherited from NodeHandler. It will register the strategies first,
-        and rewrite the memory_cost and compute_cost of the strategy using the MetaInfo class.
+        and rewrite the memory_cost and compute_cost of the strategy using the ShardMetaInfo class.
         """
         super().register_strategy(compute_resharding_cost=compute_resharding_cost)
         target = self.get_target_function()
         # Currently we haven't patched all the torch functions and modules, so if the target
         # is not patched, we will use the default cost model to compute the cost.
         # TODO: patch all torch functions and modules to make it clean
         if meta_register.has(target.__class__) or meta_register.has(target):
-            metainfo_vector = []
+            strategies_info = []
             for strategy in self.strategies_vector:
-                metainfo = MetaInfo(strategy, target)
+                metainfo = ShardMetaInfo(strategy, target)
                 strategy.compute_cost = metainfo.compute_cost
                 strategy.memory_cost = metainfo.memory_cost
-                metainfo_vector.append(metainfo)
+                strategies_info.append(metainfo)
 
             # attach metainfos to the handler
-            setattr(self, "metainfo_vector", metainfo_vector)
+            setattr(self, "strategies_info", strategies_info)
 
         else:
             logger = get_dist_logger()
             logger.warning(f'The target function {target} is not patched yet, ')
 
         return self.strategies_vector
 
@@ -309,30 +309,30 @@
     Note: this class will be integrated into the ModuleHandler class in the future, after
     all the modules are patched.
     """
 
     def register_strategy(self, compute_resharding_cost: bool = True) -> StrategiesVector:
         """
         This method is inherited from NodeHandler. It will register the strategies first,
-        and rewrite the memory_cost and compute_cost of the strategy using the MetaInfo class.
+        and rewrite the memory_cost and compute_cost of the strategy using the ShardMetaInfo class.
         """
         super().register_strategy(compute_resharding_cost=compute_resharding_cost)
         target = self.get_target_function()
         # Currently we haven't patched all the torch functions and modules, so if the target
         # is not patched, we will use the default cost model to compute the cost.
         # TODO: patch all torch functions and modules to make it clean
         if meta_register.has(target.__class__) or meta_register.has(target):
-            metainfo_vector = []
+            strategies_info = []
             for strategy in self.strategies_vector:
-                metainfo = MetaInfo(strategy, target)
+                metainfo = ShardMetaInfo(strategy, target)
                 strategy.compute_cost = metainfo.compute_cost
                 strategy.memory_cost = metainfo.memory_cost
-                metainfo_vector.append(metainfo)
+                strategies_info.append(metainfo)
 
             # attach metainfos to the handler
-            setattr(self, "metainfo_vector", metainfo_vector)
+            setattr(self, "strategies_info", strategies_info)
 
         else:
             logger = get_dist_logger()
             logger.warning(f'The target function {target} is not patched yet')
 
         return self.strategies_vector
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/registry.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py`

 * *Files 0% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 class BatchNormStrategyGenerator(StrategyGenerator):
     """
     A StrategyGenerator which deals with the sharding strategies of batch normalization.
 
     To keep the math consistency, there are two way to do BatchNorm if the input
     shards on batch dimension:
     1. We gather the input partitions through batch dimension, then do the normal BatchNorm.
-    2. We do the SyncBatchNorm on the each input partition seperately, the SyncBN op will help
+    2. We do the SyncBatchNorm on the each input partition separately, the SyncBN op will help
        us to keep the computing correctness.
     In this generator, both methods will be considered.
     """
 
     def validate(self) -> bool:
         '''
         In sanity check, we need make sure the input data having correct dimension size.
@@ -40,15 +40,15 @@
         assert input_op_data.data.dim() in (
             3, 4, 5), f'We suppose the dim of input fed into conv op should in range of [3, 5].'
 
     def update_compute_cost(self, strategy: ShardingStrategy):
         '''
         Compute the computation cost per device with this specific strategy.
 
-        Note: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        Note: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         '''
         # TODO: a constant coefficient need to be added.
         # 1D: (L) * N * Cin
         # 2D: (H * W) * N  * Cin
         # 3D: (H * W  * D) * N  * Cin
         sharded_input_shape = strategy.sharding_specs[self.op_data['input']].get_sharded_shape_per_device()
         sharded_output_shape = strategy.sharding_specs[self.op_data['output']].get_sharded_shape_per_device()
@@ -208,15 +208,15 @@
         if self.has_bias:
             dim_partition_dict_mapping["bias"] = {}
 
         sharding_spec_mapping = self.to_sharding_spec_mapping(dim_partition_dict_mapping)
 
         # set communication action
         # For SyncBN case, we don't need to do communication for weight and bias.
-        # TODO: the communication happens interally at SyncBN operation. We need to replace the BN operation
+        # TODO: the communication happens internally at SyncBN operation. We need to replace the BN operation
         # to SyncBN operation instead of inserting a communication node.
         output_comm_action = self.get_communication_action(
             sharding_spec=sharding_spec_mapping["output"],
             communication_pattern=CollectiveCommPattern.ALLREDUCE_FWD_IDENTITY_BWD,
             logical_process_axis=mesh_dim_0,
             comm_type=CommType.IMPLICIT)
 
@@ -246,15 +246,15 @@
         if self.has_bias:
             dim_partition_dict_mapping["bias"] = {}
 
         sharding_spec_mapping = self.to_sharding_spec_mapping(dim_partition_dict_mapping)
 
         # set communication action
         # For SyncBN case, we don't need to do communication for gradients of weight and bias.
-        # TODO: the communication happens interally at SyncBN operation. We need to replace the BN operation
+        # TODO: the communication happens internally at SyncBN operation. We need to replace the BN operation
         # to SyncBN operation instead of inserting a communication node.
         output_comm_action = self.get_communication_action(
             sharding_spec=sharding_spec_mapping["output"],
             communication_pattern=CollectiveCommPattern.ALLREDUCE_FWD_IDENTITY_BWD,
             logical_process_axis=[mesh_dim_0, mesh_dim_1],
             comm_type=CommType.IMPLICIT)
 
@@ -294,15 +294,15 @@
                 0: [mesh_dim_1],
             }
 
         sharding_spec_mapping = self.to_sharding_spec_mapping(dim_partition_dict_mapping)
 
         # set communication action
         # For SyncBN case, we don't need to do communication for gradients of weight and bias.
-        # TODO: the communication happens interally at SyncBN operation. We need to replace the BN operation
+        # TODO: the communication happens internally at SyncBN operation. We need to replace the BN operation
         # to SyncBN operation instead of inserting a communication node.
         output_comm_action = self.get_communication_action(
             sharding_spec=sharding_spec_mapping["output"],
             communication_pattern=CollectiveCommPattern.ALLREDUCE_FWD_IDENTITY_BWD,
             logical_process_axis=[mesh_dim_0],
             comm_type=CommType.IMPLICIT)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,15 +47,15 @@
 
     def update_memory_cost(self, strategy: ShardingStrategy) -> ShardingStrategy:
         # all input, output and outputs have the same shape
         shape = strategy.sharding_specs[self.op_data['input']].get_sharded_shape_per_device()
 
         # compute fwd memory cost in bytes
         # as the elementwise ops are not memory-intensive
-        # we approximate the fwd memroy cost to be the output
+        # we approximate the fwd memory cost to be the output
         # and the backward memory cost to be grad of input and other
         input_bytes = self._compute_size_in_bytes(strategy, 'input')
         other_bytes = self._compute_size_in_bytes(strategy, 'other')
         output_bytes = self._compute_size_in_bytes(strategy, 'output')
         fwd_memory_cost = MemoryCost(activation=output_bytes)
         bwd_memory_cost = MemoryCost(activation=input_bytes + other_bytes)
         total_memory_cost = MemoryCost(activation=input_bytes + other_bytes + output_bytes)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,17 +34,17 @@
         assert input_op_data.data.dim() in (
             3, 4, 5), f'We suppose the dim of input fed into conv op should in range of [3, 5].'
 
     def update_compute_cost(self, strategy: ShardingStrategy):
         '''
         Compute the computation cost per device with this specific strategy.
 
-        Note: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        Note: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         '''
-        # TODO: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        # TODO: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         # 1D: (L) * N * Cout * Cin * kernel
         # 2D: (H * W) * N * Cout * Cin * kernel
         # 3D: (H * W  * D) * N * Cout * Cin * kernel
         sharded_input_shape = strategy.sharding_specs[self.op_data['input']].get_sharded_shape_per_device()
         sharded_other_shape = strategy.sharding_specs[self.op_data['other']].get_sharded_shape_per_device()
         sharded_output_shape = strategy.sharding_specs[self.op_data['output']].get_sharded_shape_per_device()
         if self.has_bias:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,17 +30,17 @@
     def validate(self) -> bool:
         return super().validate()
 
     def update_compute_cost(self, strategy: ShardingStrategy):
         '''
         Compute the computation cost per device with this specific strategy.
 
-        Note: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        Note: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         '''
-        # TODO: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        # TODO: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         # TODO: a constant coefficient need to be added.
 
         sharded_input_shape = strategy.sharding_specs[self.op_data['input']].get_sharded_shape_per_device()
         sharded_weight_shape = strategy.sharding_specs[self.op_data['other']].get_sharded_shape_per_device()
         if self.has_bias:
             # bias add is an element wise operation, so the cost is equal to product of output shape.
             bias_compute_cost = reduce(operator.mul, sharded_weight_shape)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py`

 * *Files 0% similar despite different names*

```diff
@@ -772,18 +772,14 @@
         other_op_data = self.op_data['other']
         assert len(input_op_data.logical_shape) == 3 or len(other_op_data.logical_shape) == 3
 
         if 'bias' in self.op_data:
             bias_op_data = self.op_data['bias']
             assert bias_op_data.data.dim() < 3 and len(bias_op_data.logical_shape) == 2
 
-        if self.op_data['output'].data.dim() == 2:
-            # addbmm will shrink the first batch dim
-            self.squeeze_batch_dim = True
-
     def update_compute_cost(self, strategy: ShardingStrategy) -> ShardingStrategy:
         fwd_compute_cost = self.op_data['input'].data.shape[-1] * reduce(operator.mul,
                                                                          self.op_data['output'].data.shape)
         bwd_compute_cost = fwd_compute_cost * 2
         compute_cost = TrainCycleItem(fwd=fwd_compute_cost,
                                       bwd=bwd_compute_cost,
                                       total=fwd_compute_cost + bwd_compute_cost)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 from .strategy_generator import StrategyGenerator
 
 
 class NormalPoolStrategyGenerator(StrategyGenerator):
     """
     NormalPoolStrategyGenerator is a generic class to generate strategies for pool operation like MaxPoolxd.
     The reason we call this normal pool is AvgPoolxd and MaxPoolxd are taking the kernel size element from image,
-    and reduce them depening on the operation type.
+    and reduce them depending on the operation type.
     """
 
     def validate(self) -> bool:
         '''
         In sanity check, we need make sure the input data having correct dimension size.
         For Pool1d, the dim of input data should be 3([N, C, L]).
         For Pool2d, the dim of input data should be 4([N, C, H, W]).
@@ -31,17 +31,17 @@
         assert input_op_data.data.dim() in (
             3, 4, 5), f'We suppose the dim of input fed into Pool op should in range of [3, 5].'
 
     def update_compute_cost(self, strategy: ShardingStrategy) -> TrainCycleItem:
         '''
         Compute the computation cost per device with this specific strategy.
 
-        Note: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        Note: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         '''
-        # TODO: compute_cost need to be devided by TFLOPS, now it just shows the computation size.
+        # TODO: compute_cost need to be divided by TFLOPS, now it just shows the computation size.
         # 1D: (Lout) * N * C * kernel
         # 2D: (H * W) * N * Cout * Cin * kernel
         # 3D: (H * W  * D) * N * Cout * Cin * kernel
         sharded_output_shape = strategy.sharding_specs[self.op_data['output']].get_sharded_shape_per_device()
         sharded_input_shape = strategy.sharding_specs[self.op_data['input']].get_sharded_shape_per_device()
 
         kernel_size = self.op_data["other"].data
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py`

 * *Files 2% similar despite different names*

```diff
@@ -221,23 +221,23 @@
                 'sharding_spec of op_data should be a list of sharding specs if op_data.data is a tuple.'
             total_bytes = 0
             for index, sharding_spec in enumerate(strategy.sharding_specs[op_data]):
                 meta_data = op_data.data[index]
                 if isinstance(meta_data, torch.Tensor):
                     element_bytes = _compute_size_in_bytes_helper(sharding_spec, meta_data)
                 else:
-                    # if meta_data is not a tensor, we count the memroy as 0
+                    # if meta_data is not a tensor, we count the memory as 0
                     element_bytes = 0
                 total_bytes += element_bytes
 
         else:
             if isinstance(op_data.data, torch.Tensor):
                 total_bytes = _compute_size_in_bytes_helper(strategy.sharding_specs[op_data], op_data.data)
             else:
-                # if op_data.data is not a tensor, we count the memroy as 0
+                # if op_data.data is not a tensor, we count the memory as 0
                 total_bytes = 0
 
         return total_bytes
 
     def generate(self) -> List[ShardingStrategy]:
         """
         Generate all possible sharding strategies for this operation.
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/options.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/options.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/sharding_strategy.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/sharding_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 
 class CostGraph:
     '''
     A graph data structure to simplify the edge cost graph. It has two main functions:
     1. To feed the quadratic resharding costs into solver, we need to linearize it. We build edge_cost in
     CostGraph, and it stored every combinations of strategies for a src-dst node pair in an 1D list.
     2. To reduce the searching space, we merge computationally-trivial operators, such as
-    element-wise operators, transpose, and reduction, into their following nodes. The merging infomation will
+    element-wise operators, transpose, and reduction, into their following nodes. The merging information will
     be given by the StrategiesVector depending on the type of target node and following nodes.
 
     Argument:
         leaf_strategies(List[StrategiesVector]): It stores StrategiesVector of every nodes on the graph.
         simplify(bool, optional): The generated cost graph will be simplified if it is true. (default to True)
     '''
 
@@ -86,15 +86,15 @@
 
             setattr(dst_node, 'parents', parent_nodes)
             setattr(dst_node, 'children', children_nodes)
 
             if self.simplify and strategies_vector.check_merge():
                 for followed_node in strategies_vector.predecessor_nodes:
                     # we only merge node pairs which src node has a tensor element inside.
-                    # This is necessay because the node without a tensor element inside will not
+                    # This is necessary because the node without a tensor element inside will not
                     # be assigned any strategy.
                     if _check_tensor_in_node(followed_node._meta_data):
                         self.merge_pair.append((followed_node, dst_node))
 
     def get_edge_cost(self, src_node, dst_node):
         return self.edge_costs[(src_node, dst_node)]
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py`

 * *Files 1% similar despite different names*

```diff
@@ -79,35 +79,35 @@
         """
         Return the Graph object associated with this analyser.
         """
         return self._graph
 
     def liveness_analysis(self) -> List[LiveStage]:
         """
-        Analyse the graph to obtain the variable liveness information. This function returns
+        Analyses the graph to obtain the variable liveness information. This function returns
         an ordered dictionary where the key is the compute stage ID and the value is a LivenessStage object.
         """
         compute_nodes = self.graph.nodes
         liveness_list = []
 
         # checked: record all variables created since the first stage
         # all: record the live variables only exist until the current stage.
-        #       this can be different from the `checked list`` as some varialbes may be destroyed prior to this stage.
+        #       this can be different from the `checked list`` as some variables may be destroyed prior to this stage.
         # unique: record the unique live variables only exist until the current stage.
         #       this is different from `all list` as some variables are duplicated.
         checked_variables = LiveVariableVector()
         all_live_variables = LiveVariableVector()
         unique_live_vars = LiveVariableVector()
 
         for idx, node in enumerate(compute_nodes):
             #############################
             # find new living variables #
             #############################
             # detect whether the current op is an in-place op
-            # if it is an in-place op, we would deem it as a duplciate var
+            # if it is an in-place op, we would deem it as a duplicate var
             is_inplace = False
             if node.op == 'call_function':
                 # check if this is an inplace op such as torch.nn.functional.relu(x, inplace=True)
                 if node.kwargs.get('inplace', False):
                     is_inplace = True
             elif node.op == 'call_module':
                 # to check if this is an inplace op such as torch.nn.Relu(inplace=True)
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/solver.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/solver.py`

 * *Files 1% similar despite different names*

```diff
@@ -40,15 +40,15 @@
                  verbose=False):
         '''
         Solver class will integrate information provided by the components and use ILP solver to find a possible optimal strategies combination for target computing graph.
         Argument:
             graph: The computing graph to be optimized.
             strategies_constructor: It will provide all the possible strategies for each node in the computing graph.
             cost_graph: A graph data structure to simplify the edge cost graph.
-            graph_analyser: graph_analyser will analyse the graph to obtain the variable liveness information, which will be used to generate memory constraints.
+            graph_analyser: graph_analyser will analyses the graph to obtain the variable liveness information, which will be used to generate memory constraints.
             memory_budget: Memory constraint for the solution.
             solution_numbers: If solution_numbers is larger than one, solver will us a serious of solutions based on different memory budget.
             memory_increasing_coefficient: If solution_numbers is larger than one, we will use this coefficient to generate new memory budget.
         '''
         self.graph = graph
         self.strategies_constructor = strategies_constructor
         self.cost_graph = cost_graph
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py`

 * *Files 10% similar despite different names*

```diff
@@ -133,43 +133,43 @@
                 submod_type = type(submod)
                 handler = operator_registry.get(submod_type)(node,
                                                              self.device_mesh,
                                                              strategies_vector,
                                                              shard_option=self.solver_options.shard_option,
                                                              solver_perference=self.solver_options.solver_perference)
                 handler.register_strategy()
-                # attach metainfo_vector to node
-                if hasattr(handler, 'metainfo_vector'):
-                    setattr(node, 'metainfo_vector', handler.metainfo_vector)
+                # attach strategies_info to node
+                if hasattr(handler, 'strategies_info'):
+                    setattr(node, 'strategies_info', handler.strategies_info)
 
             # call_function node
             elif node.op == 'call_function':
                 target = node.target
                 handler = operator_registry.get(target)(node,
                                                         self.device_mesh,
                                                         strategies_vector,
                                                         shard_option=self.solver_options.shard_option,
                                                         solver_perference=self.solver_options.solver_perference)
                 handler.register_strategy()
-                # attach metainfo_vector to node
-                if hasattr(handler, 'metainfo_vector'):
-                    setattr(node, 'metainfo_vector', handler.metainfo_vector)
+                # attach strategies_info to node
+                if hasattr(handler, 'strategies_info'):
+                    setattr(node, 'strategies_info', handler.strategies_info)
 
             # call_method node
             elif node.op == 'call_method':
                 method = getattr(node.args[0]._meta_data.__class__, node.target)
                 handler = operator_registry.get(method)(node,
                                                         self.device_mesh,
                                                         strategies_vector,
                                                         shard_option=self.solver_options.shard_option,
                                                         solver_perference=self.solver_options.solver_perference)
                 handler.register_strategy()
-                # attach metainfo_vector to node
-                if hasattr(handler, 'metainfo_vector'):
-                    setattr(node, 'metainfo_vector', handler.metainfo_vector)
+                # attach strategies_info to node
+                if hasattr(handler, 'strategies_info'):
+                    setattr(node, 'strategies_info', handler.strategies_info)
 
             # output node
             elif node.op == 'output':
                 if self.solver_options.dataloader_option == DataloaderOption.DISTRIBUTED:
                     output_option = 'distributed'
                 else:
                     assert self.solver_options.dataloader_option == DataloaderOption.REPLICATED, f'placeholder_option {self.solver_options.dataloader_option} is not supported'
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/__init__.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/broadcast.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/broadcast.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     'BroadcastType', 'is_broadcastable', 'get_broadcast_shape', 'recover_sharding_spec_for_broadcast_shape',
     'comm_actions_for_oprands'
 ]
 
 
 class BroadcastType(Enum):
     EQUAL = auto()
-    PADDDING = auto()
+    PADDING = auto()
     MULTIPLE = auto()
 
 
 def is_broadcastable(shape1: torch.Size, shape2: torch.Size) -> bool:
     """
     Check if two shapes are broadcastable to each other.
     """
@@ -65,26 +65,26 @@
 
     # track the dim and its broadcasting type
     logical_dim_broadcast_info = {}
 
     for i in range(logical_num_dims):
         # get the trailing dim size
         logical_dim_idx = logical_num_dims - i - 1
-        phyiscal_dim_idx = physical_num_dims - i - 1
+        physical_dim_idx = physical_num_dims - i - 1
         logical_dim_size = logical_shape[logical_dim_idx]
 
-        if phyiscal_dim_idx >= 0:
-            physical_dim_size = physical_shape[phyiscal_dim_idx]
+        if physical_dim_idx >= 0:
+            physical_dim_size = physical_shape[physical_dim_idx]
 
             if physical_dim_size == logical_dim_size:
                 logical_dim_broadcast_info[logical_dim_idx] = BroadcastType.EQUAL
             elif physical_dim_size == 1 and physical_dim_size != logical_dim_size:
                 logical_dim_broadcast_info[logical_dim_idx] = BroadcastType.MULTIPLE
         else:
-            logical_dim_broadcast_info[logical_dim_idx] = BroadcastType.PADDDING
+            logical_dim_broadcast_info[logical_dim_idx] = BroadcastType.PADDING
 
     return logical_dim_broadcast_info
 
 
 def recover_sharding_spec_for_broadcast_shape(logical_sharding_spec: ShardingSpec, logical_shape: torch.Size,
                                               physical_shape: torch.Size) -> ShardingSpec:
     """
@@ -113,15 +113,15 @@
     # generate the sharding spec for the physical shape
     physical_dim_partition = {}
     logical_dim_partition = logical_sharding_spec.dim_partition_dict
 
     for shape_dim, mesh_dim in logical_dim_partition.items():
         logical_broadcast_type = logical_dim_broadcast_info[shape_dim]
 
-        if logical_broadcast_type == BroadcastType.PADDDING or logical_broadcast_type == BroadcastType.MULTIPLE:
+        if logical_broadcast_type == BroadcastType.PADDING or logical_broadcast_type == BroadcastType.MULTIPLE:
             removed_dims.extend(mesh_dim)
         else:
             # get the corresponding physical dim
             physical_dim = physical_num_dims - (logical_num_dims - shape_dim)
             physical_dim_partition[physical_dim] = mesh_dim
 
     physical_sharding_spec = ShardingSpec(device_mesh=logical_sharding_spec.device_mesh,
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/factory.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/factory.py`

 * *Files 0% similar despite different names*

```diff
@@ -26,15 +26,15 @@
     Args:
         input_ (Union[Node, torch.Tensor]): the input can be a Node object or a PyTorch tensor. If a node is used, it will look for its meta data associated with this node.
         device_mesh (DeviceMesh): a DeviceMesh object which contains the meta information about the cluster.
         dim_partition_dict (Dict[int, List[int]]): a dictionary to specify the sharding specs, the key is the tensor dimension and the value is the mesh dimension for sharding.
     """
 
     if isinstance(input_, Node):
-        assert hasattr(input_, '_meta_data'), f'The given node has no attribte _meta_data'
+        assert hasattr(input_, '_meta_data'), f'The given node has no attribute _meta_data'
         meta_tensor = input_._meta_data
         assert meta_tensor is not None, "The given node's _meta_data attribute is None"
         shape = meta_tensor.shape
     elif isinstance(input_, torch.Tensor):
         shape = input_.shape
     else:
         raise TypeError(
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/misc.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/misc.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/reshape.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/reshape.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,20 +2,20 @@
 from typing import Dict, List, Tuple
 
 import torch
 
 
 class PreviousStatus(Enum):
     """
-    This class shows the status of previous comparision.
+    This class shows the status of previous comparison.
     """
     RESET = 0
-    # ORIGIN means the dimension size of original tensor is larger in the previous comparision.
+    # ORIGIN means the dimension size of original tensor is larger in the previous comparison.
     ORIGIN = 1
-    # TGT means the dimension size of target tensor is larger in the previous comparision.
+    # TGT means the dimension size of target tensor is larger in the previous comparison.
     TGT = 2
 
 
 def detect_reshape_mapping(origin_shape: torch.Size, tgt_shape: torch.Size) -> Dict[Tuple[int], Tuple[int]]:
     """
     This method is used to detect the reshape mapping between original tensor and target tensor.
 
@@ -87,15 +87,15 @@
 
             previous_label = PreviousStatus.RESET
 
         elif original_dimension_size > tgt_dimension_size:
             tgt_index += 1
 
             if previous_label == PreviousStatus.TGT:
-                # if the target dimension size is larger in the previous comparision, which means
+                # if the target dimension size is larger in the previous comparison, which means
                 # the origin dimension size has already accumulated larger than target dimension size, so
                 # we need to offload the origin dims and tgt dims into the reshape_mapping_dict.
                 reshape_mapping_dict[tuple(origin_dims)] = tuple(tgt_dims)
                 original_dimension_size = original_dimension_size // tgt_dimension_size
                 origin_dims = [origin_len - origin_index - 1]
                 tgt_dimension_size = tgt_shape[tgt_index]
                 tgt_dims = [tgt_len - tgt_index - 1, tgt_len - tgt_index]
@@ -107,15 +107,15 @@
                 tgt_dims.append(tgt_len - tgt_index - 1)
                 previous_label = PreviousStatus.ORIGIN
 
         else:
             origin_index += 1
 
             if previous_label == PreviousStatus.ORIGIN:
-                # if the origin element is larger in the previous comparision, which means
+                # if the origin element is larger in the previous comparison, which means
                 # the target element has already accumulated larger than origin element, so
                 # we need to offload the origin dims and tgt dims into the reshape_mapping_dict.
                 reshape_mapping_dict[tuple(origin_dims)] = tuple(tgt_dims)
                 tgt_dimension_size = tgt_dimension_size // original_dimension_size
                 tgt_dims = [tgt_len - tgt_index - 1]
                 original_dimension_size = origin_shape[origin_index]
                 origin_dims = [origin_len - origin_index - 1, origin_len - origin_index]
@@ -135,15 +135,15 @@
     """
     This method is used to check whether the reshape operation could implement without converting
     the input to fully replicated status.
 
     Rule:
         For a sharded dimension of input tensor, if it is not the minimum element of the input tuple,
         the function will return false.
-        To illustrate this issue, there are two cases to analyse:
+        To illustrate this issue, there are two cases to analyze:
         1. no sharded dims in the input tuple: we could do the reshape operation safely just as the normal
         operation without distributed tensor.
         2. sharded dims in the input tuple: the sharded dim must be the minimum element, then during shape
         consistency process, torch.cat will be implemented on the sharded dim, and everything after the sharded
         dim get recovered.
 
     Examples:
```

### Comparing `colossalai-0.2.8/colossalai/auto_parallel/tensor_shard/utils/sharding.py` & `colossalai-0.3.0/colossalai/auto_parallel/tensor_shard/utils/sharding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/booster/accelerator.py` & `colossalai-0.3.0/colossalai/booster/accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/booster/booster.py` & `colossalai-0.3.0/colossalai/booster/plugin/torch_fsdp_plugin.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,174 +1,221 @@
-import warnings
-from contextlib import contextmanager
-from typing import Callable, Iterator, List, Optional, Tuple, Union
+from pathlib import Path
+from typing import Callable, Iterable, Iterator, List, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
+import warnings
+from packaging import version
+from torch.distributed import ProcessGroup
+
+if version.parse(torch.__version__) >= version.parse('1.12.0'):
+    from torch.distributed.fsdp import FullStateDictConfig
+    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
+    from torch.distributed.fsdp import StateDictType
+    from torch.distributed.fsdp.fully_sharded_data_parallel import (
+        BackwardPrefetch,
+        CPUOffload,
+        FullStateDictConfig,
+        MixedPrecision,
+        ShardingStrategy,
+    )
+else:
+    raise RuntimeError("FSDP is not supported while torch version under 1.12.0.")
+
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils.data import DataLoader
 
-from colossalai.checkpoint_io import GeneralCheckpointIO
+from colossalai.checkpoint_io import CheckpointIO, GeneralCheckpointIO, utils
+from colossalai.cluster import DistCoordinator
+from colossalai.interface import ModelWrapper, OptimizerWrapper
+
+from .dp_plugin_base import DPPluginBase
+
+__all__ = ['TorchFSDPPlugin']
+
+
+class TorchFSDPCheckpointIO(GeneralCheckpointIO):
+
+    def __init__(self) -> None:
+        super().__init__()
+        self.coordinator = DistCoordinator()
+
+    def load_unsharded_model(self, model: nn.Module, checkpoint: str, strict: bool):
+        checkpoint = utils.load_state_dict(checkpoint)
+        model.load_state_dict(checkpoint)
+
+    def load_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: Path):
+        checkpoint = utils.load_state_dict(checkpoint)
+        fsdp_model = optimizer.unwrap_model()
+        sharded_osd = FSDP.scatter_full_optim_state_dict(checkpoint, fsdp_model)
+        optimizer.load_state_dict(sharded_osd)
+
+    def save_unsharded_model(self, model: nn.Module, checkpoint: str, gather_dtensor: bool, use_safetensors: bool):
+        """
+        Save model to checkpoint but only on master process.
+        """
+        # the model should be unwrapped in self.load_model via ModelWrapper.unwrap
+        cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
+        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, cfg):
+            full_model_state = model.state_dict()
+        utils.save_state_dict(full_model_state, checkpoint_file_path=checkpoint, use_safetensors=use_safetensors)
+
+    def save_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: str, gather_dtensor: bool):
+        """
+        Save optimizer to checkpoint but only on master process.
+        """
+        assert isinstance(optimizer, FSDPOptimizerWrapper)
+        fsdp_model = optimizer.unwrap_model()
+        full_optimizer_state = FSDP.full_optim_state_dict(fsdp_model, optim=optimizer, rank0_only=True)
+        utils.save_state_dict(full_optimizer_state, checkpoint_file_path=checkpoint, use_safetensors=False)
 
-from .accelerator import Accelerator
-from .mixed_precision import MixedPrecision, mixed_precision_factory
-from .plugin import Plugin
+    def save_sharded_model(self, model: nn.Module, checkpoint: str, gather_dtensor: bool, variant: Optional[str],
+                           size_per_shard: int, use_safetensors: bool):
+        """
+        Save model to checkpoint but only on master process.
+        """
+        raise NotImplementedError("Sharded model checkpoint is not supported yet.")
 
-__all__ = ['Booster']
+    def load_sharded_model(self,
+                           model: nn.Module,
+                           checkpoint_index_file: Path,
+                           strict: bool = False,
+                           use_safetensors: bool = False,
+                           load_sub_module: bool = True):
+        """
+        Load model to checkpoint but only on master process.
+        """
+        raise NotImplementedError("Sharded model checkpoint is not supported yet.")
 
+    def save_sharded_optimizer(self, optimizer: Optimizer, checkpoint: str, gather_dtensor: bool):
+        """
+        Save optimizer to checkpoint but only on master process.
+        """
+        raise NotImplementedError("Sharded optimizer checkpoint is not supported yet.")
 
-class Booster:
+    def load_sharded_optimizer(self, optimizer: Optimizer, index_file_path: str, prefix: str, size_per_shard: int):
+        """
+        Load optimizer to checkpoint but only on master process.
+        """
+        raise NotImplementedError("Sharded optimizer checkpoint is not supported yet.")
+
+    def save_lr_scheduler(self, lr_scheduler: LRScheduler, checkpoint: str):
+        """
+        Save model to checkpoint but only on master process.
+        """
+        if self.coordinator.is_master():
+            super().save_lr_scheduler(lr_scheduler, checkpoint)
+
+
+class TorchFSDPModel(ModelWrapper):
+
+    def __init__(self, module: nn.Module, *args, **kwargs) -> None:
+        super().__init__(module)
+        self.module = FSDP(module, *args, **kwargs)
+
+    def unwrap(self):
+        return self.module
+
+
+class FSDPOptimizerWrapper(OptimizerWrapper):
+
+    def __init__(self, optimizer: Optimizer, model: nn.Module):
+        self.model = model
+        super().__init__(optimizer)
+
+    def unwrap_model(self) -> nn.Module:
+        return self.model
+
+
+class TorchFSDPPlugin(DPPluginBase):
     """
-    Booster is a high-level API for training neural networks. It provides a unified interface for
-    training with different precisio, accelerator, and plugin.
+    Plugin for PyTorch FSDP.
 
-    Examples:
-        >>> colossalai.launch(...)
-        >>> plugin = GeminiPlugin(stage=3, ...)
-        >>> booster = Booster(precision='fp16', plugin=plugin)
+    Example:
+        >>> from colossalai.booster import Booster
+        >>> from colossalai.booster.plugin import TorchFSDPPlugin
         >>>
-        >>> model = GPT2()
-        >>> optimizer = Adam(model.parameters())
-        >>> dataloader = Dataloader(Dataset)
-        >>> lr_scheduler = LinearWarmupScheduler()
-        >>> criterion = GPTLMLoss()
-        >>>
-        >>> model, optimizer, lr_scheduler, dataloader = booster.boost(model, optimizer, lr_scheduler, dataloader)
-        >>>
-        >>> for epoch in range(max_epochs):
-        >>>     for input_ids, attention_mask in dataloader:
-        >>>         outputs = model(input_ids, attention_mask)
-        >>>         loss = criterion(outputs.logits, input_ids)
-        >>>         booster.backward(loss, optimizer)
-        >>>         optimizer.step()
-        >>>         lr_scheduler.step()
-        >>>         optimizer.zero_grad()
+        >>> model, train_dataset, optimizer, criterion = ...
+        >>> plugin = TorchFSDPPlugin()
 
+        >>> train_dataloader = plugin.prepare_train_dataloader(train_dataset, batch_size=8)
+        >>> booster = Booster(plugin=plugin)
+        >>> model, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)
 
     Args:
-        device (str or torch.device): The device to run the training. Default: 'cuda'.
-        mixed_precision (str or MixedPrecision): The mixed precision to run the training. Default: None.
-                                If the argument is a string, it can be 'fp16', 'fp16_apex', 'bf16', or 'fp8'.
-                                'fp16' would use PyTorch AMP while `fp16_apex` would use Nvidia Apex.
-        plugin (Plugin): The plugin to run the training. Default: None.
+        See https://pytorch.org/docs/stable/fsdp.html for details.
     """
 
-    def __init__(self,
-                 device: str = 'cuda',
-                 mixed_precision: Union[MixedPrecision, str] = None,
-                 plugin: Optional[Plugin] = None) -> None:
-        if plugin is not None:
-            assert isinstance(
-                plugin, Plugin), f'Expected the argument plugin to be an instance of Plugin, but got {type(plugin)}.'
-        self.plugin = plugin
-
-        # set accelerator
-        if self.plugin and self.plugin.control_device():
-            self.accelerator = None
-            warnings.warn('The plugin will control the accelerator, so the device argument will be ignored.')
-        else:
-            self.accelerator = Accelerator(device)
-
-        # set precision
-        if self.plugin and self.plugin.control_precision():
-            warnings.warn('The plugin will control the precision, so the mixed_precision argument will be ignored.')
-            self.mixed_precision = None
-        elif mixed_precision is None:
-            self.mixed_precision = None
-        else:
-            # validate and set precision
-            if isinstance(mixed_precision, str):
-                # the user will take the default arguments for amp training
-                self.mixed_precision = mixed_precision_factory(mixed_precision)
-            elif isinstance(mixed_precision, MixedPrecision):
-                # the user can customize the arguments by passing the precision object
-                self.mixed_precision = mixed_precision
-            else:
-                raise ValueError(
-                    f'Expected the argument mixed_precision to be a string or an instance of Precision, but got {type(mixed_precision)}.'
-                )
-
-        if self.plugin is not None and self.plugin.control_checkpoint_io():
-            self.checkpoint_io = self.plugin.get_checkpoint_io()
-        else:
-            self.checkpoint_io = GeneralCheckpointIO()
+    if version.parse(torch.__version__) >= version.parse('1.12.0'):
+
+        def __init__(
+            self,
+            process_group: Optional[ProcessGroup] = None,
+            sharding_strategy: Optional[ShardingStrategy] = None,
+            cpu_offload: Optional[CPUOffload] = None,
+            auto_wrap_policy: Optional[Callable] = None,
+            backward_prefetch: Optional[BackwardPrefetch] = None,
+            mixed_precision: Optional[MixedPrecision] = None,
+            ignored_modules: Optional[Iterable[torch.nn.Module]] = None,
+            param_init_fn: Optional[Callable[[nn.Module], None]] = None,
+            sync_module_states: bool = False,
+        ):
+            super().__init__()
+            self.fsdp_kwargs = dict(process_group=process_group,
+                                    sharding_strategy=sharding_strategy,
+                                    cpu_offload=cpu_offload,
+                                    auto_wrap_policy=auto_wrap_policy,
+                                    backward_prefetch=backward_prefetch,
+                                    mixed_precision=mixed_precision,
+                                    ignored_modules=ignored_modules,
+                                    param_init_fn=param_init_fn,
+                                    sync_module_states=sync_module_states)
+    else:
+        raise RuntimeError("FSDP is not supported while torch version under 1.12.0.")
+
+    def support_no_sync(self) -> bool:
+        False
+
+    def no_sync(self, model: nn.Module) -> Iterator[None]:
+        raise NotImplementedError("Torch fsdp no_sync func not supported yet.")
+
+    def control_precision(self) -> bool:
+        return True
+
+    def supported_precisions(self) -> List[str]:
+        return ['fp16', 'bf16']
 
-    def boost(
+    def control_device(self) -> bool:
+        return True
+
+    def supported_devices(self) -> List[str]:
+        return ['cuda']
+
+    def configure(
         self,
         model: nn.Module,
         optimizer: Optimizer,
         criterion: Callable = None,
         dataloader: DataLoader = None,
         lr_scheduler: LRScheduler = None,
-    ) -> List[Union[nn.Module, Optimizer, LRScheduler, DataLoader]]:
-        """
-        Boost the model, optimizer, criterion, lr_scheduler, and dataloader.
+    ) -> Tuple[Union[nn.Module, OptimizerWrapper, LRScheduler, DataLoader]]:
 
-        Args:
-            model (nn.Module): The model to be boosted.
-            optimizer (Optimizer): The optimizer to be boosted.
-            criterion (Callable): The criterion to be boosted.
-            dataloader (DataLoader): The dataloader to be boosted.
-            lr_scheduler (LRScheduler): The lr_scheduler to be boosted.
-        """
-        # TODO(FrankLeeeee): consider multi-model and multi-optimizer case
-        # TODO(FrankLeeeee): consider multi-dataloader case
-        # transform model for mixed precision
-        if self.plugin:
-            model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
-                model, optimizer, criterion, dataloader, lr_scheduler)
-
-        if self.plugin and not self.plugin.control_device():
-            # transform model for accelerator
-            model = self.accelerator.configure(model)
-
-        if self.mixed_precision and (self.plugin is None or self.plugin and not self.plugin.control_precision()):
-            # transform model for mixed precision
-            # when mixed_precision is specified and the plugin is not given or does not control the precision
-            model, optimizer, criterion = self.mixed_precision.configure(model, optimizer, criterion)
-
-        return model, optimizer, criterion, dataloader, lr_scheduler
-
-    def backward(self, loss: torch.Tensor, optimizer: Optimizer) -> None:
-        # TODO: implement this method with plugin
-        optimizer.backward(loss)
-
-    def execute_pipeline(self,
-                         data_iter: Iterator,
-                         model: nn.Module,
-                         criterion: Callable[[torch.Tensor], torch.Tensor],
-                         optimizer: Optimizer,
-                         return_loss: bool = True,
-                         return_outputs: bool = False) -> Tuple[Optional[torch.Tensor], ...]:
-        # TODO: implement this method
-        # run pipeline forward backward pass
-        # return loss or outputs if needed
-        pass
-
-    def no_sync(self, model: nn.Module) -> contextmanager:
-        assert self.plugin is not None, f'no_sync is only enabled when a plugin is provided and the plugin supports no_sync.'
-        assert self.plugin.support_no_sync, f'The plugin {self.plugin.__class__.__name__} does not support no_sync.'
-        return self.plugin.no_sync(model)
-
-    def load_model(self, model: nn.Module, checkpoint: str, strict: bool = True):
-        self.checkpoint_io.load_model(model, checkpoint, strict)
-
-    def save_model(self,
-                   model: nn.Module,
-                   checkpoint: str,
-                   prefix: str = None,
-                   shard: bool = False,
-                   size_per_shard: int = 1024):
-        self.checkpoint_io.save_model(model, checkpoint, prefix, shard, size_per_shard)
+        # wrap the model with PyTorch FSDP
+        fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
 
-    def load_optimizer(self, optimizer: Optimizer, checkpoint: str):
-        self.checkpoint_io.load_optimizer(optimizer, checkpoint)
+        if len(optimizer.param_groups) > 1:
+            warnings.warn(
+                'TorchFSDPPlugin does not support optimizer that use multi param groups. The results may not be as expected if used.'
+            )
+        optimizer.__init__(fsdp_model.parameters(), **optimizer.defaults)
 
-    def save_optimizer(self, optimizer: Optimizer, checkpoint: str, shard: bool = False, size_per_shard: int = 1024):
-        self.checkpoint_io.save_optimizer(optimizer, checkpoint, shard, size_per_shard)
+        if not isinstance(optimizer, FSDPOptimizerWrapper):
+            optimizer = FSDPOptimizerWrapper(optimizer, fsdp_model)
 
-    def save_lr_scheduler(self, lr_scheduler: LRScheduler, checkpoint: str):
-        self.checkpoint_io.save_lr_scheduler(lr_scheduler, checkpoint)
+        return fsdp_model, optimizer, criterion, dataloader, lr_scheduler
+
+    def control_checkpoint_io(self) -> bool:
+        return True
 
-    def load_lr_scheduler(self, lr_scheduler: LRScheduler, checkpoint: str):
-        self.checkpoint_io.load_lr_scheduler(lr_scheduler, checkpoint)
+    def get_checkpoint_io(self) -> CheckpointIO:
+        return TorchFSDPCheckpointIO()
```

### Comparing `colossalai-0.2.8/colossalai/booster/mixed_precision/__init__.py` & `colossalai-0.3.0/colossalai/booster/mixed_precision/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 from .bf16 import BF16MixedPrecision
 from .fp8 import FP8MixedPrecision
 from .fp16_apex import FP16ApexMixedPrecision
+from .fp16_naive import FP16NaiveMixedPrecision
 from .fp16_torch import FP16TorchMixedPrecision
 from .mixed_precision_base import MixedPrecision
 
 __all__ = [
     'MixedPrecision', 'mixed_precision_factory', 'FP16_Apex_MixedPrecision', 'FP16_Torch_MixedPrecision',
-    'FP32_MixedPrecision', 'BF16_MixedPrecision', 'FP8_MixedPrecision'
+    'FP32_MixedPrecision', 'BF16_MixedPrecision', 'FP8_MixedPrecision', 'FP16NaiveMixedPrecision'
 ]
 
 _mixed_precision_mapping = {
     'fp16': FP16TorchMixedPrecision,
     'fp16_apex': FP16ApexMixedPrecision,
+    'fp16_naive': FP16NaiveMixedPrecision,
     'bf16': BF16MixedPrecision,
     'fp8': FP8MixedPrecision
 }
 
 
 def mixed_precision_factory(mixed_precision_type: str) -> MixedPrecision:
     """
```

### Comparing `colossalai-0.2.8/colossalai/booster/mixed_precision/fp16_torch.py` & `colossalai-0.3.0/colossalai/booster/mixed_precision/fp16_torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/booster/mixed_precision/mixed_precision_base.py` & `colossalai-0.3.0/colossalai/booster/mixed_precision/mixed_precision_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/booster/plugin/plugin_base.py` & `colossalai-0.3.0/colossalai/booster/plugin/plugin_base.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from abc import ABC, abstractmethod
-from typing import Callable, List, Tuple, Union
+from typing import Callable, Iterator, List, Tuple, Union
 
 import torch.nn as nn
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
-from torch.utils.data import DataLoader
+from torch.utils.data import DataLoader, Dataset
 
 from colossalai.checkpoint_io import CheckpointIO
 from colossalai.interface import OptimizerWrapper
 
 __all__ = ['Plugin']
 
 
@@ -55,7 +55,29 @@
 
     @abstractmethod
     def get_checkpoint_io(self) -> CheckpointIO:
         """
         Get checkpoint io object for this plugin, only invoked when control_checkpoint_io is True.
         """
         pass
+
+    @abstractmethod
+    def no_sync(self, model: nn.Module) -> Iterator[None]:
+        """
+        Context manager to disable gradient synchronization.
+        """
+        pass
+
+    @abstractmethod
+    def prepare_dataloader(self,
+                           dataset: Dataset,
+                           batch_size: int,
+                           shuffle: bool = False,
+                           seed: int = 1024,
+                           drop_last: bool = False,
+                           pin_memory: bool = False,
+                           num_workers: int = 0,
+                           **kwargs):
+        """Prepare a dataloader for distributed training. The dataloader will be wrapped by
+        `torch.utils.data.DataLoader`
+        """
+        pass
```

### Comparing `colossalai-0.2.8/colossalai/booster/plugin/torch_ddp_plugin.py` & `colossalai-0.3.0/colossalai/booster/plugin/torch_ddp_plugin.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,20 @@
-import random
-from typing import Callable, List, Tuple, Union
+from typing import Callable, Iterator, List, Optional, Tuple, Union
 
-import numpy as np
-import torch
-import torch.distributed as dist
 import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils.data import DataLoader
-from torch.utils.data.distributed import DistributedSampler
 
 from colossalai.checkpoint_io import CheckpointIO, GeneralCheckpointIO
 from colossalai.cluster import DistCoordinator
 from colossalai.interface import ModelWrapper, OptimizerWrapper
 
-from .plugin_base import Plugin
+from .dp_plugin_base import DPPluginBase
 
 __all__ = ['TorchDDPPlugin']
 
 
 class TorchDDPCheckpointIO(GeneralCheckpointIO):
 
     def __init__(self) -> None:
@@ -29,59 +24,69 @@
     def load_unsharded_model(self, model: nn.Module, checkpoint: str, strict: bool = True):
         """
         Load model from checkpoint with automatic unwrapping.
         """
         # the model should be unwrapped in self.load_model via ModelWrapper.unwrap
         return super().load_unsharded_model(model, checkpoint, strict=strict)
 
-    def save_unsharded_model(self, model: nn.Module, checkpoint: str):
+    def save_unsharded_model(self, model: nn.Module, checkpoint: str, gather_dtensor: bool, use_safetensors: bool):
         """
         Save model to checkpoint but only on master process.
         """
         # the model should be unwrapped in self.load_model via ModelWrapper.unwrap
         if self.coordinator.is_master():
-            super().save_unsharded_model(model, checkpoint)
+            super().save_unsharded_model(model, checkpoint, gather_dtensor, use_safetensors)
 
-    def save_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: str):
+    def save_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: str, gather_dtensor: bool):
         """
         Save optimizer to checkpoint but only on master process.
         """
         if self.coordinator.is_master():
-            super().save_unsharded_optimizer(optimizer, checkpoint)
+            super().save_unsharded_optimizer(optimizer, checkpoint, gather_dtensor)
 
     def save_lr_scheduler(self, lr_scheduler: LRScheduler, checkpoint: str):
         """
         Save model to checkpoint but only on master process.
         """
         if self.coordinator.is_master():
             super().save_lr_scheduler(lr_scheduler, checkpoint)
 
+    def save_sharded_model(self,
+                           model: nn.Module,
+                           checkpoint_path: str,
+                           gather_dtensor: bool = False,
+                           variant: Optional[str] = None,
+                           max_shard_size: int = 1024,
+                           use_safetensors: bool = False):
+        if self.coordinator.is_master():
+            super().save_sharded_model(model, checkpoint_path, gather_dtensor, variant, max_shard_size, use_safetensors)
+
 
 class TorchDDPModel(ModelWrapper):
 
     def __init__(self, module: nn.Module, *args, **kwargs) -> None:
         super().__init__(module)
         self.module = DDP(module, *args, **kwargs)
 
     def unwrap(self):
         return self.module.module
 
 
-class TorchDDPPlugin(Plugin):
+class TorchDDPPlugin(DPPluginBase):
     """
     Plugin for PyTorch DDP.
 
     Example:
         >>> from colossalai.booster import Booster
         >>> from colossalai.booster.plugin import TorchDDPPlugin
         >>>
         >>> model, train_dataset, optimizer, criterion = ...
         >>> plugin = TorchDDPPlugin()
 
-        >>> train_dataloader = plugin.prepare_train_dataloader(train_dataset, batch_size=8)
+        >>> train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)
         >>> booster = Booster(plugin=plugin)
         >>> model, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)
 
     Args:
         broadcast_buffers (bool, optional): Whether to broadcast buffers in the beginning of training. Defaults to True.
         bucket_cap_mb (int, optional): The bucket size in MB. Defaults to 25.
         find_unused_parameters (bool, optional): Whether to find unused parameters. Defaults to False.
@@ -93,19 +98,15 @@
     def __init__(self,
                  broadcast_buffers: bool = True,
                  bucket_cap_mb: int = 25,
                  find_unused_parameters: bool = False,
                  check_reduction: bool = False,
                  gradient_as_bucket_view: bool = False,
                  static_graph: bool = False) -> None:
-
-        assert dist.is_initialized(
-        ), 'torch.distributed is not initialized, please use colossalai.launch to create the distributed environment'
-        self.rank = dist.get_rank()
-        self.world_size = dist.get_world_size()
+        super().__init__()
         self.ddp_kwargs = dict(broadcast_buffers=broadcast_buffers,
                                bucket_cap_mb=bucket_cap_mb,
                                find_unused_parameters=find_unused_parameters,
                                check_reduction=check_reduction,
                                gradient_as_bucket_view=gradient_as_bucket_view,
                                static_graph=static_graph)
 
@@ -120,65 +121,14 @@
 
     def control_device(self) -> bool:
         return True
 
     def supported_devices(self) -> List[str]:
         return ['cuda']
 
-    def prepare_train_dataloader(self,
-                                 dataset,
-                                 batch_size,
-                                 shuffle=False,
-                                 seed=1024,
-                                 drop_last=False,
-                                 pin_memory=False,
-                                 num_workers=0,
-                                 **kwargs):
-        r"""
-        Prepare a dataloader for distributed training. The dataloader will be wrapped by
-        `torch.utils.data.DataLoader` and `torch.utils.data.DistributedSampler`.
-
-        Note:
-            1. Evaluation datasets should not be passed to this function.
-
-        Args:
-            dataset (`torch.utils.data.Dataset`): The dataset to be loaded.
-            shuffle (bool, optional): Whether to shuffle the dataset. Defaults to False.
-            seed (int, optional): Random worker seed for sampling, defaults to 1024.
-            add_sampler: Whether to add ``DistributedDataParallelSampler`` to the dataset. Defaults to True.
-            drop_last (bool, optional): Set to True to drop the last incomplete batch, if the dataset size
-                is not divisible by the batch size. If False and the size of dataset is not divisible by
-                the batch size, then the last batch will be smaller, defaults to False.
-            pin_memory (bool, optional): Whether to pin memory address in CPU memory. Defaults to False.
-            num_workers (int, optional): Number of worker threads for this dataloader. Defaults to 0.
-            kwargs (dict): optional parameters for ``torch.utils.data.DataLoader``, more details could be found in
-                    `DataLoader <https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader>`_.
-
-        Returns:
-            :class:`torch.utils.data.DataLoader`: A DataLoader used for training or testing.
-        """
-        _kwargs = kwargs.copy()
-        sampler = DistributedSampler(dataset, num_replicas=self.world_size, rank=self.rank, shuffle=shuffle)
-
-        # Deterministic dataloader
-        def seed_worker(worker_id):
-            worker_seed = seed
-            np.random.seed(worker_seed)
-            torch.manual_seed(worker_seed)
-            random.seed(worker_seed)
-
-        return DataLoader(dataset,
-                          batch_size=batch_size,
-                          sampler=sampler,
-                          worker_init_fn=seed_worker,
-                          drop_last=drop_last,
-                          pin_memory=pin_memory,
-                          num_workers=num_workers,
-                          **_kwargs)
-
     def configure(
         self,
         model: nn.Module,
         optimizer: Optimizer,
         criterion: Callable = None,
         dataloader: DataLoader = None,
         lr_scheduler: LRScheduler = None,
@@ -198,7 +148,11 @@
         return model, optimizer, criterion, dataloader, lr_scheduler
 
     def control_checkpoint_io(self) -> bool:
         return True
 
     def get_checkpoint_io(self) -> CheckpointIO:
         return TorchDDPCheckpointIO()
+
+    def no_sync(self, model: nn.Module) -> Iterator[None]:
+        assert isinstance(model, TorchDDPModel), 'Model is not boosted by TorchDDPPlugin.'
+        return model.module.no_sync()
```

### Comparing `colossalai-0.2.8/colossalai/builder/builder.py` & `colossalai-0.3.0/colossalai/builder/builder.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/checkpoint_io/checkpoint_io_base.py` & `colossalai-0.3.0/colossalai/checkpoint_io/checkpoint_io_base.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,43 +1,60 @@
-import json
 from abc import ABC, abstractmethod
 from pathlib import Path
-from typing import Any, Union
+from typing import Optional, Union
 
 import torch
 import torch.nn as nn
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 
 from colossalai.interface import ModelWrapper
 
-__all__ = ['CheckpointIO', 'ShardCheckpointIndexFile']
+from .utils import has_index_file
+
+__all__ = ['CheckpointIO']
 
 
 class CheckpointIO(ABC):
     """
     CheckpointIO is the base class for all checkpoint IO classes. It defines the interface for checkpoint IO.
 
 
     Examples:
         >>> from colossalai.checkpoint_io import GeneralCheckpointIO
         >>> checkpoint_io = CheckpointIO()
         >>>
         >>> # load model from checkpoint
         >>> model = checkpoint_io.load_model(model, 'model.pt')
         >>>
-        >>> # save model to checkpoint
+        >>> # save model to checkpoint, any distributed tensor is gathered by default
         >>> checkpoint_io.save_model(model, 'model.pt')
         >>>
+        >>> # if the model contains distributed tensor, and you don't want to gather it
+        >>> # each rank will save its own shard of the distributed tensor
+        >>> checkpoint_io.save_model(model, 'model.pt', gather_dtensor=False)
+        >>>
         >>> # save model to sharded checkpoints
         >>> checkpoint_io.save_model(model, './checkpoints/', shard=True)
         >>>
+        >>> # save model to sharded  and assume we don't want to gather distributed tensors
+        >>> checkpoint_io.save_model(model, './checkpoints/', shard=True, gather_dtensor=False)
+        >>>
+        >>> # Note:
+        >>> # 1. we don't support loading from distributed tensors, conversion from distributed tensors
+        >>> # checkpoints to full tensor checkpoint should be done offline via our CLI
+        >>> # 2. you don't have to specify whether the model is sharded or not when loading the model
+        >>> # as it will be automatically detected
+        >>>
         >>> # load model from sharded checkpoints
         >>> model = checkpoint_io.load_model(model, './checkpoints/')
         >>>
+        >>> # load model from unsharded checkpoints
+        >>> model = checkpoint_io.load_model(model, './checkpoints/')
+        >>>
         >>> # load optimizer from checkpoint
         >>> optimizer = checkpoint_io.load_optimizer(optimizer, 'optimizer.pt')
         >>>
         >>> # save optimizer to checkpoint
         >>> checkpoint_io.save_optimizer(optimizer, 'optimizer.pt')
     """
 
@@ -49,43 +66,50 @@
                    checkpoint: str,
                    strict: bool = True) -> Union[nn.Module, ModelWrapper]:
         """
         Load model from checkpoint.
 
         Args:
             model (nn.Module): model to be loaded.
-            checkpoint (str): checkpoint path. This value is made compatiblity with the model checkpoints in the
+            checkpoint (str): checkpoint path. This value is made compatibility with the model checkpoints in the
                         mainstream model zoos such as Hugging Face and TIMM. The checkpoint path can be:
                         1. a file path, e.g. 'model.pt'
                         2. a path to a json file which defines the index to the sharded checkpoint
                         3. a path to a folder containing a unique .index.json file for sharded checkpoint
+                        Distributed tensors cannot be loaded directly unless gathered offline via our CLI.
             strict (bool): whether to strictly enforce that the param name in
                 the checkpoint match the keys returned by this module's.
         """
-        ckpt_path = Path(checkpoint)
-        is_sharded = self.is_sharded_checkpoint(ckpt_path)
+        # since we only support loaded sharded and unsharded weight format
+        # containing no distributed tensors, dtensor -> full tensor conversion
+        # should be done offline via our CLI
+        # the existence of index file means it is a sharded checkpoint
+        index_file_exists, index_file_path = has_index_file(checkpoint)
 
+        # return the origin model instead of the unwrapped model
         origin_model = model
 
         if isinstance(model, ModelWrapper):
             model = model.unwrap()
 
-        if is_sharded:
-            self.load_sharded_model(model, ckpt_path, strict)
+        if index_file_exists:
+            self.load_sharded_model(model, index_file_path, strict)
         else:
-            self.load_unsharded_model(model, ckpt_path, strict)
+            self.load_unsharded_model(model, checkpoint, strict)
 
         return origin_model
 
     def save_model(self,
                    model: Union[nn.Module, ModelWrapper],
                    checkpoint: str,
                    shard: bool = False,
-                   prefix: str = None,
-                   size_per_shard: int = 1024):
+                   gather_dtensor: bool = True,
+                   variant: str = None,
+                   size_per_shard: int = 1024,
+                   use_safetensors: bool = False):
         """
         Save model to checkpoint.
 
         Examples:
             >>> from colossalai.checkpoint_io import GeneralCheckpointIO
             >>> checkpoint_io = CheckpointIO()
             >>>
@@ -97,132 +121,147 @@
 
         Args:
             model (nn.Module): model to be saved.
             checkpoint (str): checkpoint path. The checkpoint path can be :
                 1. a file path, e.g. 'model.pt'
                 2. a directory path to save the sharded checkpoint, e.g. './checkpoints/' when shard = True.
             shard (bool): whether to shard the checkpoint. Default: False. If set to True, the checkpoint will be sharded into
-                multiple files. The model shards will be specificed by a `model.index.json` file. When shard = True, please ensure
+                multiple files. The model shards will be specified by a `model.index.json` file. When shard = True, please ensure
                 that the checkpoint path is a directory path instead of a file path.
-            prefix (str): prefix for the model checkpoint file name when shard=True. Default: None.
+            gather_dtensor (bool): whether to gather the distributed tensor to the first device. Default: True.
+            variant (str): If specified, weights are saved in the format pytorch_model.<variant>.bin. Default: None.
             size_per_shard (int): size per shard in MB. Default: 1024. This value is only used when shard = True.
+            use_safetensors (bool): whether to use safe tensors. Default: False. If set to True, the checkpoint will be saved
         """
 
         if isinstance(model, ModelWrapper):
             model = model.unwrap()
 
         if shard:
-            self.save_sharded_model(model, checkpoint, prefix, size_per_shard)
+            self.save_sharded_model(model, checkpoint, gather_dtensor, variant, size_per_shard, use_safetensors)
         else:
-            self.save_unsharded_model(model, checkpoint)
+            self.save_unsharded_model(model, checkpoint, gather_dtensor, use_safetensors)
 
     def load_optimizer(self, optimizer: Optimizer, checkpoint: str):
         """
         Load optimizer from checkpoint.
 
         Args:
             optimizer (Optimizer): optimizer to be loaded.
-            checkpoint (str): checkpoint path. This value is made compatiblity with the model checkpoints in the
+            checkpoint (str): checkpoint path. This value is made compatibility with the model checkpoints in the
         """
-        ckpt_path = Path(checkpoint)
-        is_sharded = self.is_sharded_checkpoint(ckpt_path)
+        index_file_exists, index_file_path = has_index_file(checkpoint)
+
+        if Path(checkpoint).is_dir() and not index_file_exists:
+            # if the checkpoint is a directory and there is no index file, raise error
+            raise ValueError(f'Cannot find index file in {checkpoint}')
 
-        if is_sharded:
-            self.load_sharded_optimizer(optimizer, ckpt_path)
+        if index_file_exists:
+            # the existence of index file means it is a sharded checkpoint
+            self.load_sharded_optimizer(optimizer, index_file_path)
         else:
-            self.load_unsharded_optimizer(optimizer, ckpt_path)
+            self.load_unsharded_optimizer(optimizer, checkpoint)
 
     def save_optimizer(self,
                        optimizer: Optimizer,
                        checkpoint: str,
                        shard: bool = False,
+                       gather_dtensor=True,
                        prefix: str = None,
                        size_per_shard: int = 1024):
         """
-        Save optimizer to checkpoint.
+        Save optimizer to checkpoint. Optimizer states saving is not compatible with safetensors.
 
         Args:
             optimizer (Optimizer): optimizer to be saved.
             checkpoint (str): checkpoint path. The checkpoint path can be :
                 1. a file path, e.g. 'model.pt'
                 2. a path to a json file which defines the index to the sharded checkpoint for the optimizer
                 3. a path to a folder containing a unique .index.json file for sharded checkpoint
             shard (bool): whether to shard the checkpoint. Default: False. If set to True, the checkpoint will be sharded into
-                multiple files. The optimizer shards will be specificed by a `optimizer.index.json` file.
+                multiple files. The optimizer shards will be specified by a `optimizer.index.json` file.
+            gather_dtensor (bool): whether to gather the distributed tensor to the first device. Default: True.
             prefix (str): prefix for the optimizer checkpoint when shard = True. Default: None.
             size_per_shard (int): size per shard in MB. Default: 1024. This value is only used when shard is set to True.
         """
         if shard:
-            self.save_sharded_optimizer(optimizer, checkpoint, prefix, size_per_shard)
+            self.save_sharded_optimizer(optimizer, checkpoint, gather_dtensor, prefix, size_per_shard)
         else:
-            self.save_unsharded_optimizer(optimizer, checkpoint)
+            self.save_unsharded_optimizer(optimizer, checkpoint, gather_dtensor)
 
     # ========================================================
     # Abstract methods for model loading/saving implementation
     # ========================================================
     @abstractmethod
-    def load_sharded_model(self, model: nn.Module, checkpoint: Path, strict: bool):
+    def load_sharded_model(self, model: nn.Module, index_file_path: str, strict: bool):
         """
         Load model from sharded checkpoint.
 
         Args:
             model (nn.Module): model to be loaded.
-            checkpoint (str): checkpoint path. It should be path to the .index.json file or a path to a directory which contains a .index.json file.
+            index_file_path (str): checkpoint path. It should be path to the .index.json file or a path to a directory which contains a .index.json file.
+            strict (bool): whether to strictly enforce that the param name in
+                the checkpoint match the keys returned by this module's.
         """
         pass
 
     @abstractmethod
-    def load_unsharded_model(self, model: nn.Module, checkpoint: Path, strict: bool):
+    def load_unsharded_model(self, model: nn.Module, checkpoint: str, strict: bool):
         """
         Load model from unsharded checkpoint.
 
         Args:
             model (nn.Module): model to be loaded.
             checkpoint (str): checkpoint path. It should be a single file path pointing to a model weight binary.
             strict (bool): whether to strictly enforce that the param name in
                 the checkpoint match the keys returned by this module's.
         """
         pass
 
     @abstractmethod
-    def save_sharded_model(self, model: nn.Module, checkpoint: Path, prefix: str, size_per_shard: int):
+    def save_sharded_model(self, model: nn.Module, checkpoint: str, gather_dtensor: bool, variant: Optional[str],
+                           size_per_shard: int, use_safetensors: bool):
         """
         Save model to sharded checkpoint.
 
         Args:
             model (nn.Module): model to be saved.
-            checkpoint (Path): checkpoint path. It should be a directory path.
+            checkpoint (str): checkpoint path. It should be a directory path.
+            gather_dtensor (bool): whether to gather the distributed tensor to the first device.
             prefix (str): prefix for the model checkpoint.
             size_per_shard (int): size per shard in MB.
+            use_safetensors (bool): whether to use safe tensors.
         """
         pass
 
     @abstractmethod
-    def save_unsharded_model(self, model: nn.Module, checkpoint: Path):
+    def save_unsharded_model(self, model: nn.Module, checkpoint: str, gather_dtensor: bool, use_safetensors: bool):
         """
         Save model to unsharded checkpoint.
 
         Args:
             model (nn.Module): model to be saved.
-            checkpoint (Path): checkpoint path. It should be a single file path pointing to a model weight binary.
+            checkpoint (str): checkpoint path. It should be a single file path pointing to a model weight binary.
+            gather_dtensor (bool): whether to gather the distributed tensor to the first device.
+            use_safetensors (bool): whether to use safe tensors.
         """
         pass
 
     # ========================================================
     # Abstract methods for optimizer loading/saving implementation
     # ========================================================
 
     @abstractmethod
-    def load_sharded_optimizer(self, optimizer: Optimizer, checkpoint: Path, prefix: str, size_per_shard: int):
+    def load_sharded_optimizer(self, optimizer: Optimizer, index_file_path: str, prefix: str, size_per_shard: int):
         """
         Load optimizer from sharded checkpoint.
 
         Args:
             optimizer (Optimizer): optimizer to be loaded.
-            checkpoint (str): checkpoint path. It should be path to the .index.json file or a path to a directory which contains a .index.json file.
+            index_file_path (str): checkpoint path. It should be path to the .index.json file or a path to a directory which contains a .index.json file.
             prefix (str): prefix for the optimizer checkpoint.
             size_per_shard (int): size per shard in MB.
         """
         pass
 
     @abstractmethod
     def load_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: Path):
@@ -232,43 +271,45 @@
         Args:
             optimizer (Optimizer): optimizer to be loaded.
             checkpoint (str): checkpoint path. It should be a single file path pointing to a model weight binary.
         """
         pass
 
     @abstractmethod
-    def save_sharded_optimizer(self, optimizer: Optimizer, checkpoint: Path, prefix: str, size_per_shard: int):
+    def save_sharded_optimizer(self, optimizer: Optimizer, checkpoint: Path, gather_dtensor: bool, prefix: str,
+                               size_per_shard: int):
         """
         Save optimizer to sharded checkpoint.
 
         Args:
             optimizer (Optimizer): optimizer to be saved.
             checkpoint (Path): checkpoint path. It should be a directory path.
+            gather_dtensor (bool): whether to gather the distributed tensor to the first device.
             prefix (str): prefix for the optimizer checkpoint.
             size_per_shard (int): size per shard in MB.
         """
         pass
 
     @abstractmethod
-    def save_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: Path):
+    def save_unsharded_optimizer(self, optimizer: Optimizer, checkpoint: Path, gather_dtensor: bool):
         """
         Save optimizer to unsharded checkpoint.
 
         Args:
             optimizer (Optimizer): optimizer to be saved.
             checkpoint (str): checkpoint path. It should be a single file path pointing to a model weight binary.
+            gather_dtensor (bool): whether to gather the distributed tensor to the first device.
         """
         pass
 
     # ============================================
     # methods for loading and saving lr scheduler
     # as this is quite standard, there is no need
     # to make them abstract
     # ============================================
-
     def save_lr_scheduler(self, lr_scheduler: LRScheduler, checkpoint: str):
         """
         Save lr scheduler to checkpoint.
 
         Args:
             lr_scheduler (LRScheduler): lr scheduler to be saved.
             checkpoint: checkpoint path. The checkpoint path can only be a file path.
@@ -281,235 +322,7 @@
 
         Args:
             lr_scheduler (LRScheduler): lr scheduler to be loaded.
             checkpoint (str): the path for a single checkpoint file.
         """
         state_dict = torch.load(checkpoint)
         lr_scheduler.load_state_dict(state_dict)
-
-    # ========================================
-    # Helper functions for loading state dict
-    # ========================================
-
-    def get_sharded_checkpoint_index_file(self, checkpoint_path: Path):
-        """
-        Get the index file path for a sharded checkpoint.
-
-        Args:
-            checkpoint_path (Path): path to the checkpoint.
-
-        Returns:
-            Path: path to the index file.
-        """
-        if checkpoint_path.is_file():
-            # check if it is .index.json
-            if checkpoint_path.name.endswith('.index.json'):
-                return checkpoint_path
-            else:
-                raise ValueError(f'Invalid checkpoint path: {checkpoint_path}. ')
-        elif checkpoint_path.is_dir():
-            # check if there is only one a file ending with .index.json in this directory
-            index_files = list(checkpoint_path.glob('*.index.json'))
-            if len(index_files) == 1:
-                return index_files[0]
-            else:
-                raise ValueError(f'Found {len(index_files)} index files in {checkpoint_path}. ')
-
-    def is_sharded_checkpoint(self, checkpoint_path: Path):
-        """
-        Check whether the checkpoint is sharded.
-
-        Args:
-            checkpoint (str): checkpoint path.
-
-        Returns:
-            bool: whether the checkpoint is sharded.
-        """
-        if checkpoint_path.is_file():
-            # check if it is .index.json
-            if checkpoint_path.name.endswith('.index.json'):
-                return True
-            else:
-                return False
-        elif checkpoint_path.is_dir():
-            # check if there is only one a file ending with .index.json in this directory
-            index_files = list(checkpoint_path.glob('*.index.json'))
-            if len(index_files) == 1:
-                return True
-            else:
-                raise ValueError(f'Found {len(index_files)} index files in {checkpoint_path}. ')
-
-    def get_checkpoint_shard_filenames(self, index_file_path: Path):
-        """
-        Get checkpoint shard filenames from a json file.
-
-        Args:
-            index_file_path (Path): path to the json file.
-
-        Returns:
-            list: checkpoint shard filenames.
-        """
-        with open(str(index_file_path), 'r') as f:
-            shard_filenames = json.load(f)
-
-        if "weight_map" in index:
-            index = index["weight_map"]
-
-        checkpoint_root_path = index_file_path.absolute().parent
-
-        # read the checkpoint file list from the json file and get a list of unique file names
-        checkpoint_files = sorted(list(set(index.values())))
-
-        # get the absolute paths for all checkpoint files
-        checkpoint_files = [checkpoint_root_path.joinpath(f) for f in checkpoint_files]
-        return shard_filenames
-
-    def load_safetensors_state_dict(self, *args, **kwargs):
-        """
-        Load safetensors state dict from checkpoint.
-        """
-        # TODO(FrankLeeeee): support huggingface safetensors
-        raise NotImplementedError("This method is not implemented to support safe tensors")
-
-    def load_state_dict(self, checkpoint_file_path: Path):
-        """
-        Load state dict from checkpoint.
-
-        Args:
-            checkpoint_file_path (Path): path to the checkpoint file.
-
-        Returns:
-            dict: state dict.
-        """
-        return torch.load(str(checkpoint_file_path))
-
-    # ======================================
-    # Helper functions for saving state dict
-    # ======================================
-
-    def save_safetensors_state_dict(self, *args, **kwargs):
-        """
-        Save safetensors state dict to checkpoint.
-        """
-        # TODO(FrankLeeeee): support huggingface safetensors
-        raise NotImplementedError("This method is not implemented to support safe tensors")
-
-    def generate_checkpoint_shard_file_name(self, index: int, total_number: int, prefix: str = None):
-        """
-        Generate checkpoint shard file name.
-
-        Args:
-            index (int): index of the shard.
-            total_number (int): total number of shards.
-            prefix (str): prefix of the shard file name. Default: None.
-        """
-        if prefix is None:
-            return f"{index}-of-{total_number}.bin"
-        else:
-            return f"{prefix}-{index}-of-{total_number}.bin"
-
-    def save_checkpoint(self, state_dict: dict, checkpoint_file_path: Path):
-        """
-        Save state dict to checkpoint.
-
-        Args:
-            state_dict (dict): state dict.
-            checkpoint_file_path (Path): path to the checkpoint file.
-        """
-        torch.save(state_dict, str(checkpoint_file_path))
-
-    def save_state_dict_as_shard(self, state_dict: dict, index: int, total_number: int, prefix: str,
-                                 checkpoint_path: Path):
-        """
-        Save state dict as shard.
-
-        Args:
-            state_dict (dict): state dict.
-            checkpoint_path (Path): path to the checkpoint file.
-        """
-        # generate the shard name
-        shard_file_name = self.generate_checkpoint_shard_file_name(index, total_number, prefix)
-        shard_file_path = checkpoint_path.joinpath(shard_file_name)
-
-        # save the shard
-        self.save_checkpoint(state_dict, shard_file_path)
-
-    def calculate_param_size(self, param: torch.Tensor):
-        """
-        Calculate the size of a parameter in MB. Used to compute whether a group of params exceed the shard size.
-        If so, a new shard should be created.
-
-        ArgsL
-            param (torch.Tensor): parameter tensor.
-        """
-        # TODO(FrankLeeeee): check if this tensor is a DTensor, compute its global size if so
-        return param.numel() * param.element_size() / 1024 / 1024
-
-
-class ShardCheckpointIndexFile:
-    """
-    This class is a data structure to keep the content in the index.json file for sharded checkpoint.
-
-    Example:
-        >>> index = ShardCheckpointIndexFile()
-        >>> index.load('index.json')
-        >>> index.append_metadata('model_type', 'bert')
-        >>> index.append_weight_map('bert.embeddings.word_embeddings.weight', 'bert.embeddings.word_embeddings.weight-0-of-2.bin')
-        >>> index.export('index.json')
-    """
-
-    def __init__(self) -> None:
-        self.metadata: dict = dict()
-        self.weight_map: dict = dict()
-
-    def load(self, json_path: str):
-        """
-        Load the index file from a json file.
-
-        Args:
-            json_path (str): path to the json file.
-        """
-        # load the json file
-        with open(json_path, 'r') as f:
-            index = json.load(f)
-
-        # assign attributes if exists
-        if "metadata" in index:
-            self.metadata = index["metadata"]
-        if "weight_map" in index:
-            self.weight_map = index["weight_map"]
-
-    def export(self, json_path: str):
-        """
-        Export the index file to a json file.
-
-        Args:
-            json_path (str): path to the json file.
-        """
-        # create the index file
-        index = dict()
-        index["metadata"] = self.metadata
-        index["weight_map"] = self.weight_map
-
-        # export the index file
-        with open(json_path, 'w') as f:
-            json.dump(index, f, indent=4)
-
-    def append_weight_map(self, param_name: str, shard_file: str):
-        """
-        Append a weight map entry to the index file.
-
-        Args:
-            param_name (str): name of the parameter.
-            shard_file (str): name of the shard file.
-        """
-        self.weight_map[param_name] = shard_file
-
-    def append_meta_data(self, name: str, val: Any):
-        """
-        Append a metadata entry to the index file.
-
-        Args:
-            name (str): name of the metadata.
-            val (Any): value of the metadata.
-        """
-        self.metadata[name] = val
```

### Comparing `colossalai-0.2.8/colossalai/cli/benchmark/__init__.py` & `colossalai-0.3.0/colossalai/cli/benchmark/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cli/benchmark/benchmark.py` & `colossalai-0.3.0/colossalai/cli/benchmark/benchmark.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,16 @@
 
 import colossalai
 from colossalai.cli.benchmark.utils import find_all_configs, get_batch_data, profile_model
 from colossalai.context import Config
 from colossalai.context.random import reset_seeds
 from colossalai.core import global_context as gpc
 from colossalai.logging import disable_existing_loggers, get_dist_logger
-from colossalai.utils import MultiTimer, free_port
+from colossalai.testing import free_port
+from colossalai.utils import MultiTimer
 
 from .models import MLP
 
 
 def run_benchmark(args: Config) -> None:
     """
     Run benchmarking with torch.multiprocessing.
```

### Comparing `colossalai-0.2.8/colossalai/cli/benchmark/utils.py` & `colossalai-0.3.0/colossalai/cli/benchmark/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cli/check/check_installation.py` & `colossalai-0.3.0/colossalai/cli/check/check_installation.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,15 @@
     ```
 
     Returns: A table of installation information.
     """
     found_aot_cuda_ext = _check_aot_built_cuda_extension_installed()
     cuda_version = _check_cuda_version()
     torch_version, torch_cuda_version = _check_torch_version()
-    colossalai_verison, prebuilt_torch_version_required, prebuilt_cuda_version_required = _parse_colossalai_version()
+    colossalai_version, prebuilt_torch_version_required, prebuilt_cuda_version_required = _parse_colossalai_version()
 
     # if cuda_version is None, that means either
     # CUDA_HOME is not found, thus cannot compare the version compatibility
     if not cuda_version:
         sys_torch_cuda_compatibility = None
     else:
         sys_torch_cuda_compatibility = _is_compatible([cuda_version, torch_cuda_version])
@@ -53,15 +53,15 @@
     if prebuilt_torch_version_required is None:
         torch_compatibility = None
     else:
         torch_compatibility = _is_compatible([torch_version, prebuilt_torch_version_required])
 
     click.echo(f'#### Installation Report ####')
     click.echo(f'\n------------ Environment ------------')
-    click.echo(f"Colossal-AI version: {to_click_output(colossalai_verison)}")
+    click.echo(f"Colossal-AI version: {to_click_output(colossalai_version)}")
     click.echo(f"PyTorch version: {to_click_output(torch_version)}")
     click.echo(f"System CUDA version: {to_click_output(cuda_version)}")
     click.echo(f"CUDA version required by PyTorch: {to_click_output(torch_cuda_version)}")
     click.echo("")
     click.echo(f"Note:")
     click.echo(f"1. The table above checks the versions of the libraries/tools in the current environment")
     click.echo(f"2. If the System CUDA version is N/A, you can set the CUDA_HOME environment variable to locate it")
@@ -72,27 +72,27 @@
     click.echo(f'\n------------ CUDA Extensions AOT Compilation ------------')
     click.echo(f"Found AOT CUDA Extension: {to_click_output(found_aot_cuda_ext)}")
     click.echo(f"PyTorch version used for AOT compilation: {to_click_output(prebuilt_torch_version_required)}")
     click.echo(f"CUDA version used for AOT compilation: {to_click_output(prebuilt_cuda_version_required)}")
     click.echo("")
     click.echo(f"Note:")
     click.echo(
-        f"1. AOT (ahead-of-time) compilation of the CUDA kernels occurs during installation when the environment varialbe CUDA_EXT=1 is set"
+        f"1. AOT (ahead-of-time) compilation of the CUDA kernels occurs during installation when the environment variable CUDA_EXT=1 is set"
     )
     click.echo(f"2. If AOT compilation is not enabled, stay calm as the CUDA kernels can still be built during runtime")
 
     click.echo(f"\n------------ Compatibility ------------")
     click.echo(f'PyTorch version match: {to_click_output(torch_compatibility)}')
     click.echo(f"System and PyTorch CUDA version match: {to_click_output(sys_torch_cuda_compatibility)}")
     click.echo(f"System and Colossal-AI CUDA version match: {to_click_output(sys_colossalai_cuda_compatibility)}")
     click.echo(f"")
     click.echo(f"Note:")
     click.echo(f"1. The table above checks the version compatibility of the libraries/tools in the current environment")
     click.echo(
-        f"   - PyTorch version mistach: whether the PyTorch version in the current environment is compatible with the PyTorch version used for AOT compilation"
+        f"   - PyTorch version mismatch: whether the PyTorch version in the current environment is compatible with the PyTorch version used for AOT compilation"
     )
     click.echo(
         f"   - System and PyTorch CUDA version match: whether the CUDA version in the current environment is compatible with the CUDA version required by PyTorch"
     )
     click.echo(
         f"   - System and Colossal-AI CUDA version match: whether the CUDA version in the current environment is compatible with the CUDA version used for AOT compilation"
     )
@@ -133,23 +133,23 @@
         torch_version_for_aot_build: PyTorch version used for AOT compilation of CUDA kernels.
         cuda_version_for_aot_build: CUDA version used for AOT compilation of CUDA kernels.
     """
     # colossalai version can be in two formats
     # 1. X.X.X+torchX.XXcuXX.X (when colossalai is installed with CUDA extensions)
     # 2. X.X.X (when colossalai is not installed with CUDA extensions)
     # where X represents an integer.
-    colossalai_verison = colossalai.__version__.split('+')[0]
+    colossalai_version = colossalai.__version__.split('+')[0]
 
     try:
         torch_version_for_aot_build = colossalai.__version__.split('torch')[1].split('cu')[0]
         cuda_version_for_aot_build = colossalai.__version__.split('cu')[1]
     except:
         torch_version_for_aot_build = None
         cuda_version_for_aot_build = None
-    return colossalai_verison, torch_version_for_aot_build, cuda_version_for_aot_build
+    return colossalai_version, torch_version_for_aot_build, cuda_version_for_aot_build
 
 
 def _check_aot_built_cuda_extension_installed():
     """
     According to `op_builder/README.md`, the CUDA extension can be built with either
     AOT (ahead-of-time) or JIT (just-in-time) compilation.
     AOT compilation will build CUDA extensions to `colossalai._C` during installation.
```

### Comparing `colossalai-0.2.8/colossalai/cli/launcher/__init__.py` & `colossalai-0.3.0/colossalai/cli/launcher/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cli/launcher/hostinfo.py` & `colossalai-0.3.0/colossalai/cli/launcher/hostinfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cli/launcher/multinode_runner.py` & `colossalai-0.3.0/colossalai/cli/launcher/multinode_runner.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cli/launcher/run.py` & `colossalai-0.3.0/colossalai/cli/launcher/run.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cluster/device_mesh_manager.py` & `colossalai-0.3.0/colossalai/cluster/device_mesh_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/cluster/dist_coordinator.py` & `colossalai-0.3.0/colossalai/cluster/dist_coordinator.py`

 * *Files 2% similar despite different names*

```diff
@@ -177,15 +177,15 @@
             >>>
             >>> @dist_coordinator.on_master_only()
             >>> def print_on_master(msg):
             >>>     print(msg)
         """
         is_master = self.is_master(process_group)
 
-        # define an inner functiuon
+        # define an inner function
         def decorator(func):
 
             @functools.wraps(func)
             def wrapper(*args, **kwargs):
                 if is_master:
                     return func(*args, **kwargs)
```

### Comparing `colossalai-0.2.8/colossalai/cluster/process_group_manager.py` & `colossalai-0.3.0/colossalai/cluster/process_group_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/communication/__init__.py` & `colossalai-0.3.0/colossalai/communication/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/communication/collective.py` & `colossalai-0.3.0/colossalai/communication/collective.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/communication/p2p.py` & `colossalai-0.3.0/colossalai/communication/p2p.py`

 * *Files 1% similar despite different names*

```diff
@@ -99,18 +99,18 @@
                           set to None).
         object_send_prev (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): tensor to send to prev rank (no tensor sent if
                           set to None).
         recv_prev (bool): boolean for whether tensor should be received from
                    previous rank.
         recv_next (bool): boolean for whether tensor should be received from
                    next rank.
-        recv_prev_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the previous stage, defualts to None.
-        recv_next_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the next stage, defualts to None.
-        prev_rank (int): the rank of the previous pipeline stage, defualts to None,
-        next_rank (int): the rank of the next pipeline stage, defualts to None,
+        recv_prev_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the previous stage, defaults to None.
+        recv_next_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the next stage, defaults to None.
+        prev_rank (int): the rank of the previous pipeline stage, defaults to None,
+        next_rank (int): the rank of the next pipeline stage, defaults to None,
         dtype (torch.dtype): data type of intermediate buffers, defaults to None
         scatter_gather_tensors (bool): whether to scatter and gather tensor between pipeline stages, defaults to False
 
     Returns:
         Tuple[Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]]: returns tensor_recv_prev, tensor_recv_next
     """
 
@@ -213,15 +213,15 @@
     """Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage.
 
     Args:
         output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.
         next_rank (int, optional): The rank of the source of the tensor.
 
     Returns:
-        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor or gradident tensor list.
+        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor or gradient tensor list.
     """
     if gpc.is_pipeline_last_stage():
         output_tensor_grad = None
     else:
         _, output_tensor_grad = _communicate(recv_next=True,
                                              recv_next_shape=output_grad_shape,
                                              next_rank=next_rank,
```

### Comparing `colossalai-0.2.8/colossalai/communication/p2p_v2.py` & `colossalai-0.3.0/colossalai/communication/p2p_v2.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 
 TensorShape = Union[torch.Size, List[int], Tuple[int]]
 _pg_manager = {}
 _unpickler = pickle.Unpickler
 
 
 def init_process_group():
-    """intialise process group by dist.new_group in the adjacent stages
+    """initialise process group by dist.new_group in the adjacent stages
 
     Args:
         None
 
     Returns:
         None
     """
@@ -226,15 +226,15 @@
     """Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage.
 
     Args:
         output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.
         next_rank (int, optional): The rank of the source of the tensor.
 
     Returns:
-        Any: The input gradient tensor or gradident tensor list.
+        Any: The input gradient tensor or gradient tensor list.
     """
     if gpc.is_pipeline_last_stage():
         output_tensor_grad = None
     else:
         if next_rank is None:
             next_rank = gpc.get_next_global_rank(ParallelMode.PIPELINE)
         output_tensor_grad = _recv_object(next_rank)
```

### Comparing `colossalai-0.2.8/colossalai/communication/ring.py` & `colossalai-0.3.0/colossalai/communication/ring.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/communication/utils.py` & `colossalai-0.3.0/colossalai/communication/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/constants.py` & `colossalai-0.3.0/colossalai/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/config.py` & `colossalai-0.3.0/colossalai/context/config.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/moe_context.py` & `colossalai-0.3.0/colossalai/context/moe_context.py`

 * *Files 1% similar despite different names*

```diff
@@ -60,15 +60,15 @@
         assert torch.cuda.is_available(), "MoE requires to enable CUDA first"
 
         self.world_size = dist.get_world_size()
 
         from colossalai.core import global_context as gpc
         self.max_ep_size = gpc.config.get('max_ep_size', self.world_size)
         assert self.world_size % self.max_ep_size == 0, \
-            "Maximum epxert parallel size must be a factor of the number of GPUs"
+            "Maximum expert parallel size must be a factor of the number of GPUs"
         self.min_dp_size = self.world_size // self.max_ep_size
 
         # Enabling kernel optimization may raise error in some cases
         # Users can close kernel optimization manually
         self.use_kernel_optim = use_kernel_optim
 
         from .random import moe_set_seed
```

### Comparing `colossalai-0.2.8/colossalai/context/parallel_context.py` & `colossalai-0.3.0/colossalai/context/parallel_context.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,23 +6,24 @@
 from collections import Counter
 from threading import local
 from typing import Union
 
 import numpy as np
 import torch
 import torch.distributed as dist
+
 from colossalai.constants import ALLOWED_MODES, INITIALIZER_MAPPING
 from colossalai.context.config import Config
+from colossalai.context.singleton_meta import SingletonMeta
 from colossalai.global_variables import tensor_parallel_env as env
 from colossalai.logging import get_dist_logger
 from colossalai.registry import DIST_GROUP_INITIALIZER
 
 from .parallel_mode import ParallelMode
 from .random import add_seed, get_seeds, set_mode
-from colossalai.context.singleton_meta import SingletonMeta
 
 
 class ParallelContext(metaclass=SingletonMeta):
     """This class provides interface functions for users to get the parallel context,
     such as the global rank, the local rank, the world size, etc. of each device.
 
     Note:
@@ -39,15 +40,15 @@
         self._groups = dict()
         self._cpu_groups = dict()
         self._ranks_in_group = dict()
 
         # load config from file
         self._config = None
 
-        # default 3D parallel args, will be overwritten during process group intialization
+        # default 3D parallel args, will be overwritten during process group initialization
         self.world_size = 1
         self.data_parallel_size = 1
         self.pipeline_parallel_size = 1
         self.tensor_parallel_size = 1
         self.num_processes_on_current_node = -1
         self.virtual_pipeline_parallel_size = None
         self.virtual_pipeline_parallel_rank = None
@@ -259,15 +260,15 @@
         self._check_parallel_mode(parallel_mode)
         return self._world_sizes[parallel_mode]
 
     def _add_world_size(self, parallel_mode: ParallelMode, world_size: int):
         """Adds world size for `parallel_mode`.
 
         Args:
-            parallel_mode (:class:`colossalai.context.ParallelMode`): The parallel mode correponding to the process group
+            parallel_mode (:class:`colossalai.context.ParallelMode`): The parallel mode corresponding to the process group
             world_size (int): The world size to be added
 
         Raises:
             AssertionError: Raises an AssertionError if `parallel_mode` is not an instance
                 of :class:`colossalai.context.ParallelMode`.
         """
         self._check_parallel_mode(parallel_mode)
```

### Comparing `colossalai-0.2.8/colossalai/context/parallel_mode.py` & `colossalai-0.3.0/colossalai/context/parallel_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/__init__.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_1d.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_2d.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_2p5d.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_3d.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_3d.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
 import math
 
 import torch.distributed as dist
+
 from colossalai.global_variables import tensor_parallel_env as env
 from colossalai.registry import DIST_GROUP_INITIALIZER
 
 from ..parallel_mode import ParallelMode
 from .process_group_initializer import ProcessGroupInitializer
 
 
@@ -209,15 +210,16 @@
         group_world_size = None
         mode = ParallelMode.PARALLEL_3D_INPUT_X_WEIGHT
         env.input_x_weight_group_3d = mode
 
         for h in range(self.num_group):
             for k in range(self.depth):
                 ranks = [
-                    h * self.depth**3 + i + self.depth * (j + self.depth * k) for j in range(self.depth)
+                    h * self.depth**3 + i + self.depth * (j + self.depth * k)
+                    for j in range(self.depth)
                     for i in range(self.depth)
                 ]
                 group = dist.new_group(ranks)
                 group_cpu = dist.new_group(ranks, backend='gloo') if dist.get_backend() != 'gloo' else group
 
                 if self.rank in ranks:
                     local_rank = ranks.index(self.rank)
@@ -262,15 +264,16 @@
         group_world_size = None
         mode = ParallelMode.PARALLEL_3D_OUTPUT_X_WEIGHT
         env.output_x_weight_group_3d = mode
 
         for h in range(self.num_group):
             for j in range(self.depth):
                 ranks = [
-                    h * self.depth**3 + i + self.depth * (j + self.depth * k) for k in range(self.depth)
+                    h * self.depth**3 + i + self.depth * (j + self.depth * k)
+                    for k in range(self.depth)
                     for i in range(self.depth)
                 ]
                 group = dist.new_group(ranks)
                 group_cpu = dist.new_group(ranks, backend='gloo') if dist.get_backend() != 'gloo' else group
 
                 if self.rank in ranks:
                     local_rank = ranks.index(self.rank)
```

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_data.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_data.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
 from torch import distributed as dist
 
 from colossalai.registry import DIST_GROUP_INITIALIZER
-from .process_group_initializer import ProcessGroupInitializer
+
 from ..parallel_mode import ParallelMode
+from .process_group_initializer import ProcessGroupInitializer
 
 
 @DIST_GROUP_INITIALIZER.register_module
 class Initializer_Data(ProcessGroupInitializer):
     """A ProcessGroupInitializer for data parallelism.
 
     Args:
```

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_model.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_pipeline.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_pipeline.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_sequence.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_sequence.py`

 * *Files 1% similar despite different names*

```diff
@@ -87,15 +87,15 @@
         Returns:
             List[Tuple (local_rank, group_world_size, process_group, ranks_in_group, mode)]:
                 A Sequence parallelism's information in list of tuples.
         """
 
         parallel_setting = []
 
-        local_rank, group_world_size, process_group, cpu_grop, ranks_in_group, mode = \
+        local_rank, group_world_size, process_group, cpu_group, ranks_in_group, mode = \
             self._sequence_initializer.init_dist_group()
         # change mode to sequence
         mode = ParallelMode.SEQUENCE
 
-        parallel_setting.append((local_rank, group_world_size, process_group, cpu_grop, ranks_in_group, mode))
+        parallel_setting.append((local_rank, group_world_size, process_group, cpu_group, ranks_in_group, mode))
         parallel_setting.append(self._sequence_dp_initializer.init_dist_group())
         return parallel_setting
```

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/initializer_tensor.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/initializer_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/process_group_initializer/process_group_initializer.py` & `colossalai-0.3.0/colossalai/context/process_group_initializer/process_group_initializer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/random/_helper.py` & `colossalai-0.3.0/colossalai/context/random/_helper.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/context/random/seed_manager.py` & `colossalai-0.3.0/colossalai/context/random/seed_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,31 +55,31 @@
             # save the current state for current mode
             self._seed_states[self._current_mode] = torch.cuda.get_rng_state()
 
         # set the new state for new mode
         self._current_mode = parallel_mode
         torch.cuda.set_rng_state(self._seed_states[parallel_mode])
 
-    def add_seed(self, parallel_mode: ParallelMode, seed: int, overwrtie: bool = False):
+    def add_seed(self, parallel_mode: ParallelMode, seed: int, overwrite: bool = False):
         """Adds a seed to the seed manager for `parallel_mode`.
 
         Args:
             parallel_mode (:class:`colossalai.context.ParallelMode`): The chosen parallel mode.
             seed (int): The seed to be added.
-            overwrtie (bool, optional): Whether allows to overwrite the seed that has been set already
+            overwrite (bool, optional): Whether allows to overwrite the seed that has been set already
 
         Raises:
             AssertionError: Raises an AssertionError if `parallel_mode` is not an instance of :class:`colossalai.context.ParallelMode`
                 or the seed for `parallel_mode` has been added.
         """
         assert isinstance(parallel_mode, ParallelMode), 'A valid ParallelMode must be provided'
-        if overwrtie is False:
+        if overwrite is False:
             assert parallel_mode not in self._seed_states, f'The seed for {parallel_mode} has been added'
         elif parallel_mode in self._seed_states:
-            print(f"Warnning: {parallel_mode} seed has been overwritten.", flush=True)
+            print(f"Warning: {parallel_mode} seed has been overwritten.", flush=True)
 
         current_state = torch.cuda.get_rng_state()
         torch.cuda.manual_seed(seed)
         self._seed_states[parallel_mode] = torch.cuda.get_rng_state()
         self._seeds[parallel_mode] = seed
         torch.cuda.set_rng_state(current_state)
```

### Comparing `colossalai-0.2.8/colossalai/context/singleton_meta.py` & `colossalai-0.3.0/colossalai/context/singleton_meta.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/device/alpha_beta_profiler.py` & `colossalai-0.3.0/colossalai/device/alpha_beta_profiler.py`

 * *Files 0% similar despite different names*

```diff
@@ -377,12 +377,12 @@
             broadcast_object = [latency, bandwidth]
             dist.broadcast_object_list(broadcast_object, src=pg[0])
             return broadcast_object
 
         first_latency, first_bandwidth = _extract_alpha_beta(first_axis, first_axis_process_group)
         second_latency, second_bandwidth = _extract_alpha_beta(second_axis, second_axis_process_group)
         mesh_alpha = [first_latency, second_latency]
-        # The beta values have been enlarged by 1e10 times temporarilly because the computation cost
+        # The beta values have been enlarged by 1e10 times temporarily because the computation cost
         # is still estimated in the unit of TFLOPs instead of time. We will remove this factor in future.
         mesh_beta = [1e10 / first_bandwidth, 1e10 / second_bandwidth]
 
         return mesh_alpha, mesh_beta
```

### Comparing `colossalai-0.2.8/colossalai/device/calc_pipeline_strategy.py` & `colossalai-0.3.0/colossalai/device/calc_pipeline_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/device/device_mesh.py` & `colossalai-0.3.0/colossalai/device/device_mesh.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/engine/_base_engine.py` & `colossalai-0.3.0/colossalai/engine/_base_engine.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,16 +6,16 @@
 
 from torch import Tensor
 from torch.nn import Module
 from torch.nn.modules.loss import _Loss
 
 from colossalai.engine.gradient_handler import BaseGradientHandler
 from colossalai.engine.schedule import BaseSchedule, InterleavedPipelineSchedule, NonPipelineSchedule, PipelineSchedule
-from colossalai.gemini.ophooks import BaseOpHook, register_ophooks_recursively
 from colossalai.logging import get_dist_logger
+from colossalai.zero.legacy.gemini import BaseOpHook, register_ophooks_recursively
 
 
 class Engine:
     """Basic engine class for training and evaluation. It runs a specific process method
     :meth:`step` which is based on the given :attr:`schedule` over each batch of a dataset.
     It controls a iteration in training.
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_accumulation/__init__.py` & `colossalai-0.3.0/colossalai/engine/gradient_accumulation/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,21 @@
+from typing import Iterable, List
+
 import torch.nn as nn
-from typing import List
-from colossalai.engine import BaseGradientHandler
-from typing import Iterable
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler
-from ._gradient_accumulation import GradAccumDataloader, GradAccumOptimizer, GradAccumLrSchedulerByStep, GradAccumGradientHandler
+
+from colossalai.engine import BaseGradientHandler
+
+from ._gradient_accumulation import (
+    GradAccumDataloader,
+    GradAccumGradientHandler,
+    GradAccumLrSchedulerByStep,
+    GradAccumOptimizer,
+)
 
 __all__ = [
     'accumulate_gradient', 'GradAccumDataloader', 'GradAccumOptimizer', 'GradAccumLrSchedulerByStep',
     'GradAccumGradientHandler'
 ]
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_accumulation/_gradient_accumulation.py` & `colossalai-0.3.0/colossalai/engine/gradient_accumulation/_gradient_accumulation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
-from typing import Union
+from typing import Any, Iterable, Tuple, Union
+
 import torch.nn as nn
 from torch import Tensor
-from typing import Iterable, Any, Tuple
-from colossalai.nn.optimizer import ColossalaiOptimizer
 from torch.nn.parallel.distributed import DistributedDataParallel
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler
 from torch.utils.data import DataLoader
-from colossalai.utils import conditional_context
+
 from colossalai.engine import BaseGradientHandler
+from colossalai.nn.optimizer import ColossalaiOptimizer
+from colossalai.utils import conditional_context
 
 
 class GradAccumOptimizer(ColossalaiOptimizer):
-    """A wrapper for the optimizer to enable gradient accumulation by skipping the steps 
+    """A wrapper for the optimizer to enable gradient accumulation by skipping the steps
     before accumulation size is reached.
 
     Args:
         optim (:class:`torch.optim.Optimizer`): Your optimizer object for gradient accumulation.
         accumulate_size (int): The number of steps to accumulate gradients.
         model (:class:`torch.nn.Module`):
             Your model object to check if it is DistributedDataParallel for special handling of no_sync() context.
@@ -157,15 +158,15 @@
                         break
             return data
         else:
             raise StopIteration
 
 
 class GradAccumLrSchedulerByStep(_LRScheduler):
-    """A wrapper for the LR scheduler to enable gradient accumulation by skipping the steps 
+    """A wrapper for the LR scheduler to enable gradient accumulation by skipping the steps
     before accumulation size is reached.
 
     Args:
         lr_scheduler (:class:`torch.optim.lr_scheduler._LRScheduler`):
             Your ``lr_scheduler`` object for gradient accumulation.
         accumulate_size (int): The number of steps to accumulate gradients.
     """
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/__init__.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 from ._base_gradient_handler import BaseGradientHandler
 from ._data_parallel_gradient_handler import DataParallelGradientHandler
-from ._zero_gradient_handler import ZeROGradientHandler
-from ._sequence_parallel_gradient_handler import SequenceParallelGradientHandler
-from ._pipeline_parallel_gradient_handler import PipelineSharedModuleGradientHandler
 from ._moe_gradient_handler import MoeGradientHandler
+from ._pipeline_parallel_gradient_handler import PipelineSharedModuleGradientHandler
 from ._sequence_parallel_gradient_handler import SequenceParallelGradientHandler
+from ._zero_gradient_handler import ZeROGradientHandler
 
 __all__ = [
     'BaseGradientHandler', 'DataParallelGradientHandler', 'ZeROGradientHandler', 'PipelineSharedModuleGradientHandler',
     'MoeGradientHandler', 'SequenceParallelGradientHandler'
 ]
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/_base_gradient_handler.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/_base_gradient_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
 from abc import ABC, abstractmethod
 
 
 class BaseGradientHandler(ABC):
-    """A basic helper class to handle all-reduce operations of gradients across different parallel groups 
+    """A basic helper class to handle all-reduce operations of gradients across different parallel groups
     before optimization.
 
     Args:
         model (Module): Model where the gradients accumulate.
         optimizer (Optimizer): Optimizer for updating the parameters.
     """
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/_data_parallel_gradient_handler.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/_data_parallel_gradient_handler.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 from colossalai.core import global_context as gpc
 from colossalai.registry import GRADIENT_HANDLER
-from ._base_gradient_handler import BaseGradientHandler
+
 from ...context.parallel_mode import ParallelMode
+from ._base_gradient_handler import BaseGradientHandler
 from .utils import bucket_allreduce
 
 
 @GRADIENT_HANDLER.register_module
 class DataParallelGradientHandler(BaseGradientHandler):
     """A helper class to handle all-reduce operations in a data parallel group.
-    A all-reduce collective communication will be operated in 
+    A all-reduce collective communication will be operated in
     :func:`handle_gradient` among a data parallel group.
-    For better performance, it bucketizes the gradients of all parameters that are 
+    For better performance, it bucketizes the gradients of all parameters that are
     the same type to improve the efficiency of communication.
 
     Args:
         model (Module): Model where the gradients accumulate.
         optimizer (Optimizer): Optimizer for updating the parameters.
     """
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/_moe_gradient_handler.py` & `colossalai-0.3.0/colossalai/utils/moe.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,45 +1,52 @@
+import torch.nn as nn
+import torch.distributed as dist
 from colossalai.core import global_context as gpc
-from colossalai.registry import GRADIENT_HANDLER
-from colossalai.utils.moe import get_moe_epsize_param_dict
-from ._base_gradient_handler import BaseGradientHandler
-from ...context.parallel_mode import ParallelMode
-from .utils import bucket_allreduce
 from colossalai.context.moe_context import MOE_CONTEXT
+from colossalai.context import ParallelMode
+from .common import is_using_ddp
+from typing import Dict, List
 
 
-@GRADIENT_HANDLER.register_module
-class MoeGradientHandler(BaseGradientHandler):
-    """A helper class to handle all-reduce operations in a data parallel group and
-    moe model parallel. A all-reduce collective communication will be operated in
-    :func:`handle_gradient` among a data parallel group.
-    For better performance, it bucketizes the gradients of all parameters that are
-    the same type to improve the efficiency of communication.
+def get_moe_epsize_param_dict(model: nn.Module) -> Dict[int, List[nn.Parameter]]:
+    """Returns a parameter dictionary, the key of which is the expert parallel
+    size of every parameter. Since the parameters in data parallelism is replicated
+    in each GPU, we set their ep_size to 1.
 
     Args:
-        model (Module): Model where the gradients accumulate.
-        optimizer (Optimizer): Optimizer for updating the parameters.
+        model (:class:`torch.nn.Module`): A pyTorch `nn.Module` from which we get dict.
     """
+    epsize_param_dict = dict()
+    for param in model.parameters():
+        if not hasattr(param, 'moe_info'):
+            ep_size = 1    # set ep_size to 1 for dp parameters
+        else:
+            ep_size = param.moe_info.ep_size
+        if ep_size not in epsize_param_dict:
+            epsize_param_dict[ep_size] = []
+        epsize_param_dict[ep_size].append(param)
 
-    def __init__(self, model, optimizer=None):
-        super().__init__(model, optimizer)
+    return epsize_param_dict
 
-    def handle_gradient(self):
-        """A method running an all-reduce operation in a data parallel group.
-        Then running an all-reduce operation for all parameters in experts
-        across moe model parallel group
-        """
-        global_data = gpc.data_parallel_size
-
-        if global_data > 1:
-            epsize_param_dict = get_moe_epsize_param_dict(self._model)
-
-            # epsize is 1, indicating the params are replicated among processes in data parallelism
-            # use the ParallelMode.DATA to get data parallel group
-            # reduce gradients for all parameters in data parallelism
-            if 1 in epsize_param_dict:
-                bucket_allreduce(param_list=epsize_param_dict[1], group=gpc.get_group(ParallelMode.DATA))
-
-            for ep_size in epsize_param_dict:
-                if ep_size != 1 and ep_size != MOE_CONTEXT.world_size:
-                    bucket_allreduce(param_list=epsize_param_dict[ep_size],
-                                     group=MOE_CONTEXT.parallel_info_dict[ep_size].dp_group)
+
+def sync_moe_model_param(model: nn.Module):
+    """Make sure model parameters are consistent in MoE parallel context.
+
+    Args:
+        model (:class:`torch.nn.Module`): A pyTorch model on whose parameters you check the consistency.
+    """
+    if is_using_ddp():
+
+        param_dict = get_moe_epsize_param_dict(model)
+
+        # synchronize the parameters whose dp_group is the whole world
+        if 1 in param_dict:
+            src_rank = gpc.get_ranks_in_group(ParallelMode.DATA)[0]
+            for param in param_dict[1]:
+                dist.broadcast(param, src=src_rank, group=gpc.get_group(ParallelMode.DATA))
+
+        for ep_size in param_dict:
+            # When ep_size = world_size, communication is not needed
+            if ep_size != 1 and ep_size != MOE_CONTEXT.world_size:
+                src_rank = dist.get_rank(MOE_CONTEXT.parallel_info_dict[ep_size].ep_group)
+                for param in param_dict[ep_size]:
+                    dist.broadcast(param, src=src_rank, group=param.moe_info.dp_group)
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/_pipeline_parallel_gradient_handler.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/_pipeline_parallel_gradient_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 #!/usr/bin/env python
 
 from collections import defaultdict
 
 import torch
 import torch.distributed as dist
+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
+
 from colossalai.core import global_context as gpc
 from colossalai.registry import GRADIENT_HANDLER
-from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
 
 from ._base_gradient_handler import BaseGradientHandler
 
 
 @GRADIENT_HANDLER.register_module
 class PipelineSharedModuleGradientHandler(BaseGradientHandler):
     """A helper class to handle all-reduce operations in sub parallel groups.
-    A all-reduce collective communication will be operated in 
+    A all-reduce collective communication will be operated in
     :func:`handle_gradient` among all sub pipeline parallel groups.
-    For better performance, it bucketizes the gradients of all parameters that are 
+    For better performance, it bucketizes the gradients of all parameters that are
     the same type to improve the efficiency of communication.
 
     Args:
         model (Module): Model where the gradients accumulate.
         optimizer (Optimizer): Optimizer for updating the parameters.
     """
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/_sequence_parallel_gradient_handler.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/_sequence_parallel_gradient_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 from colossalai.core import global_context as gpc
 from colossalai.registry import GRADIENT_HANDLER
-from ._base_gradient_handler import BaseGradientHandler
+
 from ...context.parallel_mode import ParallelMode
+from ._base_gradient_handler import BaseGradientHandler
 from .utils import bucket_allreduce
 
 
 @GRADIENT_HANDLER.register_module
 class SequenceParallelGradientHandler(BaseGradientHandler):
     """A helper class to handle all-reduce operations in a data parallel group.
-    A all-reduce collective communication will be operated in 
+    A all-reduce collective communication will be operated in
     :func:`handle_gradient` among a data parallel group.
-    For better performance, it bucketizes the gradients of all parameters that are 
+    For better performance, it bucketizes the gradients of all parameters that are
     the same type to improve the efficiency of communication.
 
     Args:
         model (Module): Model where the gradients accumulate.
         optimizer (Optimizer): Optimizer for updating the parameters.
     """
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/_zero_gradient_handler.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/_zero_gradient_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from colossalai.registry import GRADIENT_HANDLER
+
 from ._base_gradient_handler import BaseGradientHandler
 
 
 @GRADIENT_HANDLER.register_module
 class ZeROGradientHandler(BaseGradientHandler):
     """A helper class to handle all-reduce operations in a data parallel group.
     A all-reduce collective communication will be operated in
```

### Comparing `colossalai-0.2.8/colossalai/engine/gradient_handler/utils.py` & `colossalai-0.3.0/colossalai/engine/gradient_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/engine/schedule/_base_schedule.py` & `colossalai-0.3.0/colossalai/engine/schedule/_base_schedule.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
 from abc import ABC, abstractmethod
+from typing import Callable, Iterable
 
 import torch
 
-from typing import Iterable, Callable
 from colossalai.logging import get_dist_logger
 from colossalai.utils import get_current_device
 
 
 class BaseSchedule(ABC):
     """A basic helper class to control the process of training or evaluation.
     It mainly composes of forward_backward_step for gradient backward and
```

### Comparing `colossalai-0.2.8/colossalai/engine/schedule/_non_pipeline_schedule.py` & `colossalai-0.3.0/colossalai/engine/schedule/_non_pipeline_schedule.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
-from typing import Iterable
+import inspect
+from typing import Callable, Iterable
 
 import torch
-import inspect
-from ._base_schedule import BaseSchedule
+
 from colossalai.utils import conditional_context
-from typing import Callable
+
+from ._base_schedule import BaseSchedule
 
 
 class NonPipelineSchedule(BaseSchedule):
     """A helper schedule class for no pipeline parallelism running environment.
     During one process, it loads a batch of dataset and feeds it to the model.
     After getting the output and calculating the loss, it will use :meth:`step`
     to update the parameters if it is in training mode.
```

### Comparing `colossalai-0.2.8/colossalai/engine/schedule/_pipeline_schedule.py` & `colossalai-0.3.0/colossalai/engine/schedule/_pipeline_schedule.py`

 * *Files 2% similar despite different names*

```diff
@@ -148,20 +148,20 @@
             return [val[offset:offset + self.microbatch_size] for val in data]
         elif isinstance(data, dict):
             return {k: v[offset:offset + self.microbatch_size] for k, v in data.items()}
         else:
             raise TypeError(f"Expected data to be of type torch.Tensor, list, tuple, or dict, but got {type(data)}")
 
     def load_micro_batch(self):
-        mciro_batch_data = self._get_data_slice(self.batch_data, self.microbatch_offset)
+        micro_batch_data = self._get_data_slice(self.batch_data, self.microbatch_offset)
         self.microbatch_offset += self.microbatch_size
-        return self._move_to_device(mciro_batch_data)
+        return self._move_to_device(micro_batch_data)
 
     def pre_processing(self, engine):
-        from colossalai.zero.sharded_model.sharded_model_v2 import ShardedModelV2
+        from colossalai.zero.legacy import ShardedModelV2
 
         # TODO: remove this after testing new zero with pipeline parallelism
         model = engine.model
         if isinstance(model, NaiveAMPModel):
             self.dtype = torch.half
             model = model.model
         if isinstance(model, ShardedModelV2):
```

### Comparing `colossalai-0.2.8/colossalai/engine/schedule/_pipeline_schedule_v2.py` & `colossalai-0.3.0/colossalai/engine/schedule/_pipeline_schedule_v2.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
-from typing import Tuple, Iterable
+from typing import Iterable, Tuple
 
-from colossalai import engine
-import colossalai.communication.p2p_v2 as comm
 import torch.cuda
+
+import colossalai.communication.p2p_v2 as comm
+from colossalai import engine
 from colossalai.context.parallel_mode import ParallelMode
 from colossalai.core import global_context as gpc
 from colossalai.utils.cuda import get_current_device
 
 from ._pipeline_schedule import PipelineSchedule
 
 
@@ -31,25 +32,25 @@
         label = {k: torch.cat(v, dim=0) for k, v in merged_label.items()}
     return output, label
 
 
 class PipelineScheduleV2(PipelineSchedule):
     """Derived class of PipelineSchedule, the only difference is that
        forward_backward_step is reconstructed with p2p_v2
-    
+
     Args:
         num_microbatches (int): The number of microbatches.
         data_process_func (Callable, optional):
             The preprocessing function which receives a batch of data, and it will be executed in `load_batch`.
         tensor_shape (torch.Size, optional): Specified shape in pipeline communication.
         scatter_gather_tensors (bool, optional):
             If set to `True`, communication will be reduced over pipeline when using 1D tensor parallelization.
-    
+
     Example:
-    
+
         # this shows an example of customized data_process_func
         def data_process_func(stage_output, dataloader_output):
             output1, output2 = stage_output
             item1, item2, item3 = dataloader_output
 
             # assume item2 is not needed
             data = (output1, output2, item1)
@@ -79,15 +80,15 @@
             Tuple[:class:`torch.Tensor`]: A tuple of (output, label, loss), loss and label could be None.
         """
 
         assert forward_only or return_loss, \
             'The argument \'return_loss\' has to be True when \'forward_only\' is False, but got False.'
         self.load_batch(data_iter)
 
-        # num_warmup_microbatches is the step when not all the processers are working
+        # num_warmup_microbatches is the step when not all the processes are working
         num_warmup_microbatches = \
             (gpc.get_world_size(ParallelMode.PIPELINE)
              - gpc.get_local_rank(ParallelMode.PIPELINE) - 1)
         num_warmup_microbatches = min(num_warmup_microbatches, self.num_microbatches)
         num_microbatches_remaining = self.num_microbatches - num_warmup_microbatches
 
         # Input, output tensors only need to be saved when doing backward passes
```

### Comparing `colossalai-0.2.8/colossalai/fx/_compatibility.py` & `colossalai-0.3.0/colossalai/fx/_compatibility.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,17 +10,15 @@
 elif TORCH_MAJOR == 1 and TORCH_MINOR == 12:
     from . import _meta_regist_12
     META_COMPATIBILITY = True
 elif TORCH_MAJOR == 1 and TORCH_MINOR == 13:
     from . import _meta_regist_13
     META_COMPATIBILITY = True
 elif TORCH_MAJOR == 2:
-    from . import _meta_regist_13
     META_COMPATIBILITY = True
-    raise UserWarning("Colossalai is not tested with torch2.0 yet!!!")
 
 
 def compatibility(is_backward_compatible: bool = False) -> Callable:
     """A decorator to make a function compatible with different versions of PyTorch.
 
     Args:
         is_backward_compatible (bool, optional): Whether the function is backward compatible. Defaults to False.
```

### Comparing `colossalai-0.2.8/colossalai/fx/_meta_regist_12.py` & `colossalai-0.3.0/colossalai/fx/_meta_regist_12.py`

 * *Files 0% similar despite different names*

```diff
@@ -382,15 +382,15 @@
     return 0
 
 
 # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorCompare.cpp
 @register_meta(aten.where.self)
 def meta_where_self(condition: torch.Tensor, self: torch.Tensor, other: torch.Tensor):
     result_type = torch.result_type(self, other)
-    return torch.empty_like(self, dtype=result_type)
+    return torch.empty_like(condition + self + other, dtype=result_type)
 
 
 @register_meta(aten.index.Tensor)
 def meta_index_Tensor(self, indices):
     assert indices, "at least one index must be provided"
     # aten::index is the internal advanced indexing implementation
     # checkIndexTensorTypes and expandTensors
```

### Comparing `colossalai-0.2.8/colossalai/fx/_meta_regist_13.py` & `colossalai-0.3.0/colossalai/fx/_meta_regist_13.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/codegen/activation_checkpoint_codegen.py` & `colossalai-0.3.0/colossalai/fx/codegen/activation_checkpoint_codegen.py`

 * *Files 0% similar despite different names*

```diff
@@ -301,15 +301,15 @@
 def emit_ckpt_func(body,
                    ckpt_func,
                    node_list: List[Node],
                    emit_node_func,
                    delete_unused_value_func,
                    level=0,
                    in_ckpt=False):
-    """Emit ckpt fuction in nested way
+    """Emit ckpt function in nested way
     Args:
         body: forward code, in recursive calls, this part will be checkpoint
         functions code
         ckpt_func: checkpoint functions code, in recursive calls, this part
         will be a buffer
         node_list (List[Node]): list of torch.fx.Node
         emit_node_func: function to emit a node
@@ -519,15 +519,15 @@
         inputs, outputs = _find_input_and_output_nodes(offload_node_list)
         offload_inputs.append(inputs)
         offload_outputs.append(outputs)
 
     # append code text to body
     for idx, node in enumerate(node_list):
         # if this is the first node of the ckpt region
-        # append the ckpt function defition
+        # append the ckpt function definition
         if idx in start_idx:
             label = start_idx.index(idx)
             ckpt_fn_def = _gen_ckpt_fn_def(label, input_vars[label])
             ckpt_func.append(f'{ckpt_fn_def}\n')
             within_ckpt_region = True
 
         if idx in offload_starts:
```

### Comparing `colossalai-0.2.8/colossalai/fx/graph_module.py` & `colossalai-0.3.0/colossalai/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/passes/adding_split_node_pass.py` & `colossalai-0.3.0/colossalai/fx/passes/adding_split_node_pass.py`

 * *Files 1% similar despite different names*

```diff
@@ -202,15 +202,15 @@
                 split_node = mod_graph.create_node('call_function', pipe_split)
     gm.recompile()
     return gm
 
 
 def avgnode_split_pass(gm: torch.fx.GraphModule, pp_size: int):
     """
-    In avgnode_split_pass, simpliy split graph by node number.
+    In avgnode_split_pass, simply split graph by node number.
     """
     mod_graph = gm.graph
     avg_num_node = len(mod_graph.nodes) // pp_size
     accumulate_num_node = 0
     for node in mod_graph.nodes:
         if pp_size <= 1:
             break
```

### Comparing `colossalai-0.2.8/colossalai/fx/passes/concrete_info_prop.py` & `colossalai-0.3.0/colossalai/fx/passes/concrete_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/passes/meta_info_prop.py` & `colossalai-0.3.0/colossalai/fx/passes/meta_info_prop.py`

 * *Files 0% similar despite different names*

```diff
@@ -27,15 +27,15 @@
     shape: torch.Size
     dtype: torch.dtype
     requires_grad: bool
     stride: Tuple[int]
     numel: int
     is_tensor: bool
     # TODO: we can add a list of sharding spec here, and record the sharding
-    # behaviour by appending sharding spec into list.
+    # behavior by appending sharding spec into list.
 
 
 def _extract_tensor_metadata(result: torch.Tensor) -> TensorMetadata:
     """
     Extract a TensorMetadata NamedTuple describing `result`.
     """
     shape = result.shape
```

### Comparing `colossalai-0.2.8/colossalai/fx/passes/passes_for_gpt2_test.py` & `colossalai-0.3.0/colossalai/fx/passes/passes_for_gpt2_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,18 +1,19 @@
+import inspect
+from typing import Any, Callable, Dict, List, Optional
+
 import torch
-from torch.fx.graph_module import GraphModule
-from typing import Callable, List, Dict, Any, Optional
-from torch.fx._compatibility import compatibility
 from packaging import version
+from torch.fx._compatibility import compatibility
+from torch.fx.graph_module import GraphModule
+from torch.fx.node import Node
+
+from colossalai.fx.passes.adding_split_node_pass import balanced_split_pass, pipe_split
 from colossalai.fx.passes.meta_info_prop import TensorMetadata
-import inspect
-from typing import List
 from colossalai.fx.passes.split_module import Partition
-from colossalai.fx.passes.adding_split_node_pass import pipe_split, balanced_split_pass
-from torch.fx.node import Node
 
 
 def customized_split_pass_for_gpt2(gm: torch.fx.GraphModule, pp_size: int, partition_list: List[int]):
     '''
     This pass is only used to do the gpt2 performance test, it may move into adding_split_node_pass.py, and will be deprecated in future.
     '''
     mod_graph = gm.graph
@@ -225,15 +226,15 @@
             if use_partition_name is not None:
                 use_partition = partitions[use_partition_name]
                 use_partition.inputs.setdefault(def_node.name)
                 if def_partition_name is not None:
                     use_partition.partitions_dependent_on.setdefault(def_partition_name)
 
     node_process_list = list(m.graph.nodes)
-    # split nodes into parititons
+    # split nodes into partitions
     while node_process_list:
         node = node_process_list.pop(0)
         orig_nodes[node.name] = node
 
         if node.op in ["placeholder"]:
             continue
         if node.op == 'output':
@@ -272,15 +273,15 @@
         for dependent in partitions[root_partition].partition_dependents:
             partitions[dependent].partitions_dependent_on.pop(root_partition)
             if not partitions[dependent].partitions_dependent_on:
                 root_partitions.append(dependent)
     if len(sorted_partitions) != len(partitions):
         raise RuntimeError("cycle exists between partitions!")
 
-    # add placeholders to parititons
+    # add placeholders to partitions
     for partition_name in sorted_partitions:
         partition = partitions[partition_name]
         for input in partition.inputs:
             placeholder = partition.graph.placeholder(input)
             placeholder.meta = orig_nodes[input].meta.copy()
             partition.environment[orig_nodes[input]] = placeholder
```

### Comparing `colossalai-0.2.8/colossalai/fx/passes/shard_1d_pass.py` & `colossalai-0.3.0/colossalai/fx/passes/shard_1d_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/passes/split_module.py` & `colossalai-0.3.0/colossalai/fx/passes/split_module.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
+import inspect
+from typing import Any, Callable, Dict, List, Optional
+
 import torch
-from torch.fx.graph_module import GraphModule
-from typing import Callable, List, Dict, Any, Optional
-from torch.fx._compatibility import compatibility
 from packaging import version
-import inspect
+from torch.fx._compatibility import compatibility
+from torch.fx.graph_module import GraphModule
 
 
 @compatibility(is_backward_compatible=True)
 class Partition:
     """
     Adapted from https://github.com/pytorch/pytorch/blob/master/torch/fx/passes/split_module.py
     """
@@ -24,25 +25,25 @@
         self.targets: Dict[str, Any] = {}
 
     def __repr__(self) -> str:
         return f"name: {self.name},\n" \
             f" nodes: {self.node_names},\n" \
             f" inputs: {self.inputs},\n" \
             f" outputs: {self.outputs},\n" \
-            f" partitions depenent on: {self.partitions_dependent_on},\n" \
-            f" parition dependents: {self.partition_dependents}"
+            f" partitions dependent on: {self.partitions_dependent_on},\n" \
+            f" partition dependents: {self.partition_dependents}"
 
 
 # Creates subgraphs out of main graph
 @compatibility(is_backward_compatible=True)
 def split_module(
     m: GraphModule,
     root_m: torch.nn.Module,
     split_callback: Callable[[torch.fx.node.Node], int],
-    merge_output = False,
+    merge_output=False,
 ):
     """
     Adapted from https://github.com/pytorch/pytorch/blob/master/torch/fx/passes/split_module.py
     Creates subgraphs out of main graph
     Args:
         m (GraphModule): Graph module to split
         root_m (torch.nn.Module): root nn module. Not currently used. Included
@@ -128,18 +129,16 @@
                     def_partition.partition_dependents.setdefault(use_partition_name)
 
             if use_partition_name is not None:
                 use_partition = partitions[use_partition_name]
                 use_partition.inputs.setdefault(def_node.name)
                 if def_partition_name is not None:
                     use_partition.partitions_dependent_on.setdefault(def_partition_name)
-                    
-    def record_output(
-        def_node: torch.fx.node.Node, use_node: Optional[torch.fx.node.Node]
-    ):  # noqa: B950
+
+    def record_output(def_node: torch.fx.node.Node, use_node: Optional[torch.fx.node.Node]):    # noqa: B950
         def_partition_name = getattr(def_node, "_fx_partition", None)
         use_partition_name = getattr(use_node, "_fx_partition", None)
         if def_partition_name != use_partition_name:
             if def_partition_name is not None:
                 def_partition = partitions[def_partition_name]
                 def_partition.outputs.setdefault(def_node.name)
                 if use_partition_name is not None:
@@ -152,15 +151,15 @@
                     use_partition.partitions_dependent_on.setdefault(def_partition_name)
             use_partition.outputs.setdefault(def_node.name)
         else:
             if use_partition_name is not None:
                 use_partition = partitions[use_partition_name]
                 use_partition.outputs.setdefault(def_node.name)
 
-    # split nodes into parititons
+    # split nodes into partitions
     for node in m.graph.nodes:
         orig_nodes[node.name] = node
 
         if node.op in ["placeholder"]:
             continue
         if node.op == 'output':
             if merge_output:
@@ -195,15 +194,15 @@
         for dependent in partitions[root_partition].partition_dependents:
             partitions[dependent].partitions_dependent_on.pop(root_partition)
             if not partitions[dependent].partitions_dependent_on:
                 root_partitions.append(dependent)
     if len(sorted_partitions) != len(partitions):
         raise RuntimeError("cycle exists between partitions!")
 
-    # add placeholders to parititons
+    # add placeholders to partitions
     for partition_name in sorted_partitions:
         partition = partitions[partition_name]
         for input in partition.inputs:
             placeholder = partition.graph.placeholder(input)
             placeholder.meta = orig_nodes[input].meta.copy()
             partition.environment[orig_nodes[input]] = placeholder
 
@@ -287,11 +286,11 @@
 
     for node in m.graph.nodes:
         if node.op == 'output':
             base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))    # noqa: B950
 
     for partition_name in sorted_partitions:
         partition = partitions[partition_name]
-    
+
     new_gm = torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)
 
     return new_gm
```

### Comparing `colossalai-0.2.8/colossalai/fx/passes/utils.py` & `colossalai-0.3.0/colossalai/fx/passes/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/__init__.py` & `colossalai-0.3.0/colossalai/fx/profiler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/constants.py` & `colossalai-0.3.0/colossalai/fx/profiler/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/dataflow.py` & `colossalai-0.3.0/colossalai/fx/profiler/dataflow.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/constants.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/activation_function.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/embedding.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/normalization.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/pooling.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/activation_function.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/attention.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/convolution.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/normalization.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/pooling.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/profiler_module/rnn.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/profiler_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/registry.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/experimental/shard_utils.py` & `colossalai-0.3.0/colossalai/fx/profiler/experimental/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/memory_utils.py` & `colossalai-0.3.0/colossalai/fx/profiler/memory_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/opcount.py` & `colossalai-0.3.0/colossalai/fx/profiler/opcount.py`

 * *Files 1% similar despite different names*

```diff
@@ -219,15 +219,16 @@
 def zero_flop_jit(*args):
     """
         Count flops for zero flop layers.
     """
     return 0
 
 
-if version.parse(torch.__version__) >= version.parse('1.12.0'):
+if version.parse(torch.__version__) >= version.parse('1.12.0') and version.parse(
+        torch.__version__) < version.parse('2.0.0'):
     flop_mapping = {
     # gemm, gemv and dot
         aten.mm.default: matmul_flop_jit,
         aten.mv.default: matmul_flop_jit,
         aten.dot.default: matmul_flop_jit,
         aten.matmul.default: matmul_flop_jit,
         aten.addmm.default: addmm_flop_jit,
```

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/profiler.py` & `colossalai-0.3.0/colossalai/fx/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/shard_utils.py` & `colossalai-0.3.0/colossalai/fx/profiler/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/profiler/tensor.py` & `colossalai-0.3.0/colossalai/fx/profiler/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/proxy.py` & `colossalai-0.3.0/colossalai/fx/proxy.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 import operator
+from typing import Any, List, Union
+
 import torch
-from torch.fx.proxy import Proxy, Attribute
-from typing import List, Union, Any
+from torch.fx.proxy import Attribute, Proxy
+
 from colossalai.fx.tracer.meta_patch import meta_patched_function
 
 __all__ = ['ColoProxy']
 
 
 class ColoProxy(Proxy):
     """
```

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/_meta_trace.py` & `colossalai-0.3.0/colossalai/fx/tracer/_meta_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/_symbolic_trace.py` & `colossalai-0.3.0/colossalai/fx/tracer/_symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/_tracer_utils.py` & `colossalai-0.3.0/colossalai/fx/tracer/_tracer_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-from typing import List, Union, Any
-from ..proxy import ColoProxy, ColoAttribute
+from typing import Any, List, Union
+
 import torch
+
+from ..proxy import ColoAttribute, ColoProxy
 from .meta_patch import meta_patched_function, meta_patched_module
 
 __all__ = ['is_element_in_list', 'extract_meta']
 
 
 def is_element_in_list(elements: Union[List[Any], Any], list_: List[Any]):
     if isinstance(elements, (tuple, list, set)):
```

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py` & `colossalai-0.3.0/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/experimental.py` & `colossalai-0.3.0/colossalai/fx/tracer/experimental.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/convolution.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/embedding.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/normalization.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/convolution.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/normalization.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/pooling.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/meta_patch/patched_module/rnn.py` & `colossalai-0.3.0/colossalai/fx/tracer/meta_patch/patched_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/registry.py` & `colossalai-0.3.0/colossalai/fx/tracer/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/fx/tracer/tracer.py` & `colossalai-0.3.0/colossalai/fx/tracer/tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/chunk/chunk.py` & `colossalai-0.3.0/colossalai/zero/gemini/chunk/chunk.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/chunk/manager.py` & `colossalai-0.3.0/colossalai/zero/gemini/chunk/manager.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from collections import deque
 from typing import Deque, Dict, Iterable, List, Optional, Set, Tuple
 
 import torch
 
-from colossalai.gemini.chunk import Chunk, ChunkFullError, TensorState
 from colossalai.tensor import ColoTensor
 from colossalai.utils import get_current_device
 
+from .chunk import Chunk, ChunkFullError, TensorState
+
 
 class ChunkManager:
     """
     A manager class to manipulate the tensors in chunks.
 
     Args:
         chunk_configuration (Dict[int, Dict]): the configuration dictionary of this chunk manager.
@@ -97,51 +98,51 @@
             self.__close_one_chunk(self.chunk_groups[group_name][-1])
 
     def access_chunk(self, chunk: Chunk) -> None:
         """Make the chunk can be used for calculation.
         """
         if chunk in self.accessed_chunks:
             return
-        self.__sub_memroy_usage(chunk.memory_usage)
+        self.__sub_memory_usage(chunk.memory_usage)
         if chunk.device_type == 'cpu':
             chunk.shard_move(get_current_device())
         self.__add_accessed_chunk(chunk)
         self.__add_memory_usage(chunk.memory_usage)
 
     def release_chunk(self, chunk: Chunk) -> None:
         """Scatter the chunk in CUDA.
         """
         if chunk not in self.accessed_chunks:
             return
         if chunk.can_release:
-            self.__sub_memroy_usage(chunk.memory_usage)
+            self.__sub_memory_usage(chunk.memory_usage)
             self.__sub_accessed_chunk(chunk)
             self.__add_memory_usage(chunk.memory_usage)
 
     def move_chunk(self, chunk: Chunk, device: torch.device, force_copy: bool = False) -> None:
         """Move the shard of the chunk to the target device.
         """
         if not chunk.can_move or chunk.device_type == device.type:
             return
-        self.__sub_memroy_usage(chunk.memory_usage)
+        self.__sub_memory_usage(chunk.memory_usage)
         chunk.shard_move(device, force_copy)
         self.__add_memory_usage(chunk.memory_usage)
 
     def trans_tensor_state(self, tensor: torch.Tensor, state: TensorState) -> None:
         """Transit tensor state according to pre-defined state machine.
         """
         chunk = self.tensor_chunk_map[tensor]
         chunk.tensor_trans_state(tensor, state)
 
     def reduce_chunk(self, chunk: Chunk) -> bool:
         """Reduce or all reduce the chunk.
         """
         if not chunk.can_reduce:
             return False
-        self.__sub_memroy_usage(chunk.memory_usage)
+        self.__sub_memory_usage(chunk.memory_usage)
         chunk.reduce()
         self.__sub_accessed_chunk(chunk)
         self.__add_memory_usage(chunk.memory_usage)
         return True
 
     def fake_release_chunk(self, chunk: Chunk) -> None:
         """Release gathered chunk in a fake mode.
@@ -223,19 +224,19 @@
         """Register a chunk group.
         """
         if group_name not in self.chunk_groups:
             self.chunk_groups[group_name] = deque()
         return self.chunk_groups[group_name]
 
     def __close_one_chunk(self, chunk: Chunk):
-        self.__sub_memroy_usage(chunk.memory_usage)
+        self.__sub_memory_usage(chunk.memory_usage)
         chunk.close_chunk()
         self.__add_memory_usage(chunk.memory_usage)
 
-    def __sub_memroy_usage(self, usage: Dict[str, int]):
+    def __sub_memory_usage(self, usage: Dict[str, int]):
         for k, v in usage.items():
             self.total_mem[k] -= v
 
     def __add_memory_usage(self, usage: Dict[str, int]):
         for k, v in usage.items():
             self.total_mem[k] += v
```

### Comparing `colossalai-0.2.8/colossalai/gemini/chunk/search_utils.py` & `colossalai-0.3.0/colossalai/zero/gemini/chunk/search_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,27 @@
 import math
 from typing import Dict, List, Optional, Tuple
 
 import numpy as np
 import torch.distributed as dist
 import torch.nn as nn
 
-from colossalai.gemini.memory_tracer import MemStats, OrderedParamGenerator
 from colossalai.tensor import ColoParameter
 from colossalai.utils import is_ddp_ignored
+from colossalai.zero.gemini.memory_tracer import MemStats, OrderedParamGenerator
 
 
 def _filter_exlarge_params(model: nn.Module, size_dict: Dict[int, List[int]]) -> None:
-    """
+    """_filter_exlarge_params
+
     Filter those parameters whose size is too large (more than 3x standard deviations) from others.
+
+    Args:
+        model (nn.Module): the model.
+        size_dict (Dict[int, List[int]]): the size dict of parameters.
     """
     agg_size_list = []
     for key in size_dict:
         agg_size_list.extend(size_dict[key])
 
     if len(agg_size_list) == 0:
         return
@@ -29,53 +34,77 @@
 
     for key in size_dict:
         org_list = size_dict[key]
         size_dict[key] = list(filter(lambda x: x <= upper_limit, org_list))
 
 
 def _get_unused_byte(size_list: List[int], chunk_size: int) -> int:
-    """Get unused byte for a certain chunk size.
+    """_get_unused_byte
+
+    Get unused byte for a certain chunk size.
+
+    Args:
+        size_list (List[int]): the size list of parameters.
+        chunk_size (int): the chunk size.
+
+    Returns:
+        int: the unused byte.
     """
     acc = 0
     left = 0
     for s in size_list:
         if s > left:
             acc += left
             left = chunk_size
         left -= s
     return left + acc
 
 
-def _tensor_numel(local_param: ColoParameter, strict_ddp_flag: bool):
-    if strict_ddp_flag:
+def _tensor_numel(local_param: ColoParameter, strict_ddp_flag: bool) -> int:
+    """_tensor_numel
+
+    Get the number of elements of a tensor.
+
+    Args:
+        local_param (ColoParameter): The local parameter.
+        strict_ddp_flag (bool): whether to enable the strict ddp mode.
+
+    Returns:
+        int: the number of elements.
+    """
+    if strict_ddp_flag and type(local_param) is ColoParameter:
         return local_param.numel_global()
     else:
+        # if local_param is not ColoParameter, we assume it's replicated
         return local_param.numel()
 
 
 def classify_params_by_dp_degree(param_order: OrderedParamGenerator,
                                  strict_ddp_flag: bool = False) -> Dict[int, List[ColoParameter]]:
     """classify_params_by_dp_degree
 
     Classify the parameters by their dp degree
 
     Args:
-        param_order (OrderedParamGenerator): the order of param be visied
+        param_order (OrderedParamGenerator): the order of param be vised
+        strict_ddp_flag (bool, optional): whether to enable the strict ddp mode. Defaults to False.
 
     Returns:
         Dict[int, List[ColoParameter]]: a dict contains the classification results.
         The keys are dp_degrees and the values are parameters.
     """
     params_dict: Dict[int, List[ColoParameter]] = dict()
     for param in param_order.generate():
-        assert isinstance(param, ColoParameter), "please init model in the ColoInitContext"
+        # assert isinstance(param, ColoParameter), "please init model in the ColoInitContext"
         if is_ddp_ignored(param):
             continue
 
-        if strict_ddp_flag:
+        if strict_ddp_flag or type(param) is not ColoParameter:
+            # if model is not initialized with ColoInitContext, we assume it's replicated
+            # TODO(ver217): integrate DTensor
             param_key = dist.get_world_size()
         else:
             param_key = param.process_group.dp_world_size()
 
         if param_key not in params_dict:
             params_dict[param_key] = []
         params_dict[param_key].append(param)
@@ -89,14 +118,16 @@
         search_interval_byte: int,    # hidden size is the best value for the interval
         min_chunk_size_mb: float = 32,
         filter_exlarge_params: bool = True,
         strict_ddp_flag: bool = False,
         memstas: Optional[MemStats] = None) -> Tuple[Dict, int, int]:
     """search_chunk_configuration
 
+    Search the chunk configuration for a model.
+
     Args:
         model (nn.Module): torch module
         search_range_mb (float): searching range in mega byte.
         search_interval_byte (int): searching interval in byte.
         min_chunk_size_mb (float, optional): the minimum size of a distributed chunk.
         filter_exlarge_params (bool, optional): filter extreme large parameters. Defaults to True.
         strict_ddp_flag (bool, optional): whether to enable the strict ddp mode.
```

### Comparing `colossalai-0.2.8/colossalai/gemini/chunk/utils.py` & `colossalai-0.3.0/colossalai/zero/gemini/chunk/utils.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 from time import time
 from typing import Optional
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 
-from colossalai.gemini.chunk import ChunkManager
-from colossalai.gemini.chunk.search_utils import search_chunk_configuration
 from colossalai.utils import is_ddp_ignored
 
+from .manager import ChunkManager
+from .search_utils import search_chunk_configuration
+
 
 def safe_div(a, b):
     if a == 0:
         return 0
     return a / b
 
 
 def init_chunk_manager(model: nn.Module,
                        init_device: Optional[torch.device] = None,
                        hidden_dim: Optional[int] = None,
+                       verbose: bool = False,
                        **kwargs) -> ChunkManager:
     if hidden_dim:
         search_interval_byte = hidden_dim
     else:
         search_interval_byte = 1024    # defaults to 1kb
     kwargs["search_interval_byte"] = search_interval_byte
 
@@ -34,15 +36,15 @@
     dist.barrier()
     end = time()
     span_s = end - begin
     mb_size = 1024**2
     total_size /= mb_size
     wasted_size /= mb_size
 
-    if dist.get_rank() == 0:
+    if verbose and dist.get_rank() == 0:
         print("searching chunk configuration is completed in {:.2f} s.\n".format(span_s),
               "used number: {:.2f} MB, wasted number: {:.2f} MB\n".format(total_size, wasted_size),
               "total wasted percentage is {:.2f}%".format(100 * safe_div(wasted_size, total_size + wasted_size)),
               sep='',
               flush=True)
     dist.barrier()
```

### Comparing `colossalai-0.2.8/colossalai/gemini/gemini_context.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/gemini_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/gemini_mgr.py` & `colossalai-0.3.0/colossalai/zero/gemini/gemini_mgr.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,15 @@
 import functools
 from time import time
 from typing import List, Optional, Tuple
 
 import torch
 
-from colossalai.gemini.chunk import Chunk, ChunkManager
-from colossalai.gemini.memory_tracer import MemStats
-
-from .memory_tracer import ChunkMemStatsCollector
+from .chunk import Chunk, ChunkManager
+from .memory_tracer import ChunkMemStatsCollector, MemStats
 from .placement_policy import PlacementPolicyFactory
 
 
 class GeminiManager:
     """
     Stateful Tensor Manager, inspired from PatrickStar
```

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/__init__.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/chunk_memstats_collector.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import Optional
 
-from colossalai.gemini.chunk import ChunkManager
-from colossalai.gemini.memory_tracer import MemStats
 from colossalai.utils import get_current_device
 from colossalai.utils.memory import colo_device_memory_capacity
+from colossalai.zero.gemini.chunk import ChunkManager
 
+from .memory_stats import MemStats
 from .memstats_collector import MemStatsCollector
 
 
 class ChunkMemStatsCollector(MemStatsCollector):
 
     def __init__(self, chunk_manager: ChunkManager, memstats: Optional[MemStats] = None) -> None:
         """
```

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/memory_monitor.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/memory_monitor.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/memory_stats.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/memory_stats.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import Any, Dict, List, Optional
 
 import torch
 
-from colossalai.gemini.memory_tracer import OrderedParamGenerator
+from .param_runtime_order import OrderedParamGenerator
 
 
 class MemStats(object):
 
     def __init__(self) -> None:
         """
         Store the non model data statistics used for Gemini and ZeroOptimizer.
@@ -55,15 +55,15 @@
 
     def increase_preop_step(self, param_list: List[torch.nn.Parameter]):
         """
         the time step is increased. param list is used between current and the next
         time step.
 
         Args:
-            param_list (List[torch.nn.Parameter]): a list of torch paramters.
+            param_list (List[torch.nn.Parameter]): a list of torch parameters.
         """
         for p in param_list:
             if p not in self._param_step_dict:
                 self._param_step_dict[p] = [self._preop_step]
             else:
                 self._param_step_dict[p].append(self._preop_step)
             self._param_runtime_order.append(p)
```

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/memstats_collector.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/memstats_collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,11 @@
 import time
-from typing import List, Optional
-
-import torch
-
-from colossalai.gemini.memory_tracer import SyncCudaMemoryMonitor
-from colossalai.gemini.stateful_tensor import StatefulTensor
-from colossalai.utils.memory import colo_device_memory_used
+from typing import Optional
 
+from .memory_monitor import SyncCudaMemoryMonitor
 from .memory_stats import MemStats
 
 
 class MemStatsCollector:
     """
     A Memory statistic collector.
     It works in two phases.
@@ -45,15 +40,15 @@
         Returns:
             int: max non model data memory usage of current sampling period
         """
         assert not self._start_flag, 'Cannot get mem stats info during collection phase.'
         assert self._step_total > 0, 'Cannot get mem stats info before collection phase.'
         assert len(self._memstats.non_model_data_list(device_type)) > self._step_idx, \
             f"{len(self._memstats.non_model_data_list(device_type))} should be > than step idx {self._step_idx}, "\
-                f"step total {self._step_total}"
+            f"step total {self._step_total}"
         next_non_model_data = self._memstats.non_model_data_list(device_type)[self._step_idx]
         self._step_idx = (self._step_idx + 1) % self._step_total
         return next_non_model_data
 
     @property
     def sampling_time(self):
         return [t - self._sampling_time[0] for t in self._sampling_time]
@@ -71,14 +66,16 @@
 
     # deprecated
     def record_model_data_volume(self) -> None:
         """
         Sampling model data statistics.
         """
         if self._start_flag and not self.use_outside_memstats:
+            from colossalai.zero.legacy.gemini import StatefulTensor
+
             # The following code work for ZeroInitContext, which is deprecated in v0.1.12
             cuda_mem = StatefulTensor.GST_MGR.total_mem['cuda']
             self._memstats.record_max_cuda_model_data(cuda_mem)
 
     def sample_overall_data(self) -> None:
         """
         Sampling overall and non model data cuda memory statistics.
```

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/param_runtime_order.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/param_runtime_order.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/runtime_mem_tracer.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,18 @@
 import torch.nn
 
-from colossalai.gemini.memory_tracer import MemStats
-from colossalai.gemini.ophooks.runtime_mem_tracer_hook import GradMemStats, GradMemTracerHook, ParamMemTracerHook
 from colossalai.nn.parallel.data_parallel import _cast_float
 from colossalai.tensor.param_op_hook import ColoParamOpHookManager
+from colossalai.zero.legacy.gemini.ophooks.runtime_mem_tracer_hook import (
+    GradMemStats,
+    GradMemTracerHook,
+    ParamMemTracerHook,
+)
+
+from .memory_stats import MemStats
 
 __all__ = ['RuntimeMemTracer']
 
 
 class RuntimeMemTracer():
     """RuntimeMemTracer for the module training using ColoParameter.
```

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/static_memstats_collector.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 import torch
 import torch.nn as nn
 from torch.fx import symbolic_trace
 
 from colossalai.fx.passes.meta_info_prop import MetaInfoProp
 from colossalai.fx.profiler import calculate_fwd_out, calculate_fwd_tmp, is_compatible_with_meta
-from colossalai.gemini.chunk import ChunkManager
+from colossalai.zero.gemini.chunk import ChunkManager
 
 if is_compatible_with_meta():
     from colossalai.fx.profiler import MetaTensor
 
 from .chunk_memstats_collector import ChunkMemStatsCollector
```

### Comparing `colossalai-0.2.8/colossalai/gemini/memory_tracer/utils.py` & `colossalai-0.3.0/colossalai/zero/gemini/memory_tracer/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/ophooks/_shard_grad_ophook.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/_shard_grad_ophook.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 from . import BaseOpHook
 
 
 @OPHOOKS.register_module
 class ShardGradMemTracerHook(BaseOpHook):
     """
-    A hook to process sharded param before and afther FWD and BWD operator executing.
+    A hook to process sharded param before and after FWD and BWD operator executing.
     """
 
     def __init__(self):
         super().__init__()
 
     def pre_fwd_exec(self, module: torch.nn.Module, *args):
         pass
```

### Comparing `colossalai-0.2.8/colossalai/gemini/ophooks/_shard_param_ophook.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/_shard_param_ophook.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 import torch
+
 from colossalai.registry import OPHOOKS
 
 from . import BaseOpHook
 
 
 @OPHOOKS.register_module
 class ShardParamHook(BaseOpHook):
     """
-    A hook to process sharded param before and afther FWD and BWD operator executing.
+    A hook to process sharded param before and after FWD and BWD operator executing.
     """
 
     def __init__(self):
         super().__init__()
 
     def niter(self):
         return self._niter
```

### Comparing `colossalai-0.2.8/colossalai/gemini/ophooks/runtime_mem_tracer_hook.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/runtime_mem_tracer_hook.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from contextlib import contextmanager
 from enum import Enum
 from functools import partial
 from typing import List
 
 import torch
 
-from colossalai.gemini.memory_tracer import MemStats, SyncCudaMemoryMonitor
-from colossalai.gemini.tensor_utils import alloc_storage, free_storage
 from colossalai.tensor.param_op_hook import ColoParamOpHook
+from colossalai.zero.gemini.memory_tracer import MemStats, SyncCudaMemoryMonitor
+from colossalai.zero.legacy.gemini.tensor_utils import alloc_storage, free_storage
 
 
 class TrainingPhase(Enum):
     FORWARD = 0
     BACKWARD = 1
```

### Comparing `colossalai-0.2.8/colossalai/gemini/ophooks/utils.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/ophooks/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/gemini/paramhooks/_param_hookmgr.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/paramhooks/_param_hookmgr.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
+import functools
 from typing import Callable, List
+
 import torch
-import functools
 
 
 class BaseParamHookMgr(object):
 
     def __init__(self, param_list: List[torch.nn.Parameter]) -> None:
         r"""
         register backward hook on every parameters of module
```

### Comparing `colossalai-0.2.8/colossalai/gemini/placement_policy.py` & `colossalai-0.3.0/colossalai/zero/gemini/placement_policy.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 import functools
 from abc import ABC, abstractmethod
 from time import time
 from typing import Dict, List, Optional, Tuple, Type
 
 import torch
 
-from colossalai.gemini.chunk import Chunk, ChunkManager
-from colossalai.gemini.memory_tracer import ChunkMemStatsCollector
 from colossalai.utils import get_current_device
 from colossalai.utils.memory import colo_device_memory_capacity
 
+from .chunk import Chunk, ChunkManager
+from .memory_tracer import ChunkMemStatsCollector
+
 
 class PlacementPolicy(ABC):
     need_mem_stats: bool = False
 
     def __init__(self,
                  chunk_manager: ChunkManager,
                  mem_stats_collector: Optional[ChunkMemStatsCollector] = None) -> None:
```

### Comparing `colossalai-0.2.8/colossalai/gemini/stateful_tensor.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/stateful_tensor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
+
 import torch
-from typing import Union
 
-from colossalai.gemini.gemini_context import GeminiMemoryManager
+from .gemini_context import GeminiMemoryManager
 
 
 def sizeof_tensor(tensor: torch.Tensor):
     return tensor.numel() * tensor.element_size()
 
 
 class TensorState(Enum):
@@ -15,15 +15,15 @@
     HOLD = 1
     HOLD_AFTER_FWD = 2
     HOLD_AFTER_BWD = 3
     COMPUTE = 4
 
 
 class StatefulTensor(object):
-    """A Structure stores a Torch Tensor and labeled states. 
+    """A Structure stores a Torch Tensor and labeled states.
     Inspired from the paper:
     PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management
 
     https://arxiv.org/abs/2108.05818
     """
     # Global Stateful Tensor Manager
     GST_MGR = GeminiMemoryManager(TensorState)
```

### Comparing `colossalai-0.2.8/colossalai/gemini/stateful_tensor_mgr.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/stateful_tensor_mgr.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 import functools
-import torch
 import types
-from colossalai.utils.cuda import get_current_device
-from colossalai.gemini.tensor_utils import colo_model_data_tensor_move_inline, colo_tensor_mem_usage
-from colossalai.gemini.stateful_tensor import StatefulTensor, TensorState
-from colossalai.gemini.tensor_placement_policy import TensorPlacementPolicy
+from time import time
 from typing import List
+
+import torch
+
 from colossalai.logging import get_dist_logger
-from time import time
+from colossalai.utils.cuda import get_current_device
+
+from .stateful_tensor import StatefulTensor, TensorState
+from .tensor_placement_policy import TensorPlacementPolicy
+from .tensor_utils import colo_model_data_tensor_move_inline, colo_tensor_mem_usage
 
 
 class StatefulTensorMgr(object):
     """
     Stateful Tensor Manager, inspired from PatrickStar
 
     PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management
@@ -46,15 +49,15 @@
         self._warmup = False
         self._compute_idx = -1
         self._cpu_gpu_move_volume = 0
         self._layout_time = 0
         self._evict_time = 0
 
     def adjust_layout(self) -> None:
-        """ Adjust the layout of statefuil tensor according to the information provided
+        """ Adjust the layout of stateful tensor according to the information provided
         by mem_stats_collector, which should belongs to a Sharded Model.
         """
         # find stateful tensor in state COMPUTE
         cuda_demand = StatefulTensor.GST_MGR.state_mem['cpu'][TensorState.COMPUTE]
         start = time()
         move_to_cuda_tensor_list, hold_cuda_tensor_list = self._get_layout_info(self._compute_idx, self._warmup)
         self._layout_time += time() - start
```

### Comparing `colossalai-0.2.8/colossalai/gemini/tensor_placement_policy.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/tensor_placement_policy.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,20 @@
+import functools
 from abc import ABC, abstractmethod
 from time import time
-from typing import List, Optional
+from typing import List, Optional, Type
+
 import torch
+
 from colossalai.utils import get_current_device
 from colossalai.utils.memory import colo_device_memory_capacity
+from colossalai.zero.gemini.memory_tracer import MemStatsCollector
 
-from colossalai.gemini.tensor_utils import colo_model_data_tensor_move_inline, colo_tensor_mem_usage
-from colossalai.gemini.stateful_tensor import StatefulTensor
-from colossalai.gemini.memory_tracer import MemStatsCollector
-from typing import Type
-import functools
+from .stateful_tensor import StatefulTensor
+from .tensor_utils import colo_model_data_tensor_move_inline, colo_tensor_mem_usage
 
 
 class TensorPlacementPolicy(ABC):
 
     def __init__(self, device: Optional[torch.device], mem_stats_collector: Optional[MemStatsCollector] = None) -> None:
         self.device: Optional[torch.device] = device
         self.mem_stats_collector: Optional[MemStatsCollector] = mem_stats_collector
```

### Comparing `colossalai-0.2.8/colossalai/gemini/tensor_utils.py` & `colossalai-0.3.0/colossalai/zero/legacy/gemini/tensor_utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,12 @@
+from typing import Tuple, Union
+
 import torch
-from colossalai.gemini.stateful_tensor import StatefulTensor
-from typing import Union, Tuple
+
+from .stateful_tensor import StatefulTensor
 
 
 def is_storage_empty(tensor: torch.Tensor) -> bool:
     return tensor.storage().size() == 0
 
 
 def free_storage(tensor: torch.Tensor) -> None:
```

### Comparing `colossalai-0.2.8/colossalai/global_variables.py` & `colossalai-0.3.0/colossalai/global_variables.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-from typing import Optional
-
-
-class TensorParallelEnv(object):
-    _instance = None
-
-    def __new__(cls, *args, **kwargs):
-        if cls._instance is None:
-            cls._instance = object.__new__(cls, *args, **kwargs)
-        return cls._instance
-
-    def __init__(self, *args, **kwargs):
-        self.load(*args, **kwargs)
-
-    def load(self,
-             mode: Optional[str] = None,
-             vocab_parallel: bool = False,
-             parallel_input_1d: bool = False,
-             summa_dim: int = None,
-             tesseract_dim: int = None,
-             tesseract_dep: int = None,
-             depth_3d: int = None,
-             input_group_3d=None,
-             weight_group_3d=None,
-             output_group_3d=None,
-             input_x_weight_group_3d=None,
-             output_x_weight_group_3d=None):
-        self.mode = mode
-        self.vocab_parallel = vocab_parallel
-        self.parallel_input_1d = parallel_input_1d
-        self.summa_dim = summa_dim
-        self.tesseract_dim = tesseract_dim
-        self.tesseract_dep = tesseract_dep
-        self.depth_3d = depth_3d
-        self.input_group_3d = input_group_3d
-        self.weight_group_3d = weight_group_3d
-        self.output_group_3d = output_group_3d
-        self.input_x_weight_group_3d = input_x_weight_group_3d
-        self.output_x_weight_group_3d = output_x_weight_group_3d
-
-    def save(self):
-        return dict(mode=self.mode,
-                    vocab_parallel=self.vocab_parallel,
-                    parallel_input_1d=self.parallel_input_1d,
-                    summa_dim=self.summa_dim,
-                    tesseract_dim=self.tesseract_dim,
-                    tesseract_dep=self.tesseract_dep,
-                    depth_3d=self.depth_3d,
-                    input_group_3d=self.input_group_3d,
-                    weight_group_3d=self.weight_group_3d,
-                    output_group_3d=self.output_group_3d,
-                    input_x_weight_group_3d=self.input_x_weight_group_3d,
-                    output_x_weight_group_3d=self.output_x_weight_group_3d)
-
-
-tensor_parallel_env = TensorParallelEnv()
+from typing import Optional
+
+
+class TensorParallelEnv(object):
+    _instance = None
+
+    def __new__(cls, *args, **kwargs):
+        if cls._instance is None:
+            cls._instance = object.__new__(cls, *args, **kwargs)
+        return cls._instance
+
+    def __init__(self, *args, **kwargs):
+        self.load(*args, **kwargs)
+
+    def load(self,
+             mode: Optional[str] = None,
+             vocab_parallel: bool = False,
+             parallel_input_1d: bool = False,
+             summa_dim: int = None,
+             tesseract_dim: int = None,
+             tesseract_dep: int = None,
+             depth_3d: int = None,
+             input_group_3d=None,
+             weight_group_3d=None,
+             output_group_3d=None,
+             input_x_weight_group_3d=None,
+             output_x_weight_group_3d=None):
+        self.mode = mode
+        self.vocab_parallel = vocab_parallel
+        self.parallel_input_1d = parallel_input_1d
+        self.summa_dim = summa_dim
+        self.tesseract_dim = tesseract_dim
+        self.tesseract_dep = tesseract_dep
+        self.depth_3d = depth_3d
+        self.input_group_3d = input_group_3d
+        self.weight_group_3d = weight_group_3d
+        self.output_group_3d = output_group_3d
+        self.input_x_weight_group_3d = input_x_weight_group_3d
+        self.output_x_weight_group_3d = output_x_weight_group_3d
+
+    def save(self):
+        return dict(mode=self.mode,
+                    vocab_parallel=self.vocab_parallel,
+                    parallel_input_1d=self.parallel_input_1d,
+                    summa_dim=self.summa_dim,
+                    tesseract_dim=self.tesseract_dim,
+                    tesseract_dep=self.tesseract_dep,
+                    depth_3d=self.depth_3d,
+                    input_group_3d=self.input_group_3d,
+                    weight_group_3d=self.weight_group_3d,
+                    output_group_3d=self.output_group_3d,
+                    input_x_weight_group_3d=self.input_x_weight_group_3d,
+                    output_x_weight_group_3d=self.output_x_weight_group_3d)
+
+
+tensor_parallel_env = TensorParallelEnv()
```

### Comparing `colossalai-0.2.8/colossalai/initialize.py` & `colossalai-0.3.0/colossalai/initialize.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,21 +25,20 @@
 from colossalai.engine.gradient_accumulation import accumulate_gradient
 from colossalai.engine.schedule import (
     InterleavedPipelineSchedule,
     NonPipelineSchedule,
     PipelineSchedule,
     get_tensor_shape,
 )
-from colossalai.gemini.ophooks import BaseOpHook
 from colossalai.logging import get_dist_logger
 from colossalai.nn.optimizer.colossalai_optimizer import ColossalaiOptimizer
 from colossalai.utils import get_current_device, is_using_ddp, is_using_pp, is_using_sequence, sync_model_param
 from colossalai.utils.moe import sync_moe_model_param
-from colossalai.zero import convert_to_zero_v2
-from colossalai.zero.sharded_optim.sharded_optim_v2 import ShardedOptimizerV2
+from colossalai.zero.legacy import ShardedOptimizerV2, convert_to_zero_v2
+from colossalai.zero.legacy.gemini.ophooks import BaseOpHook
 
 
 def get_default_parser():
     """Reads user command line and uses an argument parser to parse the input arguments.
     Input arguments include configuration, host, port, world size, local rank, backend for torch.distributed.
 
     Returns:
```

### Comparing `colossalai-0.2.8/colossalai/interface/model.py` & `colossalai-0.3.0/colossalai/interface/model.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/interface/optimizer.py` & `colossalai-0.3.0/colossalai/interface/optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/colossal_C_frontend.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/colossal_C_frontend.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/cpu_adam.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/cpu_adam.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/cross_entropy.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/cross_entropy.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/cublas_wrappers.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/cublas_wrappers.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/cuda_util.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/cuda_util.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/dropout_kernels.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/dropout_kernels.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/general_kernels.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/general_kernels.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/block_reduce.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/block_reduce.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/context.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/context.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/cross_entropy_layer.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/cross_entropy_layer.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/cublas_wrappers.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/cublas_wrappers.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/cuda_util.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/cuda_util.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/dropout.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/dropout.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/feed_forward.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/feed_forward.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/kernels.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/kernels.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/normalize_layer.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/normalize_layer.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/softmax.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/include/strided_batch_gemm.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/include/strided_batch_gemm.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/normalize_kernels.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/normalize_kernels.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/softmax_kernels.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/softmax_kernels.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/kernels/transform_kernels.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/kernels/transform_kernels.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/layer_norm_cuda.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/layer_norm_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/layer_norm_cuda_kernel.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/layer_norm_cuda_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/moe_cuda.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/moe_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/moe_cuda_kernel.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/moe_cuda_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_adam.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_adam.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_apply.cuh` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_apply.cuh`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_l2norm_kernel.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_l2norm_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_lamb.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_lamb.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_scale_kernel.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_scale_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multi_tensor_sgd_kernel.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multi_tensor_sgd_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/multihead_attention_1d.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax_cuda.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.cpp` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax_cuda.cu` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/csrc/type_shim.h` & `colossalai-0.3.0/colossalai/kernel/cuda_native/csrc/type_shim.h`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/flash_attention.py` & `colossalai-0.3.0/colossalai/kernel/cuda_native/flash_attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/layer_norm.py` & `colossalai-0.3.0/colossalai/kernel/cuda_native/layer_norm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/multihead_attention.py` & `colossalai-0.3.0/colossalai/kernel/cuda_native/multihead_attention.py`

 * *Files 0% similar despite different names*

```diff
@@ -107,15 +107,15 @@
 
         layer_id: The layer-index counter starting from 0 and incrementing by 1 every time a layer object is instantiated,
         e.g. if a model has 24 transformer layers, layer_id goes from 0 to 23.
 
     Arguments:
         hidden_size: Total dimension of hidden_size.
         nhead: Number of parallel attention heads.
-        batch_size: Batch Size for one foward
+        batch_size: Batch Size for one forward
         max_seq_len: Max length of input sequence
         dropout: Dropout probability
         norm_first: perform LayerNorms before attention
     """
 
     layer_id = 0
```

### Comparing `colossalai-0.2.8/colossalai/kernel/cuda_native/scaled_softmax.py` & `colossalai-0.3.0/colossalai/kernel/cuda_native/scaled_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/jit/bias_dropout_add.py` & `colossalai-0.3.0/colossalai/kernel/jit/bias_dropout_add.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/jit/bias_gelu.py` & `colossalai-0.3.0/colossalai/kernel/jit/bias_gelu.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/jit/option.py` & `colossalai-0.3.0/colossalai/kernel/jit/option.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/__init__.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/builder.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 # Licensed under the MIT License.
 import importlib
 import os
 import time
 from abc import ABC, abstractmethod
 from pathlib import Path
-from typing import List
+from typing import List, Optional
 
 from .utils import check_cuda_availability, check_system_pytorch_cuda_match, print_rank_0
 
 
 class Builder(ABC):
     """
     Builder is the base class to build extensions for PyTorch.
@@ -74,15 +74,15 @@
         This function should return a list of source files for extensions.
         """
         raise NotImplementedError
 
     @abstractmethod
     def include_dirs(self) -> List[str]:
         """
-        This function should return a list of inlcude files for extensions.
+        This function should return a list of include files for extensions.
         """
         pass
 
     @abstractmethod
     def cxx_flags(self) -> List[str]:
         """
         This function should return a list of cxx compilation flags for extensions.
@@ -123,47 +123,49 @@
 
         if not TORCH_AVAILABLE:
             raise ModuleNotFoundError(
                 "PyTorch is not found. You need to install PyTorch first in order to build CUDA extensions")
 
         if CUDA_HOME is None:
             raise RuntimeError(
-                "CUDA_HOME is not found. You need to export CUDA_HOME environment vairable or install CUDA Toolkit first in order to build CUDA extensions"
+                "CUDA_HOME is not found. You need to export CUDA_HOME environment variable or install CUDA Toolkit first in order to build CUDA extensions"
             )
 
         # make sure CUDA is available for compilation during
         cuda_available = check_cuda_availability()
         if not cuda_available:
-            raise RuntimeError("CUDA is not available on your system as torch.cuda.is_avaible() returns False.")
+            raise RuntimeError("CUDA is not available on your system as torch.cuda.is_available() returns False.")
 
         # make sure system CUDA and pytorch CUDA match, an error will raised inside the function if not
         check_system_pytorch_cuda_match(CUDA_HOME)
 
-    def load(self, verbose=True):
+    def load(self, verbose: Optional[bool] = None):
         """
         load the kernel during runtime. If the kernel is not built during pip install, it will build the kernel.
         If the kernel is built during runtime, it will be stored in `~/.cache/colossalai/torch_extensions/`. If the
         kernel is built during pip install, it can be accessed through `colossalai._C`.
 
         Warning: do not load this kernel repeatedly during model execution as it could slow down the training process.
 
         Args:
             verbose (bool, optional): show detailed info. Defaults to True.
         """
+        if verbose is None:
+            verbose = os.environ.get('CAI_KERNEL_VERBOSE', '0') == '1'
         # if the kernel has be compiled and cached, we directly use it
         if self.cached_op_module is not None:
             return self.cached_op_module
 
         try:
             # if the kernel has been pre-built during installation
             # we just directly import it
             op_module = self.import_op()
             if verbose:
                 print_rank_0(
-                    f"[extension] OP {self.prebuilt_import_path} has been compileed ahead of time, skip building.")
+                    f"[extension] OP {self.prebuilt_import_path} has been compiled ahead of time, skip building.")
         except ImportError:
             # check environment
             self.check_runtime_build_environment()
 
             # time the kernel compilation
             start_build = time.time()
```

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/cpu_adam.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/cpu_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/fused_optim.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/fused_optim.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/layernorm.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/moe.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/moe.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/multi_head_attn.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/multi_head_attn.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/scaled_masked_softmax.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/scaled_masked_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/scaled_upper_triangle_masked_softmax.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/scaled_upper_triangle_masked_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/kernel/op_builder/utils.py` & `colossalai-0.3.0/colossalai/kernel/op_builder/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,15 +32,15 @@
     import torch
 
     try:
         torch_cuda_major = torch.version.cuda.split(".")[0]
         torch_cuda_minor = torch.version.cuda.split(".")[1]
     except:
         raise ValueError(
-            "[extension] Cannot retrive the CUDA version in the PyTorch binary given by torch.version.cuda")
+            "[extension] Cannot retrieve the CUDA version in the PyTorch binary given by torch.version.cuda")
     return torch_cuda_major, torch_cuda_minor
 
 
 def get_cuda_bare_metal_version(cuda_dir) -> List[int]:
     """
     Get the System CUDA version from nvcc.
 
@@ -86,15 +86,14 @@
     if bare_metal_major != torch_cuda_major:
         raise Exception(
             f'[extension] Failed to build PyTorch extension because the detected CUDA version ({bare_metal_major}.{bare_metal_minor}) '
             f'mismatches the version that was used to compile PyTorch ({torch_cuda_major}.{torch_cuda_minor}).'
             'Please make sure you have set the CUDA_HOME correctly and installed the correct PyTorch in https://pytorch.org/get-started/locally/ .'
         )
 
-    print(bare_metal_minor != torch_cuda_minor)
     if bare_metal_minor != torch_cuda_minor:
         warnings.warn(
             f"[extension] The CUDA version on the system ({bare_metal_major}.{bare_metal_minor}) does not match with the version ({torch_cuda_major}.{torch_cuda_minor}) torch was compiled with. "
             "The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. "
             "If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions"
         )
     return True
@@ -107,15 +106,15 @@
     Returns:
         A tuple of integers in the form of (major, minor, patch).
     """
     import torch
     torch_version = torch.__version__.split('+')[0]
     TORCH_MAJOR = int(torch_version.split('.')[0])
     TORCH_MINOR = int(torch_version.split('.')[1])
-    TORCH_PATCH = int(torch_version.split('.')[2])
+    TORCH_PATCH = int(torch_version.split('.')[2], 16)
     return TORCH_MAJOR, TORCH_MINOR, TORCH_PATCH
 
 
 def check_pytorch_version(min_major_version, min_minor_version) -> bool:
     """
     Compare the current PyTorch version with the minium required version.
 
@@ -152,24 +151,23 @@
     This function sets the PyTorch TORCH_CUDA_ARCH_LIST variable for ahead-of-time extension compilation.
     Ahead-of-time compilation occurs when CUDA_EXT=1 is set when running 'pip install'.
     """
     cuda_available = check_cuda_availability()
 
     # we only need to set this when CUDA is not available for cross-compilation
     if not cuda_available:
-        warnings.warn(
-            '\n[extension]  PyTorch did not find available GPUs on this system.\n'
-            'If your intention is to cross-compile, this is not an error.\n'
-            'By default, Colossal-AI will cross-compile for \n'
-            '1. Pascal (compute capabilities 6.0, 6.1, 6.2),\n'
-            '2. Volta (compute capability 7.0)\n'
-            '3. Turing (compute capability 7.5),\n'
-            '4. Ampere (compute capability 8.0, 8.6)if the CUDA version is >= 11.0\n'
-            '\nIf you wish to cross-compile for a single specific architecture,\n'
-            'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n')
+        warnings.warn('\n[extension]  PyTorch did not find available GPUs on this system.\n'
+                      'If your intention is to cross-compile, this is not an error.\n'
+                      'By default, Colossal-AI will cross-compile for \n'
+                      '1. Pascal (compute capabilities 6.0, 6.1, 6.2),\n'
+                      '2. Volta (compute capability 7.0)\n'
+                      '3. Turing (compute capability 7.5),\n'
+                      '4. Ampere (compute capability 8.0, 8.6)if the CUDA version is >= 11.0\n'
+                      '\nIf you wish to cross-compile for a single specific architecture,\n'
+                      'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n')
 
         if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None:
             bare_metal_major, bare_metal_minor = get_cuda_bare_metal_version(cuda_dir)
 
             arch_list = ['6.0', '6.1', '6.2', '7.0', '7.5']
 
             if int(bare_metal_major) == 11:
```

### Comparing `colossalai-0.2.8/colossalai/logging/__init__.py` & `colossalai-0.3.0/colossalai/logging/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/logging/logger.py` & `colossalai-0.3.0/colossalai/logging/logger.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/_utils.py` & `colossalai-0.3.0/colossalai/nn/_ops/_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-import torch
-from typing import Union, Optional, List
-from colossalai.tensor import ColoTensor
+from typing import List, Optional, Union
+
 import torch
 import torch.distributed as dist
-from colossalai.global_variables import tensor_parallel_env as env
 
+from colossalai.global_variables import tensor_parallel_env as env
 from colossalai.nn.layer.utils import divide
-from colossalai.tensor import ProcessGroup, ColoTensorSpec
+from colossalai.tensor import ColoTensor, ColoTensorSpec, ProcessGroup
 
 GeneralTensor = Union[ColoTensor, torch.Tensor]
 Number = Union[int, float]
 
 
 def convert_to_colo_tensor(tensor: Optional[GeneralTensor], pg: ProcessGroup) -> Optional[ColoTensor]:
     if tensor is not None and not isinstance(tensor, ColoTensor):
@@ -131,15 +130,15 @@
     def backward(ctx, grad_output):
         return grad_output, None
 
 
 class _SplitForwardGatherBackward(torch.autograd.Function):
     """
     Split the input and keep only the corresponding chuck to the rank.
-    
+
     Args:
         input_: input matrix.
         process_group: parallel mode.
         dim: dimension
     """
 
     @staticmethod
```

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/addmm.py` & `colossalai-0.3.0/colossalai/nn/_ops/addmm.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import torch
+
+from colossalai.tensor import ColoTensor, ColoTensorSpec, ComputePattern, ComputeSpec, ReplicaSpec, ShardSpec, distspec
 from colossalai.tensor.op_wrapper import colo_op_impl
-from colossalai.tensor import ComputePattern, ComputePattern, ComputeSpec, ColoTensor
-from colossalai.tensor import distspec, ColoTensorSpec, ShardSpec, ReplicaSpec
-from ._utils import GeneralTensor, Number, convert_to_colo_tensor
-from ._utils import reduce_input, reduce_grad
+
+from ._utils import GeneralTensor, Number, convert_to_colo_tensor, reduce_grad, reduce_input
 
 
 def colo_addmm_1Drow(input_tensor: ColoTensor, mat1: ColoTensor, mat2: ColoTensor, beta: Number,
                      alpha: Number) -> ColoTensor:
     # mat1:S[1] x mat2:S[0] = Output:P
     # beta * input + alpha * All-Reduce(Output) = res
 
@@ -65,17 +65,21 @@
     mat1 = convert_to_colo_tensor(mat1, mat2.get_process_group())
 
     # Add communication logic before and after linear call.
     ret_tensor = None
     if not mat2.has_compute_spec():    # No Model Parallel Applied
         assert mat2.is_replicate(), 'Invalid mat2 spec for native addmm op'
         assert input_tensor.is_replicate(), 'Invalid input spec for native addmm op'
-        ret_tensor = ColoTensor.from_torch_tensor(
-            tensor=torch.addmm(input_tensor, mat1, mat2, beta=beta, alpha=alpha, **kargs),
-            spec=ColoTensorSpec(mat2.get_process_group()))
+        ret_tensor = ColoTensor.from_torch_tensor(tensor=torch.addmm(input_tensor,
+                                                                     mat1,
+                                                                     mat2,
+                                                                     beta=beta,
+                                                                     alpha=alpha,
+                                                                     **kargs),
+                                                  spec=ColoTensorSpec(mat2.get_process_group()))
     elif mat2.has_compute_pattern(ComputePattern.TP1D):    # Single Model Parallel Applied
         if mat2.is_shard_1drow() and input_tensor.is_replicate():
             mode = 'row'
         elif mat2.is_shard_1dcol() and (input_tensor.is_shard_1dcol() or input_tensor.is_shard_1drow()):
             mode = 'col'
         else:
             raise NotImplementedError
```

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/batch_norm.py` & `colossalai-0.3.0/colossalai/nn/_ops/batch_norm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/element_wise.py` & `colossalai-0.3.0/colossalai/nn/_ops/element_wise.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/embedding.py` & `colossalai-0.3.0/colossalai/nn/_ops/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/embedding_bag.py` & `colossalai-0.3.0/colossalai/nn/_ops/embedding_bag.py`

 * *Files 0% similar despite different names*

```diff
@@ -84,15 +84,15 @@
                        padding_idx: Optional[int] = None):
     """Handles ``__torch_function__`` dispatch for ``torch.nn.functional.embedding_bag``.
     This method looks up an embedding table.
     """
     assert isinstance(weight, ColoTensor)
     input_tensor = convert_to_colo_tensor(input_tensor, weight.get_process_group())
 
-    # Handle differen parallel actions.
+    # Handle different parallel actions.
 
     if not weight.has_compute_spec():    # No Model Parallel Applied
         assert weight.is_replicate(), 'Invalid weight spec for native embedding op'
         return ColoTensor.from_torch_tensor(tensor=F.embedding_bag(input_tensor,
                                                                    weight,
                                                                    offsets=offsets,
                                                                    max_norm=max_norm,
```

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/layernorm.py` & `colossalai-0.3.0/colossalai/nn/_ops/layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/linear.py` & `colossalai-0.3.0/colossalai/nn/_ops/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/loss.py` & `colossalai-0.3.0/colossalai/nn/_ops/loss.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/_ops/view.py` & `colossalai-0.3.0/colossalai/nn/_ops/view.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/init.py` & `colossalai-0.3.0/colossalai/nn/init.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/base_layer.py` & `colossalai-0.3.0/colossalai/nn/layer/base_layer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/_utils.py` & `colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/dropout.py` & `colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/embedding.py` & `colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/linear.py` & `colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/colossalai_layer/normalization.py` & `colossalai-0.3.0/colossalai/nn/layer/colossalai_layer/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/moe/_operation.py` & `colossalai-0.3.0/colossalai/nn/layer/moe/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/moe/routers.py` & `colossalai-0.3.0/colossalai/nn/layer/moe/routers.py`

 * *Files 0% similar despite different names*

```diff
@@ -56,15 +56,15 @@
         reservation = self._routing_loss
         self._routing_loss = None
         return reservation
 
 
 class Top1Router(MoeRouter):
     """Top1 router that returns the dispatch mask [s, e, c] and combine weight [s, e, c]
-    for routing usage. More deailted function can be found in the paper about Switch Transformer
+    for routing usage. More detailed function can be found in the paper about Switch Transformer
     of Google.
     Args:
         capacity_factor_train (float, optional): Capacity factor in routing of training.
         capacity_factor_eval (float, optional): Capacity factor in routing of evaluation.
         min_capacity (int, optional): The minimum number of the capacity of each expert.
         select_policy (str, optional): The policy about tokens selection.
         noisy_func (:class:`typing.Callable`, optional): Noisy function used in logits.
@@ -139,15 +139,15 @@
             combine_weights = weight.unsqueeze(2) * ranks.unsqueeze(1)
             sec_mask = combine_weights.bool()
             return combine_weights, sec_mask
 
 
 class Top2Router(MoeRouter):
     """Top2 router that returns the dispatch mask [s, e, c] and combine weight [s, e, c]
-    for routing usage. More deailted function can be found in the paper about ViT-MoE.
+    for routing usage. More detailed function can be found in the paper about ViT-MoE.
     Args:
         capacity_factor_train (float, optional): Capacity factor in routing of training.
         capacity_factor_eval (float, optional): Capacity factor in routing of evaluation.
         min_capacity (int, optional): The minimum number of the capacity of each expert
         noisy_func (:class:`typing.Callable`, optional): Noisy function used in logits.
         drop_tks (bool, optional): Whether drops tokens in evaluation.
     """
```

### Comparing `colossalai-0.2.8/colossalai/nn/layer/moe/utils.py` & `colossalai-0.3.0/colossalai/nn/layer/moe/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 class ForceFP32Parameter(torch.nn.Parameter):
 
     def half(self, memory_format=None):
         return self.data.clone()
 
 
 class NormalNoiseGenerator:
-    """Generates a random noisy mask for logtis tensor.
+    """Generates a random noisy mask for logits tensor.
 
     All noise is generated from a normal distribution :math:`(0, 1 / E^2)`, where
     `E = the number of experts`.
 
     Args:
         num_experts (int): The number of experts.
     """
@@ -28,15 +28,15 @@
 
     def __call__(self, inputs: torch.Tensor):
         noisy = self.normal(inputs.shape)
         return inputs + noisy
 
 
 class UniformNoiseGenerator:
-    """Generates a random noisy mask for logtis tensor.
+    """Generates a random noisy mask for logits tensor.
     copied from mesh tensorflow:
     Multiply values by a random number between :math:`1-epsilon` and :math:`1+epsilon`.
     Makes models more resilient to rounding errors introduced by bfloat16.
     This seems particularly important for logits.
 
     Args:
         eps (float, optional): Epsilon in generator, defaults 1e-2.
```

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_1d/_operation.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_1d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_1d/_utils.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_1d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_1d/layers.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_1d/layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -435,15 +435,15 @@
         out_features (int): size of each output sample.
         bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
         dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
         gather_output (bool, optional): If true, call all-gather on output and make Y available
                     to all GPUs, otherwise, every GPU will have its output
                     which is :math:`Y_i = XA_i`, defaults to False
         skip_bias_add (bool, optional): If set to ``True``, it will skip bias add for linear layer,
-            which is preserved for kernel fusion, defaults to Fals
+            which is preserved for kernel fusion, defaults to False
         weight_initializer (:class:`typing.Callable`, optional):
             The initializer of weight, defaults to kaiming uniform initializer.
         bias_initializer (:class:`typing.Callable`, optional):
             The initializer of bias, defaults to xavier uniform initializer.
 
     More details about ``initializer`` please refer to
     `init <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/init.py>`_.
@@ -574,15 +574,15 @@
     Args:
         in_features (int): size of each input sample.
         out_features (int): size of each output sample.
         bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
         dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
         parallel_input (bool, optional): If set to ``True``, it's assumed that the input is split, defaults to False.
         skip_bias_add (bool, optional): If set to ``True``, it will skip bias add for linear layer,
-            which is preserved for kernel fusion, defaults to Fals
+            which is preserved for kernel fusion, defaults to False
         weight_initializer (:class:`typing.Callable`, optional):
             The initializer of weight, defaults to kaiming uniform initializer.
         bias_initializer (:class:`typing.Callable`, optional):
             The initializer of bias, defaults to xavier uniform initializer.
 
     More details about ``initializer`` please refer to
     `init <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/init.py>`_.
@@ -990,19 +990,19 @@
     :type in_chans: int
     :param embed_size: size of embedding
     :type embed_size: int
     :param dtype: The dtype of parameters, defaults to None
     :type dtype: torch.dtype, optional
     :param flatten: whether to flatten output tensor, defaults to True
     :type flatten: bool, optional
-    :param weight_initializer: The intializer of weight, defaults to kaiming uniform initializer
+    :param weight_initializer: The initializer of weight, defaults to kaiming uniform initializer
     :type weight_initializer: typing.Callable, optional
-    :param bias_initializer: The intializer of bias, defaults to xavier uniform initializer
+    :param bias_initializer: The initializer of bias, defaults to xavier uniform initializer
     :type bias_initializer: typing.Callable, optional
-    :param position_embed_initializer: The intializer of position embedding, defaults to zero
+    :param position_embed_initializer: The initializer of position embedding, defaults to zero
     :type position_embed_initializer: typing.Callable, optional
     """
 
     def __init__(self,
                  img_size: int,
                  patch_size: int,
                  in_chans: int,
```

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_2d/_operation.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_2d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_2d/_utils.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_2d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_2d/layers.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_2d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/_operation.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/_utils.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_2p5d/layers.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_2p5d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_3d/_operation.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_3d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_3d/_utils.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_3d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_3d/layers.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_3d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/_operation.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/parallel_sequence/layers.py` & `colossalai-0.3.0/colossalai/nn/layer/parallel_sequence/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/utils/common.py` & `colossalai-0.3.0/colossalai/nn/layer/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/vanilla/layers.py` & `colossalai-0.3.0/colossalai/nn/layer/vanilla/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/layer/wrapper/pipeline_wrapper.py` & `colossalai-0.3.0/colossalai/nn/layer/wrapper/pipeline_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/loss/__init__.py` & `colossalai-0.3.0/colossalai/nn/loss/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/loss/loss_1d.py` & `colossalai-0.3.0/colossalai/nn/loss/loss_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/loss/loss_2d.py` & `colossalai-0.3.0/colossalai/nn/loss/loss_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/loss/loss_2p5d.py` & `colossalai-0.3.0/colossalai/nn/loss/loss_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/loss/loss_3d.py` & `colossalai-0.3.0/colossalai/nn/loss/loss_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/loss/loss_moe.py` & `colossalai-0.3.0/colossalai/nn/loss/loss_moe.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/__init__.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/cosine.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/cosine.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/delayed.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/delayed.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/linear.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/multistep.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/multistep.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/onecycle.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/onecycle.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/poly.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/poly.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/lr_scheduler/torch.py` & `colossalai-0.3.0/colossalai/nn/lr_scheduler/torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/metric/__init__.py` & `colossalai-0.3.0/colossalai/nn/metric/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/metric/accuracy_2d.py` & `colossalai-0.3.0/colossalai/nn/metric/accuracy_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/metric/accuracy_2p5d.py` & `colossalai-0.3.0/colossalai/nn/metric/accuracy_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/metric/accuracy_3d.py` & `colossalai-0.3.0/colossalai/nn/metric/accuracy_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/colossalai_optimizer.py` & `colossalai-0.3.0/colossalai/nn/optimizer/colossalai_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/cpu_adam.py` & `colossalai-0.3.0/colossalai/nn/optimizer/cpu_adam.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 from .nvme_optimizer import NVMeOptimizer
 
 
 @OPTIMIZERS.register_module
 class CPUAdam(NVMeOptimizer):
     """Implements Adam algorithm.
 
-    Supports parameters updating on both GPU and CPU, depanding on the device of paramters.
+    Supports parameters updating on both GPU and CPU, depanding on the device of parameters.
     But the parameters and gradients should on the same device:
       * Parameters on CPU and gradients on CPU is allowed.
       * Parameters on GPU and gradients on GPU is allowed.
       * Parameters on GPU and gradients on CPU is **not** allowed.
 
     `CPUAdam` requires CUDA extensions which can be built during installation or runtime.
```

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/fused_adam.py` & `colossalai-0.3.0/colossalai/nn/optimizer/fused_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/fused_lamb.py` & `colossalai-0.3.0/colossalai/nn/optimizer/fused_lamb.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/fused_sgd.py` & `colossalai-0.3.0/colossalai/nn/optimizer/fused_sgd.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/hybrid_adam.py` & `colossalai-0.3.0/colossalai/nn/optimizer/hybrid_adam.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,27 +9,27 @@
 from .nvme_optimizer import NVMeOptimizer
 
 
 @OPTIMIZERS.register_module
 class HybridAdam(NVMeOptimizer):
     """Implements Adam algorithm.
 
-    Supports parameters updating on both GPU and CPU, depanding on the device of paramters.
+    Supports parameters updating on both GPU and CPU, depanding on the device of parameters.
     But the parameters and gradients should on the same device:
       * Parameters on CPU and gradients on CPU is allowed.
       * Parameters on GPU and gradients on GPU is allowed.
       * Parameters on GPU and gradients on CPU is **not** allowed.
 
-    `HybriadAdam` requires CUDA extensions which can be built during installation or runtime.
+    `HybridAdam` requires CUDA extensions which can be built during installation or runtime.
 
     This version of Hybrid Adam is an hybrid of CPUAdam and FusedAdam.
 
     * For parameters updating on CPU, it uses CPUAdam.
     * For parameters updating on GPU, it uses FusedAdam.
-    * Hybird precision calculation of fp16 and fp32 is supported, eg fp32 parameters and fp16 gradients.
+    * Hybrid precision calculation of fp16 and fp32 is supported, eg fp32 parameters and fp16 gradients.
 
     :class:`colossalai.nn.optimizer.HybridAdam` may be used as a drop-in replacement for ``torch.optim.AdamW``,
     or ``torch.optim.Adam`` with ``adamw_mode=False``
 
     Adam was been proposed in `Adam: A Method for Stochastic Optimization`_.
 
     Arguments:
@@ -127,15 +127,15 @@
                                           state['exp_avg_sq'], div_scale)
                     self._post_update(p, 'exp_avg', 'exp_avg_sq')
 
                 elif target_device.type == 'cuda':
                     assert state['exp_avg'].device.type == 'cuda', "exp_avg should stay on cuda"
                     assert state['exp_avg_sq'].device.type == 'cuda', "exp_avg should stay on cuda"
 
-                    # record the state by gruop and update at once
+                    # record the state by group and update at once
                     g_l.append(p.grad.data)
                     p_l.append(p.data)
                     m_l.append(state['exp_avg'])
                     v_l.append(state['exp_avg_sq'])
 
                 else:
                     raise RuntimeError
```

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/lamb.py` & `colossalai-0.3.0/colossalai/nn/optimizer/lamb.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/lars.py` & `colossalai-0.3.0/colossalai/nn/optimizer/lars.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/nvme_optimizer.py` & `colossalai-0.3.0/colossalai/nn/optimizer/nvme_optimizer.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,14 @@
-import torch
+import math
 import os
 import tempfile
-import math
+from typing import Callable, Dict, List, Optional
+
+import torch
 from torch.nn.parameter import Parameter
-from typing import Optional, List, Dict, Callable
 
 
 class NVMeOptimizer(torch.optim.Optimizer):
     """A base class for offloading optimizer states.
 
     Args:
         params: parameters
@@ -38,16 +39,17 @@
             backend = 'uring' if 'uring' in get_backends() else 'aio'
             self.offloader = DiskOffloader(self.offload_dir, 8, backend=backend)
         else:
             self.offload_dir = None
             self.offloader = None
         self.is_on_nvme: Dict[Parameter, bool] = {}
         self.offloaded_numel: int = 0
-        self.total_numel: int = self._get_numel()
-        self.can_offload_numel = math.floor(self.total_numel * self.nvme_offload_fraction)
+        # As param may be not materialized here, these attributes are initalized when the first step
+        self.total_numel: Optional[int] = None
+        self.can_offload_numel: Optional[int] = None
 
         self.prefetch_params: List[Parameter] = []
         self.param_to_prefetch_idx: Dict[Parameter, int] = {}
 
     def _get_numel(self) -> int:
         numel = 0
         for group in self.param_groups:
@@ -73,14 +75,17 @@
                     continue
                 if len(self.state[p]) > 0 and self.is_on_nvme[p]:
                     assert p.device.type == 'cpu'
                     self.param_to_prefetch_idx[p] = len(self.prefetch_params)
                     self.prefetch_params.append(p)
 
     def _pre_step(self, *state_keys: str) -> None:
+        if self.total_numel is None:
+            self.total_numel = self._get_numel()
+            self.can_offload_numel = math.floor(self.total_numel * self.nvme_offload_fraction)
         self._setup_prefetch_params()
         if self.offloader is None or len(self.prefetch_params) == 0:
             return
         state = self.state[self.prefetch_params[0]]
         for key in state_keys:
             self.offloader.async_read(state[key])
```

### Comparing `colossalai-0.2.8/colossalai/nn/optimizer/zero_optimizer.py` & `colossalai-0.3.0/colossalai/zero/gemini/gemini_optimizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,20 +6,23 @@
 
 import torch
 import torch.distributed as dist
 from torch.nn import Parameter
 from torch.optim import Optimizer
 
 from colossalai.amp.naive_amp.grad_scaler import DynamicGradScaler
-from colossalai.gemini.chunk import Chunk, ChunkManager
 from colossalai.logging import get_dist_logger
 from colossalai.nn.optimizer import ColossalaiOptimizer, CPUAdam, FusedAdam, HybridAdam
-from colossalai.nn.parallel.data_parallel import ZeroDDP
 from colossalai.utils import disposable, get_current_device, is_ddp_ignored
 
+from .chunk import Chunk, ChunkManager
+from .gemini_ddp import ZeroDDP
+
+__all__ = ['ZeroOptimizer', 'GeminiAdamOptimizer']
+
 _AVAIL_OPTIM_LIST = {FusedAdam, CPUAdam, HybridAdam}
 
 
 class OptimState(Enum):
     SCALED = 0
     UNSCALED = 1
 
@@ -39,48 +42,54 @@
         module (ZeroDDP): A ``ZeroDDP`` instance.
         gpu_margin_mem_ratio (float, optional): The ratio of GPU remaining memory (after the first forward-backward)
             which will be used when using hybrid CPU optimizer.
             This argument is meaningless when `placement_policy` of `GeminiManager` is not "auto".
             Defaults to 0.0.
         initial_scale (float, optional): Initial scale used by DynamicGradScaler. Defaults to 2**32.
         min_scale (float, optional): Min scale used by DynamicGradScaler. Defaults to 1.
-        growth_factor (float, optional): growth_factor used by DynamicGradScaler. Defaults to 2.
-        backoff_factor (float, optional): backoff_factor used by DynamicGradScaler. Defaults to 0.5.
-        growth_interval (float, optional): growth_interval used by DynamicGradScaler. Defaults to 1000.
-        hysteresis (float, optional): hysteresis used by DynamicGradScaler. Defaults to 2.
-        max_scale (int, optional): max_scale used by DynamicGradScaler. Defaults to 2**32.
-        """
+        growth_factor (float, optional): Growth_factor used by DynamicGradScaler. Defaults to 2.
+        backoff_factor (float, optional): Backoff_factor used by DynamicGradScaler. Defaults to 0.5.
+        growth_interval (float, optional): Growth_interval used by DynamicGradScaler. Defaults to 1000.
+        hysteresis (float, optional): Hysteresis used by DynamicGradScaler. Defaults to 2.
+        max_scale (int, optional): Max_scale used by DynamicGradScaler. Defaults to 2**32.
+        clipping_norm (float, optional): The norm value used to clip gradient. Defaults to 0.0.
+        norm_type (float, optional): The type of norm used for gradient clipping. Currently, only L2-norm (norm_type=2.0)
+            is supported in ZeroOptimizer. Defaults to 2.0.
+        verbose (bool, optional): Whether to print verbose information, including grad overflow info. Defaults to False.
+    """
 
     def __init__(self,
                  optim: Optimizer,
                  module: ZeroDDP,
                  gpu_margin_mem_ratio: float = 0.0,
                  initial_scale: float = 2**32,
                  min_scale: float = 1,
                  growth_factor: float = 2,
                  backoff_factor: float = 0.5,
                  growth_interval: int = 1000,
                  hysteresis: int = 2,
                  max_scale: float = 2**32,
                  clipping_norm: float = 0.0,
                  norm_type: float = 2.0,
+                 verbose: bool = False,
                  **defaults: Any):
         super().__init__(optim)
         assert isinstance(module, ZeroDDP)
         assert type(optim) in _AVAIL_OPTIM_LIST, "You should use an optimizer in the available list:\n" \
             f"{_AVAIL_OPTIM_LIST}"
         self.module = module
         self.gemini_manager = module.gemini_manager
         self.chunk_manager: ChunkManager = self.gemini_manager.chunk_manager
         self.optim_state = OptimState.UNSCALED
         self.param_to_range: Dict[Parameter, Tuple[int, int]] = dict()
         self.param_to_chunk32: Dict[Parameter, Chunk] = dict()
         self.chunk16_set: Set[Chunk] = set()
         self.clipping_flag = clipping_norm > 0.0
         self.max_norm = clipping_norm
+        self.verbose = verbose
 
         if self.clipping_flag:
             assert norm_type == 2.0, "ZeroOptimizer only supports L2 norm now"
 
         ddp_param_list = []
         for name, param in module.named_parameters():
             if is_ddp_ignored(param):
@@ -211,15 +220,16 @@
         self._maybe_move_fp32_params()
         self._set_grad_ptr()
 
         found_inf = self._check_overflow()
         if found_inf:
             self.optim_state = OptimState.UNSCALED    # no need to unscale grad
             self.grad_scaler.update(found_inf)    # update gradient scaler
-            self._logger.info(f'Found overflow. Skip step')
+            if self.verbose:
+                self._logger.info(f'Found overflow. Skip step')
             self._clear_global_norm()    # clear recorded norm
             self.zero_grad()    # reset all gradients
             self._update_fp16_params()
             return
 
         # get combined scale. combined scale = loss scale * clipping norm
         # so that gradient = gradient / combined scale
@@ -312,7 +322,14 @@
                 fake_param = torch.nn.Parameter(torch.empty([0], device=grad_device))
                 self.param_to_chunk32[fake_param] = chunk16.paired_chunk
                 self.param_to_range[fake_param] = range_pair
 
                 fake_params_list.append(fake_param)
 
             group['params'] = fake_params_list
+
+
+class GeminiAdamOptimizer(ZeroOptimizer):
+
+    def __init__(self, model: torch.nn.Module, **defaults: Any) -> None:
+        optimizer = HybridAdam(model.parameters(), **defaults)
+        super().__init__(optimizer, model, **defaults)
```

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/data_parallel.py` & `colossalai-0.3.0/colossalai/zero/gemini/gemini_ddp.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,236 +1,81 @@
 import itertools
 from collections import OrderedDict
+from contextlib import nullcontext
 from functools import partial
-from typing import Dict, Iterable, List, Optional, Set
+from typing import Dict, Iterator, List, Optional, Union, Tuple, Set
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 
-from colossalai.gemini.chunk import Chunk, ChunkManager, TensorState
-from colossalai.gemini.gemini_mgr import GeminiManager
-from colossalai.gemini.memory_tracer import OrderedParamGenerator
+from colossalai.checkpoint_io.utils import calculate_tensor_size
 from colossalai.logging import get_dist_logger
-from colossalai.nn.parallel.utils import get_temp_total_chunk_on_cuda
+from colossalai.nn.parallel.data_parallel import ColoDDP, _cast_float, free_storage
 from colossalai.tensor import ProcessGroup as ColoProcessGroup
 from colossalai.tensor import ReplicaSpec
 from colossalai.tensor.colo_parameter import ColoParameter, ColoTensor, ColoTensorSpec
 from colossalai.tensor.param_op_hook import ColoParamOpHookManager
 from colossalai.utils import get_current_device, is_ddp_ignored
-from colossalai.zero.utils.gemini_hook import GeminiZeROHook
+from colossalai.utils.model.experimental import LazyTensor
 
-from .reducer import Reducer
-from .utils import get_static_torch_model
+from .chunk import Chunk, ChunkManager, TensorState, init_chunk_manager
+from .gemini_hook import GeminiZeROHook
+from .gemini_mgr import GeminiManager
+from .memory_tracer import MemStats, OrderedParamGenerator
+from .utils import get_temp_total_chunk_on_cuda
 
 try:
     from torch.nn.modules.module import _EXTRA_STATE_KEY_SUFFIX, _IncompatibleKeys
 except ImportError:
     _EXTRA_STATE_KEY_SUFFIX = '_extra_state'
 
-
-def free_storage(data: torch.Tensor) -> None:
-    """Free underlying storage of a Tensor."""
-    if data.storage().size() > 0:
-        # Since we're modifying the Tensor's Storage directly, make sure the Tensor
-        # is the sole occupant of the Storage.
-        assert data.storage_offset() == 0
-        data.storage().resize_(0)
-
-
-def _cast_float(args, dtype: torch.dtype):
-    if isinstance(args, torch.Tensor) and torch.is_floating_point(args):
-        args = args.to(dtype)
-    elif isinstance(args, (list, tuple)):
-        args = type(args)(_cast_float(t, dtype) for t in args)
-    elif isinstance(args, dict):
-        args = {k: _cast_float(v, dtype) for k, v in args.items()}
-    return args
-
-
-class ColoDDP(torch.nn.Module):
-    """Distributed data parallel for ColoTensor. Nested ColoDDP is not supported now.
-
-    Example:
-        >>> from colossalai.core import global_context as gpc
-        >>> from colossalai.context import ParallelMode
-        >>> model = torch.nn.Linear(20, 1)
-        >>> pg = ProcessGroup(tp_degree = world_size//2)
-        >>> model = ColoDDP(model, pg)
-        >>> logits = model(x)
-        >>> loss = criterion(logits, labels)
-        >>> model.backward(loss)
-
-    Args:
-        module (torch.nn.Module): Module to apply DDP.
-        process_group (Optional[dist.ProcessGroup], optional): The process group which DDP uses.
-            If it's None, the default data parallel group will be used. Defaults to None.
-    """
-
-    def __init__(self,
-                 module: torch.nn.Module,
-                 process_group: ColoProcessGroup,
-                 bucket_cap_mb: int = 25,
-                 rebuild_bucket: bool = True) -> None:
-        assert not isinstance(module, ColoDDP)
-        super().__init__()
-        self.module = module
-        self.comm_stream: torch.cuda.Stream = torch.cuda.Stream()
-        assert process_group
-
-        self.process_group = process_group
-        self.dp_world_size = self.process_group.dp_world_size()
-
-        self.reducer = Reducer(bucket_cap_mb)
-        self.rebuild_bucket = rebuild_bucket
-        for p in module.parameters():
-            if is_ddp_ignored(p):
-                continue
-            if p.requires_grad:
-                p.register_hook(partial(self.grad_handle, p))
-
-    def parameters(self, recurse: bool = True):
-        return self.module.parameters(recurse)
-
-    def named_parameters(self, prefix: str = '', recurse: bool = True):
-        return self.module.named_parameters(prefix, recurse)
-
-    def named_buffers(self, prefix: str = '', recurse: bool = True):
-        return self.module.named_buffers(prefix, recurse)
-
-    def named_children(self):
-        return self.module.named_children()
-
-    def named_modules(self,
-                      memo: Optional[Set[torch.nn.Module]] = None,
-                      prefix: str = '',
-                      remove_duplicate: bool = True):
-        return self.module.named_modules(memo, prefix, remove_duplicate)
-
-    def forward(self, *args, **kwargs):
-        self.module.zero_grad(set_to_none=True)
-        return self.module(*args, **kwargs)
-
-    def backward(self, loss: torch.Tensor):
-        loss.backward()
-        with torch.cuda.stream(self.comm_stream):
-            self.reducer.flush()
-        torch.cuda.current_stream().wait_stream(self.comm_stream)
-        if self.rebuild_bucket:
-            self.reducer.free()
-        for p in self.module.parameters():
-            if is_ddp_ignored(p):
-                continue
-            if p.grad.device.type != "cpu":
-                p.grad = p._saved_grad
-
-    def grad_handle(self, p, grad):
-        if grad.device.type != "cpu":
-            empty_grad = torch.empty_like(grad)
-            free_storage(empty_grad)
-            if self.dp_world_size > 1:
-                grad = grad / self.dp_world_size
-                self.comm_stream.wait_stream(torch.cuda.current_stream())
-                with torch.cuda.stream(self.comm_stream):
-                    self.reducer.all_reduce_async(grad,
-                                                  group=self.process_group.dp_process_group(),
-                                                  callback_fn=partial(self._save_grad, p))
-                grad.record_stream(self.comm_stream)
-            else:
-                ColoDDP._save_grad(p, grad)
-            return empty_grad
-
-        else:
-            # TODO(jiaruifang) fixme
-            self.process_group.set_cpu_groups()
-            dist.all_reduce(grad, group=self.process_group.cpu_dp_process_group())
-            return grad
-
-    @staticmethod
-    def _save_grad(p, grad):
-        if hasattr(p, '_saved_grad'):
-            p._saved_grad.add_(grad)
-        else:
-            p._saved_grad = grad
-
-    def zero_grad(self, set_to_none: bool = False) -> None:
-        self.module.zero_grad(set_to_none=True)
-        for p in self.module.parameters():
-            if getattr(p, '_saved_grad', None) is not None:
-                if set_to_none:
-                    p._saved_grad = None
-                else:
-                    if p._saved_grad.grad_fn is not None:
-                        p._saved_grad.detach_()
-                    else:
-                        p._saved_grad.requires_grad_(False)
-                    p._saved_grad.zero_()
-
-    @staticmethod
-    def set_params_to_ignore(params_to_ignore: Iterable[torch.Tensor]) -> None:
-        """Sets parameters to be ignored by DDP.
-        This method must be called before initializing ColoDDP.
-
-        Example:
-            >>> params_to_ignore = []
-            >>> for p in module.parameters():
-            >>>     if should_ignore(p):
-            >>>         params_to_ignore.append(p)
-            >>> ColoDDP.set_params_to_ignore(params_to_ignore)
-            >>> module = ColoDDP(module)
-
-        Args:
-            params_to_ignore (Iterable[torch.Tensor]): A list of parameters to be ignored.
-        """
-        for p in params_to_ignore:
-            p._ddp_to_ignore = True
-
-    def state_dict(self, destination=None, prefix='', keep_vars=False):
-        return self.module.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
-
-    def load_state_dict(self, state_dict: 'OrderedDict[str, torch.Tensor]', strict: bool = True):
-        return self.module.load_state_dict(state_dict, strict)
+__all__ = [
+    'ZeroDDP',
+    'GeminiDDP',
+]
 
 
 class ZeroDDP(ColoDDP):
     """ZeRO DDP for ColoTensor.
     Warning: Nested ZeroDDP is not supported now.
     It is designed to be used with ChunkManager and GeminiManager.
     For more details, see the API reference of ``ChunkManager`` and ``GeminiManager``.
 
     Args:
         module (torch.nn.Module): Module to apply ZeRO-DP.
-        gemini_manager (GeminiManager): Manages the chunk manager and heterogeneous momery space.
+        gemini_manager (GeminiManager): Manages the chunk manager and heterogeneous memory space.
             For more details, see the API reference of ``GeminiManager``.
         pin_memory (bool): Chunks on CPU Memory use pin-memory.
         force_outputs_fp32 (bool): If set to True, outputs will be fp32. Otherwise, outputs will be fp16.
             Defaults to False.
         strict_ddp_mode (bool): If set to True, there is no tensor sharding, each tensor is replicated.
             Defaults to False. Users can set it to True, when they clearly know that they only need DDP.
+        scatter_after_inference (bool): If set to True, the model will be scattered after inference. This will save memory but slow down the consecutive inference.
     """
 
     def __init__(self,
                  module: torch.nn.Module,
                  gemini_manager: GeminiManager,
                  pin_memory: bool = False,
                  force_outputs_fp32: bool = False,
-                 strict_ddp_mode: bool = False) -> None:
-        super().__init__(module, process_group=ColoProcessGroup())
+                 strict_ddp_mode: bool = False,
+                 scatter_after_inference: bool = True) -> None:
         self.gemini_manager = gemini_manager
         self.chunk_manager: ChunkManager = gemini_manager.chunk_manager
         self.force_outputs_fp32 = force_outputs_fp32
         self.param_op_hook = GeminiZeROHook(gemini_manager)
         self.fp32_params: List[ColoTensor] = list()
         self.fp16_params: List[ColoParameter] = list()
         self.overflow_counter = 0
         self.grads_device: Dict[torch.Tensor, torch.device] = dict()
         self.param2name: Dict[nn.Parameter, str] = dict()
         self.name2param: Dict[str, nn.Parameter] = dict()
+        self.scatter_after_inference = scatter_after_inference
 
-        self._cast_buffers()
         self._logger = get_dist_logger()
 
         if self.gemini_manager._premade_memstats_:
             # build chunk in param runtime visited order.
             param_order = self.gemini_manager.memstats()._param_runtime_order
         else:
             # build chunk in param initialized order.
@@ -246,14 +91,43 @@
 
         for name, param in module.named_parameters():
             self.param2name[param] = name
         for m_name, m_var in module.named_modules():
             for p_name, p_var in m_var.named_parameters(recurse=False):
                 param_name = m_name + '.' + p_name if m_name else p_name
                 self.name2param[param_name] = p_var
+        super().__init__(module, process_group=ColoProcessGroup())
+        self._non_persistent_buffers_set=self._get_non_persistent_buffers_set(module)
+        self._cast_buffers()
+
+    def _get_non_persistent_buffers_set(self, module, memo: Optional[Set[nn.Module]] = None, prefix: str = '', remove_duplicate: bool = True):
+
+            r"""
+            Args:
+                memo: a memo to store the set of modules already added to the result
+                prefix: a prefix that will be added to the name of the module
+                remove_duplicate: whether to remove the duplicated module instances in the result
+                    or not
+            """
+
+            if memo is None:
+                memo = set()
+            self_non_persistent_set = set()
+            if module not in memo:
+                if remove_duplicate:
+                    memo.add(module)
+                self_non_persistent_set = set(map(lambda key: prefix + ('.' if prefix else '') + key, module._non_persistent_buffers_set))
+                for name, sub_module in module._modules.items():
+                    if sub_module is None:
+                        continue
+                    submodule_prefix = prefix + ('.' if prefix else '') + name
+                    child_non_persistent_set = self._get_non_persistent_buffers_set(sub_module, memo, submodule_prefix, remove_duplicate)
+                    self_non_persistent_set = set.union(self_non_persistent_set, child_non_persistent_set)
+            return self_non_persistent_set
+    
 
     def _post_forward(self):
         """This function is only triggered for inference.
         """
         access_list = list(self.chunk_manager.accessed_chunks)
         # we need to scatter all accessed chunks and move them to their original places
         for chunk in access_list:
@@ -261,37 +135,53 @@
                 self.chunk_manager.fake_release_chunk(chunk)
             else:
                 assert chunk.can_release
                 self.chunk_manager.release_chunk(chunk)
             first_param = next(iter(chunk.tensors_info))
             self.chunk_manager.move_chunk(chunk, self.grads_device[first_param])
         assert self.chunk_manager.accessed_mem == 0
-        # reset all recorded attributes
-        self.gemini_manager.reset_attributes()
 
     def forward(self, *args, **kwargs):
         # check whether we are in a inference mode
         grad_flag = torch.is_grad_enabled()
         if not grad_flag:
             assert not self.gemini_manager.need_warmup or not self.gemini_manager.is_warmup(
             ), "You should run a completed iteration as your warmup iter"
 
         args, kwargs = _cast_float(args, torch.half), _cast_float(kwargs, torch.half)
         self.module.zero_grad(set_to_none=True)
-        self.gemini_manager.pre_iter(*args)
-        with ColoParamOpHookManager.use_hooks(self.param_op_hook):
-            outputs = self.module(*args, **kwargs)
-        # scatter chunks in the inference mode
         if not grad_flag:
-            self._post_forward()
+            outputs = self._inference_forward(*args, **kwargs)
+        else:
+            self.gemini_manager.pre_iter(*args)
+            with ColoParamOpHookManager.use_hooks(self.param_op_hook):
+                outputs = self.module(*args, **kwargs)
 
         if self.force_outputs_fp32:
             return _cast_float(outputs, torch.float)
         return outputs
 
+    def _inference_forward(self, *args, **kwargs):
+        """This function is only triggered for inference.
+        """
+        fwd_ctx = ColoParamOpHookManager.use_hooks(self.param_op_hook)
+        if not self.scatter_after_inference:
+            # gather all chunks
+            for chunk in self.chunk_manager.get_chunks(self.fp16_params):
+                self.chunk_manager.access_chunk(chunk)
+            fwd_ctx = nullcontext()
+        with fwd_ctx:
+            outputs = self.module(*args, **kwargs)
+        if self.scatter_after_inference:
+            # scatter chunks
+            self._post_forward()
+        # reset all recorded attributes
+        self.gemini_manager.reset_attributes()
+        return outputs
+
     def _setup_grads_ptr(self):
         for p in self.module.parameters():
             if is_ddp_ignored(p):
                 continue
             p.grad = None
 
     def _pre_backward(self):
@@ -355,15 +245,20 @@
     def zero_grad(self, set_to_none: bool = False) -> None:
         self.module.zero_grad(set_to_none=True)
 
     def set_chunk_grad_device(self, chunk: Chunk, device: torch.device) -> None:
         for tensor in chunk.get_tensors():
             self.grads_device[tensor] = device
 
-    def state_dict(self, destination=None, prefix='', keep_vars=False, only_rank_0: bool = True):
+    def state_dict(self,
+                   destination=None,
+                   prefix='',
+                   keep_vars=False,
+                   only_rank_0: bool = True,
+                   dtype: torch.dtype = torch.float16):
         """Returns a dictionary containing a whole state of the module.
 
         Both parameters and persistent buffers (e.g. running averages) are included.
         Keys are corresponding parameter and buffer names.
         Parameters and buffers set to ``None`` are not included.
 
         Warning: The non strict state dict would ignore the parameters if the tensors of the parameters
@@ -374,52 +269,70 @@
             dict:
                 a dictionary containing a whole state of the module
         """
         if destination is None:
             destination = OrderedDict()
             destination._metadata = OrderedDict()
         destination._metadata[prefix[:-1]] = local_metadata = dict(version=self._version)
-        self._save_to_state_dict(destination, prefix, keep_vars, only_rank_0)
+        self._save_to_state_dict(destination, prefix, keep_vars, only_rank_0, dtype)
 
         for hook in self._state_dict_hooks.values():
             hook_result = hook(self, destination, prefix, local_metadata)
             if hook_result is not None:
                 destination = hook_result
         return destination
 
-    def _get_param_to_save_data(self, param_list: List[torch.nn.Parameter], only_rank_0: bool) -> Dict:
+    def _get_chunk_to_save_data(self, chunk: Chunk, only_rank_0: bool, dtype: torch.dtype = torch.float16) -> Dict:
+        """
+        get gathered chunk content.
+
+        Args:
+            chunk (Chunk): a chunk
+            only_rank_0 (bool): whether to only save data on rank 0
+
+        Returns:
+            Dict: a dict whose key is param name and value is param with correct payload
+        """
+        # save parameters
+        chunk_to_save_data = dict()
+        temp_chunk = get_temp_total_chunk_on_cuda(chunk)
+        if torch.is_floating_point(temp_chunk):
+            temp_chunk = temp_chunk.to(dtype)
+        for tensor, tensor_info in chunk.tensors_info.items():
+            record_tensor = torch.empty([0])
+            record_flag = (not only_rank_0) | (dist.get_rank(chunk.torch_pg) == 0)
+            if record_flag:
+                record_tensor = temp_chunk[tensor_info.offset:tensor_info.end].view(tensor.shape).cpu()
+
+            assert tensor not in chunk_to_save_data
+            chunk_to_save_data[tensor] = record_tensor
+
+        del temp_chunk
+        return chunk_to_save_data
+
+    def _get_param_to_save_data(self, param_list: List[torch.nn.Parameter], only_rank_0: bool,
+                                dtype: torch.dtype) -> Dict:
         """
         get param content from chunks.
 
         Args:
             param_list (_type_): a list of torch.nn.Parameters
             only_rank_0 (_type_): _description_
 
         Returns:
             Dict: a dict whose key is param name and value is param with correct payload
         """
         # save parameters
         param_to_save_data = dict()
         chunk_list = self.chunk_manager.get_chunks(param_list)
         for chunk in chunk_list:
-            temp_chunk = get_temp_total_chunk_on_cuda(chunk)
-
-            for tensor, tensor_info in chunk.tensors_info.items():
-                record_tensor = torch.empty([0])
-                record_flag = (not only_rank_0) | (dist.get_rank(chunk.torch_pg) == 0)
-                if record_flag:
-                    record_tensor = temp_chunk[tensor_info.offset:tensor_info.end].view(tensor.shape).cpu()
-
-                assert tensor not in param_to_save_data
-                param_to_save_data[tensor] = record_tensor
-
-            del temp_chunk
+            param_to_save_data.update(self._get_chunk_to_save_data(chunk, only_rank_0, dtype))
         return param_to_save_data
 
-    def _save_to_state_dict(self, destination, prefix, keep_vars, only_rank_0=True):
+    def _save_to_state_dict(self, destination, prefix, keep_vars, only_rank_0=True, dtype=torch.float16):
         r"""Saves module state to `destination` dictionary, containing a state
         of the module, but not its descendants. This is called on every
         submodule in :meth:`~torch.nn.Module.state_dict`.
 
         In rare cases, subclasses can achieve class-specific behavior by
         overriding this method with custom logic.
 
@@ -427,15 +340,16 @@
             destination (dict): a dict where state will be stored
             prefix (str): the prefix for parameters and buffers used in this
                 module
         """
         assert keep_vars is False, "`state_dict` with parameter, `keep_vars=True`, is not supported now."
 
         # get copies of fp32 parameters in CPU
-        param_to_save_data = self._get_param_to_save_data(self.fp32_params, only_rank_0)
+        # as memory of fp16_params may be reused by grad, it's not reliable, we should use fp32_params and convert to fp16
+        param_to_save_data = self._get_param_to_save_data(self.fp32_params, only_rank_0, dtype)
         # get the mapping between copies and fp16 parameters
         p_mapping = dict()
         for p, fp32_p in zip(self.fp16_params, self.fp32_params):
             name = self.param2name[p]
             assert fp32_p in param_to_save_data, "Parameter '{}' is neglected in the chunk list".format(name)
             record_parameter = param_to_save_data[fp32_p]
             p_mapping[p] = record_parameter
@@ -633,15 +547,16 @@
                     input_name = key[len(prefix):]
                     if input_name not in local_state:
                         unexpected_keys.append(key)
 
     def _init_chunks(self, param_order, strict_ddp_mode: bool, cpu_offload: bool, pin_memory: bool):
         ddp_pg = ColoProcessGroup()
         for p in param_order.generate():
-            assert isinstance(p, ColoParameter)
+            self._preprocess_param(p)
+            assert type(p) is ColoParameter
 
             # gather sharded parameters in the strict ddp mode
             if strict_ddp_mode:
                 if not p.is_replicate():
                     p.set_dist_spec(ReplicaSpec())
                 p.set_process_group(pg=ddp_pg)
 
@@ -686,10 +601,176 @@
 
             # keep gathered chunks are in CUDA
             if chunk_16.keep_gathered:
                 self.grads_device[p] = get_current_device()
 
     def _cast_buffers(self):
         for buffer in self.module.buffers():
+            if isinstance(buffer, LazyTensor):
+                buffer.materialize()
             buffer.data = buffer.cuda()
             if torch.is_floating_point(buffer):
                 buffer.data = buffer.half()
+
+    def _preprocess_param(self, p: Union[nn.Parameter, ColoParameter, 'LazyTensor']) -> None:
+        """Convert parameter to ColoParameter in-place.
+        Args:
+            p (Union[nn.Parameter, ColoParameter, LazyTensor]): parameter to be converted
+        """
+        if type(p) is ColoParameter:
+            # model is initialized with ColoInitContext
+            return
+        requires_grad = p.requires_grad
+        if isinstance(p, LazyTensor):
+            # model is initialized with LazyInitContext
+            p.materialize()
+        p.__class__ = ColoParameter
+        p.__init__(p, requires_grad=requires_grad)
+
+    def state_dict_shard(self,
+                         prefix: str = '',
+                         keep_vars: bool = False,
+                         max_shard_size: int = 1024,
+                         only_rank_0: bool = True,
+                         dtype: torch.dtype = torch.float16) -> Iterator[Tuple[OrderedDict, int]]:
+        """Returns dictionaries containing a whole state of the module one by one. The max size of dictionary shard is specified by ``max_shard_size``.
+
+        Both parameters and persistent buffers (e.g. running averages) are included.
+        Keys are corresponding parameter and buffer names.
+        Parameters and buffers set to ``None`` are not included.
+
+        Args:
+            prefix (str, optional): the prefix for parameters and buffers used in this
+                module. Defaults to ''.
+            keep_vars (bool, optional): whether to keep variables. Defaults to False.
+            max_shard_size (int, optional): max size of state dict shard (in MB). Defaults to 1024.
+            only_rank_0 (bool, optional): only get data on rank0. Defaults to True.
+
+
+        Yields:
+            Iterator[OrderedDict]: A generator of state dict shard
+        """
+        sharder = _StateDictSharder(max_shard_size)
+
+        # get the mapping between copies and fp16 parameters
+        fp16_to_fp32 = dict()
+        for p, fp32_p in zip(self.fp16_params, self.fp32_params):
+            fp16_to_fp32[p] = fp32_p
+
+        # key is fp32 param, and value is gathered param on CPU
+        gathered_param_buffer = dict()
+        for name, param in self.name2param.items():
+            if param is not None:
+                if is_ddp_ignored(param):
+                    # deal with ddp ignored parameters
+                    gathered_param = param if keep_vars else param.detach()
+                else:
+                    # as memory of fp16 param may be reused, we should use fp32 param and then convert to fp16
+                    fp32_param = fp16_to_fp32[param]
+                    if fp32_param not in gathered_param_buffer:
+                        chunk = self.chunk_manager.get_chunk(fp32_param)
+                        gathered_param_buffer.update(self._get_chunk_to_save_data(chunk, only_rank_0, dtype))
+                    gathered_param = gathered_param_buffer.pop(fp32_param)
+
+                block, block_size = sharder.append(prefix + name, gathered_param)
+                if block is not None:
+                    yield block, block_size
+
+        del fp16_to_fp32
+        del gathered_param_buffer
+
+        # save all buffers
+        for name, buf in self.named_buffers():
+            if buf is not None and name not in self._non_persistent_buffers_set:
+                buffer = buf if keep_vars else buf.detach()
+                block, block_size = sharder.append(prefix + name, buffer)
+                if block is not None:
+                    yield block, block_size
+        # save extra states
+        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX
+        if getattr(self.__class__, "get_extra_state",
+                   torch.nn.Module.get_extra_state) is not torch.nn.Module.get_extra_state:
+            extra_state = self.get_extra_state()
+            block, block_size = sharder.append(extra_state_key, extra_state)
+            if block is not None:
+                yield block, block_size
+
+        yield sharder.current_block, sharder.current_block_size
+
+
+class _StateDictSharder:
+
+    def __init__(self, max_shard_size: int) -> None:
+        self.max_shard_size = max_shard_size
+        self.current_block = OrderedDict()
+        self.current_block_size = 0
+
+    def append(self, name: str, tensor: torch.Tensor) -> Tuple[Optional[OrderedDict], int]:
+        tensor_size = calculate_tensor_size(tensor)
+        ret_block = None
+        ret_block_size = 0
+        if self.current_block_size + tensor_size > self.max_shard_size:
+            ret_block = self.current_block
+            ret_block_size = self.current_block_size
+            self.current_block = OrderedDict()
+            self.current_block_size = 0
+        self.current_block[name] = tensor
+        self.current_block_size += tensor_size
+        return ret_block, ret_block_size
+
+
+class GeminiDDP(ZeroDDP):
+
+    def __init__(self,
+                 module: torch.nn.Module,
+                 device: torch.device,
+                 placement_policy: str = "cpu",
+                 pin_memory: bool = False,
+                 force_outputs_fp32: bool = False,
+                 strict_ddp_mode: bool = False,
+                 scatter_after_inference: bool = True,
+                 search_range_mb: int = 32,
+                 hidden_dim: Optional[int] = None,
+                 min_chunk_size_mb: float = 32,
+                 memstats: Optional[MemStats] = None,
+                 verbose: bool = False) -> None:
+        """
+        A torch.Module wrapper using ZeRO-DP and Gemini.
+        ZeRO is for parallel. Gemini is for memory management.
+        WARNING: The class will modify the module inline!
+
+        Example:
+            model is initialized under the context of ColoInitContext
+            >>> model = GeminiDDP(model, torch.cuda.current_device(), "cuda")
+            >>> logits = model(x)
+            >>> loss = criterion(logits, labels)
+            >>> model.backward(loss)
+
+        Args:
+            module (torch.nn.Module): the model to be wrapped.
+            device (torch.device): device to place the model.
+            placement_policy (str, optional): "cpu", "cuda", "auto". Defaults to "cpu".
+            pin_memory (bool, optional): use pin memory on CPU. Defaults to False.
+            force_outputs_fp32 (bool, optional): force outputs are fp32. Defaults to False.
+            search_range_mb (int, optional): chunk size searching range in MegaByte. Defaults to 32.
+            hidden_dim (int, optional): the hidden dimension of DNN.
+                Users can provide this argument to speed up searching.
+                If users do not know this argument before training, it is ok. We will use a default value 1024.
+            min_chunk_size_mb (float, optional): the minimum chunk size in MegaByte.
+                If the aggregate size of parameters is still smaller than the minimum chunk size,
+                all parameters will be compacted into one small chunk.
+            memstats (MemStats, optional) the memory statistics collector by a runtime memory tracer.
+        """
+        # some ugly hotfix for the compatibility with Lightning
+        if search_range_mb is None:
+            search_range_mb = 32
+
+        chunk_manager = init_chunk_manager(model=module,
+                                           init_device=device,
+                                           hidden_dim=hidden_dim,
+                                           search_range_mb=search_range_mb,
+                                           min_chunk_size_mb=min_chunk_size_mb,
+                                           strict_ddp_flag=strict_ddp_mode,
+                                           verbose=verbose)
+        gemini_manager = GeminiManager(placement_policy, chunk_manager, memstats)
+        super().__init__(module, gemini_manager, pin_memory, force_outputs_fp32, strict_ddp_mode,
+                         scatter_after_inference)
```

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/__init__.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/__init__.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/base_embedding.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/base_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/cache_mgr.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/cache_mgr.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,16 +16,16 @@
 
 
 def _wait_for_data(t, stream: Optional[torch.cuda.streams.Stream]) -> None:
     if stream is None:
         return
     torch.cuda.current_stream().wait_stream(stream)
     # As mentioned in https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html,
-    # PyTorch uses the "caching allocator" for memroy allocation for tensors. When a tensor is
-    # freed, its memory is likely to be reused by newly constructed tenosrs.  By default,
+    # PyTorch uses the "caching allocator" for memory allocation for tensors. When a tensor is
+    # freed, its memory is likely to be reused by newly constructed tensors.  By default,
     # this allocator traces whether a tensor is still in use by only the CUDA stream where it
     # was created.   When a tensor is used by additional CUDA streams, we need to call record_stream
     # to tell the allocator about all these streams.  Otherwise, the allocator might free the
     # underlying memory of the tensor once it is no longer used by the creator stream.  This is
     # a notable programming trick when we write programs using multi CUDA streams.
     cur_stream = torch.cuda.current_stream()
     assert isinstance(t, torch.Tensor)
@@ -290,15 +290,15 @@
             )
             print(f'cuda_to_cpu_elapse {elapsed} sec')
         if self._cpu_to_cuda_numel > 0 and "5_evict_in" in self._elapsed_dict:
             elapsed = self._elapsed_dict["5_evict_in"]
             print(
                 f"CPU->CUDA BWD {self._cpu_to_cuda_numel * self.elem_size_in_byte / 1e6 / elapsed} MB/s {self._cpu_to_cuda_numel / 1e6} M elem"
             )
-            print(f'cpu_to_cuda_elpase {elapsed} sec')
+            print(f'cpu_to_cuda_elapse {elapsed} sec')
 
         for k, v in self._elapsed_dict.items():
             print(f'{k}: {v}')
 
         print(f'cache miss ratio {self._cache_miss / self._total_cache}')
 
     @torch.no_grad()
```

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/cached_embedding.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/copyer.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/copyer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/embedding_config.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/embedding_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/colo_module.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/colo_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/embedding.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/linear.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/layers/module_utils.py` & `colossalai-0.3.0/colossalai/nn/parallel/layers/module_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/reducer.py` & `colossalai-0.3.0/colossalai/nn/parallel/reducer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/utils.py` & `colossalai-0.3.0/colossalai/zero/gemini/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,17 +2,18 @@
 from copy import copy
 from typing import Optional, Set
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 
-from colossalai.gemini.chunk import Chunk
 from colossalai.utils import get_current_device
 
+from .chunk import Chunk
+
 
 def get_temp_total_chunk_on_cuda(chunk: Chunk):
     if chunk.is_gathered:
         return chunk.cuda_global_chunk
 
     if chunk.cuda_shard is not None:
         shard_temp = chunk.cuda_shard
@@ -73,15 +74,15 @@
         device (torch.device): the device of the final torch model
         dtype (torch.dtype): the dtype of the final torch model
         only_rank_0 (bool): if True, only rank0 has the coverted torch model
 
     Returns:
         torch.nn.Module: a static torch model used for saving checkpoints or numeric checks
     """
-    from colossalai.nn.parallel import ZeroDDP
+    from colossalai.zero.gemini.gemini_ddp import ZeroDDP
     assert isinstance(zero_ddp_model, ZeroDDP)
 
     state_dict = zero_ddp_model.state_dict(only_rank_0=only_rank_0)
     colo_model = zero_ddp_model.module
     torch_model = _get_shallow_copy_model(colo_model)
 
     if not only_rank_0 or dist.get_rank() == 0:
```

### Comparing `colossalai-0.2.8/colossalai/nn/parallel/zero_wrapper.py` & `colossalai-0.3.0/colossalai/zero/wrapper.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 from copy import copy
 from typing import Dict, Optional
 
 import torch
 import torch.nn as nn
 
-from .gemini_parallel import GeminiDDP
+from .gemini import GeminiDDP
 
 
-def zero_model_wrapper(model: nn.Module, zero_stage: int = 1, gemini_config: Optional[Dict] = None):
+def zero_model_wrapper(model: nn.Module,
+                       zero_stage: int = 1,
+                       gemini_config: Optional[Dict] = None,
+                       verbose: bool = False):
     """This wrapper function is used to wrap your training model for ZeRO DDP.
 
     Example:
 
         >>> with ColoInitContext():
         >>>     my_model = Bert()
         >>> my_optim = SGD(my_model.parameters(), lr = 1e-3)
@@ -19,15 +22,15 @@
         >>> zero_optim = zero_optim_wrapper(zero_model, my_optim)
 
     Args:
         model (nn.Module): The model used in ZeRO DDP.
         zero_stage (int, optional): The stage of ZeRO DDP. You can find more information in ZeRO's paper.
             https://arxiv.org/abs/1910.02054
         gemini_config (dict, optional): The configuration dictionary of `GeminiDDP`. `GeminiDDP` is enabled
-            when the stage is set to 3. You can set the arguemnts of `GeminiDDP` in the gemini_config.
+            when the stage is set to 3. You can set the arguments of `GeminiDDP` in the gemini_config.
             Here is an example where we set the device of the model, the placement policy of Gemini, and the
             size of hidden dimension to help Gemini find out a unified chunk size.
 
             Example:
 
                 >>> config_dict = dict(device=torch.cuda.current_device(), hidden_dim=1024, placement_policy='auto')
                 >>> model = zero_model_wrapper(model, zero_stage=3, gemini_config=config_dict)
@@ -36,15 +39,15 @@
 
     if gemini_config is None:
         gemini_config = dict()
 
     if zero_stage in [1, 2]:
         wrapped_model = model
     else:
-        wrapped_model = GeminiDDP(model, **gemini_config)
+        wrapped_model = GeminiDDP(model, **gemini_config, verbose=verbose)
 
     setattr(wrapped_model, "_colo_zero_stage", zero_stage)
 
     return wrapped_model
 
 
 def zero_optim_wrapper(model: nn.Module,
@@ -54,15 +57,16 @@
                        backoff_factor: float = 0.5,
                        growth_interval: int = 1000,
                        hysteresis: int = 2,
                        min_scale: float = 1,
                        max_scale: float = 2**32,
                        max_norm: float = 0.0,
                        norm_type: float = 2.0,
-                       optim_config: Optional[Dict] = None):
+                       optim_config: Optional[Dict] = None,
+                       verbose: bool = False):
     """This wrapper function is used to wrap your training optimizer for ZeRO DDP.
 
     Args:
         model (nn.Module): Your model wrapped by `zero_model_wrapper`
         optimizer (torch.optim.Optimizer): Your initialized optimizer
         initial_scale (float, optional): initial_scale used by DynamicGradScaler.
         min_scale (float, optional): min_scale used by DynamicGradScaler.
@@ -70,19 +74,20 @@
         backoff_factor (float, optional): backoff_factor used by DynamicGradScaler.
         growth_interval (float, optional): growth_interval used by DynamicGradScaler.
         hysteresis (float, optional): hysteresis used by DynamicGradScaler.
         max_scale (int, optional): max_scale used by DynamicGradScaler.
         max_norm (float, optional): max_norm used for `clip_grad_norm`. You should notice that you shall not do
             clip_grad_norm by yourself when using ZeRO DDP. The ZeRO optimizer will take care of clip_grad_norm.
         norm_type (float, optional): norm_type used for `clip_grad_norm`.
-        optim_config (dict, optinoal): The configuration used for the ZeRO optimizer.
+        optim_config (dict, optional): The configuration used for the ZeRO optimizer.
             Example:
 
                 >>> zero2_config = dict(reduce_bucket_size=12 * 1024 * 1024, overlap_communication=True)
                 >>> optim = zero_optim_wrapper(model, optim, optim_config=zero2_config)
+        verbose (bool, optional): Whether to print the verbose info.
     """
     assert hasattr(model, "_colo_zero_stage"), "You should use `zero_ddp_wrapper` first"
     zero_stage = getattr(model, "_colo_zero_stage")
 
     assert norm_type == 2.0, "Current ZeRO optimizers only support 'norm_type=2'"
 
     if optim_config is None:
@@ -95,15 +100,15 @@
     config_dict['backoff_factor'] = backoff_factor
     config_dict['growth_interval'] = growth_interval
     config_dict['hysteresis'] = hysteresis
     config_dict['min_scale'] = min_scale
     config_dict['max_scale'] = max_scale
 
     if zero_stage in [1, 2]:
-        from colossalai.zero.sharded_optim.low_level_optim import LowLevelZeroOptimizer
+        from colossalai.zero.low_level import LowLevelZeroOptimizer
         config_dict['partition_grad'] = zero_stage == 2
         config_dict['clip_grad_norm'] = max_norm
-        return LowLevelZeroOptimizer(optimizer, **config_dict)
+        return LowLevelZeroOptimizer(optimizer, **config_dict, verbose=verbose)
     else:
-        from colossalai.nn.optimizer.zero_optimizer import ZeroOptimizer
+        from colossalai.zero.gemini.gemini_optimizer import ZeroOptimizer
         config_dict['clipping_norm'] = max_norm
-        return ZeroOptimizer(optimizer, model, **config_dict)
+        return ZeroOptimizer(optimizer, model, **config_dict, verbose=verbose)
```

### Comparing `colossalai-0.2.8/colossalai/pipeline/layer_spec.py` & `colossalai-0.3.0/colossalai/pipeline/layer_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/middleware/adaptor/fx.py` & `colossalai-0.3.0/colossalai/pipeline/middleware/adaptor/fx.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/middleware/topo.py` & `colossalai-0.3.0/colossalai/pipeline/middleware/topo.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/pipelinable.py` & `colossalai-0.3.0/colossalai/pipeline/pipelinable.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/pipeline_process_group.py` & `colossalai-0.3.0/colossalai/pipeline/pipeline_process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/rpc/_pipeline_base.py` & `colossalai-0.3.0/colossalai/pipeline/rpc/_pipeline_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/rpc/_pipeline_schedule.py` & `colossalai-0.3.0/colossalai/pipeline/rpc/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/rpc/utils.py` & `colossalai-0.3.0/colossalai/pipeline/rpc/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/pipeline/utils.py` & `colossalai-0.3.0/colossalai/pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/registry/__init__.py` & `colossalai-0.3.0/colossalai/registry/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/registry/registry.py` & `colossalai-0.3.0/colossalai/registry/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/__init__.py` & `colossalai-0.3.0/colossalai/tensor/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/colo_parameter.py` & `colossalai-0.3.0/colossalai/tensor/colo_parameter.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/colo_tensor.py` & `colossalai-0.3.0/colossalai/tensor/colo_tensor.py`

 * *Files 1% similar despite different names*

```diff
@@ -180,15 +180,15 @@
             func = _COLOSSAL_OPS[func]
 
         if cls.torch_major > 1 or (cls.torch_major == 1 and cls.torch_minor >= 12):
             # in order to trigger pre-op hook in the forward of checkpoint module
             # we have to capture the `backward` function
             # and make sure that it does not in `torch._C.DisableTorchFunction()` context
             if func is torch.Tensor.backward:
-                assert len(args) == 1    # only has 1 paramter
+                assert len(args) == 1    # only has 1 parameter
                 backward_tensor = torch.Tensor(args[0])
                 tensor_kwargs = {k: torch.Tensor(v) if torch.is_tensor(v) else v for k, v in kwargs.items()}
                 return backward_tensor.backward(**tensor_kwargs)
 
         with torch._C.DisableTorchFunction():
             ret = func(*args, **kwargs)
             if func in _get_my_nowrap_functions():
@@ -224,15 +224,15 @@
 
         1. If the pg is None, then redistribute the tensor payload among the TP process group. Keep the
         DP process group not changed.
 
         2. If the pg is not not None and not equal to the current process group.
         First, convert the tensor as replicated among the TP process group.
         Second, reset the process group to the new pg.
-        Third, conver the tensor (new replicated both among the tp process group) to the new dist_spec.
+        Third, convert the tensor (new replicated both among the tp process group) to the new dist_spec.
 
         Args:
             dist_spec (_DistSpec): the new dist spec.
             pg (Optional[ProcessGroup], optional): the new process group . Defaults to None.
 
         Returns:
             ColoTensor: a redistributed colotensor
@@ -293,15 +293,15 @@
     def size_local(self, *args) -> torch.Size:
         with torch._C.DisableTorchFunction():
             return super().size(*args)
 
     def size_global(self, *args) -> torch.Size:
         """size_global
 
-        override the torch buildin size()
+        override the torch building size()
         the shape passed in must be in a replicate placement.
 
         Returns:
             torch.Size: the global tensor shape
         """
         if self.is_replicate():
             return self.size_local(*args)
```

### Comparing `colossalai-0.2.8/colossalai/tensor/comm_spec.py` & `colossalai-0.3.0/colossalai/tensor/comm_spec.py`

 * *Files 0% similar despite different names*

```diff
@@ -387,15 +387,15 @@
     1. Compute the communication cost which will be used in auto parallel solver.
     2. Convert the communication spec to real action which will be used in runtime.
     It contains comm_pattern to determine the
     communication method, sharding_spec to determine the communication size, gather_dim and shard_dim
     to determine the buffer shape, and logical_process_axis
 
     Argument:
-        comm_pattern(CollectiveCommPattern): decribe the communication method used in this spec.
+        comm_pattern(CollectiveCommPattern): describe the communication method used in this spec.
         sharding_spec(ShardingSpec): This is sharding spec of the tensor which will join the communication action.
         gather_dim(int, Optional): The gather_dim of the tensor will be gathered.
         shard_dim(int, Optional): The shard_dim of the tensor will be sharded.
         logical_process_axis(Union(int, List[int]), Optional): The mesh_dim to implement the communication action.
     '''
 
     def __init__(self,
```

### Comparing `colossalai-0.2.8/colossalai/tensor/compute_spec.py` & `colossalai-0.3.0/colossalai/tensor/compute_spec.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
     TP2D = 1
     TP2P5D = 2
     TP3D = 3
 
 
 class ComputeSpec(object):
     """ComputeSpec
-    The Specification for compuattion pattern
+    The Specification for computation pattern
 
     Args:
         compute_pattern (ComputePattern): an Enum instance for compute pattern.
     """
 
     def __init__(self, compute_pattern: ComputePattern) -> None:
         assert isinstance(compute_pattern, ComputePattern)
```

### Comparing `colossalai-0.2.8/colossalai/tensor/d_tensor/comm_spec.py` & `colossalai-0.3.0/colossalai/tensor/d_tensor/comm_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/d_tensor/d_tensor.py` & `colossalai-0.3.0/colossalai/tensor/d_tensor/d_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/d_tensor/layout.py` & `colossalai-0.3.0/colossalai/tensor/d_tensor/layout.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 from .sharding_spec import ShardingSpec
 
 
 class Layout:
     """Layout of a tensor.
 
     Attributes:
-        device_mesh: the device mesh to store the tensor distributedly.
+        device_mesh: the device mesh to store the tensor distributed.
         device_type: the type of the device mesh, e.g. 'cpu' or 'cuda'.
         sharding_spec: the sharding specification to describe how the tensor is sharded.
         entire_shape: the entire shape of the global tensor.
     """
 
     def __init__(self, device_mesh: DeviceMesh, device_type: torch.device, sharding_spec: ShardingSpec,
                  entire_shape: torch.Size):
```

### Comparing `colossalai-0.2.8/colossalai/tensor/d_tensor/layout_converter.py` & `colossalai-0.3.0/colossalai/tensor/d_tensor/layout_converter.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/d_tensor/sharding_spec.py` & `colossalai-0.3.0/colossalai/tensor/d_tensor/sharding_spec.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 SHARD_COST = 5
 STEP_PENALTY = 6
 NAN = 'nan'
 
 
 class DimSpec:
     '''
-    Sharding spec for single dimension of the sharded tensor decribe the sharding dimension of
+    Sharding spec for single dimension of the sharded tensor describe the sharding dimension of
     logical device mesh and give a method to compute the difference between them.
     This class is used internally in ShardingSpec.
 
     Argument:
         shard_list(List[int]): if shard_list is None, the dim spec will be 'R' type.
             Otherwise, the element in shard_list means the data will be sharded in that dimension.
     '''
@@ -139,15 +139,15 @@
 class ShardingSpec:
     '''
     Sharding spec describes how to shard a tensor with dim_size dimensions. The sharding sequence looks like
     [R, R, S0, S1], which means
 
     Argument:
         dim_partition_dict(Dict[int, List[int]], optional): The key is the dimension of tensor to be sharded,
-            and the value of the key decribe which logical axis will be sharded in that dimension.
+            and the value of the key describe which logical axis will be sharded in that dimension.
         sharding_sequence(List[DimSpec], optional): A straight view of ShardingSpec looks like [R, R, S0, S1].
     '''
 
     def __init__(self,
                  dim_size: int,
                  dim_partition_dict: Dict[int, List[int]] = None,
                  sharding_sequence: List[DimSpec] = None):
```

### Comparing `colossalai-0.2.8/colossalai/tensor/d_tensor/utils.py` & `colossalai-0.3.0/colossalai/tensor/d_tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/dist_spec_mgr.py` & `colossalai-0.3.0/colossalai/tensor/dist_spec_mgr.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,15 @@
 from contextlib import contextmanager
 
 import torch
 import torch.distributed as dist
 # from colossalai.nn.layer.utils import divide
 from numpy import prod
-from packaging import version
 
-from colossalai.logging import get_dist_logger
-from colossalai.tensor.distspec import _DistSpec
+from colossalai.tensor.distspec import DistPlacementPattern, _DistSpec
 from colossalai.tensor.process_group import ProcessGroup
 
 
 # TODO(jiaruifang) circle import, move the divide to colossalai.commons.
 # colossalai.tensor shall not import any submodule from colossal.nn
 def divide(numerator, denominator):
     """Only allow exact division.
@@ -57,15 +55,15 @@
     def _shard_as(tensor: torch.Tensor, old_dist_spec: _DistSpec, dist_spec: _DistSpec,
                   pg: ProcessGroup) -> torch.Tensor:
         """_shard_as: shard the tensor w.r.t a distributed specification.
         Assuming the tensor passed in is a global (replicated) tensor.
         Args:
             tensor (torch.Tensor): a global (replicated) tensor before shard
             dist_spec (_DistSpec): the distributed spec. to be sharded as.
-            pg (ProcessGrouo): the process group of the corresponding colotensor
+            pg (ProcessGroup): the process group of the corresponding colotensor
         Returns:
             torch.Tensor: a torch tensor after sharded.
         """
         assert old_dist_spec.placement.value == 'r', f"The old_dist_spec of DistSpecManager._shard_as must be REPLICATE!"
         DistSpecManager._sanity_check(old_dist_spec, dist_spec)
 
         chunk = tensor
@@ -167,19 +165,29 @@
         return DistSpecManager._shard_as(tensor, old_dist_spec, dist_spec, pg)
 
     @staticmethod
     def handle_trans_spec(tensor: torch.Tensor, old_dist_spec: _DistSpec, dist_spec: _DistSpec,
                           pg: ProcessGroup) -> torch.Tensor:
         assert isinstance(old_dist_spec, _DistSpec), f"{type(old_dist_spec)} should be _DistSpec"
         assert isinstance(dist_spec, _DistSpec), f"{type(dist_spec)} should be _DistSpec"
-        forward_trans_handle = getattr(DistSpecManager, f'_{old_dist_spec.placement.value}2{dist_spec.placement.value}')
+
+        trans_func_key = (old_dist_spec.placement, dist_spec.placement)
+        trans_funcs = {
+            (DistPlacementPattern.REPLICATE, DistPlacementPattern.REPLICATE): DistSpecManager._r2r,
+            (DistPlacementPattern.REPLICATE, DistPlacementPattern.SHARD): DistSpecManager._r2s,
+            (DistPlacementPattern.SHARD, DistPlacementPattern.REPLICATE): DistSpecManager._s2r,
+            (DistPlacementPattern.SHARD, DistPlacementPattern.SHARD): DistSpecManager._s2s
+        }
+
+        forward_trans_handle = trans_funcs[trans_func_key]
         if not DistSpecManager._use_autograd_function:
             return forward_trans_handle(tensor, old_dist_spec, dist_spec, pg)
-        backward_trans_handle = getattr(DistSpecManager,
-                                        f'_{dist_spec.placement.value}2{old_dist_spec.placement.value}')
+
+        backward_trans_handle = trans_funcs[(dist_spec.placement, old_dist_spec.placement)]
+
         return TransformDistSpec.apply(tensor, old_dist_spec, dist_spec, pg, forward_trans_handle,
                                        backward_trans_handle)
 
     @staticmethod
     @contextmanager
     def no_grad():
         try:
```

### Comparing `colossalai-0.2.8/colossalai/tensor/distspec.py` & `colossalai-0.3.0/colossalai/tensor/distspec.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 
 class _DistSpec:
     """_DistSpec
 
     A class indicates Distributed Specification.
     The DistSpec is only works for the tensor parallel process groups.
     Because the dist spec of data parallel process group can be automatically deduced.
-    This is an internal data structrue.
+    This is an internal data structure.
     The API for users should be `ShardSpec` and `ReplicaSpec`.
 
     Args:
         dist_placement_pattern (DistPlacementPattern): the pattern describing how tensors are distributed among processes.
                                                 The dist_placement_pattern is picked from a limited set, now including two patterns: replicate and shard.
         process_group (Optional[ProcessGroup], optional): the process group contains processes. Defaults to None.
     """
```

### Comparing `colossalai-0.2.8/colossalai/tensor/op_wrapper.py` & `colossalai-0.3.0/colossalai/tensor/op_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/param_op_hook.py` & `colossalai-0.3.0/colossalai/tensor/param_op_hook.py`

 * *Files 0% similar despite different names*

```diff
@@ -164,20 +164,20 @@
     for obj in args:
         if _is_grad_tensor(obj):
             return args, None
     # otherwise, the first arguement should be a tuple of grad tensors
     # if there is no grad tensor, the backward of PreFwdPostBwd can't be triggered
     arg_zero = args[0]
     if not isinstance(arg_zero, tuple):
-        raise NotImplementedError("Some torch function is incompatible because of its complcated inputs.")
+        raise NotImplementedError("Some torch function is incompatible because of its complicated inputs.")
     check_grad_flag = False
     for obj in arg_zero:
         check_grad_flag |= _is_grad_tensor(obj)
     if not check_grad_flag:
-        raise NotImplementedError("Some torch function is incompatible because of its complcated inputs.")
+        raise NotImplementedError("Some torch function is incompatible because of its complicated inputs.")
     return arg_zero, args[1:]
 
 
 def _get_colo_tensors_info(*args) -> list:
     info = []
     for arg in args:
         if isinstance(arg, ColoTensor):
```

### Comparing `colossalai-0.2.8/colossalai/tensor/process_group.py` & `colossalai-0.3.0/colossalai/tensor/process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/shape_consistency.py` & `colossalai-0.3.0/colossalai/tensor/shape_consistency.py`

 * *Files 0% similar despite different names*

```diff
@@ -69,15 +69,15 @@
         assert isinstance(value, bool)
         self._forward_only = value
 
     def get_all_all_gather_spec(self, source_spec: ShardingSpec,
                                 orig_cost_dict: Dict[str, float]) -> Dict[ShardingSpec, float]:
         '''
         Get all valid sharding specs from source_spec with single all-gather operation, and
-        accumulate commucation cost on origin cost which will finally be used in auto sharding solver.
+        accumulate communication cost on origin cost which will finally be used in auto sharding solver.
         For the all-gather operation, we just care about the S dimension.
 
         Argument:
             source_spec(ShardingSpec): the ShardingSpec of the source_spec.
             orig_cost(Dict[str, float]): the original communication cost before this operation.
 
         Return:
@@ -141,15 +141,15 @@
                 pass
         return valid_spec_dict
 
     def get_all_all_to_all_spec(self, source_spec: ShardingSpec,
                                 orig_cost_dict: Dict[str, float]) -> Dict[ShardingSpec, float]:
         '''
         Get all valid sharding specs from source_spec with single all-to-all operation, and
-        accumulate commucation cost on origin cost which will finally be used in auto sharding solver.
+        accumulate communication cost on origin cost which will finally be used in auto sharding solver.
         For the all-to-all operation, we just care about the pairs containing S dimension.
 
         Argument:
             source_spec(ShardingSpec): the ShardingSpec of the source_spec.
             orig_cost(Dict[str, float]): the original communication cost before this operation.
 
         Return:
```

### Comparing `colossalai-0.2.8/colossalai/tensor/sharding_spec.py` & `colossalai-0.3.0/colossalai/tensor/sharding_spec.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 SHARD_COST = 5
 STEP_PENALTY = 6
 NAN = 'nan'
 
 
 class _DimSpec:
     '''
-    Sharding spec for single dimension of the sharded tensor decribe the sharding dimension of
+    Sharding spec for single dimension of the sharded tensor describe the sharding dimension of
     logical device mesh and give a method to compute the difference between them.
     This class is used internally in ShardingSpec.
 
     Argument:
         shard_list(List[int]): if shard_list is None, the dim spec will be 'R' type.
             Otherwise, the element in shard_list means the data will be sharded in that dimension.
     '''
```

### Comparing `colossalai-0.2.8/colossalai/tensor/tensor_spec.py` & `colossalai-0.3.0/colossalai/tensor/tensor_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/tensor/utils.py` & `colossalai-0.3.0/colossalai/tensor/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
     We don't allow uncontiguous layout, such as all-gather(S012)->S02 is NOT allowed.
     Therefore, all gather operation just remove the last element in shard list,
     e.g.:
         all-gather(S01) -> S0
 
     Argument:
         target_pair(Tuple[int, List[int]]): The first element is the dimension of tensor to be sharded,
-        and the second element decribes which logical axis will be sharded in that dimension.
+        and the second element describes which logical axis will be sharded in that dimension.
     '''
     _, shard_list = target_pair
     new_shard_list = shard_list[:-1]
 
     return new_shard_list
 
 
@@ -32,25 +32,25 @@
     and simulate the influence of the DimSpec.
 
     We BANNED all representations which shard_list in decreasing order,
     such as S10, so all-to-all(S0, S1) -> RS01 is NOT allowed.
     Therefore, if the behind shard_list is not None, we just extend it to the front shard_list.
     Argument:
         target_pair(Tuple[int, List[int]]): The first element is the dimension of tensor to be sharded,
-        and the second element decribes which logical axis will be sharded in that dimension.
+        and the second element describes which logical axis will be sharded in that dimension.
     e.g.:
         all-to-all(S0, S1) -> [S01, R]
         all-to-all(S0, R) -> [R, S0]
     Otherwise, we extend the front shard_list to behind.
     e.g.:
         all-to-all(R, S1) -> [S1, R]
 
     Argument:
         target_pair(Tuple[int, List[int]]): The first element is the dimension of tensor to be sharded,
-        and the second element decribes which logical axis will be sharded in that dimension.
+        and the second element describes which logical axis will be sharded in that dimension.
     '''
     _, f_shard_list = f_target_pair
     _, b_shard_list = b_target_pair
     if not len(b_shard_list):
         b_shard_list.extend(f_shard_list)
         f_shard_list = []
     else:
```

### Comparing `colossalai-0.2.8/colossalai/testing/pytest_wrapper.py` & `colossalai-0.3.0/colossalai/testing/pytest_wrapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -29,12 +29,12 @@
         # this will skip your test
         pytest test_for_something.py
 
     """
     assert isinstance(name, str)
     flag = os.environ.get(name.upper(), '0')
 
-    reason = f'Environment varialbe {name} is {flag}'
+    reason = f'Environment variable {name} is {flag}'
     if flag == '1':
         return pytest.mark.skipif(False, reason=reason)
     else:
         return pytest.mark.skipif(True, reason=reason)
```

### Comparing `colossalai-0.2.8/colossalai/testing/random.py` & `colossalai-0.3.0/colossalai/testing/random.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/_trainer.py` & `colossalai-0.3.0/colossalai/trainer/_trainer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/hooks/__init__.py` & `colossalai-0.3.0/colossalai/trainer/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/hooks/_base_hook.py` & `colossalai-0.3.0/colossalai/trainer/hooks/_base_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/hooks/_checkpoint_hook.py` & `colossalai-0.3.0/colossalai/trainer/hooks/_checkpoint_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/hooks/_log_hook.py` & `colossalai-0.3.0/colossalai/trainer/hooks/_log_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/hooks/_lr_scheduler_hook.py` & `colossalai-0.3.0/colossalai/trainer/hooks/_lr_scheduler_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/trainer/hooks/_metric_hook.py` & `colossalai-0.3.0/colossalai/trainer/hooks/_metric_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/__init__.py` & `colossalai-0.3.0/colossalai/utils/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from .common import (
     clip_grad_norm_fp32,
     conditional_context,
     copy_tensor_parallel_attributes,
     count_zeros_fp32,
     disposable,
     ensure_path_exists,
-    free_port,
     is_ddp_ignored,
     is_dp_rank_0,
     is_model_parallel_parameter,
     is_no_pp_or_last_stage,
     is_tp_rank_0,
     is_using_ddp,
     is_using_pp,
@@ -33,15 +32,14 @@
     report_memory_usage,
 )
 from .tensor_detector import TensorDetector
 from .timer import MultiTimer, Timer
 
 __all__ = [
     'checkpoint',
-    'free_port',
     'print_rank_0',
     'sync_model_param',
     'is_ddp_ignored',
     'is_dp_rank_0',
     'is_tp_rank_0',
     'is_no_pp_or_last_stage',
     'is_using_ddp',
```

### Comparing `colossalai-0.2.8/colossalai/utils/activation_checkpoint.py` & `colossalai-0.3.0/colossalai/utils/activation_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint/module_checkpoint.py` & `colossalai-0.3.0/colossalai/utils/checkpoint/module_checkpoint.py`

 * *Files 0% similar despite different names*

```diff
@@ -85,15 +85,15 @@
         epoch (int): the number of epoch
         model (torch.nn.Module): a torch module initialized by ColoInitContext
         optimizer (ColossalaiOptimizer, optional): optimizers. Defaults to None.
         lr_scheduler (torch.optim.lr_scheduler._LRScheduler, optional): lr schedule. Defaults to None.
         torch_load_kwargs: (dict, optional): The kwargs of torch.load inside the function
         load_state_dict_kwargs (dict, optional): The kwargs of load_state_dict inside the function
     """
-    # initialize the default paramters
+    # initialize the default parameters
     if not torch_load_kwargs:
         torch_load_kwargs = dict()
     if not load_state_dict_kwargs:
         load_state_dict_kwargs = dict()
 
     rank = dist.get_rank()
     mapping = dict()
```

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint/utils.py` & `colossalai-0.3.0/colossalai/utils/checkpoint/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,15 @@
             if dist.get_rank() != 0:
                 colo_tensor.set_dist_spec(old_dist_spec)
 
         # synchronize all processes for unexpected problems
         dist.barrier()
 
     if dist.get_rank() == 0:
-        setattr(colo_tensor, 'save_ready', True)    # set saving signitrue
+        setattr(colo_tensor, 'save_ready', True)    # set saving signature
 
 
 def scatter_tensor(colo_tensor: ColoTensor, dist_spec: _DistSpec) -> None:
     """Reversal operation of `gather_tensor`.
     """
     if dist_spec.placement == DistPlacementPattern.REPLICATE:
         robust_broadcast(colo_tensor.data)
```

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/backend.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/backend.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/convertor.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/convertor.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/distributed.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/distributed.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/io.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/io.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/meta.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/meta.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/reader.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/reader.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/utils.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpoint_io/writer.py` & `colossalai-0.3.0/colossalai/utils/checkpoint_io/writer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/checkpointing.py` & `colossalai-0.3.0/colossalai/utils/checkpointing.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/common.py` & `colossalai-0.3.0/colossalai/utils/common.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,31 +46,14 @@
 def ensure_path_exists(filename: str):
     # ensure the path exists
     dirpath = os.path.dirname(filename)
     if not os.path.exists(dirpath):
         Path(dirpath).mkdir(parents=True, exist_ok=True)
 
 
-def free_port() -> int:
-    """Get a free port on localhost.
-
-    Returns:
-        int: A free port on localhost.
-    """
-    while True:
-        port = random.randint(20000, 65000)
-        try:
-            with socket.socket() as sock:
-                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-                sock.bind(("localhost", port))
-                return port
-        except OSError:
-            continue
-
-
 def sync_model_param(model, parallel_mode):
     r"""Make sure data parameters are consistent during Data Parallel Mode.
 
     Args:
         model (:class:`torch.nn.Module`): A pyTorch model on whose parameters you check the consistency.
         parallel_mode (:class:`colossalai.context.ParallelMode`): Parallel mode to be checked.
 
@@ -337,15 +320,15 @@
     else:
         enable_cuda_kernels = params[0].grad.device.type == 'cuda'
     # Norm parameters.
     max_norm = float(max_norm)
     norm_type = float(norm_type)
 
     # Parameters can be on CPU or CUDA
-    # If parameters are on CPU, disable CUDA kernerls
+    # If parameters are on CPU, disable CUDA kernels
 
     # Calculate norm.
     if norm_type == inf:
         total_norm = max(p.grad.data.abs().max() for p in params)
         total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])
         # Take max across all model-parallel GPUs.
         if gpc.is_initialized(ParallelMode.MODEL) and gpc.get_world_size(ParallelMode.MODEL) > 1:
```

### Comparing `colossalai-0.2.8/colossalai/utils/cuda.py` & `colossalai-0.3.0/colossalai/utils/cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/data_sampler/data_parallel_sampler.py` & `colossalai-0.3.0/colossalai/utils/data_sampler/data_parallel_sampler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/memory.py` & `colossalai-0.3.0/colossalai/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/model/colo_init_context.py` & `colossalai-0.3.0/colossalai/zero/gemini/colo_init_context.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,14 @@
 from typing import Any, Dict, Iterator, Optional, Tuple, Union
 
 import torch
 from torch import nn
 
-from colossalai.nn.parallel.layers import ColoEmbedding, ColoLinear, register_colo_module
 from colossalai.tensor import ColoParameter, ColoTensor, ProcessGroup
-
-from .utils import InsertPostInitMethodToModuleSubClasses
+from colossalai.utils.model.utils import InsertPostInitMethodToModuleSubClasses
 
 # find named_params includes replica
 
 
 def _named_params_with_replica(
     module: nn.Module,
     prefix: str = '',
@@ -72,27 +70,28 @@
                  device: torch.device = torch.device('cpu'),
                  dtype: torch.dtype = torch.float,
                  default_pg: Optional[ProcessGroup] = None,
                  default_dist_spec=None):
         """
         Args:
             device (torch.device): the device where parameters initialized are resident. Defaults to torch.device('cpu').
-            dtype (torch.dtype): the dtype of parameters initialized. Defults to torch.float.
+            dtype (torch.dtype): the dtype of parameters initialized. Defaults to torch.float.
             default_pg (ProcessGroup): the default process group for all initialized parameters.
             default_dist_spec: the default distributed specifications.
         """
         super().__init__()
         self._device = device
         self._dtype = dtype
 
         self._register_colo_modules()
         self._default_pg = default_pg
         self._default_dist_spec = default_dist_spec
 
     def _register_colo_modules(self):
+        from colossalai.nn.parallel.layers import ColoEmbedding, ColoLinear, register_colo_module
         register_colo_module(torch.nn.Linear, ColoLinear())
         register_colo_module(torch.nn.Embedding, ColoEmbedding())
 
     def _pre_context_exec(self):
         pass
 
     def _post_init_method(self, module: torch.nn.Module, *args, **kwargs):
@@ -161,15 +160,15 @@
 
     This function is called after `ColoInitContext`.
 
     Args:
         model (torch.nn.module): the model
         device (torch.device, optional): device type of the model params. Defaults to torch.device('cpu').
         dtype (torch.dtype, optional): dtype of the model params. Defaults to torch.float.
-        default_pg (Optional[ProcessGroup], optional): default process group. Defaults to None. Inidicates a DP-only process group.
+        default_pg (Optional[ProcessGroup], optional): default process group. Defaults to None. Indicates a DP-only process group.
         default_dist_spec (Any, optional): default dist spec of params. Defaults to None.
 
     Raises:
         RuntimeError: raise error if
     """
 
     torch_params = []
```

### Comparing `colossalai-0.2.8/colossalai/utils/model/experimental.py` & `colossalai-0.3.0/colossalai/utils/model/experimental.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,23 +3,23 @@
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 from torch import Tensor
 from torch.utils._pytree import tree_map
 
-from colossalai.fx.profiler.tensor import MetaTensor
+from colossalai._analyzer._subclasses import MetaTensor
 from colossalai.tensor.d_tensor.d_tensor import DTensor
 from colossalai.tensor.d_tensor.layout import Layout
 
 # reference: https://pytorch.org/cppdocs/notes/tensor_creation.html
 _NORMAL_FACTORY = [
     "arange",
-    "empty",
     "full",
+    "empty",
     "linspace",
     "logspace",
     "ones",
     "rand",
     "randn",
     "randint",
     "randperm",
@@ -33,15 +33,15 @@
 ]
 
 _EARLY_MATERIALIZED_OPS = ['__getitem__', 'split']
 
 # If your intent is to change the metadata of a Tensor (such as sizes / strides / storage / storage_offset)
 # without autograd tracking the change, remove the .data / .detach() call and wrap the change in a `with torch.no_grad():` block.
 # These ops cannot be unwrapped using .data
-_CHANGE_META_OPS = ['_cudnn_rnn_flatten_weight', 'requires_grad_', '__get__']
+_CHANGE_META_OPS = ['_cudnn_rnn_flatten_weight', 'requires_grad_', '__get__', '__set__']
 
 _LEGACY_TENSOR_CONSTRUCTOR = {
     'FloatTensor': torch.float,
     'DoubleTensor': torch.double,
     'HalfTensor': torch.half,
     'BFloat16Tensor': torch.bfloat16,
     'ByteTensor': torch.uint8,
@@ -71,14 +71,20 @@
 
     @classmethod
     def __torch_function__(cls, func, types, args=(), kwargs=None):
         cls._pre_op_fn()
         return super().__torch_function__(func, types, args, kwargs)
 
 
+def _data_tolist(tensor: torch.Tensor) -> list:
+    """tolist() method is not allowed for a subclass of tensor. Tensor.data returns a Tensor.
+    """
+    return tensor.data.tolist()
+
+
 def _convert_cls(tensor: 'LazyTensor', target: torch.Tensor) -> torch.Tensor:
     """Convert a lazy tensor's class to target's class, with target's data.
 
     The reason why we change the class of a lazy tensor in-place is that this can easily handle shared modules/parameters, which is common in huggingface models.
     If we create a new tensor and update the module by ``setattr(module, name, param)``, the shared parameters will not be updated. And we have to track all shared parameters and update them manually.
 
     Args:
@@ -90,15 +96,15 @@
     """
     cls_to_become = nn.Parameter if isinstance(tensor, nn.Parameter) else torch.Tensor
     tensor.__class__ = cls_to_become
     tensor.data = target
     tensor.requires_grad = target.requires_grad
     # subclass of torch.Tensor does not have tolist() method
     # overwrite this method after materialization or distribution
-    tensor.tolist = MethodType(torch.Tensor.tolist, target)
+    tensor.tolist = MethodType(_data_tolist, tensor)
     return tensor
 
 
 class LazyTensor(torch.Tensor):
     """A naive implementation of LazyTensor (https://arxiv.org/pdf/2102.13267.pdf).
 
     Usage:
@@ -140,15 +146,15 @@
         if concrete_data is not None:
             # some ops don't support meta backend and should have concrete data
             elem = concrete_data
         else:
             if meta_data is None:
                 device = kwargs.get('device', 'cpu')
                 elem = func(*args, **{**kwargs, 'device': 'meta'})
-                meta_data = MetaTensor(elem, fake_device=device)
+                meta_data = MetaTensor(elem, device=device)
             elem = meta_data._tensor
         # As a meta tensor cannot be modified __class__ to torch.Tensor, we should use an empty real tensor here
         r = torch.Tensor._make_subclass(cls, _EMPTY_DATA, require_grad=elem.requires_grad)
         r._meta_data = meta_data
         return r
 
     def __init__(self, func, *args, meta_data=None, concrete_data=None, **kwargs):
@@ -251,15 +257,15 @@
         if kwargs is None:
             kwargs = {}
         if func.__name__ in _EARLY_MATERIALIZED_OPS:
             # These OPs cannot be lazy and related tensors should be early materialized
             tree_map(cls._replace_with_materialized, args)
             tree_map(cls._replace_with_materialized, kwargs)
         is_inplace: bool = (func.__name__.endswith('_') and not (func.__name__.endswith('__'))
-                            or func.__name__ == "__setitem__")
+                            or func.__name__ in ('__setitem__', '__set__'))
 
         is_change_meta_op: bool = func.__name__ in _CHANGE_META_OPS
 
         if isinstance(func, torch._C.ScriptMethod):
             # FIXME(ver217): torch script functions are not verified
 
             target = None
@@ -314,23 +320,45 @@
     @classmethod
     def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
         pass    # skip
 
     def clone(self) -> "LazyTensor":
 
         def factory_fn():
-            return self.materialize().clone()
+            # if self is materialized, return self
+            new_tensor = self.materialize() if type(self) is LazyTensor else self
+            return new_tensor.clone()
 
         target = LazyTensor(factory_fn, meta_data=self._meta_data)
 
         return target
 
     def detach(self) -> Tensor:
         return self
 
+    def __deepcopy__(self, memo):
+        if not self.is_leaf:
+            raise RuntimeError("Only Tensors created explicitly by the user "
+                               "(graph leaves) support the deepcopy protocol at the moment")
+        if id(self) in memo:
+            return memo[id(self)]
+
+        def factory_fn():
+            # if self is materialized, return self
+            new_tensor = self.materialize() if type(self) is LazyTensor else self
+            copied = new_tensor.detach().clone()
+            if new_tensor.requires_grad:
+                copied.requires_grad_()
+            return copied
+
+        target = LazyTensor(factory_fn, meta_data=self._meta_data)
+
+        memo[id(self)] = target
+        return target
+
     @property
     def data(self):
         return self
 
     @data.setter
     def data(self, other: 'LazyTensor'):
         """This is sightly different from oringinal `data` setter.
```

### Comparing `colossalai-0.2.8/colossalai/utils/model/lazy_init_context.py` & `colossalai-0.3.0/colossalai/utils/model/lazy_init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/model/utils.py` & `colossalai-0.3.0/colossalai/utils/model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py` & `colossalai-0.3.0/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/profiler/legacy/comm_profiler.py` & `colossalai-0.3.0/colossalai/utils/profiler/legacy/comm_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/profiler/legacy/pcie_profiler.py` & `colossalai-0.3.0/colossalai/utils/profiler/legacy/pcie_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/profiler/legacy/prof_utils.py` & `colossalai-0.3.0/colossalai/utils/profiler/legacy/prof_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/profiler/profiler.py` & `colossalai-0.3.0/colossalai/utils/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/profiler/stateful_tensor_mem_extention.py` & `colossalai-0.3.0/colossalai/utils/profiler/stateful_tensor_mem_extention.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/rank_recorder/rank_recorder.py` & `colossalai-0.3.0/colossalai/utils/rank_recorder/rank_recorder.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/utils/tensor_detector/tensor_detector.py` & `colossalai-0.3.0/colossalai/utils/tensor_detector/tensor_detector.py`

 * *Files 0% similar despite different names*

```diff
@@ -51,15 +51,15 @@
         memory_size = tensor.element_size() * tensor.storage().size()
         if (tensor.is_leaf or tensor.retains_grad) and tensor.grad is not None:
             grad_memory_size = tensor.grad.element_size() * tensor.grad.storage().size()
             memory_size += grad_memory_size
         return self.mem_format(memory_size)
 
     def mem_format(self, real_memory_size):
-        # format the tensor memory into a reasonal magnitude
+        # format the tensor memory into a reasonable magnitude
         if real_memory_size >= 2**30:
             return str(real_memory_size / (2**30)) + ' GB'
         if real_memory_size >= 2**20:
             return str(real_memory_size / (2**20)) + ' MB'
         if real_memory_size >= 2**10:
             return str(real_memory_size / (2**10)) + ' KB'
         return str(real_memory_size) + ' B'
@@ -67,28 +67,28 @@
     def collect_tensors_state(self):
         for obj in gc.get_objects():
             if torch.is_tensor(obj):
                 # skip cpu tensor when include_cpu is false and the tensor we have collected before
                 if (not self.include_cpu) and obj.device == torch.device('cpu'):
                     continue
                 self.detected.append(id(obj))
-                # skip paramters we had added in __init__ when module is an instance of nn.Module for the first epoch
+                # skip parameters we had added in __init__ when module is an instance of nn.Module for the first epoch
                 if id(obj) not in self.tensor_info:
 
                     name = type(obj).__name__
                     # after backward, we want to update the records, to show you the change
                     if isinstance(self.module, nn.Module) and name == 'Parameter':
                         if obj.grad is not None:
                             # with grad attached
                             for par_name, param in self.module.named_parameters():
                                 if param.requires_grad and param.grad.equal(obj.grad):
                                     name = par_name + ' (with grad)'
                         else:
                             # with no grad attached
-                            # there will be no new paramters created during running
+                            # there will be no new parameters created during running
                             # so it must be in saved_tensor_info
                             continue
                     # we can also marked common tensors as tensor(with grad)
                     if name == 'Tensor' and (obj.is_leaf or obj.retains_grad):
                         if obj.grad is not None:
                             name = name + ' (with grad)'
                     # in fact, common tensor have no grad
@@ -151,15 +151,15 @@
 
         self.info += LINE
         self.info += f"Detect Location: {locate_msg}\n"
         for device in self.devices:
             if device == torch.device('cpu'):
                 continue
             gpu_mem_alloc = self.mem_format(torch.cuda.memory_allocated(device))
-            self.info += f"Totle GPU Memery Allocated on {device} is {gpu_mem_alloc}\n"
+            self.info += f"Total GPU Memory Allocated on {device} is {gpu_mem_alloc}\n"
         self.info += LINE
         self.info += '\n\n'
         if self.show_info:
             print(self.info)
         if self.log is not None:
             with open(self.log + '.log', 'a') as f:
                 f.write(self.info)
```

### Comparing `colossalai-0.2.8/colossalai/utils/timer.py` & `colossalai-0.3.0/colossalai/utils/timer.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/zero/__init__.py` & `colossalai-0.3.0/colossalai/zero/legacy/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 from typing import Tuple
 
 import torch
 import torch.nn as nn
 
 from colossalai.logging import get_dist_logger
-from colossalai.zero.sharded_model.sharded_model_v2 import ShardedModelV2
-from colossalai.zero.sharded_optim import LowLevelZeroOptimizer, ShardedOptimizerV2
 
-from ..nn.optimizer.zero_optimizer import ZeroOptimizer
+from .init_ctx import ZeroInitContext, no_shard_zero_context, no_shard_zero_decrator
+from .shard_utils import BucketTensorShardStrategy, TensorShardStrategy
+from .sharded_model import ShardedModelV2
+from .sharded_optim import ShardedOptimizerV2
 
 
 def convert_to_zero_v2(model: nn.Module, optimizer: torch.optim.Optimizer, model_config,
                        optimizer_config) -> Tuple[ShardedModelV2, ShardedOptimizerV2]:
     """
     A helper function to integrate the model and optimizer with ZeRO optimizer and off-loading
 
@@ -34,8 +35,11 @@
         model_config = dict()
 
     zero_model = ShardedModelV2(model, **model_config)
     zero_optimizer = ShardedOptimizerV2(zero_model, optimizer, **optimizer_config)
     return zero_model, zero_optimizer
 
 
-__all__ = ['convert_to_zero_v2', 'LowLevelZeroOptimizer', 'ShardedModelV2', 'ShardedOptimizerV2', 'ZeroOptimizer']
+__all__ = [
+    'convert_to_zero_v2', 'ShardedModelV2', 'ShardedOptimizerV2', 'ZeroInitContext', 'no_shard_zero_context',
+    'no_shard_zero_decrator', 'TensorShardStrategy', 'BucketTensorShardStrategy'
+]
```

### Comparing `colossalai-0.2.8/colossalai/zero/init_ctx/init_context.py` & `colossalai-0.3.0/colossalai/zero/legacy/init_ctx/init_context.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,18 +9,18 @@
 import torch.nn as nn
 
 from colossalai.context.parallel_mode import ParallelMode
 from colossalai.context.singleton_meta import SingletonMeta
 from colossalai.core import global_context as gpc
 from colossalai.logging import get_dist_logger
 from colossalai.utils.model.utils import InsertPostInitMethodToModuleSubClasses
-from colossalai.zero.shard_utils import BaseShardStrategy
-from colossalai.zero.sharded_model._utils import cast_tensor_to_fp16
-from colossalai.zero.sharded_model.sharded_model_v2 import ShardedModelV2
-from colossalai.zero.sharded_param import ShardedParamV2
+from colossalai.zero.legacy.shard_utils import BaseShardStrategy
+from colossalai.zero.legacy.sharded_model._utils import cast_tensor_to_fp16
+from colossalai.zero.legacy.sharded_model.sharded_model_v2 import ShardedModelV2
+from colossalai.zero.legacy.sharded_param import ShardedParamV2
 
 
 @dataclass
 class ZeroContextConfig:
     """The configuration used to control zero context initialization.
 
     Args:
@@ -93,15 +93,15 @@
         return self.config.shard_param
 
     @staticmethod
     def calc_fanin_fanout(tensor: torch.Tensor):
         """We use this function to substitute fan-in and fan-out calculation in torch.nn.init.
         This can help us get correct fan-in and fan-out for sharded tensor.
         """
-        assert isinstance(tensor, nn.Parameter), "Sharded tensor initilization is only allowed for paramters"
+        assert isinstance(tensor, nn.Parameter), "Sharded tensor initialization is only allowed for parameters"
 
         # get correct shape of input tensor
         if not hasattr(tensor, 'colo_attr') or not tensor.colo_attr.param_is_sharded:
             tensor_shape = tensor.shape
         else:
             tensor_shape = tensor.colo_attr.sharded_data_tensor.origin_shape
```

### Comparing `colossalai-0.2.8/colossalai/zero/shard_utils/base_shard_strategy.py` & `colossalai-0.3.0/colossalai/zero/legacy/shard_utils/base_shard_strategy.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from abc import ABC, abstractmethod
 from typing import List, Optional
 
 import torch.distributed as dist
-from colossalai.zero.sharded_param.sharded_tensor import ShardedTensor
+
+from colossalai.zero.legacy.sharded_param.sharded_tensor import ShardedTensor
 
 
 class BaseShardStrategy(ABC):
 
     def __init__(self) -> None:
         """Abstract Shard Strategy. Use to shard a tensors on multiple GPUs.
         """
```

### Comparing `colossalai-0.2.8/colossalai/zero/shard_utils/bucket_tensor_shard_strategy.py` & `colossalai-0.3.0/colossalai/zero/legacy/shard_utils/bucket_tensor_shard_strategy.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 from typing import List, Optional
 
 import torch
 import torch.distributed as dist
-from colossalai.utils import get_current_device
-from colossalai.zero.sharded_param.sharded_tensor import ShardedTensor
 from torch._utils import _flatten_dense_tensors as flatten
 
+from colossalai.utils import get_current_device
+from colossalai.zero.legacy.sharded_param.sharded_tensor import ShardedTensor
+
 from .tensor_shard_strategy import TensorShardStrategy
 
 
 class BucketTensorShardStrategy(TensorShardStrategy):
-    """Use the same shard scheme as `TensorShardStrategy`'s, but it gathers tensors of a sub-module together, 
-    which will fully utilize network bandwidth. 
-    It is especially useful when sub-module contains bias, 
-    since we cannot utilize network bandwidth well if we only gather a bias tensor (bias is usaully small).
+    """Use the same shard scheme as `TensorShardStrategy`'s, but it gathers tensors of a sub-module together,
+    which will fully utilize network bandwidth.
+    It is especially useful when sub-module contains bias,
+    since we cannot utilize network bandwidth well if we only gather a bias tensor (bias is usually small).
     """
 
     def gather(self, tensor_list: List[ShardedTensor], process_group: Optional[dist.ProcessGroup] = None):
 
         tensor_list: List[ShardedTensor] = [t for t in tensor_list if t.is_sharded]
         if len(tensor_list) == 0:
             return
```

### Comparing `colossalai-0.2.8/colossalai/zero/shard_utils/commons.py` & `colossalai-0.3.0/colossalai/zero/legacy/shard_utils/commons.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-import torch
-import torch.nn.functional as F
 from typing import Tuple
 
+import torch
+
 
 def get_shard(tensor: torch.Tensor, rank: int, world_size: int) -> Tuple[torch.Tensor, int]:
     """Return the local shard of a full tensor."""
     # Shard using torch.chunk to match all-gather/reduce-scatter.
     chunks = list(torch.flatten(tensor).chunk(world_size))
     while len(chunks) < world_size:
         chunks.append(chunks[0].new_empty(0))
```

### Comparing `colossalai-0.2.8/colossalai/zero/shard_utils/tensor_shard_strategy.py` & `colossalai-0.3.0/colossalai/zero/legacy/shard_utils/tensor_shard_strategy.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from typing import List, Optional
 
 import torch
 import torch.distributed as dist
+
 from colossalai.utils import get_current_device
-from colossalai.zero.shard_utils import BaseShardStrategy
-from colossalai.zero.shard_utils.commons import get_shard
-from colossalai.zero.sharded_param.sharded_tensor import ShardedTensor
-from colossalai.gemini.tensor_utils import colo_model_data_tensor_move_inline
+from colossalai.zero.legacy.gemini.tensor_utils import colo_model_data_tensor_move_inline
+from colossalai.zero.legacy.shard_utils import BaseShardStrategy
+from colossalai.zero.legacy.shard_utils.commons import get_shard
+from colossalai.zero.legacy.sharded_param.sharded_tensor import ShardedTensor
 
 
 class TensorShardStrategy(BaseShardStrategy):
     """
     A naive implementation which shard each tensor evenly over all ranks
     """
 
@@ -23,15 +24,15 @@
             self._gather_tensor(t, process_group)
 
     def _shard_tensor(self, t: ShardedTensor, process_group: Optional[dist.ProcessGroup] = None):
         """ Shard tensor among processes.
 
         Args:
             t (ShardedTensor): a tensor to be sharded.
-            process_group (Optional[dist.ProcessGroup], optional): the process group among which tensor shards. 
+            process_group (Optional[dist.ProcessGroup], optional): the process group among which tensor shards.
             Defaults to None.
         """
         if t.is_sharded:
             return
         if t.payload.device.type == 'cuda':
             assert t.payload.device == get_current_device(), f"shard tensor on cuda device index {t.payload.device.index},"\
                 f" but current cuda device is {get_current_device()}"
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_model/_utils.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_model/_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-from typing import Any, Callable, List, Tuple
+from typing import Any, Callable, List, Tuple, Union
 
 import torch
 import torch.nn.functional as F
-from typing import Union
-from colossalai.gemini.stateful_tensor import StatefulTensor
+
+from colossalai.zero.legacy.gemini.stateful_tensor import StatefulTensor
 
 
 def get_gradient_predivide_factor(world_size: int) -> float:
     factor: int = 1
     while world_size % factor == 0 and world_size / factor > factor:
         factor *= 2
     return float(factor)
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_model/reduce_scatter.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_model/reduce_scatter.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_model/sharded_model_v2.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_model/sharded_model_v2.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,36 +9,36 @@
 import torch.distributed as dist
 import torch.nn as nn
 from torch.distributed import ProcessGroup
 from torch.nn.parameter import Parameter
 
 from colossalai.context.parallel_mode import ParallelMode
 from colossalai.core import global_context as gpc
-from colossalai.gemini.memory_tracer import MemStatsCollector, StaticMemStatsCollector
-from colossalai.gemini.ophooks import register_ophooks_recursively
-from colossalai.gemini.paramhooks import BaseParamHookMgr
-from colossalai.gemini.stateful_tensor import TensorState
-from colossalai.gemini.stateful_tensor_mgr import StatefulTensorMgr
-from colossalai.gemini.tensor_placement_policy import TensorPlacementPolicy, TensorPlacementPolicyFactory
-from colossalai.gemini.tensor_utils import colo_model_data_move_to_cpu
 from colossalai.logging import get_dist_logger
 from colossalai.utils import disposable, get_current_device
 from colossalai.utils.memory import colo_device_memory_capacity
-from colossalai.zero.shard_utils import BaseShardStrategy
-from colossalai.zero.sharded_model.reduce_scatter import ReduceScatterBucketer
-from colossalai.zero.utils import ZeroHook
+from colossalai.zero.gemini.memory_tracer import MemStatsCollector, StaticMemStatsCollector
+from colossalai.zero.legacy.gemini.ophooks import register_ophooks_recursively
+from colossalai.zero.legacy.gemini.paramhooks import BaseParamHookMgr
+from colossalai.zero.legacy.gemini.stateful_tensor import TensorState
+from colossalai.zero.legacy.gemini.stateful_tensor_mgr import StatefulTensorMgr
+from colossalai.zero.legacy.gemini.tensor_placement_policy import TensorPlacementPolicy, TensorPlacementPolicyFactory
+from colossalai.zero.legacy.gemini.tensor_utils import colo_model_data_move_to_cpu
+from colossalai.zero.legacy.shard_utils import BaseShardStrategy
+from colossalai.zero.legacy.sharded_model.reduce_scatter import ReduceScatterBucketer
 
 from ._utils import (
     cast_float_arguments,
     cast_tensor_to_fp16,
     cast_tensor_to_fp32,
     chunk_and_pad,
     free_storage,
     get_gradient_predivide_factor,
 )
+from .zero_hook import ZeroHook
 
 try:
     from torch.nn.modules.module import _EXTRA_STATE_KEY_SUFFIX
 except ImportError:
     _EXTRA_STATE_KEY_SUFFIX = '_extra_state'
 
 
@@ -188,24 +188,24 @@
 
     @property
     def cpu_offload(self):
         return self._cpu_offload
 
     def dump_memory_stats(self, filename: Optional[str] = 'dump_mem_stats.log') -> None:
         """
-        dummy memory tracer collected infomation to a file.
+        dummy memory tracer collected information to a file.
         try:
             # forward: model(inputs)
             # backward: optimizer.backward()
         except Exception as e:
             model.dump_memory_stats()
             exit(0)
         """
         if self._use_memory_tracer:
-            self.logger.error(f'dump memort tracer collected infomation to a {filename}', ranks=[0])
+            self.logger.error(f'dump memort tracer collected information to a {filename}', ranks=[0])
             if gpc.get_global_rank() == 0:
                 with open(filename, 'w+') as f:
                     f.write(f'cuda reserved {torch.cuda.memory_reserved(get_current_device()) / 1e9} GB\n')
                     f.write(f'cuda max allocated {torch.cuda.max_memory_allocated(get_current_device()) / 1e9} GB\n')
                     f.write('CUDA model data (GB)\n')
                     f.write('\n')
                     f.write('CUDA non model data (GB)\n')
@@ -289,15 +289,15 @@
         self.shard_strategy.shard(tensor_list, self.process_group)
 
         # 4. set all parameters' grad to None
         for p in self.module.parameters():
             if not p.requires_grad:
                 continue
             # Leave the gradient accumulation state (_require_backward_grad_sync) as-is if not synchronizing this pass.
-            # NOTE() (no-sync)/sync pass: (not conduct)/conduct gradient allreducing between process group.
+            # NOTE() (no-sync)/sync pass: (not conduct)/conduct gradient all reducing between process group.
             # If _require_backward_grad_sync is True,
             # p.grad remains the accumulated unsharded gradient from prior no-sync passes.
             # We also allows to interleave no-sync pass with sync passes, if desired.
             if not self._require_backward_grad_sync:
                 continue
 
             p.grad = None
@@ -381,15 +381,15 @@
 
             assert param.colo_attr.saved_grad.is_null(
             ), 'Gradien accumulation is not supported when reuse_fp16_shard=True'
 
             param.colo_attr.grad_payload_reset(grad.data)
             # release the memory of param
             # we set a false None for parameter's payload
-            # so we can get paramter's device and dtype later in optimizer
+            # so we can get parameter's device and dtype later in optimizer
             param.colo_attr.data_payload_reset(torch.empty(0, device=grad.device, dtype=grad.dtype))
 
             if param.colo_attr.is_replicated:
                 param.colo_attr.sharded_data_tensor.is_sharded = True
         else:
 
             fp32_grad = cast_tensor_to_fp32(grad)
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_model/utils.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_model/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,12 @@
+import copy
+
 import torch
-from colossalai.zero.sharded_model import ShardedModelV2
 
-import copy
+from colossalai.zero.legacy.sharded_model import ShardedModelV2
 
 
 def col_model_deepcopy(sharded_model: ShardedModelV2, other_model: torch.nn.Module):
     """
     copy param of the ShardedModelV2 to other_model.
     Note the other_model has to be the same as self.
     """
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/_utils.py` & `colossalai-0.3.0/colossalai/zero/low_level/_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -87,18 +87,26 @@
     # where tensor_tmp and tensor indeed point to the same object.
     # You can check this by print(tensor.data_ptr() == tensor_tmp.data_ptr())
     tensor_tmp = tensor.expand_as(tensor)
     grad_acc_obj = tensor_tmp.grad_fn.next_functions[0][0]
     return grad_acc_obj
 
 
-def split_half_float_double(tensor_list):
+def split_by_dtype(tensor_list):
+    """
+    Splits a list of PyTorch tensors into sublists based on their data type.
+
+    :param tensor_list: A list of PyTorch tensors.
+    :type tensor_list: list[torch.Tensor]
+    :return: A list of sublists, where each sublist contains tensors of a specific data type.
+    :rtype: list[list[torch.Tensor]]
+    """
     dtypes = ["torch.cuda.HalfTensor", "torch.cuda.FloatTensor", "torch.cuda.DoubleTensor", "torch.cuda.BFloat16Tensor"]
     buckets = []
-    for i, dtype in enumerate(dtypes):
+    for _, dtype in enumerate(dtypes):
         bucket = [t for t in tensor_list if t.type() == dtype]
         if bucket:
             buckets.append(bucket)
     return buckets
 
 
 def reduce_tensor_dp_group(tensor: torch.Tensor,
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/bucket_store.py` & `colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/bucket_store.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/gradient_store.py` & `colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/gradient_store.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/parameter_store.py` & `colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/parameter_store.py`

 * *Files 10% similar despite different names*

```diff
@@ -7,17 +7,17 @@
 
 
 class ParameterStore(BaseStore):
 
     def __init__(self, torch_pg: ProcessGroup):
         super().__init__(torch_pg)
         # param partitioning data structures
-        self._fp16_param_to_rank = dict()
-        self._rank_groupid_to_fp16_param_list = dict()
-        self._rank_group_id_to_flat_fp16_param = dict()
+        self._param_to_rank = dict()
+        self._rank_group_id_to_param_list = dict()
+        self._rank_group_id_to_flat_param = dict()
 
         # param reduction data structures
         self._is_param_reduced = dict()
         self._reduced_param = []
 
     def set_param_to_rank(self, tensor: Tensor, rank: int) -> None:
         """
@@ -25,59 +25,59 @@
 
         :param tensor: A :class:`torch.Tensor` object
         :type tensor: torch.Tensor
         :param rank: The rank of which the process is responsible for updating the parameter
         :type rank: int
         """
 
-        self._fp16_param_to_rank[tensor] = rank
+        self._param_to_rank[tensor] = rank
 
     def get_param_rank(self, tensor: Tensor) -> int:
         """
         Gives the rank which the parameter belongs to
 
         :param tensor: A :class:`torch.Tensor` object
         :type tensor: torch.Tensor
         """
-        return self._fp16_param_to_rank[tensor]
+        return self._param_to_rank[tensor]
 
     def belongs_to_current_rank(self, tensor) -> bool:
         """
         Check whether a parameter is supposed to be updated by the process of the current rank
 
         :param tensor: A :class:`torch.Tensor` object
         :type tensor: torch.Tensor
 
         :return: True if the parameter should be updated by the current rank. Otherwise false.
         :rtype: bool
         """
 
-        tensor_rank = self._fp16_param_to_rank[tensor]
+        tensor_rank = self._param_to_rank[tensor]
         return tensor_rank == self._local_rank
 
-    def add_fp16_param_list_by_rank_group(self, rank, group_id, tensor_list) -> None:
-        if rank not in self._rank_groupid_to_fp16_param_list:
-            self._rank_groupid_to_fp16_param_list[rank] = dict()
+    def add_param_list_by_rank_group(self, rank, group_id, tensor_list) -> None:
+        if rank not in self._rank_group_id_to_param_list:
+            self._rank_group_id_to_param_list[rank] = dict()
 
-        if group_id not in self._rank_groupid_to_fp16_param_list[rank]:
-            self._rank_groupid_to_fp16_param_list[rank][group_id] = []
+        if group_id not in self._rank_group_id_to_param_list[rank]:
+            self._rank_group_id_to_param_list[rank][group_id] = []
 
-        self._rank_groupid_to_fp16_param_list[rank][group_id].extend(tensor_list)
+        self._rank_group_id_to_param_list[rank][group_id].extend(tensor_list)
 
-    def get_fp16_params_by_rank_group(self, rank, group_id) -> List[Tensor]:
-        return self._rank_groupid_to_fp16_param_list[rank][group_id]
+    def get_params_by_rank_group(self, rank, group_id) -> List[Tensor]:
+        return self._rank_group_id_to_param_list[rank][group_id]
 
-    def add_flat_fp16_param_by_rank_group(self, rank, group_id, tensor) -> None:
-        if rank not in self._rank_group_id_to_flat_fp16_param:
-            self._rank_group_id_to_flat_fp16_param[rank] = dict()
+    def add_flat_param_by_rank_group(self, rank, group_id, tensor) -> None:
+        if rank not in self._rank_group_id_to_flat_param:
+            self._rank_group_id_to_flat_param[rank] = dict()
 
-        self._rank_group_id_to_flat_fp16_param[rank][group_id] = tensor
+        self._rank_group_id_to_flat_param[rank][group_id] = tensor
 
-    def get_flat_fp16_param_by_rank_group(self, rank, group_id) -> Tensor:
-        return self._rank_group_id_to_flat_fp16_param[rank][group_id]
+    def get_flat_param_by_rank_group(self, rank, group_id) -> Tensor:
+        return self._rank_group_id_to_flat_param[rank][group_id]
 
     def is_param_reduced(self, tensor):
         return self._is_param_reduced[tensor]
 
     def set_param_reduction_state(self, tensor, state):
         self._is_param_reduced[tensor] = state
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/bookkeeping/tensor_bucket.py` & `colossalai-0.3.0/colossalai/zero/low_level/bookkeeping/tensor_bucket.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/low_level_optim.py` & `colossalai-0.3.0/colossalai/zero/low_level/low_level_optim.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from ._utils import (
     calculate_global_norm_from_list,
     compute_norm,
     flatten,
     has_inf_or_nan,
     reduce_tensor_dp_group,
     release_param_grad,
-    split_half_float_double,
+    split_by_dtype,
     sync_param,
 )
 from .bookkeeping import BucketStore, GradientStore, ParameterStore, TensorBucket
 
 
 class LowLevelZeroOptimizer(ColossalaiOptimizer):
     """Optimizer used for ZeRO-1 and ZeRO-2.
@@ -51,14 +51,15 @@
             forced_dtype: Optional[torch.dtype] = None):
 
         # TODO: add support for
         # 1. fp16 master weights
         # 2. contiguous gradients
         # 3. cpu offload
         # 4. support when some parameters requires_grad = False
+        # 5. support layer drop
         super(LowLevelZeroOptimizer, self).__init__(optim=optimizer)
         self._dtype = self.optim.param_groups[0]['params'][0].dtype
         self._logger = get_dist_logger()
         self._verbose = verbose
 
         # stage 2
         self._partition_grads = partition_grad
@@ -85,17 +86,18 @@
             self._dp_global_ranks = gpc.get_ranks_in_group(dp_parallel_mode)
             self._dp_torch_group = gpc.get_group(dp_parallel_mode)
             self._mp_torch_group = None
             if gpc.is_initialized(mp_parallel_mode) and gpc.get_world_size(mp_parallel_mode) > 1:
                 self._mp_torch_group = gpc.get_group(mp_parallel_mode)
         else:
             raise NotImplementedError
-        # fp16 and fp32 params for mixed precision training
-        self._fp16_param_groups = dict()
-        self._fp32_flat_param_groups_of_current_rank = dict()
+
+        # working and master params for mixed precision training
+        self._working_param_groups = dict()
+        self._master_flat_param_groups_of_current_rank = dict()
 
         # communication params
         self._overlap_communication = overlap_communication
         self._reduce_bucket_size = reduce_bucket_size
         self._communication_dtype = communication_dtype
 
         # gradient scaler
@@ -133,62 +135,62 @@
         # and add buffers to parameter store for future access
         for group_id, param_group in enumerate(self.optim.param_groups):
             group_params = list()
             for param in param_group['params']:
                 if param.requires_grad:
                     group_params.append(param)
 
-            # add the fp16 params to fp16_param_groups for bookkeeping
-            self._fp16_param_groups[group_id] = group_params
+            # add the working params to working_param_groups for bookkeeping
+            self._working_param_groups[group_id] = group_params
 
             # assign parameters to ranks
             # the params in the list are sorted
             params_per_rank = self._partition_param_list(group_params)
 
             # store the mapping between param to rank
             # each param should belong to only one rank
             for rank, params in enumerate(params_per_rank):
-                self._param_store.add_fp16_param_list_by_rank_group(rank, group_id, params)
+                self._param_store.add_param_list_by_rank_group(rank, group_id, params)
                 for param in params:
                     self._param_store.set_param_to_rank(param, rank)
 
             # move to cpu to make room to create the flat tensor
             # move_tensor(params, device='cpu')
             for param in group_params:
                 param.data = param.data.cpu()
 
             # flatten the reordered tensors
             for rank in range(self._world_size):
-                tensor_list = self._param_store.get_fp16_params_by_rank_group(rank, group_id)
+                tensor_list = self._param_store.get_params_by_rank_group(rank, group_id)
                 with torch.no_grad():
                     flat_tensor = flatten(tensor_list)
                 flat_tensor = flat_tensor.data.cuda()
-                self._param_store.add_flat_fp16_param_by_rank_group(rank, group_id, flat_tensor)
+                self._param_store.add_flat_param_by_rank_group(rank, group_id, flat_tensor)
 
             # sync parameters
             for rank in range(self._world_size):
-                flat_tensor = self._param_store.get_flat_fp16_param_by_rank_group(rank, group_id)
-                tensor_list = self._param_store.get_fp16_params_by_rank_group(rank, group_id)
+                flat_tensor = self._param_store.get_flat_param_by_rank_group(rank, group_id)
+                tensor_list = self._param_store.get_params_by_rank_group(rank, group_id)
                 sync_param(flat_tensor=flat_tensor, tensor_list=tensor_list)
 
-            # create a copy of fp32 weights of the parameters for which this rank is responsible
-            fp16_flat_current_rank = self._param_store.get_flat_fp16_param_by_rank_group(self._local_rank, group_id)
-            fp32_flat_current_rank = fp16_flat_current_rank.float()
+            # create a copy of fp32 master weights of the parameters for which this rank is responsible
+            working_flat_current_rank = self._param_store.get_flat_param_by_rank_group(self._local_rank, group_id)
+            master_flat_current_rank = working_flat_current_rank.float()
             device = 'cpu' if self._cpu_offload else get_current_device()
-            fp32_flat_current_rank = fp32_flat_current_rank.to(device)
-            fp32_flat_current_rank.requires_grad = True
-            self._fp32_flat_param_groups_of_current_rank[group_id] = fp32_flat_current_rank
+            master_flat_current_rank = master_flat_current_rank.to(device)
+            master_flat_current_rank.requires_grad = True
+            self._master_flat_param_groups_of_current_rank[group_id] = master_flat_current_rank
 
             # need to replace the params in the `params` field in the optimizer
             # so that when the optimizer calls step(), it only updates the tensors
             # managed by this data parallel rank
-            param_group['params'] = [fp32_flat_current_rank]
+            param_group['params'] = [master_flat_current_rank]
 
             # set reduction state
-            for param in self._fp16_param_groups[group_id]:
+            for param in self._working_param_groups[group_id]:
                 self._param_store.set_param_reduction_state(param, False)
 
         # intialize communication stream for
         # communication-compuation overlapping
         if self._overlap_communication:
             self._comm_stream = torch.cuda.Stream()
 
@@ -204,15 +206,15 @@
 
     @property
     def loss_scale(self):
         return self.grad_scaler.scale
 
     @property
     def num_param_groups(self):
-        return len(self._fp16_param_groups)
+        return len(self._working_param_groups)
 
     def _sanity_checks(self):
         assert torch.cuda.is_available(), 'CUDA is required'
         for param_group in self.optim.param_groups:
             group_params = param_group['params']
             for param in group_params:
                 assert param.dtype == self._dtype, \
@@ -256,18 +258,18 @@
     ###########################
 
     def _grad_handler(self, param, grad, reduce_rank):
         self._add_to_reduction_bucket(param, reduce_rank)
         return grad
 
     def _attach_reduction_hook(self):
-        # we iterate over the fp16 params
+        # we iterate over the working params
         # on each param, we register a hook to its AccumulateGrad object
         for group_id in range(self.num_param_groups):
-            param_group = self._fp16_param_groups[group_id]
+            param_group = self._working_param_groups[group_id]
             for param in param_group:
                 if param.requires_grad:
                     # determines the reduction destionation rank
                     # this is only valid for stage 2
                     # dst_rank = None means using all-reduce
                     # else using reduce
                     if self._partition_grads:
@@ -310,15 +312,15 @@
                 self._reduce_tensor_bucket(bucket=param_bucket, reduce_rank=reduce_rank)
                 param_bucket.empty()
 
         if not param_bucket.is_empty():
             self._reduce_tensor_bucket(bucket=param_bucket, reduce_rank=reduce_rank)
 
     def _reduce_grads(self, reduce_rank, grads, bucket_size):
-        grad_buckets_by_dtype = split_half_float_double(grads)
+        grad_buckets_by_dtype = split_by_dtype(grads)
 
         for tensor_list in grad_buckets_by_dtype:
             self._reduce_tensor_list_with_one_dtype(tensor_list=tensor_list,
                                                     bucket_size=bucket_size,
                                                     reduce_rank=reduce_rank)
 
     #######################
@@ -413,15 +415,15 @@
         """
         Set parameter gradients to zero. If set_to_none = True, gradient
         will be set to None to save memory.
 
         :param set_to_none: Whether set the gradient to None. Default value is True.
         :type set_to_none: bool
         """
-        for _, param_group in self._fp16_param_groups.items():
+        for _, param_group in self._working_param_groups.items():
             for param in param_group:
                 if set_to_none:
                     param.grad = None
                 else:
                     if param.grad is not None:
                         param.grad.detach()
                         param.grad.zero_()
@@ -436,83 +438,85 @@
         # check for overflow
         found_inf = self._check_overflow()
         self.grad_scaler.update(found_inf)
 
         # update loss scale if overflow occurs
         if found_inf:
             self._grad_store.reset_all_average_gradients()
+            if self._verbose:
+                self._logger.info(f'Found overflow. Skip step')
             self.zero_grad()
             return
 
-        # copy the grad of fp16 param to fp32 param
+        # copy the grad of working param to master param
         single_grad_partition_groups = []
         norm_groups = []
 
         for group_id in range(self.num_param_groups):
             # compute norm
             norm_group = compute_norm(gradients=self._grad_store.get_averaged_gradients_by_group(group_id),
-                                      params=self._param_store.get_fp16_params_by_rank_group(group_id=group_id,
-                                                                                             rank=self._local_rank),
+                                      params=self._param_store.get_params_by_rank_group(group_id=group_id,
+                                                                                        rank=self._local_rank),
                                       dp_group=self._dp_torch_group,
                                       mp_group=self._mp_torch_group)
             norm_groups.append(norm_group)
 
-            # create flat gradient for the flat fp32 params
-            fp16_avg_grads = self._grad_store.get_averaged_gradients_by_group(group_id)
-            flat_fp16_avg_grads = flatten(fp16_avg_grads)
-
-            dtype = self._fp32_flat_param_groups_of_current_rank[group_id].dtype
-            flat_fp32_avg_grads = flat_fp16_avg_grads.to(dtype)
-
-            param_shape = self._fp32_flat_param_groups_of_current_rank[group_id].shape
-            assert param_shape == flat_fp32_avg_grads.shape, \
-                f'fp32 param and grad have different shape {param_shape} vs {flat_fp32_avg_grads.shape}'
-
-            single_grad_partition_groups.append(flat_fp32_avg_grads)
-            device = self._fp32_flat_param_groups_of_current_rank[group_id].device
-            self._fp32_flat_param_groups_of_current_rank[group_id].grad = flat_fp32_avg_grads.to(device)
+            # create flat gradient for the flat fp32 master params
+            working_avg_grads = self._grad_store.get_averaged_gradients_by_group(group_id)
+            flat_working_avg_grads = flatten(working_avg_grads)
+
+            dtype = self._master_flat_param_groups_of_current_rank[group_id].dtype
+            flat_master_avg_grads = flat_working_avg_grads.to(dtype)
+
+            param_shape = self._master_flat_param_groups_of_current_rank[group_id].shape
+            assert param_shape == flat_master_avg_grads.shape, \
+                f'fp32 param and grad have different shape {param_shape} vs {flat_master_avg_grads.shape}'
+
+            single_grad_partition_groups.append(flat_master_avg_grads)
+            device = self._master_flat_param_groups_of_current_rank[group_id].device
+            self._master_flat_param_groups_of_current_rank[group_id].grad = flat_master_avg_grads.to(device)
             self._grad_store.reset_average_gradients_by_group(group_id)
 
         # unscale and clip grads
         global_norm = calculate_global_norm_from_list(norm_list=norm_groups)
         self._unscale_and_clip_grads(single_grad_partition_groups, global_norm)
 
         # update the parameters
         self.optim.step()
-        # release the fp32 grad
-        release_param_grad(self._fp32_flat_param_groups_of_current_rank.values())
+        # release the master grad
+        release_param_grad(self._master_flat_param_groups_of_current_rank.values())
 
-        # update fp16 partition updated by the current rank
-        for group_id in range(len(self._fp16_param_groups)):
-            fp16_param = self._param_store.get_flat_fp16_param_by_rank_group(rank=self._local_rank, group_id=group_id)
-            fp32_param = self._fp32_flat_param_groups_of_current_rank[group_id]
-            fp16_param.data.copy_(fp32_param)
+        # update working partition updated by the current rank
+        for group_id in range(len(self._working_param_groups)):
+            working_param = self._param_store.get_flat_param_by_rank_group(rank=self._local_rank, group_id=group_id)
+            master_param = self._master_flat_param_groups_of_current_rank[group_id]
+            working_param.data.copy_(master_param)
 
         # broadcast the updated model weights
         handles = []
         for group_id in range(self.num_param_groups):
             for index in range(self._world_size):
                 rank = self._dp_global_ranks[index]
-                fp16_param = self._param_store.get_flat_fp16_param_by_rank_group(rank=index, group_id=group_id)
-                handle = dist.broadcast(fp16_param, src=rank, group=self._dp_torch_group, async_op=True)
+                working_param = self._param_store.get_flat_param_by_rank_group(rank=index, group_id=group_id)
+                handle = dist.broadcast(working_param, src=rank, group=self._dp_torch_group, async_op=True)
                 handles.append(handle)
 
         for handle in handles:
             handle.wait()
 
-    ##################
-    # FP16 Utilities #
-    ##################
+    #############################
+    # Mixed Precision Utilities #
+    #############################
 
     def _check_overflow(self):
         # clear previous overflow record
         self._found_overflow.fill_(0.0)
 
         # check for overflow
-        for group_id in range(len(self._fp16_param_groups)):
+        for group_id in range(len(self._working_param_groups)):
             for avg_grad in self._grad_store.get_averaged_gradients_by_group(group_id):
                 if avg_grad is not None and has_inf_or_nan(avg_grad):
                     self._found_overflow.fill_(1.0)
                     break
 
         # all-reduce across dp group
         dist.all_reduce(self._found_overflow, op=dist.ReduceOp.MAX, group=self._dp_torch_group)
@@ -547,15 +551,15 @@
         # update param already reduced flag
         reduction_states = self._param_store.get_param_reduction_states()
         for tensor, _ in reduction_states.items():
             reduction_states[tensor] = False
 
         # accumulate gradient
         for group_id in range(self.num_param_groups):
-            param_group = self._param_store.get_fp16_params_by_rank_group(self._local_rank, group_id)
+            param_group = self._param_store.get_params_by_rank_group(self._local_rank, group_id)
 
             avg_gradients_group = self._grad_store.get_averaged_gradients_by_group(group_id)
 
             param_idx = 0
             for param in param_group:
                 if param.grad is not None:
                     if len(avg_gradients_group) == param_idx:
@@ -568,16 +572,16 @@
         # thus, can clear this
         self.zero_grad()
 
     def _reduce_grad_stage1(self):
         # if not overlapping communication (no reduction hook is attached)
         # we need to manually reduce these gradients
         if not self._overlap_communication:
-            for group_id in range(len(self._fp16_param_groups)):
-                param_group = self._fp16_param_groups[group_id]
+            for group_id in range(len(self._working_param_groups)):
+                param_group = self._working_param_groups[group_id]
                 for param in param_group:
                     if param.grad is not None:
                         self._add_to_reduction_bucket(param)
 
         # we need to reduce the gradients
         # left in the communication bucket
         self._run_reduction()
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_optim/sharded_optim_v2.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_optim/sharded_optim_v2.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,21 +10,21 @@
 from torch.distributed import ProcessGroup
 from torch.nn.parameter import Parameter
 from torch.optim import Optimizer
 
 from colossalai.amp.naive_amp.grad_scaler import DynamicGradScaler
 from colossalai.context.parallel_mode import ParallelMode
 from colossalai.core import global_context as gpc
-from colossalai.gemini.stateful_tensor import StatefulTensor, TensorState
-from colossalai.gemini.tensor_placement_policy import AutoTensorPlacementPolicy
-from colossalai.gemini.tensor_utils import colo_model_data_tensor_move_inline, colo_tensor_mem_usage
 from colossalai.logging import get_dist_logger
 from colossalai.nn.optimizer import ColossalaiOptimizer
-from colossalai.zero.sharded_model import ShardedModelV2
-from colossalai.zero.sharded_model._utils import cast_tensor_to_fp32
+from colossalai.zero.legacy.gemini.stateful_tensor import StatefulTensor, TensorState
+from colossalai.zero.legacy.gemini.tensor_placement_policy import AutoTensorPlacementPolicy
+from colossalai.zero.legacy.gemini.tensor_utils import colo_model_data_tensor_move_inline, colo_tensor_mem_usage
+from colossalai.zero.legacy.sharded_model import ShardedModelV2
+from colossalai.zero.legacy.sharded_model._utils import cast_tensor_to_fp32
 
 
 class OptimState(Enum):
     SCALED = 1
     UNSCALED = 2
 
 
@@ -63,16 +63,16 @@
         initial_scale (float, optional): Initial scale used by DynamicGradScaler. Defaults to 2**32.
         min_scale (float, optional): Min scale used by DynamicGradScaler. Defaults to 1.
         growth_factor (float, optional): growth_factor used by DynamicGradScaler. Defaults to 2.
         backoff_factor (float, optional): backoff_factor used by DynamicGradScaler. Defaults to 0.5.
         growth_interval (float, optional): growth_interval used by DynamicGradScaler. Defaults to 1000.
         hysteresis (float, optional): hysteresis used by DynamicGradScaler. Defaults to 2.
         max_scale (int, optional): max_scale used by DynamicGradScaler. Defaults to 2**32.
-        dp_process_group (Optional[ProcessGroup], optional): data paralle process group. Defaults to None.
-        mp_process_group (Optional[ProcessGroup], optional): model paralle process group. Defaults to None.
+        dp_process_group (Optional[ProcessGroup], optional): data parallel process group. Defaults to None.
+        mp_process_group (Optional[ProcessGroup], optional): model parallel process group. Defaults to None.
 
     .. _PatrickStar\: Parallel Training of Pre-trained Models via Chunk-based Memory Management:
         https://arxiv.org/abs/2108.05818
     """
 
     def __init__(self,
                  sharded_model: ShardedModelV2,
@@ -270,15 +270,15 @@
     def _register_master_weight(self):
         self.master_params: Dict[Parameter, StatefulTensor] = {}
         for group in self.optim.param_groups:
             for p in group['params']:
                 assert hasattr(p, 'colo_attr'), 'The parameter must be wrapped with ShardedParam'
                 shard_flag = not p.colo_attr.sharded_data_tensor.is_sharded and p.colo_attr.is_replicated
                 if shard_flag:
-                    # we always shard replicated paramters
+                    # we always shard replicated parameters
                     self.shard_strategy.shard([p.colo_attr.sharded_data_tensor], self.dp_process_group)
                 self.master_params[p] = StatefulTensor(cast_tensor_to_fp32(p.colo_attr.data_payload.to(self.device)))
                 if shard_flag:
                     # In this branch, there's no need to shard param
                     # So we gather here
                     self.shard_strategy.gather([p.colo_attr.sharded_data_tensor], self.dp_process_group)
 
@@ -308,15 +308,15 @@
             for p in group['params']:
                 if p.colo_attr.saved_grad.is_null():
                     continue
                 p.colo_attr.saved_grad.trans_state(TensorState.COMPUTE)
                 # If reuse_fp16_shard, grad fp16 which wasn't be offloaded may be evicted to CPU
                 if not p.colo_attr.offload_grad:
                     colo_model_data_tensor_move_inline(p.colo_attr.saved_grad, torch.cuda.current_device())
-                # FIXME(ver217): p.data here is an empty tensor on CUDA and has no useful infomation
+                # FIXME(ver217): p.data here is an empty tensor on CUDA and has no useful information
                 # If we change p.grad directly
                 # it may raise error because of different shape/dtype/device of p.data and p.grad
                 # We just set p.data = p.colo_attr.saved_grad.payload here
                 p.data = p.colo_attr.grad_payload
                 p.grad = p.colo_attr.grad_payload
                 # Set p.data to empty tensor, in case of memory leaking
                 p.colo_attr.set_data_none()
@@ -329,15 +329,15 @@
                 self.master_params[p].trans_state(TensorState.COMPUTE)
                 p.data = self.master_params[p].payload
                 # Now p.data is sharded
                 # So optimizer states are sharded naturally
 
     def _copy_master_model_to_model_fp16(self):
         # Copy master param data (fp32) to payload of colo_attr (fp16)
-        # TODO() improve efficiency by gathering tensors into a chunk and transfering
+        # TODO() improve efficiency by gathering tensors into a chunk and transferring
         # a chunk.
         for group in self.optim.param_groups:
             for p in group['params']:
                 self._copy_master_param_to_param_fp16(p)
 
     def _copy_master_param_to_param_fp16(self, p):
         # flush gradient
@@ -346,15 +346,15 @@
             # in order to use copy below, we should give sharded data tensor a payload
             p.colo_attr.sharded_data_tensor.payload_relay(p.colo_attr.saved_grad)
         else:
             p.colo_attr.saved_grad.set_null()
 
         p.data = self.master_params[p].payload
 
-        # we need to allocate new memory for keep_not_shard paramters
+        # we need to allocate new memory for keep_not_shard parameters
         # in order to use copy, otherwise, the sizes of tensor is not compatible
         if p.colo_attr.data_payload.numel() != p.data.numel():
             p.colo_attr.data_payload_reset(
                 torch.empty(p.data.shape, dtype=p.colo_attr.data_payload.dtype, device=p.colo_attr.data_payload.device))
 
         # TODO() optimize this line CPU (fp32) -> GPU (fp16)
         p.colo_attr.sharded_data_tensor.payload_copy(p.half().detach())
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_param/sharded_param.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_param/sharded_param.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
+from typing import List, Optional, Tuple
+
 import torch
-from typing import Optional, Tuple
-from colossalai.zero.sharded_param.sharded_tensor import ShardedTensor
-from colossalai.gemini.tensor_utils import colo_tensor_mem_usage
-from colossalai.gemini.stateful_tensor import StatefulTensor, TensorState
-from typing import List
+
+from colossalai.zero.legacy.gemini.stateful_tensor import StatefulTensor, TensorState
+from colossalai.zero.legacy.gemini.tensor_utils import colo_tensor_mem_usage
+
+from .sharded_tensor import ShardedTensor
 
 EMPTY_TENSOR_DICT = {}
 
 
 def get_empty_tensor(device: torch.device, dtype: torch.dtype):
     key = (device, dtype)
     if key not in EMPTY_TENSOR_DICT:
```

### Comparing `colossalai-0.2.8/colossalai/zero/sharded_param/sharded_tensor.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_param/sharded_tensor.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import torch
-from colossalai.gemini.stateful_tensor import StatefulTensor, TensorState
+
+from colossalai.zero.legacy.gemini.stateful_tensor import StatefulTensor, TensorState
 
 
 class ShardedTensor(StatefulTensor):
 
     def __init__(self, tensor: torch.Tensor, state: TensorState = TensorState.HOLD) -> None:
         r"""
         A tensor sharded in multiple processes. Constructed from an existing torch.Tensor instance.
```

### Comparing `colossalai-0.2.8/colossalai/zero/utils/gemini_hook.py` & `colossalai-0.3.0/colossalai/zero/gemini/gemini_hook.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from contextlib import contextmanager
 from enum import Enum
 from functools import partial
 from typing import List
 
 import torch
 
-from colossalai.gemini import TensorState
-from colossalai.gemini.gemini_mgr import GeminiManager
 from colossalai.tensor.param_op_hook import ColoParamOpHook
 from colossalai.utils import is_ddp_ignored
+from colossalai.zero.gemini import TensorState
+from colossalai.zero.gemini.gemini_mgr import GeminiManager
 
 
 class TrainingPhase(Enum):
     FORWARD = 0
     BACKWARD = 1
```

### Comparing `colossalai-0.2.8/colossalai/zero/utils/zero_hook.py` & `colossalai-0.3.0/colossalai/zero/legacy/sharded_model/zero_hook.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 from typing import Optional
 
 import torch
 import torch.distributed as dist
 
-from colossalai.gemini.memory_tracer import MemStatsCollector
-from colossalai.gemini.ophooks import BaseOpHook
-from colossalai.gemini.stateful_tensor import TensorState
-from colossalai.gemini.stateful_tensor_mgr import StatefulTensorMgr
 from colossalai.logging import get_dist_logger
 from colossalai.registry import OPHOOKS
 from colossalai.utils import get_current_device
-from colossalai.zero.shard_utils import BaseShardStrategy
+from colossalai.zero.gemini.memory_tracer import MemStatsCollector
+from colossalai.zero.legacy.gemini.ophooks import BaseOpHook
+from colossalai.zero.legacy.gemini.stateful_tensor import TensorState
+from colossalai.zero.legacy.gemini.stateful_tensor_mgr import StatefulTensorMgr
+from colossalai.zero.legacy.shard_utils import BaseShardStrategy
 
 
 @OPHOOKS.register_module
 class ZeroHook(BaseOpHook):
     """
     A hook to process sharded param for ZeRO method.
     Warning: this class has been deprecated after version 0.1.12
```

### Comparing `colossalai-0.2.8/colossalai.egg-info/PKG-INFO` & `colossalai-0.3.0/colossalai.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: colossalai
-Version: 0.2.8
+Version: 0.3.0
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
 Project-URL: Github, https://github.com/hpcaitech/ColossalAI
 Description: # Colossal-AI
         <div id="top" align="center">
         
            [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)
         
-           Colossal-AI: Making large AI models cheaper, faster and more accessible
+           Colossal-AI: Making large AI models cheaper, faster, and more accessible
         
            <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> |
            <a href="https://www.colossalai.org/"> Documentation </a> |
            <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> |
            <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> |
            <a href="https://medium.com/@hpcaitech"> Blog </a></h3>
         
@@ -33,26 +33,35 @@
         
            | [English](README.md) | [中文](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
         * [2023/03] [ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+        * [2023/03] [Intel and Colossal-AI Partner to Deliver Cost-Efficient Open-Source Solution for Protein Folding Structure Prediction](https://www.hpc-ai.tech/blog/intel-habana)
         * [2023/03] [AWS and Google Fund Colossal-AI with Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-google-fund-colossal-ai-with-startup-cloud-programs)
         * [2023/02] [Open Source Solution Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU Memory](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt)
         * [2023/01] [Hardware Savings Up to 46 Times for AIGC and  Automatic Parallelism](https://medium.com/pytorch/latest-colossal-ai-boasts-novel-automatic-parallelism-and-offers-savings-up-to-46x-for-stable-1453b48f3f02)
         * [2022/11] [Diffusion Pretraining and Hardware Fine-Tuning Can Be Almost 7X Cheaper](https://www.hpc-ai.tech/blog/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper)
         * [2022/10] [Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference Sequence Exceeding 10,000](https://www.hpc-ai.tech/blog/use-a-laptop-to-analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding)
         * [2022/09] [HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising](https://www.hpc-ai.tech/blog/hpc-ai-tech-completes-6-million-seed-and-angel-round-fundraising-led-by-bluerun-ventures-in-the)
         
         ## Table of Contents
         <ul>
          <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>
          <li><a href="#Features">Features</a> </li>
          <li>
+           <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
+           <ul>
+             <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
+             <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
+             <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
+           </ul>
+         </li>
+         <li>
            <a href="#Parallel-Training-Demo">Parallel Training Demo</a>
            <ul>
              <li><a href="#GPT-3">GPT-3</a></li>
              <li><a href="#GPT-2">GPT-2</a></li>
              <li><a href="#BERT">BERT</a></li>
              <li><a href="#PaLM">PaLM</a></li>
              <li><a href="#OPT">OPT</a></li>
@@ -71,22 +80,14 @@
            <a href="#Inference-Energon-AI-Demo">Inference (Energon-AI) Demo</a>
            <ul>
              <li><a href="#GPT-3-Inference">GPT-3</a></li>
              <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
              <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
-           <li>
-           <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
-           <ul>
-             <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
-             <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
-             <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
-           </ul>
-         </li>
          <li>
            <a href="#Installation">Installation</a>
            <ul>
              <li><a href="#PyPI">PyPI</a></li>
              <li><a href="#Install-From-Source">Install From Source</a></li>
            </ul>
          </li>
@@ -121,29 +122,121 @@
           - [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)
           - [Auto-Parallelism](https://arxiv.org/abs/2302.02599)
         
         - Heterogeneous Memory Management
           - [PatrickStar](https://arxiv.org/abs/2108.05818)
         
         - Friendly Usage
-          - Parallelism based on configuration file
+          - Parallelism based on the configuration file
         
         - Inference
           - [Energon-AI](https://github.com/hpcaitech/EnergonAI)
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
+        ## Colossal-AI in the Real World
+        
+        ### ColossalChat
+        
+        <div align="center">
+           <a href="https://www.youtube.com/watch?v=HcTiHzApHm0">
+           <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png" width="700" />
+           </a>
+        </div>
+        
+        [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)
+        [[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+        [[demo]](https://www.youtube.com/watch?v=HcTiHzApHm0)
+        [[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)
+        
+        <p id="ColossalChat-Speed" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg" width=450/>
+        </p>
+        
+        - Up to 10 times faster for RLHF PPO Stage3 Training
+        
+        <p id="ColossalChat_scaling" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>
+        </p>
+        
+        - Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference
+        
+        <p id="ColossalChat-1GPU" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>
+        </p>
+        
+        - Up to 10.3x growth in model capacity on one GPU
+        - A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)
+        
+        <p id="ColossalChat-LoRA" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>
+        </p>
+        
+        - Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU
+        - Keep at a sufficiently high running speed
+        
+        <p align="right">(<a href="#top">back to top</a>)</p>
+        
+        
+        ### AIGC
+        Acceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).
+        <p id="diffusion_train" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>
+        </p>
+        
+        - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).
+        
+        <p id="diffusion_demo" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>
+        </p>
+        
+        - [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.
+        
+        <p id="inference" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>
+        </p>
+        
+        - [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.
+        
+        
+        <p align="right">(<a href="#top">back to top</a>)</p>
+        
+        ### Biomedicine
+        Acceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)
+        
+        <p id="FastFold" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>
+        </p>
+        
+        - [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.
+        
+        <p id="FastFold-Intel" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>
+        </p>
+        
+        - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.
+        
+        <p id="xTrimoMultimer" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>
+        </p>
+        
+        - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
+        
+        
+        <p align="right">(<a href="#top">back to top</a>)</p>
+        
         ## Parallel Training Demo
         
         ### GPT-3
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png" width=700/>
         </p>
         
-        - Save 50% GPU resources, and 10.7% acceleration
+        - Save 50% GPU resources and 10.7% acceleration
         
         ### GPT-2
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png" width=800/>
         
         - 11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism
         
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png" width=800>
@@ -157,15 +250,15 @@
         
         ### PaLM
         - [PaLM-colossalai](https://github.com/hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)).
         
         ### OPT
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png" width=800/>
         
-        - [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because public pretrained model weights.
+        - [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.
         - 45% speedup fine-tuning OPT at low cost in lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/opt_service)
         
         Please visit our [documentation](https://www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more details.
         
         ### ViT
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png" width="450" />
@@ -221,104 +314,24 @@
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
         </p>
         
         - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
-        ## Colossal-AI in the Real World
-        
-        ### ColossalChat
-        
-        <div align="center">
-           <a href="https://chat.colossalai.org/">
-           <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Chat-demo.png" width="700" />
-           </a>
-        </div>
-        
-        [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline. [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) [[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://chat.colossalai.org)
-        
-        <p id="ColossalChat_scaling" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>
-        </p>
-        
-        - Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference
-        
-        <p id="ColossalChat-1GPU" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>
-        </p>
-        
-        - Up to 10.3x growth in model capacity on one GPU
-        - A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)
-        
-        <p id="ColossalChat-LoRA" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>
-        </p>
-        
-        - Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU
-        - Keep in a sufficiently high running speed
-        
-        <p align="right">(<a href="#top">back to top</a>)</p>
-        
-        
-        ### AIGC
-        Acceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).
-        <p id="diffusion_train" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>
-        </p>
-        
-        - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).
-        
-        <p id="diffusion_demo" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>
-        </p>
-        
-        - [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.
-        
-        <p id="inference" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>
-        </p>
-        
-        - [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.
-        
-        
-        <p align="right">(<a href="#top">back to top</a>)</p>
-        
-        ### Biomedicine
-        Acceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)
-        
-        <p id="FastFold" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>
-        </p>
-        
-        - [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.
-        
-        <p id="FastFold-Intel" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>
-        </p>
-        
-        - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.
-        
-        <p id="xTrimoMultimer" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>
-        </p>
-        
-        - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
-        
-        
-        <p align="right">(<a href="#top">back to top</a>)</p>
-        
         ## Installation
         
         Requirements:
         - PyTorch >= 1.11 (PyTorch 2.x in progress)
         - Python >= 3.7
         - CUDA >= 11.0
+        - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
+        - Linux OS
         
-        If you encounter any problem about installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+        If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
         
         ### Install from PyPI
         
         You can easily install Colossal-AI with the following command. **By default, we do not build PyTorch extensions during installation.**
         
         ```bash
         pip install colossalai
@@ -328,26 +341,26 @@
         
         However, if you want to build the PyTorch extensions during installation, you can set `CUDA_EXT=1`.
         
         ```bash
         CUDA_EXT=1 pip install colossalai
         ```
         
-        **Otherwise, CUDA kernels will be built during runtime when you actually need it.**
+        **Otherwise, CUDA kernels will be built during runtime when you actually need them.**
         
-        We also keep release the nightly version to PyPI on a weekly basis. This allows you to access the unreleased features and bug fixes in the main branch.
+        We also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch.
         Installation can be made via
         
         ```bash
         pip install colossalai-nightly
         ```
         
         ### Download From Source
         
-        > The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problem. :)
+        > The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)
         
         ```shell
         git clone https://github.com/hpcaitech/ColossalAI.git
         cd ColossalAI
         
         # install colossalai
         pip install .
@@ -356,14 +369,30 @@
         By default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
         If you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):
         
         ```shell
         CUDA_EXT=1 pip install .
         ```
         
+        For Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.
+        
+        ```bash
+        # clone the repository
+        git clone https://github.com/hpcaitech/ColossalAI.git
+        cd ColossalAI
+        
+        # download the cub library
+        wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip
+        unzip 1.8.0.zip
+        cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/
+        
+        # install
+        CUDA_EXT=1 pip install .
+        ```
+        
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Use Docker
         
         ### Pull from DockerHub
         
         You can directly pull the docker image from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The image is automatically uploaded upon release.
@@ -402,17 +431,18 @@
         You may contact us or participate in the following ways:
         1. [Leaving a Star ⭐](https://github.com/hpcaitech/ColossalAI/stargazers) to show your like and support. Thanks!
         2. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/CONTRIBUTING.md)
         3. Send your official proposal to email contact@hpcaitech.com
         
         Thanks so much to all of our amazing contributors!
         
-        <a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors"><img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/contributor_avatar.png" width="800px"></a>
+        <a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors">
+          <img src="https://contrib.rocks/image?repo=hpcaitech/ColossalAI"  width="800px"/>
+        </a>
         
-        *The order of contributor avatars is randomly shuffled.*
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## CI/CD
         
         We leverage the power of [GitHub Actions](https://github.com/features/actions) to automate our development, release and deployment workflows. Please check out this [documentation](.github/workflows/README.md) on how the automated workflows are operated.
@@ -429,15 +459,15 @@
           title={Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
           author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
           journal={arXiv preprint arXiv:2110.14883},
           year={2021}
         }
         ```
         
-        Colossal-AI has been accepted as official tutorials by top conference [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
+        Colossal-AI has been accepted as official tutorial by top conferences [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Environment :: GPU :: NVIDIA CUDA
```

#### html2text {}

```diff
@@ -1,18 +1,18 @@
-Metadata-Version: 2.1 Name: colossalai Version: 0.2.8 Summary: An integrated
+Metadata-Version: 2.1 Name: colossalai Version: 0.3.0 Summary: An integrated
 large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
    [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/
   colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/
-   ) Colossal-AI: Making large AI models cheaper, faster and more accessible
+  ) Colossal-AI: Making large AI models cheaper, faster, and more accessible
            **** Paper | Documentation | Examples | Forum | Blog ****
      [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/
 ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)
      [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/
   build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/
       actions/workflows/build_on_schedule.yml) [![Documentation](https://
      readthedocs.org/projects/colossalai/badge/?version=latest)](https://
@@ -25,32 +25,39 @@
   shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w) [![WeChat badge](https://
        img.shields.io/badge/å¾®ä¿¡-å å¥-green?logo=wechat&)](https://
     raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
     WeChat.png) | [English](README.md) | [ä¸­æ](docs/README-zh-Hans.md) |
 ## Latest News * [2023/03] [ColossalChat: An Open-Source Solution for Cloning
 ChatGPT With a Complete RLHF Pipeline](https://medium.com/@yangyou_berkeley/
 colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-
-pipeline-5edf08fb538b) * [2023/03] [AWS and Google Fund Colossal-AI with
-Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-google-fund-
-colossal-ai-with-startup-cloud-programs) * [2023/02] [Open Source Solution
-Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU Memory]
-(https://www.hpc-ai.tech/blog/colossal-ai-chatgpt) * [2023/01] [Hardware
+pipeline-5edf08fb538b) * [2023/03] [Intel and Colossal-AI Partner to Deliver
+Cost-Efficient Open-Source Solution for Protein Folding Structure Prediction]
+(https://www.hpc-ai.tech/blog/intel-habana) * [2023/03] [AWS and Google Fund
+Colossal-AI with Startup Cloud Programs](https://www.hpc-ai.tech/blog/aws-and-
+google-fund-colossal-ai-with-startup-cloud-programs) * [2023/02] [Open Source
+Solution Replicates ChatGPT Training Process! Ready to go with only 1.6GB GPU
+Memory](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt) * [2023/01] [Hardware
 Savings Up to 46 Times for AIGC and Automatic Parallelism](https://medium.com/
 pytorch/latest-colossal-ai-boasts-novel-automatic-parallelism-and-offers-
 savings-up-to-46x-for-stable-1453b48f3f02) * [2022/11] [Diffusion Pretraining
 and Hardware Fine-Tuning Can Be Almost 7X Cheaper](https://www.hpc-ai.tech/
 blog/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper) *
 [2022/10] [Use a Laptop to Analyze 90% of Proteins, With a Single-GPU Inference
 Sequence Exceeding 10,000](https://www.hpc-ai.tech/blog/use-a-laptop-to-
 analyze-90-of-proteins-with-a-single-gpu-inference-sequence-exceeding) * [2022/
 09] [HPC-AI Tech Completes $6 Million Seed and Angel Round Fundraising](https:/
 /www.hpc-ai.tech/blog/hpc-ai-tech-completes-6-million-seed-and-angel-round-
 fundraising-led-by-bluerun-ventures-in-the) ## Table of Contents
     * Why_Colossal-AI
     * Features
+    * Colossal-AI_for_Real_World_Applications
+          o ColossalChat:_An_Open-Source_Solution_for_Cloning_ChatGPT_With_a
+            Complete_RLHF_Pipeline
+          o AIGC:_Acceleration_of_Stable_Diffusion
+          o Biomedicine:_Acceleration_of_AlphaFold_Protein_Structure
     * Parallel_Training_Demo
           o GPT-3
           o GPT-2
           o BERT
           o PaLM
           o OPT
           o ViT
@@ -58,19 +65,14 @@
     * Single_GPU_Training_Demo
           o GPT-2
           o PaLM
     * Inference_(Energon-AI)_Demo
           o GPT-3
           o OPT-175B_Online_Serving_for_Text_Generation
           o 176B_BLOOM
-    * Colossal-AI_for_Real_World_Applications
-          o ColossalChat:_An_Open-Source_Solution_for_Cloning_ChatGPT_With_a
-            Complete_RLHF_Pipeline
-          o AIGC:_Acceleration_of_Stable_Diffusion
-          o Biomedicine:_Acceleration_of_AlphaFold_Protein_Structure
     * Installation
           o PyPI
           o Install_From_Source
     * Use_Docker
     * Community
     * Contributing
     * Cite_Us
@@ -85,96 +87,43 @@
 kickstart distributed training and inference in a few lines. - Parallelism
 strategies - Data Parallelism - Pipeline Parallelism - 1D, [2D](https://
 arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D]
 (https://arxiv.org/abs/2105.14450) Tensor Parallelism - [Sequence Parallelism]
 (https://arxiv.org/abs/2105.13120) - [Zero Redundancy Optimizer (ZeRO)](https:/
 /arxiv.org/abs/1910.02054) - [Auto-Parallelism](https://arxiv.org/abs/
 2302.02599) - Heterogeneous Memory Management - [PatrickStar](https://
-arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on configuration
-file - Inference - [Energon-AI](https://github.com/hpcaitech/EnergonAI)
-                                                                  (back_to_top)
-## Parallel Training Demo ### GPT-3
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                 GPT3-v5.png]
-- Save 50% GPU resources, and 10.7% acceleration ### GPT-2 [https://
-raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png]
-- 11x lower GPU memory consumption, and superlinear scaling efficiency with
-Tensor Parallelism [https://raw.githubusercontent.com/hpcaitech/public_assets/
-main/colossalai/img/(updated)GPT-2.png] - 24x larger model size on the same
-hardware - over 3x acceleration ### BERT [https://raw.githubusercontent.com/
-hpcaitech/public_assets/main/colossalai/img/BERT.png] - 2x faster training, or
-50% longer sequence length ### PaLM - [PaLM-colossalai](https://github.com/
-hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways
-Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-
-model-palm-scaling-to.html)). ### OPT [https://raw.githubusercontent.com/
-hpcaitech/public_assets/main/colossalai/img/OPT_update.png] - [Open Pretrained
-Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion
-parameter AI language model released by Meta, which stimulates AI programmers
-to perform various downstream tasks and application deployments because public
-pretrained model weights. - 45% speedup fine-tuning OPT at low cost in lines. [
-[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/
-opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/
-opt_service) Please visit our [documentation](https://www.colossalai.org/) and
-[examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more
-details. ### ViT
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                   ViT.png]
-- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64 ###
-Recommendation System Models - [Cached Embedding](https://github.com/hpcaitech/
-CachedEmbedding), utilize software cache to train larger embedding tables with
-a smaller GPU memory budget.
-                                                                  (back_to_top)
-## Single GPU Training Demo ### GPT-2
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                GPT2-GPU1.png]
-- 20x larger model size on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                GPT2-NVME.png]
-- 120x larger model size on the same hardware (RTX 3080) ### PaLM
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                PaLM-GPU1.png]
-- 34x larger model size on the same hardware
-                                                                  (back_to_top)
-## Inference (Energon-AI) Demo
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             inference_GPT-3.jpg]
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
-acceleration on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             BLOOM%20serving.png]
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
-Try 175-billion-parameter OPT online services
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                            BLOOM%20Inference.PNG]
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
-Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
-times.
+arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on the
+configuration file - Inference - [Energon-AI](https://github.com/hpcaitech/
+EnergonAI)
                                                                   (back_to_top)
 ## Colossal-AI in the Real World ### ColossalChat
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                                Chat-demo.png]
+ [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
+                       chat/ColossalChat%20YouTube.png]
 [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/
 Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/
 chatgpt/) with a complete RLHF pipeline. [[code]](https://github.com/hpcaitech/
 ColossalAI/tree/main/applications/Chat) [[blog]](https://medium.com/
 @yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-
-with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://
-chat.colossalai.org)
+with-a-complete-rlhf-pipeline-5edf08fb538b) [[demo]](https://www.youtube.com/
+watch?v=HcTiHzApHm0) [[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)
+ [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
+                        chat/ColossalChat%20Speed.jpg]
+- Up to 10 times faster for RLHF PPO Stage3 Training
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                         chatgpt/ChatGPT%20scaling.png]
 - Up to 7.73 times faster for single server training and 1.42 times faster for
 single-GPU inference
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                            chatgpt/ChatGPT-1GPU.jpg]
 - Up to 10.3x growth in model capacity on one GPU - A mini demo training
 process requires only 1.62GB of GPU memory (any consumer-grade GPU)
  [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
                            chatgpt/LoRA%20data.jpg]
 - Increase the capacity of the fine-tuning model by up to 3.7 times on a single
-GPU - Keep in a sufficiently high running speed
+GPU - Keep at a sufficiently high running speed
                                                                   (back_to_top)
 ### AIGC Acceleration of AIGC (AI-Generated Content) models such as [Stable
 Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable
 Diffusion v2](https://github.com/Stability-AI/stablediffusion).
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                          Stable%20Diffusion%20v2.png]
 - [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/
@@ -202,34 +151,99 @@
 - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference
 acceleration and 39% cost reduce.
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                            xTrimoMultimer_Table.jpg]
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer):
 accelerating structure prediction of protein monomers and multimer by 11x.
                                                                   (back_to_top)
+## Parallel Training Demo ### GPT-3
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                 GPT3-v5.png]
+- Save 50% GPU resources and 10.7% acceleration ### GPT-2 [https://
+raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png]
+- 11x lower GPU memory consumption, and superlinear scaling efficiency with
+Tensor Parallelism [https://raw.githubusercontent.com/hpcaitech/public_assets/
+main/colossalai/img/(updated)GPT-2.png] - 24x larger model size on the same
+hardware - over 3x acceleration ### BERT [https://raw.githubusercontent.com/
+hpcaitech/public_assets/main/colossalai/img/BERT.png] - 2x faster training, or
+50% longer sequence length ### PaLM - [PaLM-colossalai](https://github.com/
+hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways
+Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-
+model-palm-scaling-to.html)). ### OPT [https://raw.githubusercontent.com/
+hpcaitech/public_assets/main/colossalai/img/OPT_update.png] - [Open Pretrained
+Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion
+parameter AI language model released by Meta, which stimulates AI programmers
+to perform various downstream tasks and application deployments because of
+public pre-trained model weights. - 45% speedup fine-tuning OPT at low cost in
+lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/
+language/opt) [[Online Serving]](https://colossalai.org/docs/
+advanced_tutorials/opt_service) Please visit our [documentation](https://
+www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/
+tree/main/examples) for more details. ### ViT
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                   ViT.png]
+- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64 ###
+Recommendation System Models - [Cached Embedding](https://github.com/hpcaitech/
+CachedEmbedding), utilize software cache to train larger embedding tables with
+a smaller GPU memory budget.
+                                                                  (back_to_top)
+## Single GPU Training Demo ### GPT-2
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                GPT2-GPU1.png]
+- 20x larger model size on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                GPT2-NVME.png]
+- 120x larger model size on the same hardware (RTX 3080) ### PaLM
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                                PaLM-GPU1.png]
+- 34x larger model size on the same hardware
+                                                                  (back_to_top)
+## Inference (Energon-AI) Demo
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             inference_GPT-3.jpg]
+- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
+acceleration on the same hardware
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                             BLOOM%20serving.png]
+- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
+Try 175-billion-parameter OPT online services
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                            BLOOM%20Inference.PNG]
+- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
+Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
+times.
+                                                                  (back_to_top)
 ## Installation Requirements: - PyTorch >= 1.11 (PyTorch 2.x in progress) -
-Python >= 3.7 - CUDA >= 11.0 If you encounter any problem about installation,
-you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/
-issues/new/choose) in this repository. ### Install from PyPI You can easily
-install Colossal-AI with the following command. **By default, we do not build
-PyTorch extensions during installation.** ```bash pip install colossalai ```
-**Note: only Linux is supported for now.** However, if you want to build the
-PyTorch extensions during installation, you can set `CUDA_EXT=1`. ```bash
-CUDA_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
-during runtime when you actually need it.** We also keep release the nightly
-version to PyPI on a weekly basis. This allows you to access the unreleased
-features and bug fixes in the main branch. Installation can be made via ```bash
-pip install colossalai-nightly ``` ### Download From Source > The version of
-Colossal-AI will be in line with the main branch of the repository. Feel free
-to raise an issue if you encounter any problem. :) ```shell git clone https://
-github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
-install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
-build them during runtime. If you want to install and enable CUDA kernel fusion
-(compulsory installation when using fused optimizer): ```shell CUDA_EXT=1 pip
-install . ```
+Python >= 3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
+developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
+you encounter any problem with installation, you may want to raise an [issue]
+(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
+### Install from PyPI You can easily install Colossal-AI with the following
+command. **By default, we do not build PyTorch extensions during
+installation.** ```bash pip install colossalai ``` **Note: only Linux is
+supported for now.** However, if you want to build the PyTorch extensions
+during installation, you can set `CUDA_EXT=1`. ```bash CUDA_EXT=1 pip install
+colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
+actually need them.** We also keep releasing the nightly version to PyPI every
+week. This allows you to access the unreleased features and bug fixes in the
+main branch. Installation can be made via ```bash pip install colossalai-
+nightly ``` ### Download From Source > The version of Colossal-AI will be in
+line with the main branch of the repository. Feel free to raise an issue if you
+encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
+ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
+we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
+If you want to install and enable CUDA kernel fusion (compulsory installation
+when using fused optimizer): ```shell CUDA_EXT=1 pip install . ``` For Users
+with CUDA 10.2, you can still build ColossalAI from source. However, you need
+to manually download the cub library and copy it to the corresponding
+directory. ```bash # clone the repository git clone https://github.com/
+hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
+github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
+1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
+CUDA_EXT=1 pip install . ```
                                                                   (back_to_top)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
@@ -252,32 +266,30 @@
 join and build the Colossal-AI community, making efforts towards the era of big
 AI models! You may contact us or participate in the following ways: 1. [Leaving
 a Star â­](https://github.com/hpcaitech/ColossalAI/stargazers) to show your
 like and support. Thanks! 2. Posting an [issue](https://github.com/hpcaitech/
 ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the
 guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/
 CONTRIBUTING.md) 3. Send your official proposal to email contact@hpcaitech.com
-Thanks so much to all of our amazing contributors! [https://
-raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-contributor_avatar.png] *The order of contributor avatars is randomly
-shuffled.*
+Thanks so much to all of our amazing contributors! [https://contrib.rocks/
+image?repo=hpcaitech/ColossalAI]
                                                                   (back_to_top)
 ## CI/CD We leverage the power of [GitHub Actions](https://github.com/features/
 actions) to automate our development, release and deployment workflows. Please
 check out this [documentation](.github/workflows/README.md) on how the
 automated workflows are operated. ## Cite Us This project is inspired by some
 related projects (some by our team and some by other organizations). We would
 like to credit these amazing projects as listed in the [Reference List](./docs/
 REFERENCE.md). To cite this project, you can use the following BibTeX citation.
 ``` @article{bian2021colossal, title={Colossal-AI: A Unified Deep Learning
 System For Large-Scale Parallel Training}, author={Bian, Zhengda and Liu,
 Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui
 and Cui, Fan and You, Yang}, journal={arXiv preprint arXiv:2110.14883}, year=
-{2021} } ``` Colossal-AI has been accepted as official tutorials by top
-conference [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/
+{2021} } ``` Colossal-AI has been accepted as official tutorial by top
+conferences [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/
 Conferences/AAAI-23/), [PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://
 cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), etc.
                                                                   (back_to_top)
 Platform: UNKNOWN Classifier: Programming Language :: Python :: 3 Classifier:
 License :: OSI Approved :: Apache Software License Classifier: Environment ::
 GPU :: NVIDIA CUDA Classifier: Topic :: Scientific/Engineering :: Artificial
 Intelligence Classifier: Topic :: System :: Distributed Computing Requires-
```

### Comparing `colossalai-0.2.8/colossalai.egg-info/SOURCES.txt` & `colossalai-0.3.0/colossalai.egg-info/SOURCES.txt`

 * *Files 6% similar despite different names*

```diff
@@ -12,14 +12,35 @@
 colossalai.egg-info/PKG-INFO
 colossalai.egg-info/SOURCES.txt
 colossalai.egg-info/dependency_links.txt
 colossalai.egg-info/entry_points.txt
 colossalai.egg-info/requires.txt
 colossalai.egg-info/top_level.txt
 colossalai/_C/__init__.py
+colossalai/_analyzer/__init__.py
+colossalai/_analyzer/envs.py
+colossalai/_analyzer/_subclasses/__init__.py
+colossalai/_analyzer/_subclasses/_meta_registration.py
+colossalai/_analyzer/_subclasses/_monkey_patch.py
+colossalai/_analyzer/_subclasses/flop_tensor.py
+colossalai/_analyzer/_subclasses/meta_tensor.py
+colossalai/_analyzer/fx/__init__.py
+colossalai/_analyzer/fx/codegen.py
+colossalai/_analyzer/fx/graph_module.py
+colossalai/_analyzer/fx/node_util.py
+colossalai/_analyzer/fx/symbolic_profile.py
+colossalai/_analyzer/fx/passes/__init__.py
+colossalai/_analyzer/fx/passes/graph_profile.py
+colossalai/_analyzer/fx/passes/shape_prop.py
+colossalai/_analyzer/fx/tracer/__init__.py
+colossalai/_analyzer/fx/tracer/bias_addition.py
+colossalai/_analyzer/fx/tracer/custom_leaf_module.py
+colossalai/_analyzer/fx/tracer/proxy.py
+colossalai/_analyzer/fx/tracer/symbolic_trace.py
+colossalai/_analyzer/fx/tracer/tracer.py
 colossalai/amp/__init__.py
 colossalai/amp/amp_type.py
 colossalai/amp/apex_amp/__init__.py
 colossalai/amp/apex_amp/apex_amp.py
 colossalai/amp/naive_amp/__init__.py
 colossalai/amp/naive_amp/_fp16_optimizer.py
 colossalai/amp/naive_amp/_utils.py
@@ -36,16 +57,16 @@
 colossalai/auto_parallel/checkpoint/build_c_ext.py
 colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
 colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
 colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
 colossalai/auto_parallel/checkpoint/operation.py
 colossalai/auto_parallel/meta_profiler/__init__.py
 colossalai/auto_parallel/meta_profiler/constants.py
-colossalai/auto_parallel/meta_profiler/metainfo.py
 colossalai/auto_parallel/meta_profiler/registry.py
+colossalai/auto_parallel/meta_profiler/shard_metainfo.py
 colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
 colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
 colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
 colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
 colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
 colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
 colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
@@ -134,25 +155,32 @@
 colossalai/auto_parallel/tensor_shard/utils/sharding.py
 colossalai/booster/__init__.py
 colossalai/booster/accelerator.py
 colossalai/booster/booster.py
 colossalai/booster/mixed_precision/__init__.py
 colossalai/booster/mixed_precision/bf16.py
 colossalai/booster/mixed_precision/fp16_apex.py
+colossalai/booster/mixed_precision/fp16_naive.py
 colossalai/booster/mixed_precision/fp16_torch.py
 colossalai/booster/mixed_precision/fp8.py
 colossalai/booster/mixed_precision/mixed_precision_base.py
 colossalai/booster/plugin/__init__.py
+colossalai/booster/plugin/dp_plugin_base.py
+colossalai/booster/plugin/gemini_plugin.py
+colossalai/booster/plugin/low_level_zero_plugin.py
 colossalai/booster/plugin/plugin_base.py
 colossalai/booster/plugin/torch_ddp_plugin.py
+colossalai/booster/plugin/torch_fsdp_plugin.py
 colossalai/builder/__init__.py
 colossalai/builder/builder.py
 colossalai/checkpoint_io/__init__.py
 colossalai/checkpoint_io/checkpoint_io_base.py
 colossalai/checkpoint_io/general_checkpoint_io.py
+colossalai/checkpoint_io/index_file.py
+colossalai/checkpoint_io/utils.py
 colossalai/cli/__init__.py
 colossalai/cli/cli.py
 colossalai/cli/benchmark/__init__.py
 colossalai/cli/benchmark/benchmark.py
 colossalai/cli/benchmark/models.py
 colossalai/cli/benchmark/utils.py
 colossalai/cli/check/__init__.py
@@ -291,43 +319,14 @@
 colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
 colossalai/fx/tracer/meta_patch/patched_module/convolution.py
 colossalai/fx/tracer/meta_patch/patched_module/embedding.py
 colossalai/fx/tracer/meta_patch/patched_module/linear.py
 colossalai/fx/tracer/meta_patch/patched_module/normalization.py
 colossalai/fx/tracer/meta_patch/patched_module/pooling.py
 colossalai/fx/tracer/meta_patch/patched_module/rnn.py
-colossalai/gemini/__init__.py
-colossalai/gemini/gemini_context.py
-colossalai/gemini/gemini_mgr.py
-colossalai/gemini/placement_policy.py
-colossalai/gemini/stateful_tensor.py
-colossalai/gemini/stateful_tensor_mgr.py
-colossalai/gemini/tensor_placement_policy.py
-colossalai/gemini/tensor_utils.py
-colossalai/gemini/chunk/__init__.py
-colossalai/gemini/chunk/chunk.py
-colossalai/gemini/chunk/manager.py
-colossalai/gemini/chunk/search_utils.py
-colossalai/gemini/chunk/utils.py
-colossalai/gemini/memory_tracer/__init__.py
-colossalai/gemini/memory_tracer/chunk_memstats_collector.py
-colossalai/gemini/memory_tracer/memory_monitor.py
-colossalai/gemini/memory_tracer/memory_stats.py
-colossalai/gemini/memory_tracer/memstats_collector.py
-colossalai/gemini/memory_tracer/param_runtime_order.py
-colossalai/gemini/memory_tracer/runtime_mem_tracer.py
-colossalai/gemini/memory_tracer/static_memstats_collector.py
-colossalai/gemini/memory_tracer/utils.py
-colossalai/gemini/ophooks/__init__.py
-colossalai/gemini/ophooks/_shard_grad_ophook.py
-colossalai/gemini/ophooks/_shard_param_ophook.py
-colossalai/gemini/ophooks/runtime_mem_tracer_hook.py
-colossalai/gemini/ophooks/utils.py
-colossalai/gemini/paramhooks/__init__.py
-colossalai/gemini/paramhooks/_param_hookmgr.py
 colossalai/interface/__init__.py
 colossalai/interface/model.py
 colossalai/interface/optimizer.py
 colossalai/kernel/__init__.py
 colossalai/kernel/cuda_native/__init__.py
 colossalai/kernel/cuda_native/flash_attention.py
 colossalai/kernel/cuda_native/layer_norm.py
@@ -411,14 +410,15 @@
 colossalai/nn/layer/colossalai_layer/_utils.py
 colossalai/nn/layer/colossalai_layer/dropout.py
 colossalai/nn/layer/colossalai_layer/embedding.py
 colossalai/nn/layer/colossalai_layer/linear.py
 colossalai/nn/layer/colossalai_layer/normalization.py
 colossalai/nn/layer/moe/__init__.py
 colossalai/nn/layer/moe/_operation.py
+colossalai/nn/layer/moe/checkpoint.py
 colossalai/nn/layer/moe/experts.py
 colossalai/nn/layer/moe/layers.py
 colossalai/nn/layer/moe/routers.py
 colossalai/nn/layer/moe/utils.py
 colossalai/nn/layer/parallel_1d/__init__.py
 colossalai/nn/layer/parallel_1d/_operation.py
 colossalai/nn/layer/parallel_1d/_utils.py
@@ -466,26 +466,21 @@
 colossalai/nn/metric/accuracy_3d.py
 colossalai/nn/optimizer/__init__.py
 colossalai/nn/optimizer/colossalai_optimizer.py
 colossalai/nn/optimizer/cpu_adam.py
 colossalai/nn/optimizer/fused_adam.py
 colossalai/nn/optimizer/fused_lamb.py
 colossalai/nn/optimizer/fused_sgd.py
-colossalai/nn/optimizer/gemini_optimizer.py
 colossalai/nn/optimizer/hybrid_adam.py
 colossalai/nn/optimizer/lamb.py
 colossalai/nn/optimizer/lars.py
 colossalai/nn/optimizer/nvme_optimizer.py
-colossalai/nn/optimizer/zero_optimizer.py
 colossalai/nn/parallel/__init__.py
 colossalai/nn/parallel/data_parallel.py
-colossalai/nn/parallel/gemini_parallel.py
 colossalai/nn/parallel/reducer.py
-colossalai/nn/parallel/utils.py
-colossalai/nn/parallel/zero_wrapper.py
 colossalai/nn/parallel/layers/__init__.py
 colossalai/nn/parallel/layers/colo_module.py
 colossalai/nn/parallel/layers/embedding.py
 colossalai/nn/parallel/layers/linear.py
 colossalai/nn/parallel/layers/module_utils.py
 colossalai/nn/parallel/layers/cache_embedding/__init__.py
 colossalai/nn/parallel/layers/cache_embedding/base_embedding.py
@@ -569,15 +564,14 @@
 colossalai/utils/checkpoint_io/reader.py
 colossalai/utils/checkpoint_io/utils.py
 colossalai/utils/checkpoint_io/writer.py
 colossalai/utils/data_sampler/__init__.py
 colossalai/utils/data_sampler/base_sampler.py
 colossalai/utils/data_sampler/data_parallel_sampler.py
 colossalai/utils/model/__init__.py
-colossalai/utils/model/colo_init_context.py
 colossalai/utils/model/experimental.py
 colossalai/utils/model/lazy_init_context.py
 colossalai/utils/model/utils.py
 colossalai/utils/multi_tensor_apply/__init__.py
 colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
 colossalai/utils/profiler/__init__.py
 colossalai/utils/profiler/extention.py
@@ -588,42 +582,78 @@
 colossalai/utils/profiler/legacy/pcie_profiler.py
 colossalai/utils/profiler/legacy/prof_utils.py
 colossalai/utils/rank_recorder/__init__.py
 colossalai/utils/rank_recorder/rank_recorder.py
 colossalai/utils/tensor_detector/__init__.py
 colossalai/utils/tensor_detector/tensor_detector.py
 colossalai/zero/__init__.py
-colossalai/zero/init_ctx/__init__.py
-colossalai/zero/init_ctx/init_context.py
-colossalai/zero/shard_utils/__init__.py
-colossalai/zero/shard_utils/base_shard_strategy.py
-colossalai/zero/shard_utils/bucket_tensor_shard_strategy.py
-colossalai/zero/shard_utils/commons.py
-colossalai/zero/shard_utils/tensor_shard_strategy.py
-colossalai/zero/sharded_model/__init__.py
-colossalai/zero/sharded_model/_utils.py
-colossalai/zero/sharded_model/reduce_scatter.py
-colossalai/zero/sharded_model/sharded_model_v2.py
-colossalai/zero/sharded_model/utils.py
-colossalai/zero/sharded_optim/__init__.py
-colossalai/zero/sharded_optim/_utils.py
-colossalai/zero/sharded_optim/low_level_optim.py
-colossalai/zero/sharded_optim/sharded_optim_v2.py
-colossalai/zero/sharded_optim/bookkeeping/__init__.py
-colossalai/zero/sharded_optim/bookkeeping/base_store.py
-colossalai/zero/sharded_optim/bookkeeping/bucket_store.py
-colossalai/zero/sharded_optim/bookkeeping/gradient_store.py
-colossalai/zero/sharded_optim/bookkeeping/parameter_store.py
-colossalai/zero/sharded_optim/bookkeeping/tensor_bucket.py
-colossalai/zero/sharded_param/__init__.py
-colossalai/zero/sharded_param/sharded_param.py
-colossalai/zero/sharded_param/sharded_tensor.py
-colossalai/zero/utils/__init__.py
-colossalai/zero/utils/gemini_hook.py
-colossalai/zero/utils/zero_hook.py
+colossalai/zero/wrapper.py
+colossalai/zero/gemini/__init__.py
+colossalai/zero/gemini/colo_init_context.py
+colossalai/zero/gemini/gemini_ddp.py
+colossalai/zero/gemini/gemini_hook.py
+colossalai/zero/gemini/gemini_mgr.py
+colossalai/zero/gemini/gemini_optimizer.py
+colossalai/zero/gemini/placement_policy.py
+colossalai/zero/gemini/utils.py
+colossalai/zero/gemini/chunk/__init__.py
+colossalai/zero/gemini/chunk/chunk.py
+colossalai/zero/gemini/chunk/manager.py
+colossalai/zero/gemini/chunk/search_utils.py
+colossalai/zero/gemini/chunk/utils.py
+colossalai/zero/gemini/memory_tracer/__init__.py
+colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
+colossalai/zero/gemini/memory_tracer/memory_monitor.py
+colossalai/zero/gemini/memory_tracer/memory_stats.py
+colossalai/zero/gemini/memory_tracer/memstats_collector.py
+colossalai/zero/gemini/memory_tracer/param_runtime_order.py
+colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
+colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
+colossalai/zero/gemini/memory_tracer/utils.py
+colossalai/zero/legacy/__init__.py
+colossalai/zero/legacy/gemini/__init__.py
+colossalai/zero/legacy/gemini/gemini_context.py
+colossalai/zero/legacy/gemini/stateful_tensor.py
+colossalai/zero/legacy/gemini/stateful_tensor_mgr.py
+colossalai/zero/legacy/gemini/tensor_placement_policy.py
+colossalai/zero/legacy/gemini/tensor_utils.py
+colossalai/zero/legacy/gemini/ophooks/__init__.py
+colossalai/zero/legacy/gemini/ophooks/_shard_grad_ophook.py
+colossalai/zero/legacy/gemini/ophooks/_shard_param_ophook.py
+colossalai/zero/legacy/gemini/ophooks/runtime_mem_tracer_hook.py
+colossalai/zero/legacy/gemini/ophooks/utils.py
+colossalai/zero/legacy/gemini/paramhooks/__init__.py
+colossalai/zero/legacy/gemini/paramhooks/_param_hookmgr.py
+colossalai/zero/legacy/init_ctx/__init__.py
+colossalai/zero/legacy/init_ctx/init_context.py
+colossalai/zero/legacy/shard_utils/__init__.py
+colossalai/zero/legacy/shard_utils/base_shard_strategy.py
+colossalai/zero/legacy/shard_utils/bucket_tensor_shard_strategy.py
+colossalai/zero/legacy/shard_utils/commons.py
+colossalai/zero/legacy/shard_utils/tensor_shard_strategy.py
+colossalai/zero/legacy/sharded_model/__init__.py
+colossalai/zero/legacy/sharded_model/_utils.py
+colossalai/zero/legacy/sharded_model/reduce_scatter.py
+colossalai/zero/legacy/sharded_model/sharded_model_v2.py
+colossalai/zero/legacy/sharded_model/utils.py
+colossalai/zero/legacy/sharded_model/zero_hook.py
+colossalai/zero/legacy/sharded_optim/__init__.py
+colossalai/zero/legacy/sharded_optim/sharded_optim_v2.py
+colossalai/zero/legacy/sharded_param/__init__.py
+colossalai/zero/legacy/sharded_param/sharded_param.py
+colossalai/zero/legacy/sharded_param/sharded_tensor.py
+colossalai/zero/low_level/__init__.py
+colossalai/zero/low_level/_utils.py
+colossalai/zero/low_level/low_level_optim.py
+colossalai/zero/low_level/bookkeeping/__init__.py
+colossalai/zero/low_level/bookkeeping/base_store.py
+colossalai/zero/low_level/bookkeeping/bucket_store.py
+colossalai/zero/low_level/bookkeeping/gradient_store.py
+colossalai/zero/low_level/bookkeeping/parameter_store.py
+colossalai/zero/low_level/bookkeeping/tensor_bucket.py
 op_builder/__init__.py
 op_builder/builder.py
 op_builder/cpu_adam.py
 op_builder/fused_optim.py
 op_builder/layernorm.py
 op_builder/moe.py
 op_builder/multi_head_attn.py
@@ -662,28 +692,38 @@
 tests/kit/model_zoo/torchvision/torchvision.py
 tests/kit/model_zoo/transformers/__init__.py
 tests/kit/model_zoo/transformers/albert.py
 tests/kit/model_zoo/transformers/bert.py
 tests/kit/model_zoo/transformers/gpt.py
 tests/kit/model_zoo/transformers/opt.py
 tests/kit/model_zoo/transformers/t5.py
+tests/test_analyzer/__init__.py
+tests/test_analyzer/test_fx/__init__.py
+tests/test_analyzer/test_fx/test_bias_addition.py
+tests/test_analyzer/test_fx/test_mod_dir.py
+tests/test_analyzer/test_fx/test_nested_ckpt.py
+tests/test_analyzer/test_fx/test_shape_prop.py
+tests/test_analyzer/test_fx/test_symbolic_profile.py
+tests/test_analyzer/test_fx/zoo.py
+tests/test_analyzer/test_subclasses/__init__.py
+tests/test_analyzer/test_subclasses/test_aten.py
+tests/test_analyzer/test_subclasses/test_flop_tensor.py
+tests/test_analyzer/test_subclasses/test_meta_mode.py
 tests/test_auto_parallel/__init__.py
 tests/test_auto_parallel/test_pass/__init__.py
 tests/test_auto_parallel/test_pass/test_node_converting_pass.py
 tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
 tests/test_auto_parallel/test_tensor_shard/__init__.py
 tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
 tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
 tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
 tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
 tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
 tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
 tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
-tests/test_auto_parallel/test_tensor_shard/test_param_resharding_cost.py
-tests/test_auto_parallel/test_tensor_shard/test_shape_consistency_pass.py
 tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
 tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
 tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
 tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
 tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
```

### Comparing `colossalai-0.2.8/op_builder/__init__.py` & `colossalai-0.3.0/op_builder/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/builder.py` & `colossalai-0.3.0/op_builder/builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 # Licensed under the MIT License.
 import importlib
 import os
 import time
 from abc import ABC, abstractmethod
 from pathlib import Path
-from typing import List
+from typing import List, Optional
 
 from .utils import check_cuda_availability, check_system_pytorch_cuda_match, print_rank_0
 
 
 class Builder(ABC):
     """
     Builder is the base class to build extensions for PyTorch.
@@ -74,15 +74,15 @@
         This function should return a list of source files for extensions.
         """
         raise NotImplementedError
 
     @abstractmethod
     def include_dirs(self) -> List[str]:
         """
-        This function should return a list of inlcude files for extensions.
+        This function should return a list of include files for extensions.
         """
         pass
 
     @abstractmethod
     def cxx_flags(self) -> List[str]:
         """
         This function should return a list of cxx compilation flags for extensions.
@@ -123,47 +123,49 @@
 
         if not TORCH_AVAILABLE:
             raise ModuleNotFoundError(
                 "PyTorch is not found. You need to install PyTorch first in order to build CUDA extensions")
 
         if CUDA_HOME is None:
             raise RuntimeError(
-                "CUDA_HOME is not found. You need to export CUDA_HOME environment vairable or install CUDA Toolkit first in order to build CUDA extensions"
+                "CUDA_HOME is not found. You need to export CUDA_HOME environment variable or install CUDA Toolkit first in order to build CUDA extensions"
             )
 
         # make sure CUDA is available for compilation during
         cuda_available = check_cuda_availability()
         if not cuda_available:
-            raise RuntimeError("CUDA is not available on your system as torch.cuda.is_avaible() returns False.")
+            raise RuntimeError("CUDA is not available on your system as torch.cuda.is_available() returns False.")
 
         # make sure system CUDA and pytorch CUDA match, an error will raised inside the function if not
         check_system_pytorch_cuda_match(CUDA_HOME)
 
-    def load(self, verbose=True):
+    def load(self, verbose: Optional[bool] = None):
         """
         load the kernel during runtime. If the kernel is not built during pip install, it will build the kernel.
         If the kernel is built during runtime, it will be stored in `~/.cache/colossalai/torch_extensions/`. If the
         kernel is built during pip install, it can be accessed through `colossalai._C`.
 
         Warning: do not load this kernel repeatedly during model execution as it could slow down the training process.
 
         Args:
             verbose (bool, optional): show detailed info. Defaults to True.
         """
+        if verbose is None:
+            verbose = os.environ.get('CAI_KERNEL_VERBOSE', '0') == '1'
         # if the kernel has be compiled and cached, we directly use it
         if self.cached_op_module is not None:
             return self.cached_op_module
 
         try:
             # if the kernel has been pre-built during installation
             # we just directly import it
             op_module = self.import_op()
             if verbose:
                 print_rank_0(
-                    f"[extension] OP {self.prebuilt_import_path} has been compileed ahead of time, skip building.")
+                    f"[extension] OP {self.prebuilt_import_path} has been compiled ahead of time, skip building.")
         except ImportError:
             # check environment
             self.check_runtime_build_environment()
 
             # time the kernel compilation
             start_build = time.time()
```

### Comparing `colossalai-0.2.8/op_builder/cpu_adam.py` & `colossalai-0.3.0/op_builder/cpu_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/fused_optim.py` & `colossalai-0.3.0/op_builder/fused_optim.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/layernorm.py` & `colossalai-0.3.0/op_builder/layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/moe.py` & `colossalai-0.3.0/op_builder/moe.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/multi_head_attn.py` & `colossalai-0.3.0/op_builder/multi_head_attn.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/scaled_masked_softmax.py` & `colossalai-0.3.0/op_builder/scaled_masked_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/scaled_upper_triangle_masked_softmax.py` & `colossalai-0.3.0/op_builder/scaled_upper_triangle_masked_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/op_builder/utils.py` & `colossalai-0.3.0/op_builder/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,15 +32,15 @@
     import torch
 
     try:
         torch_cuda_major = torch.version.cuda.split(".")[0]
         torch_cuda_minor = torch.version.cuda.split(".")[1]
     except:
         raise ValueError(
-            "[extension] Cannot retrive the CUDA version in the PyTorch binary given by torch.version.cuda")
+            "[extension] Cannot retrieve the CUDA version in the PyTorch binary given by torch.version.cuda")
     return torch_cuda_major, torch_cuda_minor
 
 
 def get_cuda_bare_metal_version(cuda_dir) -> List[int]:
     """
     Get the System CUDA version from nvcc.
 
@@ -86,15 +86,14 @@
     if bare_metal_major != torch_cuda_major:
         raise Exception(
             f'[extension] Failed to build PyTorch extension because the detected CUDA version ({bare_metal_major}.{bare_metal_minor}) '
             f'mismatches the version that was used to compile PyTorch ({torch_cuda_major}.{torch_cuda_minor}).'
             'Please make sure you have set the CUDA_HOME correctly and installed the correct PyTorch in https://pytorch.org/get-started/locally/ .'
         )
 
-    print(bare_metal_minor != torch_cuda_minor)
     if bare_metal_minor != torch_cuda_minor:
         warnings.warn(
             f"[extension] The CUDA version on the system ({bare_metal_major}.{bare_metal_minor}) does not match with the version ({torch_cuda_major}.{torch_cuda_minor}) torch was compiled with. "
             "The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. "
             "If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions"
         )
     return True
@@ -107,15 +106,15 @@
     Returns:
         A tuple of integers in the form of (major, minor, patch).
     """
     import torch
     torch_version = torch.__version__.split('+')[0]
     TORCH_MAJOR = int(torch_version.split('.')[0])
     TORCH_MINOR = int(torch_version.split('.')[1])
-    TORCH_PATCH = int(torch_version.split('.')[2])
+    TORCH_PATCH = int(torch_version.split('.')[2], 16)
     return TORCH_MAJOR, TORCH_MINOR, TORCH_PATCH
 
 
 def check_pytorch_version(min_major_version, min_minor_version) -> bool:
     """
     Compare the current PyTorch version with the minium required version.
 
@@ -152,24 +151,23 @@
     This function sets the PyTorch TORCH_CUDA_ARCH_LIST variable for ahead-of-time extension compilation.
     Ahead-of-time compilation occurs when CUDA_EXT=1 is set when running 'pip install'.
     """
     cuda_available = check_cuda_availability()
 
     # we only need to set this when CUDA is not available for cross-compilation
     if not cuda_available:
-        warnings.warn(
-            '\n[extension]  PyTorch did not find available GPUs on this system.\n'
-            'If your intention is to cross-compile, this is not an error.\n'
-            'By default, Colossal-AI will cross-compile for \n'
-            '1. Pascal (compute capabilities 6.0, 6.1, 6.2),\n'
-            '2. Volta (compute capability 7.0)\n'
-            '3. Turing (compute capability 7.5),\n'
-            '4. Ampere (compute capability 8.0, 8.6)if the CUDA version is >= 11.0\n'
-            '\nIf you wish to cross-compile for a single specific architecture,\n'
-            'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n')
+        warnings.warn('\n[extension]  PyTorch did not find available GPUs on this system.\n'
+                      'If your intention is to cross-compile, this is not an error.\n'
+                      'By default, Colossal-AI will cross-compile for \n'
+                      '1. Pascal (compute capabilities 6.0, 6.1, 6.2),\n'
+                      '2. Volta (compute capability 7.0)\n'
+                      '3. Turing (compute capability 7.5),\n'
+                      '4. Ampere (compute capability 8.0, 8.6)if the CUDA version is >= 11.0\n'
+                      '\nIf you wish to cross-compile for a single specific architecture,\n'
+                      'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n')
 
         if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None:
             bare_metal_major, bare_metal_minor = get_cuda_bare_metal_version(cuda_dir)
 
             arch_list = ['6.0', '6.1', '6.2', '7.0', '7.5']
 
             if int(bare_metal_major) == 11:
```

### Comparing `colossalai-0.2.8/setup.py` & `colossalai-0.3.0/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -42,15 +42,15 @@
     if not TORCH_AVAILABLE:
         raise ModuleNotFoundError(
             "[extension] PyTorch is not found while CUDA_EXT=1. You need to install PyTorch first in order to build CUDA extensions"
         )
 
     if not CUDA_HOME:
         raise RuntimeError(
-            "[extension] CUDA_HOME is not found while CUDA_EXT=1. You need to export CUDA_HOME environment vairable or install CUDA Toolkit first in order to build CUDA extensions"
+            "[extension] CUDA_HOME is not found while CUDA_EXT=1. You need to export CUDA_HOME environment variable or install CUDA Toolkit first in order to build CUDA extensions"
         )
 
     check_system_pytorch_cuda_match(CUDA_HOME)
     check_pytorch_version(MIN_PYTORCH_VERSION_MAJOR, MIN_PYTORCH_VERSION_MINOR)
     check_cuda_availability()
```

### Comparing `colossalai-0.2.8/tests/components_to_test/albert.py` & `colossalai-0.3.0/tests/components_to_test/albert.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,37 +23,37 @@
                               num_attention_heads=num_head,
                               max_position_embeddings=sequence_length,
                               num_hidden_layers=num_layer,
                               hidden_dropout_prob=0.,
                               attention_probs_dropout_prob=0.)
         print('building AlbertForSequenceClassification model')
 
-        # adapting huggingface BertForSequenceClassification for single unitest calling interface
-        class ModelAaptor(AlbertForSequenceClassification):
+        # adapting huggingface BertForSequenceClassification for single unittest calling interface
+        class ModelAdaptor(AlbertForSequenceClassification):
 
             def forward(self, input_ids, labels):
                 """
                 inputs: data, label
                 outputs: loss
                 """
                 return super().forward(input_ids=input_ids, labels=labels)[0]
 
-        model = ModelAaptor(config)
+        model = ModelAdaptor(config)
         # if checkpoint and version.parse(transformers.__version__) >= version.parse("4.11.0"):
         #     model.gradient_checkpointing_enable()
 
         return model
 
-    is_distrbuted = torch.distributed.is_initialized()
+    is_distributed = torch.distributed.is_initialized()
     trainloader = get_bert_data_loader(n_class=vocab_size,
                                        batch_size=2,
                                        total_samples=10000,
                                        sequence_length=sequence_length,
-                                       is_distrbuted=is_distrbuted)
+                                       is_distributed=is_distributed)
     testloader = get_bert_data_loader(n_class=vocab_size,
                                       batch_size=2,
                                       total_samples=10000,
                                       sequence_length=sequence_length,
-                                      is_distrbuted=is_distrbuted)
+                                      is_distributed=is_distributed)
 
     criterion = None
     return bert_model_builder, trainloader, testloader, torch.optim.Adam, criterion
```

### Comparing `colossalai-0.2.8/tests/components_to_test/beit.py` & `colossalai-0.3.0/tests/components_to_test/beit.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,20 +23,20 @@
                               device=get_current_device())
         return data, label
 
 
 @non_distributed_component_funcs.register(name='beit')
 def get_training_components():
 
-    def model_buider(checkpoint=False):
+    def model_builder(checkpoint=False):
         model = Beit(img_size=DummyDataLoader.img_size,
                      num_classes=DummyDataLoader.num_class,
                      embed_dim=32,
                      depth=2,
                      num_heads=4)
         return model
 
     trainloader = DummyDataLoader()
     testloader = DummyDataLoader()
 
     criterion = torch.nn.CrossEntropyLoss()
-    return model_buider, trainloader, testloader, torch.optim.Adam, criterion
+    return model_builder, trainloader, testloader, torch.optim.Adam, criterion
```

### Comparing `colossalai-0.2.8/tests/components_to_test/bert.py` & `colossalai-0.3.0/tests/components_to_test/bert.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,26 +9,26 @@
 
 def get_bert_data_loader(
         n_class,
         batch_size,
         total_samples,
         sequence_length,
         device=torch.device('cpu:0'),
-        is_distrbuted=False,
+        is_distributed=False,
 ):
     train_data = torch.randint(
         low=0,
         high=n_class,
         size=(total_samples, sequence_length),
         device=device,
         dtype=torch.long,
     )
     train_label = torch.randint(low=0, high=2, size=(total_samples,), device=device, dtype=torch.long)
     train_dataset = torch.utils.data.TensorDataset(train_data, train_label)
-    if is_distrbuted:
+    if is_distributed:
         sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
     else:
         sampler = SequentialSampler(train_dataset)
     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)
     return train_loader
 
 
@@ -48,37 +48,37 @@
                             num_attention_heads=num_head,
                             max_position_embeddings=sequence_length,
                             num_hidden_layers=num_layer,
                             hidden_dropout_prob=0.,
                             attention_probs_dropout_prob=0.)
         print('building BertForSequenceClassification model')
 
-        # adapting huggingface BertForSequenceClassification for single unitest calling interface
-        class ModelAaptor(BertForSequenceClassification):
+        # adapting huggingface BertForSequenceClassification for single unittest calling interface
+        class ModelAdaptor(BertForSequenceClassification):
 
             def forward(self, input_ids, labels):
                 """
                 inputs: data, label
                 outputs: loss
                 """
                 return super().forward(input_ids=input_ids, labels=labels)[0]
 
-        model = ModelAaptor(config)
+        model = ModelAdaptor(config)
         if checkpoint and version.parse(transformers.__version__) >= version.parse("4.11.0"):
             model.gradient_checkpointing_enable()
 
         return model
 
-    is_distrbuted = torch.distributed.is_initialized()
+    is_distributed = torch.distributed.is_initialized()
     trainloader = get_bert_data_loader(n_class=vocab_size,
                                        batch_size=2,
                                        total_samples=10000,
                                        sequence_length=sequence_length,
-                                       is_distrbuted=is_distrbuted)
+                                       is_distributed=is_distributed)
     testloader = get_bert_data_loader(n_class=vocab_size,
                                       batch_size=2,
                                       total_samples=10000,
                                       sequence_length=sequence_length,
-                                      is_distrbuted=is_distrbuted)
+                                      is_distributed=is_distributed)
 
     criterion = None
     return bert_model_builder, trainloader, testloader, torch.optim.Adam, criterion
```

### Comparing `colossalai-0.2.8/tests/components_to_test/gpt2.py` & `colossalai-0.3.0/tests/components_to_test/gpt2.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/hanging_param_model.py` & `colossalai-0.3.0/tests/components_to_test/hanging_param_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/inline_op_model.py` & `colossalai-0.3.0/tests/components_to_test/inline_op_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/nested_model.py` & `colossalai-0.3.0/tests/components_to_test/nested_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/registry.py` & `colossalai-0.3.0/tests/components_to_test/registry.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,18 +5,18 @@
 
     def __init__(self):
         self._registry = dict()
 
     def register(self, name):
         assert name not in self._registry
 
-        def _regsiter(callable_):
+        def _register(callable_):
             self._registry[name] = callable_
 
-        return _regsiter
+        return _register
 
     def get_callable(self, name: str):
         return self._registry[name]
 
     def __iter__(self):
         self._idx = 0
         self._len = len(self._registry)
@@ -30,10 +30,10 @@
             self._idx += 1
             return callable_
         else:
             raise StopIteration
 
 
 non_distributed_component_funcs = Registry()
-model_paralle_component_funcs = Registry()
+model_parallel_component_funcs = Registry()
 
-__all__ = ['non_distributed_component_funcs', 'model_paralle_component_funcs']
+__all__ = ['non_distributed_component_funcs', 'model_parallel_component_funcs']
```

### Comparing `colossalai-0.2.8/tests/components_to_test/repeated_computed_layers.py` & `colossalai-0.3.0/tests/components_to_test/repeated_computed_layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/resnet.py` & `colossalai-0.3.0/tests/components_to_test/resnet.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/simple_net.py` & `colossalai-0.3.0/tests/components_to_test/simple_net.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/components_to_test/utils/executor.py` & `colossalai-0.3.0/tests/components_to_test/utils/executor.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import torch
 
 
-def run_fwd_bwd(model, data, label, criterion, optimizer=None) -> torch.Tensor:
-    """run_fwd_bwd
-    run fwd and bwd for the model
+def run_fwd(model, data, label, criterion) -> torch.Tensor:
+    """run_fwd
+    run fwd for the model
 
     Args:
         model (torch.nn.Module): a PyTorch model
         data (torch.Tensor): input data
         label (torch.Tensor): label
         criterion (Optional[Callable]): a function of criterion
 
@@ -18,12 +18,29 @@
         y = model(data)
         y = y.float()
         loss = criterion(y, label)
     else:
         loss = model(data, label)
 
     loss = loss.float()
+    return loss
+
+
+def run_fwd_bwd(model, data, label, criterion, optimizer=None) -> torch.Tensor:
+    """run_fwd_bwd
+    run fwd and bwd for the model
+
+    Args:
+        model (torch.nn.Module): a PyTorch model
+        data (torch.Tensor): input data
+        label (torch.Tensor): label
+        criterion (Optional[Callable]): a function of criterion
+
+    Returns:
+        torch.Tensor: loss of fwd
+    """
+    loss = run_fwd(model, data, label, criterion)
     if optimizer:
         optimizer.backward(loss)
     else:
         loss.backward()
     return loss
```

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/diffusers/diffusers.py` & `colossalai-0.3.0/tests/kit/model_zoo/diffusers/diffusers.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 LATENTS_SHAPE = (BATCH_SIZE, IN_CHANNELS, HEIGHT // 7, WIDTH // 7)
 TIME_STEP = 3
 
 data_vae_fn = lambda: dict(sample=torch.randn(2, 3, 32, 32))
 data_unet_fn = lambda: dict(sample=torch.randn(2, 3, 32, 32), timestep=3)
 
 identity_output = lambda x: x
+clip_vision_model_output = lambda x: dict(pooler_output=x[1])
 
 
 def data_clip_model():
     input_ids = torch.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=torch.int64)
     attention_mask = torch.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=torch.int64)
     position_ids = torch.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=torch.int64)
     pixel_values = torch.zeros((BATCH_SIZE, IN_CHANNELS, HEIGHT, WIDTH), dtype=torch.float32)
@@ -61,13 +62,13 @@
                    model_fn=partial(transformers.CLIPTextModel, config=transformers.CLIPTextConfig()),
                    data_gen_fn=data_clip_text,
                    output_transform_fn=identity_output)
 
 model_zoo.register(name='diffusers_clip_vision_model',
                    model_fn=partial(transformers.CLIPVisionModel, config=transformers.CLIPVisionConfig()),
                    data_gen_fn=data_clip_vision,
-                   output_transform_fn=identity_output)
+                   output_transform_fn=clip_vision_model_output)
 
 model_zoo.register(name='diffusers_unet2d_model',
                    model_fn=diffusers.UNet2DModel,
                    data_gen_fn=data_unet_fn,
                    output_transform_fn=identity_output)
```

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/registry.py` & `colossalai-0.3.0/tests/kit/model_zoo/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/timm/timm.py` & `colossalai-0.3.0/tests/kit/model_zoo/timm/timm.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/torchaudio/torchaudio.py` & `colossalai-0.3.0/tests/kit/model_zoo/torchaudio/torchaudio.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+from functools import partial
+
 import torch
 import torchaudio.models as tm
 
 from ..registry import ModelAttribute, model_zoo
 
 INPUT_DIM = 80
 IN_FEATURES = 16
@@ -97,32 +99,30 @@
     mel_specgram_lengths = max_mel_specgram_length * torch.ones((n_batch,))
     return dict(tokens=tokens,
                 token_lengths=token_lengths,
                 mel_specgram=mel_specgram,
                 mel_specgram_lengths=mel_specgram_lengths)
 
 
-model_zoo.register(
-    name='torchaudio_tacotron',
-    model_fn=lambda: tm.Tacotron2(n_mels=N_MELS),
-    data_gen_fn=tacotron_data_gen_fn,
-    output_transform_fn=lambda outputs: dict(
-        spectrogram_before=outputs[0], spectrogram_after=outputs[1], stop_tokens=outputs[2], attn_weights=outputs[3]),
-    model_attribute=ModelAttribute(has_control_flow=True))
+model_zoo.register(name='torchaudio_tacotron',
+                   model_fn=lambda: tm.Tacotron2(n_mels=N_MELS),
+                   data_gen_fn=tacotron_data_gen_fn,
+                   output_transform_fn=lambda outputs: dict(summed_output=sum(x.sum() for x in outputs)),
+                   model_attribute=ModelAttribute(has_control_flow=True))
 
 
 def wav2vec_data_gen_fn():
     batch_size, num_frames = 4, 400
     waveforms = torch.randn(batch_size, num_frames)
     lengths = torch.randint(0, num_frames, (batch_size,))
     return dict(waveforms=waveforms, lengths=lengths)
 
 
 model_zoo.register(name='torchaudio_wav2vec2_base',
-                   model_fn=tm.wav2vec2_base,
+                   model_fn=partial(tm.wav2vec2_base, encoder_layer_drop=0.0),
                    data_gen_fn=wav2vec_data_gen_fn,
                    output_transform_fn=transformer_output_transform_fn,
                    model_attribute=ModelAttribute(has_control_flow=True))
 
 model_zoo.register(name='torchaudio_hubert_base',
                    model_fn=tm.hubert_base,
                    data_gen_fn=wav2vec_data_gen_fn,
```

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/torchrec/torchrec.py` & `colossalai-0.3.0/tests/kit/model_zoo/torchrec/torchrec.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/torchvision/torchvision.py` & `colossalai-0.3.0/tests/kit/model_zoo/torchvision/torchvision.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,20 +32,20 @@
         stochastic_depth_prob=0,    # it is originally 0.2, but we set it to 0 to make it deterministic
         weights=weights,
         progress=progress,
     )
 
 
 # special output transform fn
-google_net_output_transform_fn = lambda x: dict(output=x.logits) if isinstance(x, torchvision.models.GoogLeNetOutputs
-                                                                              ) else dict(output=x)
+google_net_output_transform_fn = lambda x: dict(output=sum(x)) if isinstance(x, torchvision.models.GoogLeNetOutputs
+                                                                            ) else dict(output=x)
 swin_s_output_output_transform_fn = lambda x: {f'output{idx}': val
                                                for idx, val in enumerate(x)} if isinstance(x, tuple) else dict(output=x)
-inception_v3_output_transform_fn = lambda x: dict(output=x.logits) if isinstance(x, torchvision.models.InceptionOutputs
-                                                                                ) else dict(output=x)
+inception_v3_output_transform_fn = lambda x: dict(output=sum(x)) if isinstance(x, torchvision.models.InceptionOutputs
+                                                                              ) else dict(output=x)
 
 model_zoo.register(name='torchvision_alexnet',
                    model_fn=tm.alexnet,
                    data_gen_fn=data_gen_fn,
                    output_transform_fn=output_transform_fn)
 model_zoo.register(name='torchvision_densenet121',
                    model_fn=tm.densenet121,
```

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/transformers/albert.py` & `colossalai-0.3.0/tests/kit/model_zoo/transformers/albert.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/transformers/bert.py` & `colossalai-0.3.0/tests/kit/model_zoo/transformers/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/transformers/gpt.py` & `colossalai-0.3.0/tests/kit/model_zoo/transformers/gpt.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/transformers/opt.py` & `colossalai-0.3.0/tests/kit/model_zoo/transformers/opt.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/kit/model_zoo/transformers/t5.py` & `colossalai-0.3.0/tests/kit/model_zoo/transformers/t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_pass/test_node_converting_pass.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_pass/test_node_converting_pass.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 import torch.nn.functional as F
 
 from colossalai.auto_parallel.passes.runtime_preparation_pass import node_args_converting_pass
 from colossalai.device.device_mesh import DeviceMesh
 from colossalai.fx.graph_module import ColoGraphModule
 from colossalai.fx.tracer import ColoTracer
 from colossalai.tensor.sharding_spec import ShardingSpec
+from colossalai.testing import clear_cache_before_run
 
 
 class TestModule(torch.nn.Module):
 
     def forward(self, x):
         x = x.view(4, 4, 2)
         return x
@@ -22,14 +23,15 @@
     view_node = list(x_node.users.keys())[0]
     new_args = list(view_node.args)
     new_args[0] = shard_node
     view_node.args = tuple(new_args)
     return gm
 
 
+@clear_cache_before_run()
 def test_node_args_converting_pass():
     model = TestModule()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     meta_args = {'x': torch.rand(4, 8).to('meta')}
     input = torch.rand(4, 8)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,18 @@
+import pytest
 import torch
 import torch.nn.functional as F
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.passes.runtime_preparation_pass import size_value_converting_pass
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.graph_module import ColoGraphModule
-from colossalai.fx.tracer import ColoTracer
 from colossalai.tensor.sharding_spec import ShardingSpec
+from colossalai.testing import clear_cache_before_run
 
 
 class TestModule(torch.nn.Module):
 
     def forward(self, x):
         size = x.size()
         return size
@@ -29,29 +32,31 @@
     size_node = list(graph.nodes)[2]
     x_node = narrow_node.args[0]
     size_node.args = (x_node,)
     graph.erase_node(narrow_node)
     return gm
 
 
+@pytest.mark.skip('ShapeProp is not compatible with PyTorch 1.11.0')
+@clear_cache_before_run()
 def test_size_value_converting_pass():
     model = TestModule()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     meta_args = {'x': torch.rand(4, 8).to('meta')}
     input = torch.rand(4, 8)
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     graph = tracer.trace(root=model, meta_args=meta_args)
-
     x_node = list(graph.nodes)[0]
     x_sharding_spec = ShardingSpec(device_mesh, entire_shape=(4, 8), dim_partition_dict={0: [0]})
     setattr(x_node, 'sharding_spec', x_sharding_spec)
     gm = ColoGraphModule(model, graph)
     gm = insert_narrow(gm, x_node)
+    shape_prop_pass(gm, *meta_args.values())
     gm.recompile()
     size = gm(input)
     assert size == torch.Size([2, 8])
 
     narrow_node = list(gm.graph.nodes)[1]
     gm = recover_narrow(gm, narrow_node)
     gm = size_value_converting_pass(gm, device_mesh)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 from functools import partial
 
 import pytest
 import torch
-import torch.multiprocessing as mp
 
-from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+try:
+    from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+    NO_CODEGEN = False
+except:
+    NO_CODEGEN = True
+
 from colossalai.device.device_mesh import DeviceMesh
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import assert_close, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 
 
 class LinearModel(torch.nn.Module):
 
     def __init__(self, in_features, out_features):
         super().__init__()
         self.linear = torch.nn.Linear(in_features, out_features)
@@ -73,19 +75,17 @@
     meta_args = {'x': torch.rand(4, 3, 64, 64).to('meta')}
     gm = initialize_model(model, meta_args=meta_args, device_mesh=device_mesh)
     output = gm(input)
     assert_close(output, output_compare)
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@pytest.mark.skipif(NO_CODEGEN, reason='No codegen found')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_bias_addition_module():
-    world_size = 4
-    run_func_linear = partial(check_linear_module, world_size=world_size, port=free_port())
-    mp.spawn(run_func_linear, nprocs=world_size)
-    run_func_conv = partial(check_conv_module, world_size=world_size, port=free_port())
-    mp.spawn(run_func_conv, nprocs=world_size)
+    spawn(check_linear_module, 4)
+    spawn(check_conv_module, 4)
 
 
 if __name__ == '__main__':
     test_bias_addition_module()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,27 +1,25 @@
-from functools import partial
-from typing import Optional, Tuple, Union
+from typing import Optional, Tuple
 
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 from torch.utils.checkpoint import checkpoint
 from transformers.pytorch_utils import Conv1D
 
-from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+try:
+    from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+    NO_CODEGEN = False
+except:
+    NO_CODEGEN = True
+
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.graph_module import ColoGraphModule
-from colossalai.fx.tracer import ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.tensor.shape_consistency import ShapeConsistencyManager
-from colossalai.testing import rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 
 HIDDEN_SIZE = 16
 
 
 class GPT2MLPWithCkpt(nn.Module):
 
     def __init__(self, intermediate_size, hidden_size):
@@ -39,32 +37,32 @@
         return hidden_states
 
 
 def check_act_ckpt(rank, world_size, port):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = GPT2MLPWithCkpt(intermediate_size=4 * HIDDEN_SIZE, hidden_size=HIDDEN_SIZE)
+    input = torch.rand(1, 64, HIDDEN_SIZE)
     input_sample = {
         'hidden_states': torch.rand(1, 64, HIDDEN_SIZE).to('meta'),
     }
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     # [[0, 1]
     #  [2, 3]]
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     gm = initialize_model(model, input_sample, device_mesh)
     code = gm.module.graph.python_code('self').src
     assert "runtime_comm_spec_apply_1 = colossalai_auto_parallel_passes_runtime_apply_pass_runtime_comm_spec_apply(linear_1, comm_actions_dict, 12, 'linear_1')" in code
-    assert "view_3 = colossalai.utils.activation_checkpoint.checkpoint(self.checkpoint_0, False, view_1, comm_actions_dict, use_reentrant=True)" in code
+    assert "view_3 = torch.utils.checkpoint.checkpoint(self.checkpoint_0, view_1, comm_actions_dict, use_reentrant=False)" in code
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@pytest.mark.skipif(NO_CODEGEN, reason='No codegen found')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_mlp_layer():
-    world_size = 4
-    run_func = partial(check_act_ckpt, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_act_ckpt, 4)
 
 
 if __name__ == '__main__':
     test_mlp_layer()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 import copy
-from functools import partial
 
 import pytest
 import torch
-import torch.multiprocessing as mp
 from torch.nn.parallel import DistributedDataParallel as DDP
 
-from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+try:
+    from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+    NO_CODEGEN = False
+except:
+    NO_CODEGEN = True
+
 from colossalai.device.device_mesh import DeviceMesh
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import assert_close, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 
 
 class MLP(torch.nn.Module):
 
     def __init__(self, in_features):
         super().__init__()
         self.linear_1 = torch.nn.Linear(in_features, 4 * in_features, bias=False)
@@ -89,17 +90,16 @@
     if rank in (1, 3):
         assert_close(gm.module.module.linear_1.weight.grad, grad_compare.narrow(0, 8, 8))
 
     print(f'gradient on rank{rank} is correct')
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@pytest.mark.skipif(NO_CODEGEN, reason='No codegen found')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_compatibility_with_ddp():
-    world_size = 4
-    run_func = partial(check_compatibility_with_ddp, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_compatibility_with_ddp, 4)
 
 
 if __name__ == '__main__':
     test_compatibility_with_ddp()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 import copy
-from functools import partial
 
 import pytest
 import torch
-import torch.multiprocessing as mp
-from torch.nn.parallel import DistributedDataParallel as DDP
 
-from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+try:
+    from colossalai.auto_parallel.tensor_shard.initialize import initialize_model
+    NO_CODEGEN = False
+except:
+    NO_CODEGEN = True
+
 from colossalai.device.device_mesh import DeviceMesh
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
 from colossalai.nn.optimizer import HybridAdam
-from colossalai.nn.parallel import zero_model_wrapper, zero_optim_wrapper
 from colossalai.tensor.process_group import ProcessGroup
-from colossalai.testing import assert_close, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port, get_current_device
-from colossalai.utils.model.colo_init_context import ColoInitContext, post_process_colo_init_ctx
+from colossalai.testing import assert_close, rerun_if_address_is_in_use, run_on_environment_flag, spawn
+from colossalai.utils import get_current_device
+from colossalai.zero import post_process_colo_init_ctx, zero_model_wrapper, zero_optim_wrapper
 
 
 class MLP(torch.nn.Module):
 
     def __init__(self, in_features):
         super().__init__()
         self.linear_1 = torch.nn.Linear(in_features, 4 * in_features, bias=False)
@@ -98,17 +98,16 @@
     if rank in (1, 3):
         assert_close(list(optimizer.optim.state.values())[0]['exp_avg'].half(), grad_compare.narrow(0, 8, 8).flatten())
 
     print(f'gradient on rank{rank} is correct')
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@pytest.mark.skipif(NO_CODEGEN, reason='No codegen found')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_auto_parallel_with_gemini():
-    world_size = 4
-    run_func = partial(check_auto_parallel_with_gemini, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_auto_parallel_with_gemini, 4)
 
 
 if __name__ == '__main__':
     test_auto_parallel_with_gemini()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 from typing import Optional, Tuple
 
 import torch
 import torch.nn as nn
 from torch.fx import GraphModule
 from transformers.pytorch_utils import Conv1D
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes import shape_prop_pass
+# from colossalai.fx.tracer.tracer import ColoTracer
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.utils.factory import find_repeat_blocks
-from colossalai.fx.tracer.tracer import ColoTracer
-from colossalai.testing import parameterize
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
+from colossalai.testing import clear_cache_before_run, parameterize, run_on_environment_flag
 
 NUM_REPEAT_BLOCKS = 4
 BATCH_SIZE = 1
 SEQ_LENGTH = 32
 HIDDEN_DIM = 384
 
 
@@ -74,24 +76,26 @@
         for block in self.blocks:
             x = block(x)
 
         return x
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 @parameterize('model_cls', [RepeatModel, NonRepeatModel])
 def test_repeat_blocks(model_cls):
 
     model = model_cls(4 * HIDDEN_DIM, HIDDEN_DIM, NUM_REPEAT_BLOCKS)
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     input_sample = {'x': torch.rand(BATCH_SIZE, SEQ_LENGTH, HIDDEN_DIM).to('meta')}
     graph = tracer.trace(root=model, meta_args=input_sample)
 
     gm = GraphModule(model, graph, model.__class__.__name__)
+    shape_prop_pass(gm, *input_sample.values())
     gm.recompile()
 
     node_list = list(graph.nodes)
     root_module = graph.owning_module
     common_blocks = find_repeat_blocks(node_list, root_module, common_length_threshold=10)
 
     total_num_nodes = len(list(graph.nodes))
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py`

 * *Files identical despite different names*

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,39 @@
 import copy
 import random
-from functools import partial
 from typing import Dict
 
 import numpy as np
 import pytest
 import torch
-import torch.multiprocessing as mp
 import transformers
 from torch.fx import GraphModule
 
-from colossalai.auto_parallel.tensor_shard.initialize import (
-    ModuleWrapper,
-    build_strategy_constructor,
-    solve_solution,
-    transform_to_sharded_model,
-)
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+# from colossalai.fx.tracer.tracer import ColoTracer
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
+
+try:
+    from colossalai.auto_parallel.tensor_shard.initialize import (
+        ModuleWrapper,
+        build_strategy_constructor,
+        solve_solution,
+        transform_to_sharded_model,
+    )
+    NO_CODEGEN = False
+except:
+    NO_CODEGEN = True
+
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import ShardingSpec
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.tracer.tracer import ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
 from colossalai.tensor.shape_consistency import to_global
-from colossalai.testing import assert_close, assert_close_loose, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import assert_close, assert_close_loose, parameterize, rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_gpt.gpt_modules import GPT2MLP, GPT2Attention, GPT2Block, GPT2Model
 
 BATCH_SIZE = 1
 SEQ_LENGTH = 32
 HIDDEN_DIM = 768
 
 seed = 128
@@ -48,29 +53,28 @@
         origin_param_grad = origin_param_dict[name].grad
         atoms = name.split('.')
         new_name = '_'.join(atoms)
         if new_name in best_sharding_spec_dict:
             param_sharding_spec = best_sharding_spec_dict[new_name]
             grad_to_compare = copy.deepcopy(param_grad)
             param_grad_global = to_global(grad_to_compare, param_sharding_spec)
-
             try:
-                assert_close_loose(param_grad_global, origin_param_grad, rtol=1e-03, atol=1e-03)
+                assert_close_loose(param_grad_global, origin_param_grad, rtol=1e-03, atol=1e-05)
             except:
                 difference = param_grad_global - origin_param_grad
                 avg_diff = difference.abs().sum() / difference.numel()
                 assert avg_diff < 0.001
                 print(f'{name} param has {avg_diff} average difference')
 
 
 def check_attention_layer(rank, model_cls, world_size, port):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
 
-    config = transformers.GPT2Config(n_position=64, n_layer=1, n_head=16, n_embd=HIDDEN_DIM)
+    config = transformers.GPT2Config(n_position=64, n_layer=2, n_head=16, n_embd=HIDDEN_DIM)
 
     if model_cls == GPT2MLP:
         model = model_cls(intermediate_size=4 * config.hidden_size, config=config).to('cuda')
     else:
         model = model_cls(config=config).to('cuda')
     test_model = copy.deepcopy(model)
 
@@ -107,23 +111,25 @@
         }
 
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     # [[0, 1]
     #  [2, 3]]
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
 
     graph = tracer.trace(root=model, meta_args=meta_input_sample)
     gm = GraphModule(model, graph, model.__class__.__name__)
+    shape_prop_pass(gm, *meta_input_sample.values())
     gm.recompile()
 
     strategies_constructor = build_strategy_constructor(graph, device_mesh, 'standard', 'replicated', 'standard')
     solution = solve_solution(gm, strategies_constructor, memory_budget=-1)
-    gm, sharding_spec_dicts = transform_to_sharded_model(gm, solution, device_mesh, strategies_constructor)
+    gm, sharding_spec_dicts = transform_to_sharded_model(gm, meta_input_sample, solution, device_mesh,
+                                                         strategies_constructor)
     gm = ModuleWrapper(gm, *sharding_spec_dicts)
 
     nodes = [strategies_vector.node for strategies_vector in strategies_constructor.leaf_strategies]
     best_sharding_spec_dict = {}
     for index, node in enumerate(nodes):
         best_sharding_spec_dict[node.name] = node.sharding_spec
 
@@ -172,18 +178,17 @@
 
         print(f'computation cost is {computation_cost}')
         print(f'communication cost is {communication_cost}')
         print(f'memory cost is {memory_cost}')
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@pytest.mark.skipif(NO_CODEGEN, reason="no codegen module")
 @pytest.mark.dist
 @parameterize('model_cls', [GPT2MLP, GPT2Block, GPT2Attention, GPT2Model])
 @rerun_if_address_is_in_use()
 def test_mlp_layer(model_cls):
-    world_size = 4
-    run_func = partial(check_attention_layer, model_cls=model_cls, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_attention_layer, 4, model_cls=model_cls)
 
 
 if __name__ == '__main__':
     test_mlp_layer()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,43 +1,44 @@
 import torch
-import torch.nn as nn
 import transformers
 from torch.fx import GraphModule
 
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.constants import BATCHNORM_MODULE_OP
 from colossalai.auto_parallel.tensor_shard.options import SolverOptions
-from colossalai.auto_parallel.tensor_shard.solver import CostGraph, GraphAnalyser, Solver, StrategiesConstructor
+from colossalai.auto_parallel.tensor_shard.solver import CostGraph, Solver, StrategiesConstructor
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.tracer.tracer import ColoTracer
 from colossalai.tensor.shape_consistency import ShapeConsistencyManager
-from colossalai.testing import parameterize
+from colossalai.testing import clear_cache_before_run, parameterize
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
 from tests.test_auto_parallel.test_tensor_shard.test_gpt.gpt_modules import GPT2MLP, GPT2Attention, GPT2Block, GPT2Model
 
 BATCH_SIZE = 1
 SEQ_LENGTH = 32
 HIDDEN_DIM = 384
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 @parameterize('model_cls', [GPT2Block, GPT2Attention, GPT2MLP, GPT2Model])
 def test_self_attention_block(model_cls):
-    config = transformers.GPT2Config(n_position=64, n_layer=12, n_head=16, n_embd=HIDDEN_DIM)
+    config = transformers.GPT2Config(n_position=64, n_layer=2, n_head=16, n_embd=HIDDEN_DIM)
     if model_cls == GPT2MLP:
         model = model_cls(intermediate_size=4 * config.hidden_size, config=config)
     else:
         model = model_cls(config=config)
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     # [[0, 1]
     #  [2, 3]]
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     shape_consistency_manager = ShapeConsistencyManager()
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     if model_cls == GPT2MLP:
         input_sample = {
             'hidden_states': torch.rand(BATCH_SIZE, SEQ_LENGTH, HIDDEN_DIM).to('meta'),
         }
     elif model_cls in (GPT2Attention, GPT2Block):
         input_sample = {
             'hidden_states': torch.rand(BATCH_SIZE, SEQ_LENGTH, HIDDEN_DIM).to('meta'),
@@ -48,14 +49,15 @@
         attention_mask = torch.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=torch.int64)
         kwargs = dict(input_ids=input_ids, attention_mask=attention_mask)
         input_sample = {k: v.to('meta') for k, v in kwargs.items()}
 
     graph = tracer.trace(root=model, meta_args=input_sample)
 
     gm = GraphModule(model, graph, model.__class__.__name__)
+    shape_prop_pass(gm, *input_sample.values())
     print(gm.graph)
     gm.recompile()
     solver_options = SolverOptions()
     strategies_constructor = StrategiesConstructor(graph, device_mesh, solver_options)
     strategies_constructor.build_strategies_and_cost()
 
     cost_graph = CostGraph(strategies_constructor.leaf_strategies)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,12 +1,17 @@
+import pytest
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.solver import GraphAnalyser
 from colossalai.fx import ColoGraphModule, ColoTracer
+from colossalai.testing import clear_cache_before_run
 
 
 class LinearModel(nn.Module):
 
     def __init__(self):
         super().__init__()
         self.linear1 = nn.Linear(4, 4)
@@ -18,23 +23,23 @@
         x1 = self.linear1(x1)
         x1 = self.relu(x1)
         x1 = self.linear2(x1)
         out = x1 + x2
         return out
 
 
+@pytest.mark.skip('meta tensor has some bugs in 1.11')
+@clear_cache_before_run()
 def test_liveness_analysis():
     model = LinearModel()
-    tracer = ColoTracer()
-    graph = tracer.trace(model,
-                         meta_args={
-                             'x1': torch.rand(4, 4, device='meta'),
-                             'x2': torch.rand(4, 4, device='meta')
-                         })
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {'x1': torch.rand(4, 4, device='meta'), 'x2': torch.rand(4, 4, device='meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(root=model, graph=graph, class_name=model.__class__.__name__)
+    shape_prop_pass(gm, *meta_args.values())
 
     graph_analyser = GraphAnalyser(gm)
     liveness_list = graph_analyser.liveness_analysis()
     stage_count = len(liveness_list)
 
     # if a LiveStage is covered by another LiveStage, we just keep the larger one.
     assert stage_count == 1
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,18 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
 from colossalai.auto_parallel.tensor_shard.node_handler import BMMFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
 from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class AddBMMTensorMethodModule(nn.Module):
 
     def __init__(self, using_kwargs):
         super().__init__()
@@ -41,15 +36,15 @@
         if self.using_kwargs:
             output = torch.addbmm(bias, x1, x2, alpha=2, beta=3)
         else:
             output = torch.addbmm(bias, x1, x2)
         return output
 
 
-def check_2d_device_mesh(rank, module, bias_shape, using_kwargs, world_size, port):
+def check_2d_device_mesh(rank, world_size, port, module, bias_shape, using_kwargs):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = module(using_kwargs).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     x1 = torch.rand(4, 8, 16).cuda()
@@ -245,38 +240,36 @@
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @parameterize('module', [AddBMMTorchFunctionModule, AddBMMTensorMethodModule])
 @parameterize('bias_shape', [[8], [1, 8], [8, 8]])
 @parameterize('using_kwargs', [True, False])
 @rerun_if_address_is_in_use()
 def test_2d_device_mesh(module, bias_shape, using_kwargs):
-    world_size = 4
-    run_func = partial(check_2d_device_mesh,
-                       module=module,
-                       bias_shape=bias_shape,
-                       world_size=world_size,
-                       using_kwargs=using_kwargs,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(
+        check_2d_device_mesh,
+        4,
+        module=module,
+        bias_shape=bias_shape,
+        using_kwargs=using_kwargs,
+    )
 
 
 @pytest.mark.skip("skip due to bias cases not ready")
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @parameterize('module', [AddBMMTorchFunctionModule, AddBMMTensorMethodModule])
 @parameterize('bias_shape', [[8], [1, 8], [8, 8]])
 @parameterize('using_kwargs', [True, False])
 @rerun_if_address_is_in_use()
 def test_1d_device_mesh(module, bias_shape, using_kwargs):
-    world_size = 4
-    run_func = partial(check_1d_device_mesh,
-                       module=module,
-                       bias_shape=bias_shape,
-                       using_kwargs=using_kwargs,
-                       world_size=world_size,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(
+        check_1d_device_mesh,
+        4,
+        module=module,
+        bias_shape=bias_shape,
+        using_kwargs=using_kwargs,
+    )
 
 
 if __name__ == '__main__':
     test_1d_device_mesh()
     test_2d_device_mesh()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,24 @@
-from faulthandler import disable
-from functools import partial
-from xml.dom import WrongDocumentErr
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
-from typing_extensions import Self
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
-    OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
 )
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class AddmmModel(nn.Module):
 
     def __init__(self):
         super().__init__()
@@ -43,15 +36,15 @@
         self.bias = torch.nn.Parameter(torch.rand(bias_shape))
 
     def forward(self, m1):
         x = torch.addmm(self.bias, m1, self.weight, beta=3, alpha=2)
         return x
 
 
-def check_addmm_function_handler(rank, input_shape, model_cls, world_size, port):
+def check_addmm_function_handler(rank, world_size, port, input_shape, model_cls):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     if model_cls == AddmmModel:
         model = AddmmModel().cuda()
     else:
         model = AddmmModel_with_param(weight_shape=(8, 16), bias_shape=input_shape).cuda()
     physical_mesh_id = torch.arange(0, 4)
@@ -92,27 +85,28 @@
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names,
                                      node_type='bias_module')
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %m1 : torch.Tensor [#users=1] = placeholder[target=m1]
     #     %m2 : torch.Tensor [#users=1] = placeholder[target=m2]
     #     %transpose : [#users=1] = call_function[target=torch.transpose](args = (%m2, 0, 1), kwargs = {})
     #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%m1, %transpose), kwargs = {})
     #     %mul : [#users=1] = call_function[target=operator.mul](args = (%input_1, 3), kwargs = {})
     #     %mul_1 : [#users=1] = call_function[target=operator.mul](args = (2, %linear), kwargs = {})
     #     %add : [#users=1] = call_function[target=operator.add](args = (%mul_1, %mul), kwargs = {})
     #     return add
     graph = tracer.trace(model, meta_args=meta_args_for_tracer)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args_for_tracer.values())
     # [input_1, m1, m2, addmm, output]
     node_list = list(graph.nodes)
     linear_node = node_list[4]
     strategies_vector = StrategiesVector(linear_node)
 
     # build handler
     handler = LinearFunctionHandler(node=linear_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
@@ -186,18 +180,12 @@
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @parameterize('input_shape', [(16,), (4, 16)])
 @parameterize('model_cls', [AddmmModel, AddmmModel_with_param])
 @rerun_if_address_is_in_use()
 def test_addmm_handler(input_shape, model_cls):
-    world_size = 4
-    run_func_function = partial(check_addmm_function_handler,
-                                input_shape=input_shape,
-                                model_cls=model_cls,
-                                world_size=world_size,
-                                port=free_port())
-    mp.spawn(run_func_function, nprocs=world_size)
+    spawn(check_addmm_function_handler, 4, input_shape=input_shape, model_cls=model_cls)
 
 
 if __name__ == '__main__':
     test_addmm_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.batch_norm_handler import BatchNormModuleHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_bn_module_handler(rank, world_size, port):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = nn.Sequential(nn.BatchNorm2d(16)).cuda()
@@ -34,21 +31,23 @@
     strategy_number = 4
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input],
                                      meta_arg_names=['input'])
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %_0 : [#users=1] = call_module[target=0](args = (%input_1,), kwargs = {})
     #     return _0
-    graph = tracer.trace(model, meta_args={"input": torch.rand(4, 16, 64, 64).to('meta')})
+    meta_args = {"input": torch.rand(4, 16, 64, 64).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     bn_mod_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(bn_mod_node)
 
     # build handler
     handler = BatchNormModuleHandler(node=bn_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
     # check operation data mapping
@@ -106,14 +105,12 @@
     # assert 'S01R = S01R x R WITH SYNC_BN' in strategy_name_list
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_bn_module_handler():
-    world_size = 4
-    run_func = partial(check_bn_module_handler, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_bn_module_handler, 4)
 
 
 if __name__ == '__main__':
     test_bn_module_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,33 +1,25 @@
-from faulthandler import disable
-from functools import partial
-from xml.dom import WrongDocumentErr
-
 import pytest
 import torch
-import torch.multiprocessing as mp
-import torch.nn as nn
 import torch.nn.functional as F
-from typing_extensions import Self
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import LinearFunctionHandler, LinearModuleHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
 )
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.testing.utils import parameterize
-from colossalai.utils import free_port
+from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 WEIGHT_SHAPE = (32, 16)
 
 
 class LinearModule(torch.nn.Module):
 
@@ -62,24 +54,26 @@
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names,
                                      node_type='bias_module')
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %x : torch.Tensor [#users=1] = placeholder[target=x]
     #     %weight : [#users=1] = get_attr[target=weight]
     #     %bias : [#users=1] = get_attr[target=bias]
     #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%x, %weight), kwargs = {})
     #     %add : [#users=1] = call_function[target=operator.add](args = (%linear, %bias), kwargs = {})
     #     return add
-    graph = tracer.trace(model, meta_args={"x": torch.rand(4, 4, 4, 16).to('meta')})
+    meta_args = {"x": torch.rand(4, 4, 4, 16).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     linear_mod_node = list(graph.nodes)[3]
     strategies_vector = StrategiesVector(linear_mod_node)
 
     # build handler
     handler = LinearFunctionHandler(node=linear_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
     # check operation data mapping
@@ -164,14 +158,12 @@
         assert weight_sharding_spec.sharding_sequence[0] == output_sharding_spec.sharding_sequence[-1]
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_linear_handler():
-    world_size = 4
-    run_func_module = partial(check_linear_module_handler, world_size=world_size, port=free_port())
-    mp.spawn(run_func_module, nprocs=world_size)
+    spawn(check_linear_module_handler)
 
 
 if __name__ == '__main__':
     test_linear_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,47 +1,39 @@
-from faulthandler import disable
-from functools import partial
-from xml.dom import WrongDocumentErr
-
 import pytest
 import torch
-import torch.multiprocessing as mp
-import torch.nn as nn
-from typing_extensions import Self
 
-from colossalai.auto_parallel.tensor_shard.node_handler import LinearFunctionHandler, LinearModuleHandler
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
+from colossalai.auto_parallel.tensor_shard.node_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
 )
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.testing.utils import parameterize
-from colossalai.utils import free_port
+from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class LinearModule(torch.nn.Module):
 
     def __init__(self, in_features, out_features, bias):
         super().__init__()
         self.linear = torch.nn.Linear(in_features, out_features, bias=bias)
 
     def forward(self, x):
         x = self.linear(x)
         return x
 
 
-def check_linear_module_handler(rank, bias, world_size, port):
+def check_linear_module_handler(rank, world_size, port, bias):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = LinearModule(16, 32, bias=bias).cuda()
 
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
@@ -58,17 +50,19 @@
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names,
                                      node_type='bias_module')
 
-    tracer = ColoTracer()
-    graph = tracer.trace(model, meta_args={"x": torch.rand(4, 4, 4, 16).to('meta')})
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {"x": torch.rand(4, 4, 4, 16).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     linear_mod_node = list(graph.nodes)[3]
     strategies_vector = StrategiesVector(linear_mod_node)
 
     # build handler
     handler = LinearFunctionHandler(node=linear_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
     # check operation data mapping
@@ -153,14 +147,12 @@
         assert weight_sharding_spec.sharding_sequence[0] == output_sharding_spec.sharding_sequence[-1]
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_linear_handler(bias=True):
-    world_size = 4
-    run_func_module = partial(check_linear_module_handler, bias=bias, world_size=world_size, port=free_port())
-    mp.spawn(run_func_module, nprocs=world_size)
+    spawn(check_linear_module_handler, bias=bias)
 
 
 if __name__ == '__main__':
     test_linear_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,24 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import BinaryElementwiseHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
-def check_binary_elementwise_handler_with_tensor(rank, op, other_dim, world_size, port):
+def check_binary_elementwise_handler_with_tensor(rank, world_size, port, op, other_dim):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
 
     class BinaryElementwiseOpModel(nn.Module):
 
         def __init__(self, op):
             super().__init__()
@@ -48,18 +45,19 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     meta_args = {'x1': torch.rand(4, 4).to('meta'), 'x2': torch.rand([4] * other_dim).to('meta')}
     graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     op_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(op_node)
 
     # build handler
     handler = BinaryElementwiseHandler(node=op_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
@@ -142,15 +140,15 @@
         self.const = const
 
     def forward(self, x1):
         out = self.op(x1, self.const)
         return out
 
 
-def check_binary_elementwise_handler_with_int(rank, op, other_dim, model_cls, world_size, port):
+def check_binary_elementwise_handler_with_int(rank, world_size, port, op, other_dim, model_cls):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
 
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     if model_cls == BEOpModelWithNodeConst:
@@ -168,20 +166,19 @@
     meta_arg_names = ['x1']
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     meta_args = {'x1': torch.rand(4, 4).to('meta')}
     graph = tracer.trace(model, meta_args=meta_args)
-    print(graph)
-    # assert False
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     if model_cls == BEOpModelWithNodeConst:
         op_node = list(graph.nodes)[2]
     else:
         op_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(op_node)
 
@@ -230,36 +227,34 @@
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @parameterize('op', [torch.add])
 @parameterize('other_dim', [1, 2])
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_binary_elementwise_handler_with_tensor(op, other_dim):
-    world_size = 4
-    run_func_tensor = partial(check_binary_elementwise_handler_with_tensor,
-                              op=op,
-                              other_dim=other_dim,
-                              world_size=world_size,
-                              port=free_port())
-    mp.spawn(run_func_tensor, nprocs=world_size)
+    spawn(
+        check_binary_elementwise_handler_with_tensor,
+        4,
+        op=op,
+        other_dim=other_dim,
+    )
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @parameterize('op', [torch.add])
 @parameterize('other_dim', [1, 2])
 @parameterize('model_cls', [BEOpModelWithNodeConst, BEOpModelWithIntConst])
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_binary_elementwise_handler_with_int(op, model_cls, other_dim):
-    world_size = 4
-    run_func_int = partial(check_binary_elementwise_handler_with_int,
-                           op=op,
-                           model_cls=model_cls,
-                           other_dim=other_dim,
-                           world_size=world_size,
-                           port=free_port())
-    mp.spawn(run_func_int, nprocs=world_size)
+    spawn(
+        check_binary_elementwise_handler_with_int,
+        4,
+        op=op,
+        model_cls=model_cls,
+        other_dim=other_dim,
+    )
 
 
 if __name__ == '__main__':
     test_binary_elementwise_handler_with_tensor()
     test_binary_elementwise_handler_with_int()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import BMMFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class BMMTensorMethodModule(nn.Module):
 
     def forward(self, x1, x2):
         return x1.bmm(x2)
@@ -48,21 +45,19 @@
     meta_arg_names = ['x1', 'x2']
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
-    tracer = ColoTracer()
-    graph = tracer.trace(model,
-                         meta_args={
-                             "x1": torch.rand(4, 8, 16).to('meta'),
-                             'x2': torch.rand(4, 16, 8).to('meta')
-                         })
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {'x1': torch.rand(4, 8, 16).to('meta'), 'x2': torch.rand(4, 16, 8).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     linear_mod_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(linear_mod_node)
 
     # build handler
     handler = BMMFunctionHandler(node=linear_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
@@ -143,21 +138,19 @@
     meta_arg_names = ['x1', 'x2']
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
-    tracer = ColoTracer()
-    graph = tracer.trace(model,
-                         meta_args={
-                             "x1": torch.rand(4, 8, 16).to('meta'),
-                             'x2': torch.rand(4, 16, 8).to('meta')
-                         })
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {'x1': torch.rand(4, 8, 16).to('meta'), 'x2': torch.rand(4, 16, 8).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     linear_mod_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(linear_mod_node)
 
     # build handler
     handler = BMMFunctionHandler(node=linear_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
     # check operation data mapping
@@ -201,19 +194,17 @@
         assert input_sharding_spec.sharding_sequence[:-1] == output_sharding_spec.sharding_sequence[:-1]
         assert other_sharding_spec.sharding_sequence[1] == input_sharding_spec.sharding_sequence[-1]
         assert other_sharding_spec.sharding_sequence[-1] == output_sharding_spec.sharding_sequence[-1]
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @parameterize('module', [BMMTensorMethodModule, BMMTorchFunctionModule])
+@parameterize('module', [BMMTensorMethodModule, BMMTorchFunctionModule])
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_bmm_handler(module):
-    world_size = 4
-    run_func_2d = partial(check_2d_device_mesh, module=module, world_size=world_size, port=free_port())
-    mp.spawn(run_func_2d, nprocs=world_size)
-    run_func_1d = partial(check_1d_device_mesh, module=module, world_size=world_size, port=free_port())
-    mp.spawn(run_func_1d, nprocs=world_size)
+    spawn(check_2d_device_mesh, 4, module=module)
+    spawn(check_1d_device_mesh, 4, module=module)
 
 
 if __name__ == '__main__':
     test_bmm_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,24 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler, ConvModuleHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
-def check_conv_module_handler(rank, bias, world_size, port):
+def check_conv_module_handler(rank, world_size, port, bias):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = nn.Sequential(nn.Conv2d(4, 16, 3, padding=1, bias=bias)).cuda()
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %_0 : [#users=1] = call_module[target=0](args = (%input_1,), kwargs = {})
     #     return _0
@@ -37,17 +34,19 @@
     strategy_number = 16
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input],
                                      meta_arg_names=['input'])
-    tracer = ColoTracer()
-    graph = tracer.trace(model, meta_args={"input": torch.rand(4, 4, 64, 64).to('meta')})
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {'input': torch.rand(4, 4, 64, 64).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     conv_mod_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(conv_mod_node)
 
     # build handler
     handler = ConvModuleHandler(node=conv_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
     # check operation data mapping
@@ -147,15 +146,15 @@
         super().__init__()
 
     def forward(self, input, others, bias=None):
         x = nn.functional.conv2d(input, others, bias=bias, padding=1)
         return x
 
 
-def check_conv_function_handler(rank, bias, world_size, port):
+def check_conv_function_handler(rank, world_size, port, bias):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = ConvModel().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 4, 64, 64).cuda()
@@ -174,25 +173,26 @@
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names,
                                      input_kwargs=input_kwargs)
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %others : torch.Tensor [#users=1] = placeholder[target=others]
     #     %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %others), kwargs = {})
     #     return conv2d
     meta_args = {"input": torch.rand(4, 4, 64, 64).to('meta'), "others": torch.rand(16, 4, 3, 3).to('meta')}
     if bias:
         meta_args['bias'] = torch.rand(16).to('meta')
     graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     if bias:
         conv_mod_node = list(graph.nodes)[3]
     else:
         conv_mod_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(conv_mod_node)
 
@@ -293,27 +293,23 @@
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 # We temporarily ban the bias option before doing bias add
 # before all reduce communication may encounter correctness issue.
 # @parameterize('bias', [True, False])
 @rerun_if_address_is_in_use()
 def test_conv_module_handler(bias=False):
-    world_size = 4
-    run_func = partial(check_conv_module_handler, bias=bias, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_conv_module_handler, 4, bias=bias)
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 # We temporarily ban the bias option before doing bias add
 # before all reduce communication may encounter correctness issue.
 # @parameterize('bias', [True, False])
 @rerun_if_address_is_in_use()
 def test_conv_function_handler(bias=False):
-    world_size = 4
-    run_func = partial(check_conv_function_handler, bias=bias, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_conv_function_handler, 4, bias=bias)
 
 
 if __name__ == '__main__':
     test_conv_module_handler()
     test_conv_function_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,45 +1,49 @@
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import DefaultReshapeHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
+from colossalai.testing import clear_cache_before_run, run_on_environment_flag
 
 
 class ReshapeModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
     def forward(self, input, other):
         conv_node = nn.functional.conv2d(input, other)
         reshape_node = conv_node.view(2, -1)
         return reshape_node
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_reshape_handler():
     model = ReshapeModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %other : torch.Tensor [#users=1] = placeholder[target=other]
     #     %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %other), kwargs = {})
     #     %view : [#users=1] = call_method[target=view](args = (%conv2d, 2, -1), kwargs = {})
     #     return view
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(4, 4, 64, 64).to('meta'),
-                             "other": torch.rand(4, 16, 3, 3).to('meta'),
-                         })
+    meta_args = {
+        "input": torch.rand(4, 4, 64, 64).to('meta'),
+        "other": torch.rand(16, 4, 3, 3).to('meta'),
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     conv_mod_node = list(graph.nodes)[2]
     reshape_node = list(graph.nodes)[3]
     reshape_strategies_vector = StrategiesVector(reshape_node)
@@ -63,21 +67,21 @@
     for name, op_data in mapping.items():
         op_data: OperationData
         # make sure they have valid values
         assert op_data.data is not None
 
     assert mapping['input'].name == "conv2d"
     assert mapping['input'].data.is_meta
-    assert mapping['input'].data.shape == torch.Size([4, 4, 62, 62])
+    assert mapping['input'].data.shape == torch.Size([4, 16, 62, 62])
     assert mapping['input'].type == OperationDataType.ARG
-    assert mapping['input'].logical_shape == torch.Size([4, 4, 62, 62])
+    assert mapping['input'].logical_shape == torch.Size([4, 16, 62, 62])
 
     assert mapping['output'].name == "view"
     assert mapping['output'].data.is_meta
-    assert mapping['output'].data.shape == torch.Size([2, 30752])
+    assert mapping['output'].data.shape == torch.Size([2, 123008])
     assert mapping['output'].type == OperationDataType.OUTPUT
 
     # reshape handler is a following strategy handler, so the number of strategies is equal to the predecessor node.
     assert len(reshape_strategies_vector) == len(conv_strategies_vector)
 
 
 if __name__ == '__main__':
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.embedding_handler import (
     EmbeddingFunctionHandler,
     EmbeddingModuleHandler,
 )
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 NUM_EMBEDDINGS = 16
 EMBEDDING_DIMS = 32
 
 
 class EmbeddingModule(nn.Module):
@@ -56,17 +54,19 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input],
                                      meta_arg_names=['input'])
 
-    tracer = ColoTracer()
-    graph = tracer.trace(model, meta_args={"input": torch.rand(4, 16, 16).to('meta')})
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {"input": torch.randint(NUM_EMBEDDINGS, (4, 16, 16)).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     embedding_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(embedding_node)
 
     # build handler
     handler = EmbeddingModuleHandler(node=embedding_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
     # check operation data mapping
@@ -167,26 +167,27 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names,
                                      input_kwargs=input_kwargs)
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %others : torch.Tensor [#users=1] = placeholder[target=others]
     #     %embedding : [#users=1] = call_function[target=torch.nn.functional.embedding](args = (%input_1, %others), kwargs = {padding_idx: None, max_norm: None, norm_type: 2.0, scale_grad_by_freq: False, sparse: False})
     #     return embedding
     meta_args = {
-        "input": torch.rand(4, 16, 16).to('meta'),
+        "input": torch.randint(NUM_EMBEDDINGS, (4, 16, 16)).to('meta'),
         "others": torch.rand(NUM_EMBEDDINGS, EMBEDDING_DIMS).to('meta')
     }
     graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     embedding_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(embedding_node)
 
     # build handler
     handler = EmbeddingFunctionHandler(node=embedding_node,
                                        device_mesh=device_mesh,
@@ -263,24 +264,20 @@
         assert input_sharding_spec.sharding_sequence == output_sharding_spec.sharding_sequence[:-1]
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_embedding_module_handler():
-    world_size = 4
-    run_func = partial(check_embedding_module_handler, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_embedding_module_handler, 4)
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_embedding_function_handler():
-    world_size = 4
-    run_func = partial(check_embedding_function_handler, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_embedding_function_handler, 4)
 
 
 if __name__ == '__main__':
     test_embedding_module_handler()
     test_embedding_function_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,44 @@
+import pytest
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.getattr_handler import GetattrHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
+from colossalai.testing import clear_cache_before_run
 
 
 class GetattrModel(nn.Module):
 
     def __init__(self):
         super().__init__()
         self.conv = nn.Conv2d(4, 16, 3, padding=1, bias=False)
 
     def forward(self, input):
         weight = self.conv.weight
         return weight
 
 
+@pytest.mark.skip('ShapeProp is not compatible with PyTorch 1.11.0')
+@clear_cache_before_run()
 def test_getattr_handler():
     model = GetattrModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=0] = placeholder[target=input]
     #     %conv_weight : [#users=1] = get_attr[target=conv.weight]
     #     return conv_weight
-    graph = tracer.trace(model, meta_args={'input': torch.rand(4, 4, 64, 64).to('meta')})
+    meta_args = {'input': torch.rand(4, 4, 64, 64).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     getattr_node = list(graph.nodes)[1]
     getattr_strategies_vector = StrategiesVector(getattr_node)
 
     # build handler
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,26 @@
 from functools import partial
 
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.default_reshape_handler import DefaultReshapeHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.getitem_handler import GetItemHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.linear_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.placeholder_handler import PlaceholderHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.fx.tracer.meta_patch.patched_module import linear
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import clear_cache_before_run, parameterize, rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class GetItemFromTensorModel(nn.Module):
 
     def __init__(self, getitem_index):
         super().__init__()
@@ -54,23 +53,23 @@
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input, other],
                                      meta_arg_names=['input', 'other'],
                                      node_type='following')
 
-    tracer = ColoTracer()
-
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(8, 16, 64, 32).to('meta'),
-                             "other": torch.rand(64, 32).to('meta'),
-                         })
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {
+        "input": torch.rand(8, 16, 64, 32).to('meta'),
+        "other": torch.rand(64, 32).to('meta'),
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
 
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *list(meta_args.values()))
     linear_mod_node = list(graph.nodes)[2]
     getitem_mod_node = list(graph.nodes)[3]
     getitem_strategies_vector = StrategiesVector(getitem_mod_node)
     linear_strategies_vector = StrategiesVector(linear_mod_node)
 
     # build handler
     linear_handler = LinearFunctionHandler(node=linear_mod_node,
@@ -97,46 +96,44 @@
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 # @parameterize('getitem_index', [slice(0, 2), (slice(None), slice(None))])
 @parameterize('getitem_index', [1, (1, 4), slice(0, 2), (slice(None), slice(None))])
 def test_getitem_from_tensor_handler(getitem_index):
-    world_size = 4
-    run_func = partial(check_getitem_from_tensor_handler,
-                       getitem_index=getitem_index,
-                       world_size=world_size,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_getitem_from_tensor_handler, 4)
 
 
 class GetItemFromTupleModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
     def forward(self, input):
         split_node = torch.split(input, 2, 0)
         x = split_node[1]
         return x
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_getitem_from_tuple_handler():
     model = GetItemFromTupleModel()
     tracer = ColoTracer()
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %split : [#users=1] = call_function[target=torch.functional.split](args = (%conv2d, 2), kwargs = {dim: 0})
     #     %getitem : [#users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})
     #     return getitem
-    graph = tracer.trace(model, meta_args={
+    meta_args = {
         "input": torch.rand(4, 4, 64, 64).to('meta'),
-    })
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     input_node = list(graph.nodes)[0]
     split_node = list(graph.nodes)[1]
     getitem_node = list(graph.nodes)[2]
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.layer_norm_handler import LayerNormModuleHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.fx.tracer.meta_patch.patched_module import linear
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_ln_module_handler(rank, world_size, port):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = nn.Sequential(nn.LayerNorm(16)).cuda()
@@ -36,21 +33,23 @@
     meta_arg_names = ['input']
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %_0 : [#users=1] = call_module[target=0](args = (%input_1,), kwargs = {})
     #     return _0
-    graph = tracer.trace(model, meta_args={"input": torch.rand(4, 16).to('meta')})
+    meta_args = {"input": torch.rand(4, 16).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     ln_mod_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(ln_mod_node)
 
     # build handler
     handler = LayerNormModuleHandler(node=ln_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
@@ -96,14 +95,12 @@
     assert '[S01, R] = [S01, R] x [R]' in strategy_name_list
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_ln_module_handler():
-    world_size = 4
-    run_func = partial(check_ln_module_handler, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_ln_module_handler, 4)
 
 
 if __name__ == '__main__':
     test_ln_module_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,33 +1,31 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import LinearFunctionHandler, LinearModuleHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
 )
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
 from colossalai.testing.utils import parameterize
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
-def check_linear_module_handler(rank, bias, input_shape, world_size, port):
+def check_linear_module_handler(rank, world_size, port, bias, input_shape):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = nn.Sequential(nn.Linear(16, 32, bias=bias)).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(input_shape).cuda()
@@ -45,17 +43,19 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
 
-    tracer = ColoTracer()
-    graph = tracer.trace(model, meta_args={"input": torch.rand(input_shape).to('meta')})
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {"input": torch.rand(input_shape).cuda()}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     linear_mod_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(linear_mod_node)
 
     # build handler
     handler = LinearModuleHandler(node=linear_mod_node, device_mesh=device_mesh, strategies_vector=strategies_vector)
 
@@ -164,15 +164,15 @@
         super().__init__()
 
     def forward(self, input, others, bias=None):
         x = nn.functional.linear(input, others, bias=bias)
         return x
 
 
-def check_linear_function_handler(rank, bias, input_shape, world_size, port):
+def check_linear_function_handler(rank, world_size, port, bias, input_shape):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = LinearModel().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
 
@@ -192,21 +192,20 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=input_args,
                                      meta_arg_names=meta_arg_names)
 
-    tracer = ColoTracer()
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(input_shape).to('meta'),
-                             'others': torch.rand(32, 16).to('meta')
-                         })
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {'input': torch.rand(input_shape).to('meta'), 'others': torch.rand(32, 16).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
+
     if bias:
         linear_func_node = list(graph.nodes)[3]
     else:
         linear_func_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(linear_func_node)
 
     # build handler
@@ -306,24 +305,23 @@
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @parameterize('input_shape', [(1, 4, 4, 16), (4, 4, 4, 16)])
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 def test_linear_handler(input_shape, bias=False):
-    world_size = 4
-    run_func_module = partial(check_linear_module_handler,
-                              bias=bias,
-                              input_shape=input_shape,
-                              world_size=world_size,
-                              port=free_port())
-    mp.spawn(run_func_module, nprocs=world_size)
-    run_func_function = partial(check_linear_function_handler,
-                                bias=bias,
-                                input_shape=input_shape,
-                                world_size=world_size,
-                                port=free_port())
-    mp.spawn(run_func_function, nprocs=world_size)
+    spawn(
+        check_linear_module_handler,
+        4,
+        bias=bias,
+        input_shape=input_shape,
+    )
+    spawn(
+        check_linear_function_handler,
+        4,
+        bias=bias,
+        input_shape=input_shape,
+    )
 
 
 if __name__ == '__main__':
     test_linear_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,35 +1,38 @@
 import pytest
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.matmul_handler import (
     MatMulHandler,
     MatMulType,
     _get_bmm_logical_shape,
     get_matmul_type,
 )
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import (
     OperationData,
     OperationDataType,
     ShardingStrategy,
     StrategiesVector,
 )
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.testing.utils import parameterize
+from colossalai.testing.utils import clear_cache_before_run, parameterize
 
 
 class MatMulModule(nn.Module):
 
     def forward(self, x1, x2):
         return torch.matmul(x1, x2)
 
 
 @pytest.mark.skipif(torch.__version__ < '1.12.0', reason="need pytorch 1.12.0 or higher for aten level operations")
+@clear_cache_before_run()
 @parameterize(
     'tensor_shapes',
     [
         [[8], [8]],    # dot product
         [[4, 8], [8]],    # mat-vec product
         [[4, 8], [8, 16]],    # mat-mat product
         [[8], [8, 16]],    # mat-mat product
@@ -53,17 +56,19 @@
     output_shape = list(torch.matmul(x1, x2).shape)
 
     # get matmul type
     matmul_type = get_matmul_type(x1.dim(), x2.dim())
 
     model = MatMulModule()
 
-    tracer = ColoTracer()
-    graph = tracer.trace(model, meta_args={"x1": x1.to('meta'), 'x2': x2.to('meta')})
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {"x1": x1.to('meta'), 'x2': x2.to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     print(graph)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     mod_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(mod_node)
@@ -120,15 +125,14 @@
         assert len(set(strategy_name_list)) == len(strategy_name_list), strategy_name_list
 
     for strategy in strategies_vector:
         strategy: ShardingStrategy
         input_sharding_spec = strategy.get_sharding_spec_by_name('x1')
         other_sharding_spec = strategy.get_sharding_spec_by_name('x2')
         output_sharding_spec = strategy.get_sharding_spec_by_name('matmul')
-
         if matmul_type == MatMulType.DOT:
             # dot product will produce a scaler
             # results should fulfill:
             # 1. the input and other operands have the same sharding spec
             # 2. the output has no sharding
             assert input_sharding_spec.sharding_sequence == other_sharding_spec.sharding_sequence
             assert len(output_sharding_spec.sharding_sequence) == 0
@@ -155,14 +159,17 @@
             # bmm should fulfil
             # 1. of the other tensor is not a 1d tensor, the last dim of other and output have the same sharding
             # 2. if the input has more than 2 dim, the second last dim of input and output have the same sharding
             # 3. if the other have more than 2 dim, the second last dim of other and the last dim of input should have the same sharding
             if len(other_shape) > 1:
                 assert other_sharding_spec.sharding_sequence[-1] == output_sharding_spec.sharding_sequence[-1]
             if len(input_shape) > 1:
-                assert input_sharding_spec.sharding_sequence[-2] == output_sharding_spec.sharding_sequence[-2]
+                if len(other_shape) == 1:
+                    assert input_sharding_spec.sharding_sequence[-2] == output_sharding_spec.sharding_sequence[-1]
+                else:
+                    assert input_sharding_spec.sharding_sequence[-2] == output_sharding_spec.sharding_sequence[-2]
             if len(other_shape) > 2:
                 assert other_sharding_spec.sharding_sequence[-2] == input_sharding_spec.sharding_sequence[-1]
 
 
 if __name__ == '__main__':
     test_matmul_node_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,30 +1,33 @@
-import pytest
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.normal_pooling_handler import NormPoolingHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.fx.tracer.meta_patch.patched_module import linear
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
+from colossalai.testing import clear_cache_before_run, run_on_environment_flag
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_norm_pool_handler():
     model = nn.Sequential(nn.MaxPool2d(4, padding=1).to('meta'))
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %_0 : [#users=1] = call_module[target=0](args = (%input_1,), kwargs = {})
     #     return _0
-    graph = tracer.trace(model, meta_args={"input": torch.rand(4, 4, 64, 64).to('meta')})
+    meta_args = {"input": torch.rand(4, 4, 64, 64).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
 
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     conv_mod_node = list(graph.nodes)[1]
     strategies_vector = StrategiesVector(conv_mod_node)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,40 +1,44 @@
+import pytest
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.output_handler import OutputHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import clear_cache_before_run, parameterize
 
 
 class OutputModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
     def forward(self, x):
         y = x * 2
         return x, y
 
 
+@pytest.mark.skip('ShapeProp is not compatible with PyTorch 1.11.0')
 @parameterize('output_option', ['distributed', 'replicated'])
-@rerun_if_address_is_in_use()
+@clear_cache_before_run()
 def test_output_handler(output_option):
     model = OutputModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %x : torch.Tensor [#users=2] = placeholder[target=x]
     #     %mul : [#users=1] = call_function[target=operator.mul](args = (%x, 2), kwargs = {})
     #     return (x, mul)
-    graph = tracer.trace(model, meta_args={
-        "x": torch.rand(4, 4, 64, 64).to('meta'),
-    })
+    meta_args = {'x': torch.rand(4, 4, 64, 64).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     output_node = list(graph.nodes)[2]
     output_strategies_vector = StrategiesVector(output_node)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 from functools import partial
 
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import PermuteHandler, TransposeHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.linear_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class ConvReshapeModel(nn.Module):
 
     def __init__(self, reshape_dims, call_function):
         super().__init__()
@@ -49,15 +49,15 @@
         if self.call_function == torch.permute:
             permute_node = self.call_function(linear_node, self.reshape_dims)
         else:
             permute_node = self.call_function(linear_node, *self.reshape_dims)
         return permute_node
 
 
-def check_view_handler(rank, call_function, reshape_dims, model_cls, world_size, port):
+def check_view_handler(rank, world_size, port, call_function, reshape_dims, model_cls):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     if call_function == torch.permute:
         reshape_dims = reshape_dims[0]
     elif call_function == torch.transpose:
         reshape_dims = reshape_dims[1]
     model = model_cls(reshape_dims, call_function).cuda()
@@ -84,42 +84,43 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input, other],
                                      meta_arg_names=['input', 'other'],
                                      node_type='following')
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     if model_cls.__name__ == 'ConvReshapeModel':
         # graph():
         #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
         #     %other : torch.Tensor [#users=1] = placeholder[target=other]
         #     %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %other), kwargs = {bias: None})
         #     %permute : [#users=1] = call_function[target=torch.permute](args = (%conv2d, (0, 2, 1, 3)), kwargs = {})
         #     return permute
-        graph = tracer.trace(model,
-                             meta_args={
-                                 "input": torch.rand(8, 8, 66, 66).to('meta'),
-                                 "other": torch.rand(16, 8, 3, 3).to('meta'),
-                             })
+        meta_args = {
+            'input': torch.rand(8, 8, 66, 66).to('meta'),
+            'other': torch.rand(16, 8, 3, 3).to('meta'),
+        }
+        graph = tracer.trace(model, meta_args=meta_args)
 
     if model_cls.__name__ == 'LinearReshapeModel':
         # graph():
         #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
         #     %other : torch.Tensor [#users=1] = placeholder[target=other]
         #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%input_1, %other), kwargs = {bias: None})
         #     %permute : [#users=1] = call_method[target=view](args = (%linear, 32, 4, 32, 32, 4), kwargs = {})
         #     return permute
-        graph = tracer.trace(model,
-                             meta_args={
-                                 "input": torch.rand(8, 16, 64, 32).to('meta'),
-                                 "other": torch.rand(64, 32).to('meta'),
-                             })
+        meta_args = {
+            'input': torch.rand(8, 16, 64, 32).to('meta'),
+            'other': torch.rand(64, 32).to('meta'),
+        }
+        graph = tracer.trace(model, meta_args=meta_args)
 
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     previous_mod_node = list(graph.nodes)[2]
     reshape_node = list(graph.nodes)[3]
     view_strategies_vector = StrategiesVector(reshape_node)
     previous_strategies_vector = StrategiesVector(previous_mod_node)
 
     # build handler
@@ -321,19 +322,18 @@
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @parameterize('call_function', [torch.permute, torch.transpose])
 @parameterize('reshape_dims', [((0, 2, 1, 3), (1, 2)), ((2, 0, 1, 3), (1, 3))])
 @parameterize('model_cls', [ConvReshapeModel, LinearReshapeModel])
 def test_view_handler(call_function, reshape_dims, model_cls):
-    world_size = 4
-    run_func = partial(check_view_handler,
-                       call_function=call_function,
-                       reshape_dims=reshape_dims,
-                       model_cls=model_cls,
-                       world_size=world_size,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(
+        check_view_handler,
+        4,
+        call_function=call_function,
+        reshape_dims=reshape_dims,
+        model_cls=model_cls,
+    )
 
 
 if __name__ == '__main__':
     test_view_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,38 +1,44 @@
+import pytest
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.placeholder_handler import PlaceholderHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import clear_cache_before_run, parameterize
 
 
 class PlaceholderModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
     def forward(self, input):
         return input
 
 
+@pytest.mark.skip('ShapeProp is not compatible with PyTorch 1.11.0')
 @parameterize('placeholder_option', ['distributed', 'replicated'])
-@rerun_if_address_is_in_use()
+@clear_cache_before_run()
 def test_placeholder_handler(placeholder_option):
     model = PlaceholderModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     return input_1
-    graph = tracer.trace(model, meta_args={
+    meta_args = {
         "input": torch.rand(4, 4, 64, 64).to('meta'),
-    })
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     placeholder_node = list(graph.nodes)[0]
     placeholder_strategies_vector = StrategiesVector(placeholder_node)
     # build handler
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,18 @@
-from functools import partial
-
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.options import ShardOption
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.testing import parameterize
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.testing.utils import parameterize
+from colossalai.testing import clear_cache_before_run, run_on_environment_flag
 
 
 class LinearModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
@@ -26,21 +23,19 @@
 
 def check_shard_option(shard_option):
     model = LinearModel().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
 
-    tracer = ColoTracer()
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(4, 4, 4, 16).to('meta'),
-                             'others': torch.rand(32, 16).to('meta')
-                         })
+    tracer = ColoTracer(bias_addition_split=True)
+    meta_args = {'input': torch.rand(4, 4, 4, 16).to('meta'), 'others': torch.rand(32, 16).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     linear_func_node = list(graph.nodes)[2]
     strategies_vector = StrategiesVector(linear_func_node)
 
     # build handler
     handler = LinearFunctionHandler(node=linear_func_node,
                                     device_mesh=device_mesh,
                                     strategies_vector=strategies_vector,
@@ -108,14 +103,15 @@
         assert 'RS1 = RR x RS1' in strategy_name_list
 
         # RR = RR x RR
         assert 'RR = RR x RR' in strategy_name_list
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_shard_option():
     # for shard_option in [ShardOption.STANDARD, ShardOption.SHARD, ShardOption.FULL_SHARD, ShardOption.SHARD_LAST_AXIS]:
     for shard_option in [ShardOption.SHARD_LAST_AXIS]:
         check_shard_option(shard_option)
 
 
 if __name__ == '__main__':
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,22 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 import torch.nn.functional as F
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.linear_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.softmax_handler import SoftmaxHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class LinearSplitModel(nn.Module):
 
     def __init__(self, softmax_dim):
         super().__init__()
@@ -27,15 +24,15 @@
 
     def forward(self, input, other):
         linear_node = F.linear(input, other, bias=None)
         softmax_node = F.softmax(linear_node, self.softmax_dim)
         return softmax_node
 
 
-def check_split_handler(rank, softmax_dim, model_cls, world_size, port):
+def check_split_handler(rank, world_size, port, softmax_dim, model_cls):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = model_cls(softmax_dim=softmax_dim).cuda()
 
     input = torch.rand(8, 16, 64, 32).to('cuda')
     other = torch.rand(64, 32).to('cuda')
     # index of linear node in computation graph
@@ -50,29 +47,30 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input, other],
                                      meta_arg_names=['input', 'other'],
                                      node_type='following')
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
 
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %other : torch.Tensor [#users=1] = placeholder[target=other]
     #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%input_1, %other), kwargs = {bias: None})
     #     %softmax : [#users=1] = call_method[target=split](args = (%linear,), kwargs = {})
     #     return split
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(8, 16, 64, 32).to('meta'),
-                             "other": torch.rand(64, 32).to('meta'),
-                         })
+    meta_args = {
+        'input': torch.rand(8, 16, 64, 32).to('meta'),
+        'other': torch.rand(64, 32).to('meta'),
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
 
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     previous_mod_node = list(graph.nodes)[2]
     split_node = list(graph.nodes)[3]
     split_strategies_vector = StrategiesVector(split_node)
     previous_strategies_vector = StrategiesVector(previous_mod_node)
 
     # build handler
@@ -169,18 +167,12 @@
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @parameterize('softmax_dim', [0, 1, 2, 3])
 @parameterize('model_cls', [LinearSplitModel])
 def test_split_handler(softmax_dim, model_cls):
-    world_size = 4
-    run_func = partial(check_split_handler,
-                       softmax_dim=softmax_dim,
-                       model_cls=model_cls,
-                       world_size=world_size,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_split_handler, 4, softmax_dim=softmax_dim, model_cls=model_cls)
 
 
 if __name__ == '__main__':
     test_split_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,22 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import SplitHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.linear_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class ConvSplitModel(nn.Module):
 
     def __init__(self, split_size, split_dim):
         super().__init__()
@@ -41,15 +38,15 @@
 
     def forward(self, input, other):
         linear_node = nn.functional.linear(input, other, bias=None)
         split_node = linear_node.split(self.split_size, dim=self.split_dim)
         return split_node
 
 
-def check_split_handler(rank, split_size, split_dim, model_cls, world_size, port):
+def check_split_handler(rank, world_size, port, split_size, split_dim, model_cls):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = model_cls(split_size=split_size, split_dim=split_dim).cuda()
 
     if model_cls.__name__ == 'ConvSplitModel':
         input = torch.rand(8, 8, 66, 66).to('cuda')
         other = torch.rand(16, 8, 3, 3).to('cuda')
@@ -72,42 +69,43 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input, other],
                                      meta_arg_names=['input', 'other'],
                                      node_type='following')
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     if model_cls.__name__ == 'ConvSplitModel':
         # graph():
         #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
         #     %other : torch.Tensor [#users=1] = placeholder[target=other]
         #     %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %other), kwargs = {})
         #     %split : [#users=1] = call_method[target=split](args = (%conv2d,), kwargs = {})
         #     return split
-        graph = tracer.trace(model,
-                             meta_args={
-                                 "input": torch.rand(8, 8, 66, 66).to('meta'),
-                                 "other": torch.rand(16, 8, 3, 3).to('meta'),
-                             })
+        meta_args = {
+            'input': torch.rand(8, 8, 66, 66).to('meta'),
+            'other': torch.rand(16, 8, 3, 3).to('meta'),
+        }
+        graph = tracer.trace(model, meta_args=meta_args)
 
     if model_cls.__name__ == 'LinearSplitModel':
         # graph():
         #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
         #     %other : torch.Tensor [#users=1] = placeholder[target=other]
         #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%input_1, %other), kwargs = {bias: None})
         #     %split : [#users=1] = call_method[target=split](args = (%linear,), kwargs = {})
         #     return split
-        graph = tracer.trace(model,
-                             meta_args={
-                                 "input": torch.rand(8, 16, 64, 32).to('meta'),
-                                 "other": torch.rand(64, 32).to('meta'),
-                             })
+        meta_args = {
+            'input': torch.rand(8, 16, 64, 32).to('meta'),
+            'other': torch.rand(64, 32).to('meta'),
+        }
+        graph = tracer.trace(model, meta_args=meta_args)
 
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     previous_mod_node = list(graph.nodes)[2]
     split_node = list(graph.nodes)[3]
     split_strategies_vector = StrategiesVector(split_node)
     previous_strategies_vector = StrategiesVector(previous_mod_node)
 
     # build handler
@@ -251,19 +249,12 @@
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @parameterize('split_size', [2])
 @parameterize('split_dim', [0, 1, 2])
 @parameterize('model_cls', [ConvSplitModel, LinearSplitModel])
 def test_split_handler(split_size, split_dim, model_cls):
-    world_size = 4
-    run_func = partial(check_split_handler,
-                       split_size=split_size,
-                       split_dim=split_dim,
-                       model_cls=model_cls,
-                       world_size=world_size,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_split_handler, 4, split_size=split_size, split_dim=split_dim, model_cls=model_cls)
 
 
 if __name__ == '__main__':
     test_split_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,21 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
-from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.linear_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.sum_handler import SumHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class LinearSumModel(nn.Module):
 
     def __init__(self, sum_dims, keepdim):
         super().__init__()
@@ -31,15 +27,15 @@
         if self.sum_dims is not None:
             sum_node = torch.sum(linear_node, self.sum_dims, keepdim=self.keepdim)
         else:
             sum_node = torch.sum(linear_node, keepdim=self.keepdim)
         return sum_node
 
 
-def check_sum_handler(rank, sum_dims, keepdim, world_size, port):
+def check_sum_handler(rank, world_size, port, sum_dims, keepdim):
     disable_existing_loggers()
     launch(config={}, rank=rank, world_size=world_size, host='localhost', port=port, backend='nccl')
     model = LinearSumModel(sum_dims=sum_dims, keepdim=keepdim).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
 
@@ -54,28 +50,29 @@
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input, other],
                                      meta_arg_names=['input', 'other'],
                                      node_type='following')
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
 
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %other : torch.Tensor [#users=1] = placeholder[target=other]
     #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%input_1, %other), kwargs = {bias: None})
     #     %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%linear,), kwargs = {})
     #     return sum_1
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(8, 16, 64, 32).to('meta'),
-                             "other": torch.rand(64, 32).to('meta'),
-                         })
+    meta_args = {
+        "input": torch.rand(8, 16, 64, 32).to('meta'),
+        "other": torch.rand(64, 32).to('meta'),
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     previous_mod_node = list(graph.nodes)[2]
     sum_node = list(graph.nodes)[3]
     sum_strategies_vector = StrategiesVector(sum_node)
     previous_strategies_vector = StrategiesVector(previous_mod_node)
 
     # build handler
@@ -112,124 +109,122 @@
     assert mapping['output'].name == "sum_1"
     sum_node_shape = torch.empty([8, 16, 64, 64]).sum(sum_dims, keepdim=keepdim).shape
     assert mapping['output'].logical_shape == sum_node_shape
     assert mapping['output'].type == OperationDataType.OUTPUT
 
     # check strategy name
     if sum_dims == (0, 2) and keepdim == False:
-        assert '[R, R, R, S1] -> [R, S1]_0' in strategy_name_list
-        assert '[R, S0, R, S1] -> [S0, S1]_1' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, S1]_2' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, S0]_3' in strategy_name_list
-        assert '[R, S1, R, S0] -> [S1, S0]_4' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, S0]_5' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_6' in strategy_name_list
-        assert '[R, S0, R, R] -> [S0, R]_7' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_0' in strategy_name_list
+        assert '[R, S01, R, R] -> [S01, R]_1' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_2' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_3' in strategy_name_list
+        assert '[R, R, R, S01] -> [R, S01]_4' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, S1]_5' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, S0]_6' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_7' in strategy_name_list
         assert '[R, R, R, R] -> [R, R]_8' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_9' in strategy_name_list
-        assert '[R, S1, R, R] -> [S1, R]_10' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_11' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, S1]_12' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, S0]_13' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_14' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_15' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, S0]_9' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, S1]_10' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, S1]_11' in strategy_name_list
+        assert '[R, S0, R, S1] -> [S0, S1]_12' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, S1]_13' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, S0]_14' in strategy_name_list
+        assert '[R, S1, R, S0] -> [S1, S0]_15' in strategy_name_list
         assert '[R, R, R, S0] -> [R, S0]_16' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, S1]_17' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_18' in strategy_name_list
-        assert '[R, S01, R, R] -> [S01, R]_19' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_17' in strategy_name_list
+        assert '[R, S0, R, R] -> [S0, R]_18' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_19' in strategy_name_list
         assert '[R, R, R, R] -> [R, R]_20' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R]_21' in strategy_name_list
-        assert '[R, R, R, S01] -> [R, S01]_22' in strategy_name_list
+        assert '[R, S1, R, R] -> [S1, R]_21' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R]_22' in strategy_name_list
         assert '[R, R, R, R] -> [R, R]_23' in strategy_name_list
 
     if sum_dims == (0, 2) and keepdim == True:
-        assert '[R, R, R, S1] -> [R, R, R, S1]_0' in strategy_name_list
-        assert '[R, S0, R, S1] -> [R, S0, R, S1]_1' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, R, S1]_2' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, R, S0]_3' in strategy_name_list
-        assert '[R, S1, R, S0] -> [R, S1, R, S0]_4' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, R, S0]_5' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_6' in strategy_name_list
-        assert '[R, S0, R, R] -> [R, S0, R, R]_7' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_0' in strategy_name_list
+        assert '[R, S01, R, R] -> [R, S01, R, R]_1' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_2' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_3' in strategy_name_list
+        assert '[R, R, R, S01] -> [R, R, R, S01]_4' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, R, S1]_5' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, R, S0]_6' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_7' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R, R]_8' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_9' in strategy_name_list
-        assert '[R, S1, R, R] -> [R, S1, R, R]_10' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_11' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, R, S1]_12' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, R, S0]_13' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_14' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_15' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, R, S0]_9' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, R, S1]_10' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, R, S1]_11' in strategy_name_list
+        assert '[R, S0, R, S1] -> [R, S0, R, S1]_12' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, R, S1]_13' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, R, S0]_14' in strategy_name_list
+        assert '[R, S1, R, S0] -> [R, S1, R, S0]_15' in strategy_name_list
         assert '[R, R, R, S0] -> [R, R, R, S0]_16' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, R, S1]_17' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_18' in strategy_name_list
-        assert '[R, S01, R, R] -> [R, S01, R, R]_19' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_17' in strategy_name_list
+        assert '[R, S0, R, R] -> [R, S0, R, R]_18' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_19' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R, R]_20' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_21' in strategy_name_list
-        assert '[R, R, R, S01] -> [R, R, R, S01]_22' in strategy_name_list
+        assert '[R, S1, R, R] -> [R, S1, R, R]_21' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_22' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R, R]_23' in strategy_name_list
 
     if sum_dims == 1 and keepdim == False:
-        assert '[S0, R, R, S1] -> [S0, R, S1]_0' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, S1]_1' in strategy_name_list
-        assert '[R, R, S0, S1] -> [R, S0, S1]_2' in strategy_name_list
-        assert '[S1, R, R, S0] -> [S1, R, S0]_3' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, S0]_4' in strategy_name_list
-        assert '[R, R, S1, S0] -> [R, S1, S0]_5' in strategy_name_list
-        assert '[S0, R, R, R] -> [S0, R, R]_6' in strategy_name_list
+        assert '[S01, R, R, R] -> [S01, R, R]_0' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R]_1' in strategy_name_list
+        assert '[R, R, S01, R] -> [R, S01, R]_2' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R]_3' in strategy_name_list
+        assert '[R, R, R, S01] -> [R, R, S01]_4' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, S1]_5' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, S0]_6' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R]_7' in strategy_name_list
-        assert '[R, R, S0, R] -> [R, S0, R]_8' in strategy_name_list
-        assert '[S1, R, R, R] -> [S1, R, R]_9' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R]_10' in strategy_name_list
-        assert '[R, R, S1, R] -> [R, S1, R]_11' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R]_8' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, S0]_9' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, S1]_10' in strategy_name_list
+        assert '[S0, R, R, S1] -> [S0, R, S1]_11' in strategy_name_list
         assert '[R, R, R, S1] -> [R, R, S1]_12' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, S0]_13' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R]_14' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R]_15' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, S0]_16' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, S1]_17' in strategy_name_list
-        assert '[S01, R, R, R] -> [S01, R, R]_18' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R]_19' in strategy_name_list
-        assert '[R, R, S01, R] -> [R, S01, R]_20' in strategy_name_list
+        assert '[R, R, S0, S1] -> [R, S0, S1]_13' in strategy_name_list
+        assert '[S1, R, R, S0] -> [S1, R, S0]_14' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, S0]_15' in strategy_name_list
+        assert '[R, R, S1, S0] -> [R, S1, S0]_16' in strategy_name_list
+        assert '[S0, R, R, R] -> [S0, R, R]_17' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R]_18' in strategy_name_list
+        assert '[R, R, S0, R] -> [R, S0, R]_19' in strategy_name_list
+        assert '[S1, R, R, R] -> [S1, R, R]_20' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R]_21' in strategy_name_list
-        assert '[R, R, R, S01] -> [R, R, S01]_22' in strategy_name_list
+        assert '[R, R, S1, R] -> [R, S1, R]_22' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R]_23' in strategy_name_list
 
     if sum_dims == 1 and keepdim == True:
-        assert '[S0, R, R, S1] -> [S0, R, R, S1]_0' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, R, S1]_1' in strategy_name_list
-        assert '[R, R, S0, S1] -> [R, R, S0, S1]_2' in strategy_name_list
-        assert '[S1, R, R, S0] -> [S1, R, R, S0]_3' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, R, S0]_4' in strategy_name_list
-        assert '[R, R, S1, S0] -> [R, R, S1, S0]_5' in strategy_name_list
-        assert '[S0, R, R, R] -> [S0, R, R, R]_6' in strategy_name_list
+        assert '[S01, R, R, R] -> [S01, R, R, R]_0' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_1' in strategy_name_list
+        assert '[R, R, S01, R] -> [R, R, S01, R]_2' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_3' in strategy_name_list
+        assert '[R, R, R, S01] -> [R, R, R, S01]_4' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, R, S1]_5' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, R, S0]_6' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R, R]_7' in strategy_name_list
-        assert '[R, R, S0, R] -> [R, R, S0, R]_8' in strategy_name_list
-        assert '[S1, R, R, R] -> [S1, R, R, R]_9' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_10' in strategy_name_list
-        assert '[R, R, S1, R] -> [R, R, S1, R]_11' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_8' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, R, S0]_9' in strategy_name_list
+        assert '[R, R, R, S1] -> [R, R, R, S1]_10' in strategy_name_list
+        assert '[S0, R, R, S1] -> [S0, R, R, S1]_11' in strategy_name_list
         assert '[R, R, R, S1] -> [R, R, R, S1]_12' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, R, S0]_13' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_14' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_15' in strategy_name_list
-        assert '[R, R, R, S0] -> [R, R, R, S0]_16' in strategy_name_list
-        assert '[R, R, R, S1] -> [R, R, R, S1]_17' in strategy_name_list
-        assert '[S01, R, R, R] -> [S01, R, R, R]_18' in strategy_name_list
-        assert '[R, R, R, R] -> [R, R, R, R]_19' in strategy_name_list
-        assert '[R, R, S01, R] -> [R, R, S01, R]_20' in strategy_name_list
+        assert '[R, R, S0, S1] -> [R, R, S0, S1]_13' in strategy_name_list
+        assert '[S1, R, R, S0] -> [S1, R, R, S0]_14' in strategy_name_list
+        assert '[R, R, R, S0] -> [R, R, R, S0]_15' in strategy_name_list
+        assert '[R, R, S1, S0] -> [R, R, S1, S0]_16' in strategy_name_list
+        assert '[S0, R, R, R] -> [S0, R, R, R]_17' in strategy_name_list
+        assert '[R, R, R, R] -> [R, R, R, R]_18' in strategy_name_list
+        assert '[R, R, S0, R] -> [R, R, S0, R]_19' in strategy_name_list
+        assert '[S1, R, R, R] -> [S1, R, R, R]_20' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R, R]_21' in strategy_name_list
-        assert '[R, R, R, S01] -> [R, R, R, S01]_22' in strategy_name_list
+        assert '[R, R, S1, R] -> [R, R, S1, R]_22' in strategy_name_list
         assert '[R, R, R, R] -> [R, R, R, R]_23' in strategy_name_list
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @parameterize('sum_dims', [(0, 2), 1])
 @parameterize('keepdim', [False, True])
 def test_sum_handler(sum_dims, keepdim):
-    world_size = 4
-    run_func = partial(check_sum_handler, sum_dims=sum_dims, keepdim=keepdim, world_size=world_size, port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_sum_handler, 4, sum_dims=sum_dims, keepdim=keepdim)
 
 
 if __name__ == '__main__':
     test_sum_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,43 +1,46 @@
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.tensor_constructor_handler import TensorConstructorHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
+from colossalai.testing import clear_cache_before_run, run_on_environment_flag
 
 
 class TensorConstructorModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
     def forward(self, x):
         arange_node = torch.arange(x.size()[0])
         x = x + arange_node
         return x
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_where_handler():
     model = TensorConstructorModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %x : torch.Tensor [#users=2] = placeholder[target=x]
     #     %size : [#users=1] = call_method[target=size](args = (%x,), kwargs = {})
     #     %getitem : [#users=1] = call_function[target=operator.getitem](args = (%size, 0), kwargs = {})
     #     %arange : [#users=1] = call_function[target=torch.arange](args = (%getitem,), kwargs = {})
     #     %add : [#users=1] = call_function[target=operator.add](args = (%x, %arange), kwargs = {})
     #     return add
-    graph = tracer.trace(model, meta_args={
-        "x": torch.rand(10).to('meta'),
-    })
+    meta_args = {'x': torch.rand(10).to('meta')}
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     arange_node = list(graph.nodes)[3]
     strategies_vector = StrategiesVector(arange_node)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 import torch
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.unary_elementwise_handler import UnaryElementwiseHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.fx.tracer.meta_patch.patched_module import linear
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
+from colossalai.testing import clear_cache_before_run, run_on_environment_flag
 
 
 class ReLuModel(nn.Module):
 
     def __init__(self):
         super().__init__()
         self.act = torch.nn.ReLU()
@@ -19,29 +20,31 @@
     def forward(self, input, other):
         conv_node = nn.functional.conv2d(input, other)
         relu_node = self.act(conv_node)
         return relu_node
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_elementwise_handler():
     model = ReLuModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %other : torch.Tensor [#users=1] = placeholder[target=other]
     #     %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %other), kwargs = {})
     #     %act : [#users=1] = call_module[target=act](args = (%conv2d,), kwargs = {})
     #     return act
-    graph = tracer.trace(model,
-                         meta_args={
-                             "input": torch.rand(4, 4, 64, 64).to('meta'),
-                             "other": torch.rand(4, 16, 3, 3).to('meta'),
-                         })
+    meta_args = {
+        'input': torch.rand(4, 4, 64, 64).to('meta'),
+        'other': torch.rand(16, 4, 3, 3).to('meta'),
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     conv_mod_node = list(graph.nodes)[2]
     relu_mod_node = list(graph.nodes)[3]
     relu_strategies_vector = StrategiesVector(relu_mod_node)
@@ -65,21 +68,21 @@
     for name, op_data in mapping.items():
         op_data: OperationData
         # make sure they have valid values
         assert op_data.data is not None
 
     assert mapping['input'].name == "conv2d"
     assert mapping['input'].data.is_meta
-    assert mapping['input'].data.shape == torch.Size([4, 4, 62, 62])
+    assert mapping['input'].data.shape == torch.Size([4, 16, 62, 62])
     assert mapping['input'].type == OperationDataType.ARG
-    assert mapping['input'].logical_shape == torch.Size([4, 4, 62, 62])
+    assert mapping['input'].logical_shape == torch.Size([4, 16, 62, 62])
 
     assert mapping['output'].name == "act"
     assert mapping['output'].data.is_meta
-    assert mapping['output'].data.shape == torch.Size([4, 4, 62, 62])
+    assert mapping['output'].data.shape == torch.Size([4, 16, 62, 62])
     assert mapping['output'].type == OperationDataType.OUTPUT
 
     # getitem is a following strategy handler, so the number of strategies is equal to the predecessor node.
     assert len(relu_strategies_vector) == len(conv_strategies_vector)
 
 
 if __name__ == '__main__':
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,23 @@
-from functools import partial
-
 import pytest
 import torch
-import torch.multiprocessing as mp
 import torch.nn as nn
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.node_handler import ViewHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.conv_handler import ConvFunctionHandler
 from colossalai.auto_parallel.tensor_shard.node_handler.linear_handler import LinearFunctionHandler
 from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
 from colossalai.initialize import launch
 from colossalai.logging import disable_existing_loggers
-from colossalai.testing import assert_close, parameterize, rerun_if_address_is_in_use
+from colossalai.testing import parameterize, rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
-from colossalai.utils import free_port
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 class ConvViewModel(nn.Module):
 
     def __init__(self, tgt_shape):
         super().__init__()
@@ -70,42 +68,40 @@
     numerical_test_for_node_strategy(model=model,
                                      device_mesh=device_mesh,
                                      node_index=node_index,
                                      strategy_number=strategy_number,
                                      input_args=[input, other],
                                      meta_arg_names=['input', 'other'],
                                      node_type='following')
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     if model_cls.__name__ == 'ConvViewModel':
         # graph():
         #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
         #     %other : torch.Tensor [#users=1] = placeholder[target=other]
         #     %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %other), kwargs = {})
         #     %view : [#users=1] = call_method[target=view](args = (%conv2d, 2, -1), kwargs = {})
         #     return view
-        graph = tracer.trace(model,
-                             meta_args={
-                                 "input": torch.rand(8, 8, 66, 66).to('meta'),
-                                 "other": torch.rand(16, 8, 3, 3).to('meta'),
-                             })
+        meta_args = {'input': torch.rand(8, 8, 66, 66).to('meta'), 'other': torch.rand(16, 8, 3, 3).to('meta')}
+        graph = tracer.trace(model, meta_args=meta_args)
 
     if model_cls.__name__ == 'LinearViewModel':
         # graph():
         #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
         #     %other : torch.Tensor [#users=1] = placeholder[target=other]
         #     %linear : [#users=1] = call_function[target=torch._C._nn.linear](args = (%input_1, %other), kwargs = {bias: None})
         #     %view : [#users=1] = call_method[target=view](args = (%linear, 32, 4, 32, 32, 4), kwargs = {})
         #     return view
-        graph = tracer.trace(model,
-                             meta_args={
-                                 "input": torch.rand(8, 16, 64, 32).to('meta'),
-                                 "other": torch.rand(64, 32).to('meta'),
-                             })
+        meta_args = {
+            'input': torch.rand(8, 16, 64, 32).to('meta'),
+            'other': torch.rand(64, 32).to('meta'),
+        }
+        graph = tracer.trace(model, meta_args=meta_args)
 
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
 
     previous_mod_node = list(graph.nodes)[2]
     view_node = list(graph.nodes)[3]
     view_strategies_vector = StrategiesVector(view_node)
     previous_strategies_vector = StrategiesVector(previous_mod_node)
 
     # build handler
@@ -251,18 +247,12 @@
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @parameterize('tgt_shape', [(32, 4, 64, 16, 4), (8, 4, 4, 64, 16, 4)])
 @parameterize('model_cls', [ConvViewModel, LinearViewModel])
 def test_view_handler(tgt_shape, model_cls):
-    world_size = 4
-    run_func = partial(check_view_handler,
-                       tgt_shape=tgt_shape,
-                       model_cls=model_cls,
-                       world_size=world_size,
-                       port=free_port())
-    mp.spawn(run_func, nprocs=world_size)
+    spawn(check_view_handler, 4, tgt_shape=tgt_shape, model_cls=model_cls)
 
 
 if __name__ == '__main__':
     test_view_handler()
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,44 +1,49 @@
+import pytest
 import torch
 import torch.nn as nn
 
-from colossalai.auto_parallel.tensor_shard.node_handler.where_handler import \
-    WhereHandler
-from colossalai.auto_parallel.tensor_shard.sharding_strategy import (OperationData, OperationDataType, StrategiesVector)
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
+from colossalai.auto_parallel.tensor_shard.node_handler.where_handler import WhereHandler
+from colossalai.auto_parallel.tensor_shard.sharding_strategy import OperationData, OperationDataType, StrategiesVector
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx import ColoGraphModule, ColoTracer
-from colossalai.fx.tracer.meta_patch.patched_module import linear
+from colossalai.testing import clear_cache_before_run
 
 
 class ConvModel(nn.Module):
 
     def __init__(self):
         super().__init__()
 
     def forward(self, condition, x, y):
         output = torch.where(condition, x, y)
         return output
 
 
+@pytest.mark.skip('ShapeProp is not compatible with PyTorch 1.11.0')
+@clear_cache_before_run()
 def test_where_handler():
     model = ConvModel()
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     # graph():
     #     %condition : torch.Tensor [#users=1] = placeholder[target=condition]
     #     %x : torch.Tensor [#users=1] = placeholder[target=x]
     #     %y : torch.Tensor [#users=1] = placeholder[target=y]
     #     %where : [#users=1] = call_function[target=torch.where](args = (%condition, %x, %y), kwargs = {})
     #     return where
-    graph = tracer.trace(model,
-                         meta_args={
-                             "condition": torch.rand(4, 4, 64, 64).to('meta'),
-                             "x": torch.rand(4, 1, 64, 64).to('meta'),
-                             "y": torch.rand(1, 4, 64, 64).to('meta')
-                         })
+    meta_args = {
+        'condition': torch.rand(4, 4, 64, 64).to('meta'),
+        'x': torch.rand(4, 1, 64, 64).to('meta'),
+        'y': torch.rand(1, 4, 64, 64).to('meta')
+    }
+    graph = tracer.trace(model, meta_args=meta_args)
     gm = ColoGraphModule(model, graph)
+    shape_prop_pass(gm, *meta_args.values())
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     where_node = list(graph.nodes)[3]
     strategies_vector = StrategiesVector(where_node)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 import copy
 from typing import Dict, List
 
 import torch
 from torch.fx import GraphModule
 
+from colossalai._analyzer.fx.graph_module import ColoGraphModule
+from colossalai._analyzer.fx.passes.shape_prop import shape_prop_pass
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.passes.runtime_apply_pass import runtime_apply_pass
 from colossalai.auto_parallel.passes.runtime_preparation_pass import runtime_preparation_pass
 from colossalai.auto_parallel.tensor_shard.options import SolverOptions
 from colossalai.auto_parallel.tensor_shard.solver import StrategiesConstructor
 from colossalai.auto_parallel.tensor_shard.solver.cost_graph import CostGraph
 from colossalai.auto_parallel.tensor_shard.solver.solver import Solver
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.tracer.tracer import ColoTracer
 from colossalai.tensor.shape_consistency import to_global
 from colossalai.testing.comparison import assert_close
 
 
 def _build_model_to_compare(model: torch.nn.Module, input_args: List[torch.Tensor],
                             input_kwargs: Dict[str, torch.Tensor], grad_dict: Dict[any, torch.Tensor]):
 
@@ -75,22 +77,24 @@
         grad_to_compare_dict = {}
         grad_to_shard_dict = {}
         model_to_compare, args_to_compare, kwargs_to_compare = _build_model_to_compare(
             model, input_args, input_kwargs, grad_to_compare_dict)
         model_to_shard, args_to_shard, kwargs_to_shard = _build_model_to_compare(model, input_args, input_kwargs,
                                                                                  grad_to_shard_dict)
 
-        tracer = ColoTracer()
+        tracer = ColoTracer(bias_addition_split=True)
         input_sample = {}
         for input_arg, meta_arg_name in zip(input_args, meta_arg_names):
-            input_sample[meta_arg_name] = torch.rand(input_arg.shape).to('meta')
+            input_sample[meta_arg_name] = torch.empty(input_arg.shape, dtype=input_arg.dtype).to('meta')
         for meta_kwarg_name, input_kwarg in input_kwargs.items():
-            input_sample[meta_kwarg_name] = torch.rand(input_kwarg.shape).to('meta')
+            input_sample[meta_kwarg_name] = torch.empty(input_kwarg.shape, dtype=input_kwarg.dtype).to('meta')
         graph = tracer.trace(root=model_to_shard, meta_args=input_sample)
-        gm = GraphModule(model_to_shard, graph, model_to_shard.__class__.__name__)
+        gm = ColoGraphModule(model_to_shard, graph, model_to_shard.__class__.__name__)
+        shape_prop_pass(gm, *input_sample.values())
+
         solver_options = SolverOptions()
         strategies_constructor = StrategiesConstructor(graph, device_mesh, solver_options)
         strategies_constructor.build_strategies_and_cost()
         target_node = [strategies_vector.node for strategies_vector in strategies_constructor.leaf_strategies
                       ][node_index]
         if node_type == 'normal':
             solution_len = len(strategies_constructor.leaf_strategies)
```

### Comparing `colossalai-0.2.8/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py` & `colossalai-0.3.0/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,30 +1,33 @@
 import torch
 from torch.fx import GraphModule
 from torchvision.models import resnet50
 
+from colossalai._analyzer.fx.passes import shape_prop_pass
+# from colossalai.fx.tracer.tracer import ColoTracer
+from colossalai._analyzer.fx.tracer.tracer import ColoTracer
 from colossalai.auto_parallel.tensor_shard.constants import BATCHNORM_MODULE_OP
 from colossalai.auto_parallel.tensor_shard.options import SolverOptions
 from colossalai.auto_parallel.tensor_shard.solver import CostGraph, GraphAnalyser, Solver, StrategiesConstructor
 from colossalai.device.device_mesh import DeviceMesh
-from colossalai.fx.tracer.tracer import ColoTracer
 from colossalai.tensor.shape_consistency import ShapeConsistencyManager
-from colossalai.testing.pytest_wrapper import run_on_environment_flag
+from colossalai.testing import clear_cache_before_run, run_on_environment_flag
 
 
 @run_on_environment_flag(name='AUTO_PARALLEL')
+@clear_cache_before_run()
 def test_cost_graph():
     physical_mesh_id = torch.arange(0, 8)
     mesh_shape = (2, 4)
     # [[0, 1]
     #  [2, 3]]
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape)
     shape_consistency_manager = ShapeConsistencyManager()
 
-    tracer = ColoTracer()
+    tracer = ColoTracer(bias_addition_split=True)
     model = resnet50(num_classes=100000)
     input_sample = {'x': torch.rand(128, 3, 224, 224).to('meta')}
 
     graph = tracer.trace(root=model, meta_args=input_sample)
     # graph():
     #     %x : torch.Tensor [#users=1] = placeholder[target=x]
     #     %conv1 : [#users=1] = call_module[target=conv1](args = (%x,), kwargs = {})
@@ -46,14 +49,15 @@
     #     %add_1 : [#users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})
     #     ...
     #     %avgpool : [#users=1] = call_module[target=avgpool](args = (%layer4_2_relu_1,), kwargs = {})
     #     %flatten : [#users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})
     #     %fc : [#users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})
     #     return fc
     gm = GraphModule(model, graph, model.__class__.__name__)
+    shape_prop_pass(gm, *input_sample.values())
     gm.recompile()
 
     solver_options = SolverOptions()
     strategies_constructor = StrategiesConstructor(graph, device_mesh, solver_options)
     strategies_constructor.build_strategies_and_cost()
 
     cost_graph = CostGraph(strategies_constructor.leaf_strategies)
```

