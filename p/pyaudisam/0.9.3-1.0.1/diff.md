# Comparing `tmp/pyaudisam-0.9.3.tar.gz` & `tmp/pyaudisam-1.0.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "C:\git\pyaudisam\dist\tmph7fyh1nq\pyaudisam-0.9.3.tar", last modified: Sun Jan 23 11:10:28 2022, max compression
+gzip compressed data, was "pyaudisam-1.0.1.tar", last modified: Thu May 25 19:10:55 2023, max compression
```

## Comparing `pyaudisam-0.9.3.tar` & `pyaudisam-1.0.1.tar`

### file list

```diff
@@ -1,126 +1,203 @@
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:28.042524 pyaudisam-0.9.3/
--rw-rw-rw-   0        0        0    35149 2021-10-03 09:00:39.000000 pyaudisam-0.9.3/LICENSE.txt
--rw-rw-rw-   0        0        0      496 2022-01-22 19:46:37.000000 pyaudisam-0.9.3/MANIFEST.in
--rw-rw-rw-   0        0        0     4044 2022-01-23 11:10:28.040496 pyaudisam-0.9.3/PKG-INFO
--rw-rw-rw-   0        0        0     3251 2022-01-23 11:08:55.000000 pyaudisam-0.9.3/README.md
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.238547 pyaudisam-0.9.3/pyaudisam/
--rw-rw-rw-   0        0        0     2788 2022-01-23 11:00:29.000000 pyaudisam-0.9.3/pyaudisam/__init__.py
--rw-rw-rw-   0        0        0    54003 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/__main__.py
--rw-rw-rw-   0        0        0   150784 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/analyser.py
--rw-rw-rw-   0        0        0    19408 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/analysis.py
--rw-rw-rw-   0        0        0    79553 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/data.py
--rw-rw-rw-   0        0        0    44085 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/engine.py
--rw-rw-rw-   0        0        0     7653 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/executor.py
--rw-rw-rw-   0        0        0     7881 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/log.py
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.309967 pyaudisam-0.9.3/pyaudisam/mcds/
--rw-rw-rw-   0        0        0     9831 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/anlys.htpl
--rw-rw-rw-   0        0        0     9071 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/fulltop.htpl
--rw-rw-rw-   0        0        0     6023 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/pretop.htpl
--rw-rw-rw-   0        0        0      686 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/stat-mod-notes.txt
--rw-rw-rw-   0        0        0     1988 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/stat-mod-specs.txt
--rw-rw-rw-   0        0        0     7965 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/stat-mod-trans.txt
--rw-rw-rw-   0        0        0      834 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/stat-row-specs.txt
--rw-rw-rw-   0        0        0     6429 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/mcds/top.htpl
--rw-rw-rw-   0        0        0    34064 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/optanalyser.py
--rw-rw-rw-   0        0        0    39060 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/optimisation.py
--rw-rw-rw-   0        0        0    74730 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/optimiser.py
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.367845 pyaudisam-0.9.3/pyaudisam/report/
--rw-rw-rw-   0        0        0      475 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-angle-up.svg
--rw-rw-rw-   0        0        0      522 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-arrow-left-hover.svg
--rw-rw-rw-   0        0        0      522 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-arrow-left.svg
--rw-rw-rw-   0        0        0      527 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-arrow-right-hover.svg
--rw-rw-rw-   0        0        0      527 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-arrow-right.svg
--rw-rw-rw-   0        0        0      525 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-arrow-up-hover.svg
--rw-rw-rw-   0        0        0      525 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-arrow-up.svg
--rw-rw-rw-   0        0        0      673 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-feather-alt.svg
--rw-rw-rw-   0        0        0      866 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-file-excel-hover.svg
--rw-rw-rw-   0        0        0      866 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/fa-file-excel.svg
--rw-rw-rw-   0        0        0     5281 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report/report.css
--rw-rw-rw-   0        0        0   126785 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/report.py
--rw-rw-rw-   0        0        0     2099 2022-01-22 16:29:08.000000 pyaudisam-0.9.3/pyaudisam/utils.py
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.267961 pyaudisam-0.9.3/pyaudisam.egg-info/
--rw-rw-rw-   0        0        0     4044 2022-01-23 11:10:26.000000 pyaudisam-0.9.3/pyaudisam.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     4809 2022-01-23 11:10:27.000000 pyaudisam-0.9.3/pyaudisam.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2022-01-23 11:10:26.000000 pyaudisam-0.9.3/pyaudisam.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       20 2022-01-23 11:10:26.000000 pyaudisam-0.9.3/pyaudisam.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0       75 2022-01-23 11:10:26.000000 pyaudisam-0.9.3/pyaudisam.egg-info/requires.txt
--rw-rw-rw-   0        0        0       10 2022-01-23 11:10:26.000000 pyaudisam-0.9.3/pyaudisam.egg-info/top_level.txt
--rw-rw-rw-   0        0        0       91 2021-11-01 11:07:13.000000 pyaudisam-0.9.3/requirements.txt
--rw-rw-rw-   0        0        0       42 2022-01-23 11:10:28.042524 pyaudisam-0.9.3/setup.cfg
--rw-rw-rw-   0        0        0     2755 2022-01-23 11:01:50.000000 pyaudisam-0.9.3/setup.py
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.437578 pyaudisam-0.9.3/tests/
--rw-rw-rw-   0        0        0     1184 2021-10-05 17:32:46.000000 pyaudisam-0.9.3/tests/conftest.py
--rw-rw-rw-   0        0        0   149073 2021-11-20 09:47:55.000000 pyaudisam-0.9.3/tests/devarchives1.ipynb
--rw-rw-rw-   0        0        0   200194 2021-11-28 17:05:18.000000 pyaudisam-0.9.3/tests/devarchives2.ipynb
--rw-rw-rw-   0        0        0     3048 2021-10-03 10:04:40.000000 pyaudisam-0.9.3/tests/ipython_notebook_toc.js
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.823153 pyaudisam-0.9.3/tests/refin/
--rw-rw-rw-   0        0        0    57477 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt
--rw-rw-rw-   0        0        0    57750 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods
--rw-rw-rw-   0        0        0    13349 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx
--rw-rw-rw-   0        0        0    11922 2021-12-28 09:03:01.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitSpecsEchants.ods
--rw-rw-rw-   0        0        0    18211 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx
--rw-rw-rw-   0        0        0    66148 2021-11-20 20:11:41.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-UnitestOptResultats.ods
--rw-rw-rw-   0        0        0     5686 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dec-dist.txt
--rw-rw-rw-   0        0        0     5686 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt
--rw-rw-rw-   0        0        0     6771 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-6dec-dist.txt
--rw-rw-rw-   0        0        0     6340 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0    39311 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods
--rw-rw-rw-   0        0        0    29263 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods
--rw-rw-rw-   0        0        0    16242 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx
--rw-rw-rw-   0        0        0     2493 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ANTTRI-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     4881 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-COLPAL-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0   190595 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-DonneesBrutesPourAutoDS.xlsx
--rw-rw-rw-   0        0        0     3170 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-EMBCIR-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     2620 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-EMBCIT-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     6105 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-LUSMEG-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     2313 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-MILCAL-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     4165 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-PHYCOL-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0    11401 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-SYLATR-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     9638 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-1dec-dist.txt
--rw-rw-rw-   0        0        0    11753 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-6dec-dist.txt
--rw-rw-rw-   0        0        0    11227 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-ttdec-dist.txt
--rw-rw-rw-   0        0        0     7440 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-TURMER-AB-5mn-1dec-dist.txt
--rw-rw-rw-   0        0        0    51263 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refin/TURMER-10mn-1dec-hnorm-cos.odt
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.873318 pyaudisam-0.9.3/tests/refout/
--rw-rw-rw-   0        0        0    58058 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Naturalist-ExtraitOptResultats.ods
--rw-rw-rw-   0        0        0    25032 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Naturalist-ExtraitPreResultats.ods
--rw-rw-rw-   0        0        0    48727 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Naturalist-ExtraitResultats.ods
--rw-rw-rw-   0        0        0    24594 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-distance-73.xlsx
--rw-rw-rw-   0        0        0    19410 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-postcomp.ods
--rw-rw-rw-   0        0        0     6120 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-saisie-5-cols.txt
--rw-rw-rw-   0        0        0    13142 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.txt
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.913914 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.956642 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/
--rw-rw-rw-   0        0        0    68916 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/boostrap.txt
--rw-rw-rw-   0        0        0      569 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/cmd.txt
--rw-rw-rw-   0        0        0    11487 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/data.txt
--rw-rw-rw-   0        0        0    38295 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/log.txt
--rw-rw-rw-   0        0        0    44459 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/output.txt
--rw-rw-rw-   0        0        0     3060 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/stats.txt
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:27.996329 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/
--rw-rw-rw-   0        0        0    68916 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/boostrap.txt
--rw-rw-rw-   0        0        0      569 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/cmd.txt
--rw-rw-rw-   0        0        0     9534 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/data.txt
--rw-rw-rw-   0        0        0    38318 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/log.txt
--rw-rw-rw-   0        0        0    45774 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/output.txt
--rw-rw-rw-   0        0        0     3254 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/stats.txt
--rw-rw-rw-   0        0        0    55416 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-order-sens.odt
--rw-rw-rw-   0        0        0     4739 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-order-sens.txt
--rw-rw-rw-   0        0        0    11007 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-order-sens.xlsx
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:28.024539 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7-dist-order/
--rw-rw-rw-   0        0        0      758 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7-dist-order/cmd.txt
--rw-rw-rw-   0        0        0    11487 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7-dist-order/data.txt
--rw-rw-rw-   0        0        0    39057 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7-dist-order/log.txt
--rw-rw-rw-   0        0        0    44559 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7-dist-order/output.txt
-drwxrwxrwx   0        0        0        0 2022-01-23 11:10:28.033548 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7.dat/
--rw-rw-rw-   0        0        0   102400 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7.dat/DistData.mdb
--rw-rw-rw-   0        0        0   251904 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/dist-win7.dst
--rw-rw-rw-   0        0        0     7529 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/echanges-equipe-ds.txt
--rw-rw-rw-   0        0        0     9638 2021-09-26 19:14:02.000000 pyaudisam-0.9.3/tests/refout/dist-order-sens-min/import-data-set.txt
--rw-rw-rw-   0        0        0    17786 2022-01-22 14:52:05.000000 pyaudisam-0.9.3/tests/sensitivity.ipynb
--rw-rw-rw-   0        0        0    31415 2021-11-01 11:07:13.000000 pyaudisam-0.9.3/tests/unint_data_test.py
--rw-rw-rw-   0        0        0    17818 2021-10-05 17:57:05.000000 pyaudisam-0.9.3/tests/unint_engine_test.py
--rw-rw-rw-   0        0        0   284893 2022-01-17 18:39:24.000000 pyaudisam-0.9.3/tests/unintests.ipynb
--rw-rw-rw-   0        0        0   129281 2021-11-20 18:31:15.000000 pyaudisam-0.9.3/tests/valarchives.ipynb
--rw-rw-rw-   0        0        0    21640 2022-01-16 13:57:44.000000 pyaudisam-0.9.3/tests/valtests-ds-params.py
--rw-rw-rw-   0        0        0   122542 2022-01-17 19:04:35.000000 pyaudisam-0.9.3/tests/valtests.ipynb
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.300902 pyaudisam-1.0.1/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2484 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/MANIFEST.in
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     7073 2023-05-25 19:10:55.300902 pyaudisam-1.0.1/PKG-INFO
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     6295 2023-05-25 19:04:20.000000 pyaudisam-1.0.1/README.md
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.277568 pyaudisam-1.0.1/docs/
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.277568 pyaudisam-1.0.1/docs/how-it-works/
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.280902 pyaudisam-1.0.1/docs/how-it-works/common/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      469 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-angle-up.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      518 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-left-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      518 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-left.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      523 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-right-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      523 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-right.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      521 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-up-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      521 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-up.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      669 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-feather-alt.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      862 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-file-excel-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      862 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/fa-file-excel.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5022 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/common/report.css
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    18569 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/how-it-works-en.md
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    20069 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/how-it-works-fr.md
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.280902 pyaudisam-1.0.1/docs/how-it-works/optanlys/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   387006 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/ACDC2019-Nat-optanalyses-report-partview.ExAicMQua-r925m8q3d12.jpg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   100947 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/ACDC2019-Nat-optanalyses-report.ExAicMQua-r925m8q3d12.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   431912 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/ACDC2019-Nat-optanalyses-report.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   195089 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/ACDC2019-Nat.230430-153402.log
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    35567 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/ACDC2019-Nat.230430-162324.log
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.280902 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      826 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/cmd.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12457 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/data.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    43733 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/detprob1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44166 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/disthist.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    66444 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/index.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3386 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/log.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    19586 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/output.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    32936 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/plots.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44116 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/probdens1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    34280 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/qqplot.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2740 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ma-7ak3cu9l/stats.txt
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.284235 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      856 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/cmd.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12457 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/data.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    43600 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/detprob1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44166 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/disthist.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    63551 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/index.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3386 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/log.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    16797 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/output.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    31617 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/plots.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    47130 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/probdens1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    34486 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/qqplot.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2740 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/stats.txt
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.284235 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      797 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/cmd.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12457 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/data.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44906 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/detprob1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44166 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/disthist.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    65003 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/index.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3386 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/log.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    17882 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/output.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    30575 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/plots.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    48239 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/probdens1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    34267 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/qqplot.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2740 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-ma-w3hq64b3/stats.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    19306 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/optanlys/acdc-2019-nat-ds-params.py
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.284235 pyaudisam-1.0.1/docs/how-it-works/preanlys/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   231698 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/ACDC2019-Nat-preanalyses-report-partview.jpg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12984 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/ACDC2019-Nat-preanalyses-report.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    21757 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/ACDC2019-Nat-preanalyses-report.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     9051 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/ACDC2019-Nat.230416-180310.log
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    13107 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/ACDC2019-Nat.230416-185538.log
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.287569 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      769 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/cmd.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2366 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/data.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44290 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/detprob1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44964 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/detprob2.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    45850 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/detprob3.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    41578 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/disthist.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    69061 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/index.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3539 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/log.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    36608 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/output.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    49179 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/plots.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    46974 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/probdens1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    45282 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/probdens2.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44681 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/probdens3.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    34997 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/qqplot.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2932 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/stats.txt
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.290902 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      768 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/cmd.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12457 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/data.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44906 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/detprob1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    43261 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/detprob2.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44019 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/detprob3.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44166 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/disthist.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    74210 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/index.html
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3461 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/log.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    41589 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/output.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    66839 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/plots.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    48239 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/probdens1.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    44532 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/probdens2.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    45563 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/probdens3.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    34267 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/qqplot.png
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2932 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/stats.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    19306 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/how-it-works/preanlys/acdc-2019-nat-ds-params.py
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.290902 pyaudisam-1.0.1/docs/howto-acdc19-nat/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   328909 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/ACDC2019-108-points-300m-circles.kml
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    75485 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/ACDC2019-Nat-ObsIndivDist.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12243 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/ACDC2019-OptAnalysesToDo.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     6890 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/ACDC2019-Samples.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    19306 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/acdc-2019-nat-ds-params.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    70666 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/acdc-2019-nat-ds-run.ipynb
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    37871 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/howto.md
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3136 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/docs/howto-acdc19-nat/ipython_notebook_toc.js
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.290902 pyaudisam-1.0.1/pyaudisam/
+-rwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)     2730 2023-05-25 19:09:46.000000 pyaudisam-1.0.1/pyaudisam/__init__.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    53985 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/__main__.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   148625 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/analyser.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    19022 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/analysis.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    77879 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/data.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    43149 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/engine.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     7444 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/executor.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     7656 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/log.py
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.294236 pyaudisam-1.0.1/pyaudisam/mcds/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     9600 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/mcds/anlys.htpl
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     8860 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/mcds/fulltop.htpl
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5872 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/mcds/pretop.htpl
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      678 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/mcds/stat-mod-notes.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     1909 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/mcds/stat-mod-specs.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     7871 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/mcds/stat-mod-trans.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      801 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/mcds/stat-row-specs.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     6261 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/mcds/top.htpl
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    33524 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/optanalyser.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    38268 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/optimisation.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    73335 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/optimiser.py
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.294236 pyaudisam-1.0.1/pyaudisam/report/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      469 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-angle-up.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      518 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-arrow-left-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      518 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-arrow-left.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      523 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-arrow-right-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      523 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-arrow-right.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      521 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-arrow-up-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      521 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-arrow-up.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      669 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-feather-alt.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      862 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-file-excel-hover.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)      862 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/pyaudisam/report/fa-file-excel.svg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5022 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/pyaudisam/report/report.css
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   124516 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/report.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2073 2023-05-25 18:50:01.000000 pyaudisam-1.0.1/pyaudisam/utils.py
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.294236 pyaudisam-1.0.1/pyaudisam.egg-info/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     7073 2023-05-25 19:10:55.000000 pyaudisam-1.0.1/pyaudisam.egg-info/PKG-INFO
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     9982 2023-05-25 19:10:55.000000 pyaudisam-1.0.1/pyaudisam.egg-info/SOURCES.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)        1 2023-05-25 19:10:55.000000 pyaudisam-1.0.1/pyaudisam.egg-info/dependency_links.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)       75 2023-05-25 19:10:55.000000 pyaudisam-1.0.1/pyaudisam.egg-info/requires.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)       10 2023-05-25 19:10:55.000000 pyaudisam-1.0.1/pyaudisam.egg-info/top_level.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)       85 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/requirements.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)       70 2023-05-25 19:10:55.300902 pyaudisam-1.0.1/setup.cfg
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2751 2023-05-25 18:54:26.000000 pyaudisam-1.0.1/setup.py
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.297569 pyaudisam-1.0.1/tests/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     1155 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/conftest.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   149073 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/devarchives1.ipynb
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   200194 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/devarchives2.ipynb
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2951 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/ipython_notebook_toc.js
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.300902 pyaudisam-1.0.1/tests/refin/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    56752 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    57750 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    13349 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    11922 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitSpecsEchants.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    18211 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    66148 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-UnitestOptResultats.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5429 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5429 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     6514 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-6dec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     6083 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    39311 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    29263 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    16242 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2376 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ANTTRI-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     4690 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-COLPAL-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   190595 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-DonneesBrutesPourAutoDS.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3033 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-EMBCIR-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2497 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-EMBCIT-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5860 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-LUSMEG-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     2198 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-MILCAL-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     3997 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-PHYCOL-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    10962 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-SYLATR-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     9209 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-1dec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    11324 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-6dec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    10798 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-ttdec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     7109 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-TURMER-AB-5mn-1dec-dist.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    51263 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refin/TURMER-10mn-1dec-hnorm-cos.odt
+drwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)        0 2023-05-25 19:10:55.300902 pyaudisam-1.0.1/tests/refout/
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    58058 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Naturalist-ExtraitOptResultats.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    25032 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Naturalist-ExtraitPreResultats.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    48727 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Naturalist-ExtraitResultats.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    24594 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-distance-73.xlsx
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    19410 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-postcomp.ods
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)     5863 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-saisie-5-cols.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    12885 2021-10-01 21:42:31.000000 pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.txt
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    17199 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/sensitivity.ipynb
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    30798 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/unint_data_test.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    17440 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/unint_engine_test.py
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   284893 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/unintests.ipynb
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)   129281 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/valarchives.ipynb
+-rw-r--r--   0 jeanphi   (1000) jeanphi   (1000)    21640 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/valtests-ds-params.py
+-rwxr-xr-x   0 jeanphi   (1000) jeanphi   (1000)   122542 2022-01-23 08:27:30.000000 pyaudisam-1.0.1/tests/valtests.ipynb
```

### Comparing `pyaudisam-0.9.3/pyaudisam/__main__.py` & `pyaudisam-1.0.1/pyaudisam/__main__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,952 +1,966 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Package main script, for when pyaudisam is invoked through "python -m"
-import re
-import sys
-import atexit
-import tempfile
-import pathlib as pl
-import argparse
-
-import pandas as pd
-
-from . import log, runtime
-from .utils import loadPythonData
-from .analyser import MCDSAnalyser, MCDSPreAnalyser, FilterSortSchemeIdManager
-from .optanalyser import MCDSTruncationOptanalyser
-from .report import MCDSResultsPreReport, MCDSResultsFullReport, MCDSResultsFilterSortReport
-
-
-class Logger(object):
-
-    """Local logger, taking care of output log file at exit time"""
-
-    DefLogNamePrefix = 'pyaudisam-main'
-
-    def __init__(self, runTimestamp, level=log.INFO):
-
-        self.runTimestamp = runTimestamp
-        self.openOpr = 'Checking'
-        self.dOprStart = dict()
-
-        # Log to sys.stdout, and also to a temporary log file (unique folder).
-        self.runLogFileName = f'{self.DefLogNamePrefix}.{runTimestamp}.log'
-        self.runLogFileName = pl.Path(tempfile.mkdtemp(prefix='pyaudisam')) / self.runLogFileName
-        log.configure(handlers=[sys.stdout, self.runLogFileName], reset=True,
-                      loggers=[dict(name='matplotlib', level=log.WARNING),
-                               dict(name='ads', level=log.INFO2),
-                               dict(name='ads.eng', level=log.INFO),
-                               dict(name='ads.exr', level=log.INFO),
-                               # dict(name='ads.dat', level=log.DEBUG),
-                               # dict(name='ads.rep', level=log.DEBUG),
-                               dict(name='ads.anr', level=log.INFO1),
-                               dict(name='main', level=level)])
-
-        # Fallback final log file path-name as long as it is not specified : current folder, generic (timestamped) name.
-        self.finalLogFileName = pl.Path('.') / self.runLogFileName.name
-
-        # Setup an atexit handler to move and rename the session log file in a user-friendly place if possible.
-        atexit.register(self.giveBackLogFile)
-
-        # Add logging methods to this logger
-        self.logger = log.logger(name='main')
-        for meth in dir(self.logger):
-            if any(meth.startswith(prefix) for prefix in ['critical', 'error', 'warning', 'info', 'debug']):
-                setattr(self, meth, getattr(self.logger, meth))
-
-        # Done.
-        self.info1(f'Logging session to temporary ' + self.runLogFileName.as_posix())
-        self.info('Current folder: ' + pl.Path().absolute().as_posix())
-        self.info('Command line: {} {}'.format(pl.Path(sys.argv[0]).as_posix(), ' '.join(sys.argv[1:])))
-
-    def setFinalLogPrefix(self, prefix=None):
-
-        self.finalLogFileName = None if prefix is None else pl.Path(prefix + f'.{self.runTimestamp}.log')
-
-    def giveBackLogFile(self):
-
-        if any(arg in sys.argv for arg in ['-h', '--help']):
-            self.finalLogFileName = None  # No need for a log file here !
-
-        if self.finalLogFileName is not None:
-            self.info1(f'Giving back session log to {self.finalLogFileName}')
-
-        # Release the log file
-        log.configure(handlers=[sys.stdout], reset=True, loggers=[dict(name='main', level=log.INFO)])
-
-        # Actually move and rename the log file if it is needed, or delete it if not.
-        if self.finalLogFileName is not None:
-            self.finalLogFileName.parent.mkdir(parents=True, exist_ok=True)
-            self.runLogFileName.rename(self.finalLogFileName)
-        else:
-            self.runLogFileName.unlink()
-
-        # Remove initial parent folder, now empty (was specially created for).
-        self.runLogFileName.parent.rmdir()
-
-    def setRealRun(self, realRun=True):
-
-        if realRun:
-            self.openOpr = 'Running'
-            self.info('This is a real run: requested operation will be actually run !')
-        else:
-            self.openOpr = 'Checking'
-            self.warning('Not a real run, only checking requested operations ...')
-
-    def openOperation(self, oprText):
-
-        self.info(f'{self.openOpr} {oprText} ...')
-        self.dOprStart[oprText] = pd.Timestamp.now()
-
-    def closeOperation(self, oprText):
-
-        elapsed = str(pd.Timestamp.now() - self.dOprStart[oprText]).replace('0 days ', '')
-        self.info(f'Done {self.openOpr.lower()} {oprText} ({elapsed}).')
-        del self.dOprStart[oprText]
-
-
-def decodeReportArg(repArg, repName='report'):
-
-    """Decode report argument value
-
-    Mini-language examples: none ; excel ; excel,html ; html:full ; excel,html:mqua-r925,html:mqua-r950
-
-    :returns: dict(format: list(type)) with format in {'none', 'excel', 'html'}, 'none' excluding any other,
-              and list(type) empty for 'none', empty or ['full'] for 'excel' format,
-              and containing at least 1 of {full, <filter-sort method regex search string>*} for 'html' format.
-    """
-
-    # Separate format[:type] items
-    repItems = [item.lower() for item in repArg.split(',')]
-
-    # List types for each format
-    repSpecs = dict()
-    for repItem in repItems:
-        fmt, typ = repItem.split(':') + ([None] if ':' not in repItem else [])
-        if fmt not in repSpecs:
-            repSpecs[fmt] = list()
-        if typ not in repSpecs[fmt] and typ is not None:
-            repSpecs[fmt].append(typ)
-
-    # 'none' format kept iff alone
-    repSpecs = {fmt: typs for fmt, typs in repSpecs.items() if fmt != 'none' or len(repSpecs) == 1}
-    if 'none' in repSpecs:
-        repSpecs.clear()
-
-    # Check formats
-    unsupRepFmts = [fmt for fmt in repSpecs if fmt not in ['none', 'html', 'excel']]
-    if unsupRepFmts:
-        logger.error('Unsupported {} format(s) {}'.format(repName, ', '.join(unsupRepFmts)))
-        sys.exit(2)
-
-    logger.debug1(f'{repName}: {repSpecs}')
-
-    return repSpecs
-
-
-# 0. Configure logging.
-runTimestamp = pd.Timestamp.now().strftime('%y%m%d-%H%M%S')  # Date+time of the run (for log file, ... etc).
-
-logger = Logger(runTimestamp, level=log.DEBUG2 if '-v' in sys.argv or '--verbose' in sys.argv else log.INFO1)
-
-logger.info('Computation platform:')
-for k, v in runtime.items():
-    logger.info(f'* {k}: {v}')
-
-# 1. Parse, check and post-process command line args.
-argser = argparse.ArgumentParser(prog='pyaudisam',  # usage='python -m pyaudisam',
-                                 description='Prepare or run (and / or generate reports for) many distance'
-                                             ' sampling (pre-)analyses using a DS engine from Distance software',
-                                 epilog='Exit codes:'
-                                        ' 0 if OK,'
-                                        ' 2 if any command line argument issue,'
-                                        ' 1 if any other (unexpected) issue.')
-
-argser.add_argument('-u', '--run', dest='realRun', action='store_true', default=False,
-                    help='Actually run specified operation (not only run diagnosis of)'
-                         ' => as long as -u/--run is not there, you can try any option,'
-                         ' it wont start or write anything ... fell free, you are safe :-)')
-argser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,
-                    help='Display more infos about the work to be done and export sample / (opt-)analysis'
-                         ' spec. files when relevant in the current directory ;')
-argser.add_argument('-p', '--params', dest='paramFile', type=str, required=True,
-                    help='Path-name of python file (.py assumed if no extension / suffix given) specifying'
-                         ' export / (opt)analysis / report parameters')
-argser.add_argument('-s', '--speparams', dest='speParams', type=str, default='',
-                    help='Comma-separated key=value items specifying "special" parameters'
-                         'defined before the parameter file is loaded, just as overridable built-in variables'
-                         r' (syntax and limitations: string-only values, with no space or ,;\'"$&! inside'
-                         ' => use it only for few and simple args like switches and simple names')
-argser.add_argument('-w', '--workdir', dest='workDir', type=str, default='.',
-                    help='Work folder = where to store DS analyses sub-folders and output files'
-                         ' (Note: a timestamp sub-folder YYMMDD-HHMMSS is auto-appended, if not already such)')
-argser.add_argument('-n', '--notimestamp', dest='noTimestamp', action='store_true', default=False,
-                    help='Inhibit auto-timestamped work sub-folder creation (under work folder)')
-argser.add_argument('-x', '--distexport', dest='distExport', action='store_true', default=False,
-                    help='Export one Distance input file for each specified sample of the survey data'
-                         ' (Note: a sample spec. file and a survey data file must be also specified, through -p)')
-argser.add_argument('-e', '--preanalyses', dest='preAnalyses', action='store_true', default=False,
-                    help='Run pre-analyses for the specified samples of the survey data'
-                         ' (Note: a sample spec. file and a survey data file must be also specified, through -p)')
-argser.add_argument('-a', '--analyses', dest='analyses', action='store_true', default=False,
-                    help='Run analyses for the specified samples of the survey data'
-                         ' (Note: an analysis spec. file and a survey data file must be also specified, through -p)')
-argser.add_argument('-o', '--optanalyses', dest='optAnalyses', action='store_true', default=False,
-                    help='Run opt-analyses for the specified samples of the survey data'
-                         ' (Note: an opt-analysis spec. file and a survey data file must be also specified,'
-                         ' through -p)')
-argser.add_argument('-c', '--recoveropts', dest='recoverOpts', action='store_true', default=False,
-                    help='Restart optimisations at the point they were when interrupted (for any reason), from the'
-                         ' last usable recovery file found in the work folder (use in conjonction with -w & -n)'
-                         ' (Note: if no usable recovery file found, the restart will fail: no automatic restart from'
-                         ' scratch ; remove -c to do so)')
-argser.add_argument('-t', '--prereports', dest='preReports', type=str, default='none',
-                    help='Which reports to generate from pre-analyses results, through comma-separated keywords'
-                         ' among {excel, html, none} (case does not matter, none ignored if not alone)')
-argser.add_argument('-r', '--reports', dest='reports', type=str, default='none',
-                    help='Which reports to generate from analyses results, through comma-separated format:type items'
-                         ' with format among {excel, html, none}, "none" ignored if not alone ; type ignored'
-                         ' for format="excel", and among {full, <filter-sort method regex search string>*}'
-                         ' (at least one of) for format=html (case does not matter) ;'
-                         ' note: available filter-sort methods are automatically listed'
-                         ' when auto-filtering parameters are specified, so run command without -u first !'
-                         ' examples: none ; excel ; excel,html:full ; html:mqua92,html:full,excel,html:mqua950')
-argser.add_argument('-f', '--optreports', dest='optReports', type=str, default='none',
-                    help='Which reports to generate from opt-analyses results (same mini-language as for -r)')
-argser.add_argument('-l', '--logprefix', dest='logPrefix', type=str, default=None,
-                    help='Target log file path-name prefix (will be postfixed by .<YYMMDD-HHMMSS timestamp>.log)'
-                         f" (Default: <work folder>/{logger.DefLogNamePrefix} if -u/--run, else 'none' ;"
-                         "if special value 'none', no log saved)")
-argser.add_argument('-m', '--threads', dest='threads', type=int, default=0,
-                    help='Number of parallel threads to use for (pre/opt-)analyses / report generation'
-                         ' (default: 0 => auto-determined actual number of parallel threads from CPU specs ;'
-                         ' 1 for no parallelism ; for any other choice, first check your actual CPU specs)')
-argser.add_argument('-g', '--engine', dest='engineType', type=str, default='MCDS',
-                    choices=['MCDS'],
-                    help='The Distance engine to use, among MCDS, ... and no other for the moment'
-                         ' (insensitive to case)')
-
-args = argser.parse_args()
-logger.debug(f'Arguments: {args}')
-
-logger.setRealRun(args.realRun)
-
-if args.threads == 1:
-    args.threads = None  # No need for asynchronism: enforce sequential run.
-
-args.preReports = decodeReportArg(args.preReports, repName='pre-analysis report')
-args.reports = decodeReportArg(args.reports, repName='analysis report')
-args.optReports = decodeReportArg(args.optReports, repName='opt-analysis report')
-
-logger.info1('Arguments:')
-for k, v in vars(args).items():
-    logger.info1(f'* {k}: {v}')
-
-# 2. Load parameter python file, passing "special" parameters if any.
-speParamItems = args.speParams.split(',') if args.speParams else list()
-if any(item.count('=') != 1 for item in speParamItems):
-    logger.error(f'Syntax error in pre-parameters: "{args.speParams}"'
-                 ' (should be "name1=value1,name2=value2,...")')
-    sys.exit(2)
-
-speParams = dict([item.split('=') for item in speParamItems])
-paramFile, pars = loadPythonData(path=args.paramFile, **speParams)
-if not pars:
-    logger.error(f'Failed to load parameter file {paramFile.as_posix()}')
-    sys.exit(2)
-logger.debug1('Parameters: ' + ', '.join(vars(pars)))
-
-paramFiles = [paramFile.as_posix()]
-if 'paramFiles' in vars(pars):
-    paramFiles += pars.paramFiles
-
-# 3. More checks on args and parameters.
-# a. Check filter and sort report args
-if 'filsorReportSchemes' in vars(pars):
-    filsorSchemeIdMgr = FilterSortSchemeIdManager()
-    filsorReportSchemes = {filsorSchemeIdMgr.schemeId(sch): sch for sch in pars.filsorReportSchemes}
-    logger.info('Available filter & sort report schemes: ' + ', '.join(filsorReportSchemes.keys()))
-
-if 'html' in args.reports and not args.reports['html']:
-    logger.error('HTML analysis report: MUST specify type / filter & sort method')
-    sys.exit(2)
-
-if 'html' in args.reports and ('full' not in args.reports['html'] or len(args.reports['html']) > 1):
-
-    filsorMatches = {reSchId: [schId for schId in filsorReportSchemes if re.search(reSchId, schId, flags=re.I)]
-                     for reSchId in args.reports['html'] if reSchId != 'full'}
-    if any(len(matches) != 1 for matches in filsorMatches.values()) \
-       or len(filsorMatches) != len(set(match for matches in filsorMatches.values() for match in matches)):
-        logger.error('HTML analysis report: Bad or ambiguous filter & sort method specification(s) {}'
-                     .format(' ; '.join('{} => [{}]'.format(reSchId, ', '.join(matchSchemes))
-                                        for reSchId, matchSchemes in filsorMatches.items())))
-        sys.exit(2)
-
-    filsorAnlysReportSchemes = {schId: schValue for schId, schValue in filsorReportSchemes.items()
-                                if schId in set(match for matches in filsorMatches.values() for match in matches)}
-    if not filsorAnlysReportSchemes:
-        logger.error('HTML analysis report: No filter & sort method specified')
-        sys.exit(2)
-
-if 'html' in args.optReports and not args.optReports['html']:
-    logger.error('HTML opt-analysis report: MUST specify type / filter & sort method')
-    sys.exit(2)
-
-if 'html' in args.optReports and ('full' not in args.optReports['html'] or len(args.optReports['html']) > 1):
-
-    filsorMatches = {reSchId: [schId for schId in filsorReportSchemes if re.search(reSchId, schId, flags=re.I)]
-                     for reSchId in args.optReports['html'] if reSchId != 'full'}
-    if any(len(matches) != 1 for matches in filsorMatches.values()) \
-       or len(filsorMatches) != len(set(match for matches in filsorMatches.values() for match in matches)):
-        logger.error('HTML opt-analysis report: Bad or ambiguous filter & sort method specification(s) {}'
-                     .format(' ; '.join('{} => [{}]'.format(reSchId, ', '.join(matchSchemes))
-                                        for reSchId, matchSchemes in filsorMatches.items())))
-        sys.exit(2)
-
-    filsorOptAnlysReportSchemes = {schId: schValue for schId, schValue in filsorReportSchemes.items()
-                                   if schId in set(match for matches in filsorMatches.values() for match in matches)}
-    if not filsorOptAnlysReportSchemes:
-        logger.error('HTML opt-analysis report: No filter & sort method specified')
-        sys.exit(2)
-
-# 4. Output folder for results, reports ... etc.
-#    (post-fixed with the run timestamp, if not already specified)
-workDir = args.workDir if 'workDir' in vars(args) else pars.workDir if 'workDir' in vars(pars) else '.'
-workDir = pl.Path(workDir)
-if not(args.noTimestamp or re.match('.*[0-9]{6}-[0-9]{4,6}$', workDir.name)):
-    workDir = workDir / runTimestamp
-logger.info(f'Work folder: {workDir.as_posix()}')
-
-# 5. Now we can setup the final session log file path-name prefix !
-if args.logPrefix is None:
-    if args.realRun:  # Default
-        args.logPrefix = workDir.as_posix() + f'/{pars.studyName}{pars.subStudyName}'
-elif args.logPrefix.lower() == 'none':
-    args.logPrefix = None
-logger.setFinalLogPrefix(args.logPrefix)
-
-# 6. Really something to do ?
-emptyRun = not any([args.distExport, args.preAnalyses, args.preReports,
-                    args.analyses, args.reports, args.optAnalyses, args.optReports])
-if emptyRun:
-    logger.warning('No operation specified: nothing to do actually !')
-
-# 7. Load input data if needed:
-# a. Survey data
-# * individualised data with distance from observer to observed "object",
-# * point transect definition
-if not emptyRun:
-
-    surveyDataFile = pars.surveyDataFile if 'surveyDataFile' in vars(pars) else None
-    if surveyDataFile:
-        surveyDataFile = pl.Path(surveyDataFile)
-        if surveyDataFile.exists():
-            indivDistSheet = pars.indivDistDataSheet if 'indivDistDataSheet' in vars(pars) else 0
-            transectSheet = pars.transectsDataSheet if 'transectsDataSheet' in vars(pars) else 1
-            logger.info1(f'Loading survey data and transects infos from file {surveyDataFile.as_posix()} ...')
-            with pd.ExcelFile(surveyDataFile) as xlInFile:
-                dfMonoCatObs = pd.read_excel(xlInFile, sheet_name=indivDistSheet)
-                dfTransects = pd.read_excel(xlInFile, sheet_name=transectSheet)
-            logger.info1(f'... found {len(dfMonoCatObs)} mono-category sightings and {len(dfTransects)} transects')
-        else:
-            logger.error(f'Could not find survey data file {surveyDataFile.as_posix()}')
-            sys.exit(2)
-    else:
-        logger.error('No survey data file specified, can\'t export Distance file or run any type of analysis')
-        sys.exit(2)
-
-# 5.b. Sample specs
-if args.distExport or args.preAnalyses:
-
-    sampleSpecFile = pars.sampleSpecFile if 'sampleSpecFile' in vars(pars) else None
-    if sampleSpecFile:
-        sampleSpecFile = pl.Path(sampleSpecFile)
-        if not sampleSpecFile.exists():
-            logger.error(f'Could not find sample spec. file {sampleSpecFile.as_posix()}')
-            sys.exit(2)
-    else:
-        logger.error('No sample spec. file specified, can\'t export Distance file or run pre-analyses')
-        sys.exit(2)
-
-# 8. Export input files for Distance software, for manual analyses (if specified to).
-sampExplSpecFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-samples-explispecs.xlsx'
-if sampExplSpecFilePath.exists():
-    logger.info('Found sample explicit specs file ' + sampExplSpecFilePath.as_posix())
-
-if args.distExport:
-
-    oprText = 'export of input data files for Distance'
-    logger.openOperation(oprText)
-
-    # a. Create PreAnalyser object.
-    preAnlysr = MCDSPreAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                                effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                                transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                                sampleSelCols=pars.sampleSelCols,
-                                sampleDecCols=[pars.effortCol, pars.distanceCol],
-                                sampleIndCol=pars.sampleIndCol, sampleSpecCustCols=pars.sampleSpecCustCols,
-                                abbrevCol=pars.sampleAbbrevCol, abbrevBuilder=pars.sampleAbbrev,
-                                distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                                surveyType=pars.surveyType, distanceType=pars.distanceType,
-                                clustering=pars.clustering,
-                                workDir=workDir)
-
-    # b. Check sample specs.
-    dfExplSampleSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
-        preAnlysr.explicitParamSpecs(implParamSpecs=sampleSpecFile, dropDupes=True, check=True)
-
-    assert userParamSpecCols == []  # No analysis params here (auto. generated by PreAnalyser)
-    assert intParamSpecCols == []  # Idem
-    assert verdict
-    assert not reasons
-
-    logger.info2(f'Explicit sample specs:\n{dfExplSampleSpecs.to_string()}')
-    logger.info1(f'From sample specs, {len(dfExplSampleSpecs)} samples to export')
-    if args.verbose and not args.realRun:
-        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
-
-    # b. Export 1 Distance input data file for each specified sample.
-    # dfSamples = Analyser.explicitVariantSpecs(partSpecs=sampSpecFile, varIndCol=pars.sampleIndCol)
-    if args.realRun:
-        preAnlysr.exportDSInputData(implSampleSpecs=sampleSpecFile, format='Distance')
-        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
-    preAnlysr.shutdown()  # Not really needed, actually.
-
-    logger.closeOperation(oprText)
-
-# 9. Run pre-analyses (if specified to).
-PreAnalyser = MCDSPreAnalyser
-resultsWord = 'resultats' if pars.studyLang == 'fr' else 'results'
-preAnlysResFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-preanalyses-{resultsWord}.xlsx'
-if preAnlysResFilePath.exists():
-    logger.info('Found pre-analyses results file ' + preAnlysResFilePath.as_posix())
-
-if args.preAnalyses:
-
-    oprText = 'pre-analyses'
-    logger.openOperation(oprText)
-
-    # a. Create PreAnalyser object.
-    preAnlysr = PreAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                            effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                            transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                            sampleSelCols=pars.sampleSelCols,
-                            sampleDecCols=[pars.effortCol, pars.distanceCol],
-                            sampleIndCol=pars.sampleIndCol, sampleSpecCustCols=pars.sampleSpecCustCols,
-                            abbrevCol=pars.sampleAbbrevCol, abbrevBuilder=pars.sampleAbbrev,
-                            distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                            surveyType=pars.surveyType, distanceType=pars.distanceType,
-                            clustering=pars.clustering,
-                            resultsHeadCols=pars.preResultsHeadCols,
-                            workDir=workDir,
-                            runMethod=pars.runPreAnalysisMethod, runTimeOut=pars.runPreAnalysisTimeOut,
-                            logData=pars.logPreAnalysisData, logProgressEvery=pars.logPreAnalysisProgressEvery)
-
-    # b. Check sample specs.
-    dfExplSampleSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
-        preAnlysr.explicitParamSpecs(implParamSpecs=sampleSpecFile, dropDupes=True, check=True)
-
-    assert userParamSpecCols == []  # No analysis params here (auto. generated by PreAnalyser)
-    assert verdict
-    assert not reasons
-
-    logger.info2(f'Explicit sample specs:\n{dfExplSampleSpecs.to_string()}')
-    logger.info2(f'Pre-analysis model fallback strategy:\n{pd.DataFrame(pars.modelPreStrategy).to_string()}')
-    logger.info1(f'From sample specs, {len(dfExplSampleSpecs)} samples to pre-analyse')
-    if args.verbose and not args.realRun:
-        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
-
-    if any(col not in dfExplSampleSpecs.columns for col in pars.sampleSpecCustCols):
-        logger.error('Missing custom (pass-through) column(s) in sample specs: {}'
-                     .format(', '.join(col for col in pars.sampleSpecCustCols
-                                       if col not in dfExplSampleSpecs.columns)))
-        sys.exit(2)
-
-    # c. Run pre-analyses.
-    if args.realRun:
-        preResults = preAnlysr.run(implSampleSpecs=sampleSpecFile, dModelStrategy=pars.modelPreStrategy,
-                                   threads=args.threads)
-        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
-    preAnlysr.shutdown()
-
-    # d. Save results to disk.
-    # if 'dfSampleStats' in dir(): # Qq specs supplémentaires
-    #    preResults.updateSpecs(sampleStats=dfSampleStats)
-    if args.realRun:
-        preResults.toExcel(preAnlysResFilePath)
-
-    logger.closeOperation(oprText)
-
-# 10. Generate pre-analysis reports (if specified to).
-PreReport = MCDSResultsPreReport
-reportWord = 'rapport' if pars.studyLang == 'fr' else 'report'
-if args.preReports:
-
-    # a. Load pre-analyses results if not just computed
-    if not args.preAnalyses:
-
-        if not preAnlysResFilePath.exists():
-            logger.error(f'Cannot generate pre-analysis reports: results file not found {preAnlysResFilePath.as_posix()}')
-            sys.exit(2)
-
-        logger.info(f'Loading pre-analysis results from {preAnlysResFilePath.as_posix()}')
-
-        preAnlysr = PreAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                                effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                                transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                                sampleSelCols=pars.sampleSelCols,
-                                sampleDecCols=[pars.effortCol, pars.distanceCol],
-                                sampleIndCol=pars.sampleIndCol, sampleSpecCustCols=pars.sampleSpecCustCols,
-                                abbrevCol=pars.sampleAbbrevCol, abbrevBuilder=pars.sampleAbbrev,
-                                distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                                surveyType=pars.surveyType, distanceType=pars.distanceType,
-                                clustering=pars.clustering,
-                                resultsHeadCols=pars.preResultsHeadCols)
-
-        preResults = preAnlysr.setupResults()
-        preAnlysr.shutdown()  # Not really needed actually.
-
-        preResults.fromFile(preAnlysResFilePath)
-
-    # b. Check report generation parameters
-    assert isinstance(pars.preReportSortAscend, bool) or len(pars.preReportSortCols) == len(pars.preReportSortAscend)
-
-    # c. Generate specified reports
-    oprText = 'generation of {} pre-analysis report(s)'.format(','.join(args.preReports))
-    logger.openOperation(oprText)
-
-    preRepPrfx = f'{pars.studyName}{pars.subStudyName}-preanalyses-{reportWord}'
-    preReport = PreReport(resultsSet=preResults, lang=pars.studyLang,
-                          title=pars.preReportStudyTitle, subTitle=pars.preReportStudySubTitle,
-                          anlysSubTitle=pars.preReportAnlysSubTitle, description=pars.preReportStudyDescr,
-                          keywords=pars.preReportStudyKeywords, pySources=paramFiles,
-                          sampleCols=pars.preReportSampleCols, paramCols=pars.preReportParamCols,
-                          resultCols=pars.preReportResultCols, synthCols=pars.preReportSynthCols,
-                          sortCols=pars.preReportSortCols, sortAscend=pars.preReportSortAscend,
-                          tgtFolder=workDir, tgtPrefix=preRepPrfx,
-                          **pars.preReportPlotParams)
-
-    if 'excel' in args.preReports:
-        logger.info1('* Excel pre-analysis report to be generated')
-        if args.realRun:
-            preReport.toExcel(rebuild=pars.preReportRebuild)
-
-    if 'html' in args.preReports:
-        logger.info1('* HTML pre-analysis report to be generated')
-        if args.realRun:
-            preReport.toHtml(rebuild=pars.preReportRebuild,
-                             generators=1 if args.threads is None else args.threads)
-
-    logger.closeOperation(oprText)
-
-# 11. Run analyses (if specified to).
-Analyser = MCDSAnalyser
-anlysExplSpecFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-analyses-explispecs.xlsx'
-if anlysExplSpecFilePath.exists():
-    logger.info('Found analyses explicit spec. file ' + anlysExplSpecFilePath.as_posix())
-anlysResFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-analyses-{resultsWord}.xlsx'
-if anlysResFilePath.exists():
-    logger.info('Found analyses results file ' + anlysResFilePath.as_posix())
-
-if args.analyses:
-
-    # Check analysis spec. file
-    analysisSpecFile = pars.analysisSpecFile if 'analysisSpecFile' in vars(pars) else None
-    if analysisSpecFile:
-        analysisSpecFile = pl.Path(analysisSpecFile)
-        if not analysisSpecFile.exists():
-            logger.error(f'Could not find analysis spec. file {analysisSpecFile.as_posix()}')
-            sys.exit(2)
-    else:
-        logger.error('No analysis spec. file specified, can\'t run analyses')
-        sys.exit(2)
-
-    oprText = 'analyses'
-    logger.openOperation(oprText)
-
-    # a. Create Analyser object.
-    anlysr = Analyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                      effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                      transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                      sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
-                      anlysSpecCustCols=pars.analysisSpecCustCols,
-                      abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
-                      anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
-                      distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                      surveyType=pars.surveyType, distanceType=pars.distanceType,
-                      clustering=pars.clustering,
-                      resultsHeadCols=pars.resultsHeadCols,
-                      ldTruncIntrvSpecs=pars.ldTruncIntrvSpecs, truncIntrvEpsilon=pars.truncIntrvEpsilon,
-                      workDir=workDir, runMethod=pars.runAnalysisMethod, runTimeOut=pars.runAnalysisTimeOut,
-                      logData=pars.logAnalysisData, logProgressEvery=pars.logAnalysisProgressEvery,
-                      defEstimKeyFn=pars.defEstimKeyFn, defEstimAdjustFn=pars.defEstimAdjustFn,
-                      defEstimCriterion=pars.defEstimCriterion, defCVInterval=pars.defCVInterval,
-                      defMinDist=pars.defMinDist, defMaxDist=pars.defMaxDist,
-                      defFitDistCuts=pars.defFitDistCuts, defDiscrDistCuts=pars.defDiscrDistCuts)
-
-    # b. Check analysis specs.
-    dfExplAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
-        anlysr.explicitParamSpecs(implParamSpecs=analysisSpecFile, dropDupes=True, check=True)
-
-    assert userParamSpecCols == pars.analysisParamCols
-    assert verdict
-    assert not reasons
-
-    logger.info2(f'Explicit analysis specs:\n{dfExplAnlysSpecs.to_string()}')
-    logger.info1(f'From analysis specs, {len(dfExplAnlysSpecs)} analyses to run')
-    if args.verbose and not args.realRun:
-        dfExplAnlysSpecs.to_excel(anlysExplSpecFilePath, index=False)
-
-    if any(col not in dfExplAnlysSpecs.columns for col in pars.analysisSpecCustCols):
-        logger.error('Missing custom (pass-through) column(s) in analysis specs: {}'
-                     .format(', '.join(col for col in pars.analysisSpecCustCols
-                                       if col not in dfExplAnlysSpecs.columns)))
-        sys.exit(2)
-
-    # c. Run analyses.
-    if args.realRun:
-        results = anlysr.run(implParamSpecs=analysisSpecFile, threads=args.threads)
-        dfExplAnlysSpecs.to_excel(anlysExplSpecFilePath, index=False)
-    anlysr.shutdown()
-
-    # d. Save results to disk.
-    if args.realRun:
-        results.toExcel(anlysResFilePath)
-
-    logger.closeOperation(oprText)
-
-# 12. Generate analysis reports (if specified to).
-FullReport = MCDSResultsFullReport
-FilSorReport = MCDSResultsFilterSortReport
-
-if args.reports:
-
-    # a. Load analysis results if not just computed
-    if not args.analyses:
-
-        if not anlysResFilePath.exists():
-            logger.error(f'Cannot generate analysis reports: results file not found {anlysResFilePath.as_posix()}')
-            sys.exit(2)
-
-        logger.info(f'Loading analysis results from {anlysResFilePath.as_posix()}')
-
-        anlysr = Analyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                          effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                          transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                          sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
-                          anlysSpecCustCols=pars.analysisSpecCustCols,
-                          abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
-                          anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
-                          distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                          surveyType=pars.surveyType, distanceType=pars.distanceType,
-                          clustering=pars.clustering,
-                          resultsHeadCols=pars.resultsHeadCols)
-
-        results = anlysr.setupResults()
-        anlysr.shutdown()  # Not really needed actually.
-
-        results.fromFile(anlysResFilePath)
-
-    # b. Generate specified reports
-    oprText = 'generation of {} analysis report(s)'.format(','.join(args.reports))
-    logger.openOperation(oprText)
-
-    repPrfx = f'{pars.studyName}{pars.subStudyName}-analyses-{reportWord}'
-
-    # * Auto-filtered reports.
-    if ('excel' in args.reports and 'full' not in args.reports['excel'] and 'filsorReportSchemes' in dir()) \
-       or ('html' in args.reports and 'full' not in args.reports['html'] and 'filsorAnlysReportSchemes' in dir()):
-
-        assert isinstance(pars.filsorReportSortAscend, bool) \
-               or len(pars.filsorReportSortCols) == len(pars.filsorReportSortAscend)
-
-        filsorReport = FilSorReport(resultsSet=results, lang=pars.studyLang,
-                                    title=pars.anlysFilsorReportStudyTitle,
-                                    subTitle=pars.anlysFilsorReportStudySubTitle,
-                                    anlysSubTitle=pars.anlysFilsorReportAnlysSubTitle,
-                                    description=pars.anlysFilsorReportStudyDescr,
-                                    keywords=pars.anlysFilsorReportStudyKeywords, pySources=paramFiles,
-                                    sampleCols=pars.filsorReportSampleCols, paramCols=pars.filsorReportParamCols,
-                                    resultCols=pars.filsorReportResultCols, synthCols=pars.filsorReportSynthCols,
-                                    sortCols=pars.filsorReportSortCols, sortAscend=pars.filsorReportSortAscend,
-                                    filSorSchemes=pars.filsorReportSchemes,
-                                    tgtFolder=workDir, tgtPrefix=repPrfx,
-                                    **pars.filsorReportPlotParams)
-
-        if 'excel' in args.reports and 'full' not in args.reports['excel'] and 'filsorReportSchemes' in dir():
-            logger.info1('* Auto-filtered Excel analysis report to be generated: all schemes')
-            if args.realRun:
-                filsorReport.toExcel(rebuild=pars.filsorReportRebuild)
-
-        if 'html' in args.reports  and 'full' not in args.reports['html'] and 'filsorAnlysReportSchemes' in dir():
-            logger.info1('* Auto-filtered HTML analysis report(s) to be generated: {}'
-                         .format(', '.join(filsorAnlysReportSchemes.keys())))
-            if args.realRun:
-                for scheme in filsorAnlysReportSchemes.values():
-                    filsorReport.toHtml(filSorScheme=scheme, rebuild=pars.filsorReportRebuild)
-
-    # * Full reports.
-    if ('excel' in args.reports and ('full' in args.reports['excel'] or 'filsorReportSchemes' not in dir())) \
-       or ('html' in args.reports and 'full' in args.reports['html']):
-
-        assert isinstance(pars.fullReportSortAscend, bool) \
-               or len(pars.fullReportSortCols) == len(pars.fullReportSortAscend)
-
-        fullReport = FullReport(resultsSet=results, lang=pars.studyLang,
-                                title=pars.anlysFullReportStudyTitle,
-                                subTitle=pars.anlysFullReportStudySubTitle,
-                                anlysSubTitle=pars.anlysFullReportAnlysSubTitle,
-                                description=pars.anlysFullReportStudyDescr,
-                                keywords=pars.anlysFullReportStudyKeywords, pySources=paramFiles,
-                                sampleCols=pars.fullReportSampleCols, paramCols=pars.fullReportParamCols,
-                                resultCols=pars.fullReportResultCols, synthCols=pars.fullReportSynthCols,
-                                sortCols=pars.fullReportSortCols, sortAscend=pars.fullReportSortAscend,
-                                tgtFolder=workDir, tgtPrefix=repPrfx,
-                                **pars.fullReportPlotParams)
-
-        if 'excel' in args.reports and ('full' in args.reports['excel'] or 'filsorReportSchemes' not in dir()):
-            logger.info1('* Full Excel analysis report to be generated')
-            if args.realRun:
-                fullReport.toExcel(rebuild=pars.fullReportRebuild)
-
-        if 'html' in args.reports and 'full' in args.reports['html']:
-            logger.info1('* Full HTML analysis report to be generated')
-            if args.realRun:
-                fullReport.toHtml(rebuild=pars.fullReportRebuild,
-                                  generators=1 if args.threads is None else args.threads)
-
-    # if not any(rep in dir() for rep in ['fullReport', 'filsorReport']):  # This doesn't work ... !!??
-    if not any(['fullReport' in dir(), 'filsorReport' in dir()]):
-        logger.error('Neither full nor filter & sort analysis report to be generated (whatever format)')
-        sys.exit(2)
-
-    logger.closeOperation(oprText)
-
-# 13. Run opt-analyses (if specified to).
-OptAnalyser = MCDSTruncationOptanalyser
-optAnlysExplSpecFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-optanalyses-explispecs.xlsx'
-if optAnlysExplSpecFilePath.exists():
-    logger.info('Found opt-analyses explicit spec. file ' + optAnlysExplSpecFilePath.as_posix())
-optAnlysResFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-optanalyses-{resultsWord}.xlsx'
-if optAnlysResFilePath.exists():
-    logger.info('Found opt-analyses results file ' + optAnlysResFilePath.as_posix())
-if args.optAnalyses:
-
-    # Check analysis spec. file
-    optAnalysisSpecFile = pars.optAnalysisSpecFile if 'optAnalysisSpecFile' in vars(pars) else None
-    if optAnalysisSpecFile:
-        optAnalysisSpecFile = pl.Path(optAnalysisSpecFile)
-        if not optAnalysisSpecFile.exists():
-            logger.error(f'Could not find opt-analysis spec. file {optAnalysisSpecFile.as_posix()}')
-            sys.exit(2)
-    else:
-        logger.error('No opt-analysis spec. file specified, can\'t run opt-analyses')
-        sys.exit(2)
-
-    oprText = 'opt-analyses'
-    logger.openOperation(oprText)
-
-    # a. Create OptAnalyser object.
-    optAnlysr = OptAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                            effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                            transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                            sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
-                            sampleDistCol=pars.distanceCol, anlysSpecCustCols=pars.optAnalysisSpecCustCols,
-                            abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
-                            anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
-                            distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                            surveyType=pars.surveyType, distanceType=pars.distanceType,
-                            clustering=pars.clustering,
-                            resultsHeadCols=pars.optResultsHeadCols,
-                            ldTruncIntrvSpecs=pars.ldTruncIntrvSpecs, truncIntrvEpsilon=pars.truncIntrvEpsilon,
-                            workDir=workDir, logData=pars.logOptAnalysisData,
-                            runMethod=pars.runOptAnalysisMethod, runTimeOut=pars.runOptAnalysisTimeOut,
-                            logAnlysProgressEvery=pars.logOptAnalysisProgressEvery,
-                            logOptimProgressEvery=pars.logOptimisationProgressEvery,
-                            backupOptimEvery=pars.backupOptimisationsEvery,
-                            defEstimKeyFn=pars.defEstimKeyFn, defEstimAdjustFn=pars.defEstimAdjustFn,
-                            defEstimCriterion=pars.defEstimCriterion, defCVInterval=pars.defCVInterval,
-                            defExpr2Optimise=pars.defExpr2Optimise, defMinimiseExpr=pars.defMinimiseExpr,
-                            defOutliersMethod=pars.defOutliersMethod,
-                            defOutliersQuantCutPct=pars.defOutliersQuantCutPct,
-                            defFitDistCutsFctr=pars.defFitDistCutsFctr,
-                            defDiscrDistCutsFctr=pars.defDiscrDistCutsFctr,
-                            defSubmitTimes=pars.defSubmitTimes, defSubmitOnlyBest=pars.defSubmitOnlyBest,
-                            dDefSubmitOtherParams=pars.dDefSubmitOtherParams,
-                            dDefOptimCoreParams=dict(core=pars.defCoreEngine, maxIters=pars.defCoreMaxIters,
-                                                     termExprValue=pars.defCoreTermExprValue,
-                                                     algorithm=pars.defCoreAlgorithm,
-                                                     maxRetries=pars.defCoreMaxRetries))
-
-    # b. Check opt-analysis specs.
-    dfExplOptAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
-        optAnlysr.explicitParamSpecs(implParamSpecs=optAnalysisSpecFile, dropDupes=True, check=True)
-
-    assert userParamSpecCols == pars.optAnalysisParamCols
-    assert verdict
-    assert not reasons
-
-    logger.info2(f'Explicit opt-analysis specs:\n{dfExplOptAnlysSpecs.to_string()}')
-    optUserParamSpecCols = optAnlysr.zoptr4Specs.optimisationParamSpecUserNames(userParamSpecCols, intParamSpecCols)
-    sbAnlysNeedOpt = dfExplOptAnlysSpecs[optUserParamSpecCols].apply(optAnlysr.analysisNeedsOptimisationFirst,
-                                                                     axis='columns')
-    logger.info1('From opt-analysis specs, {} / {} opt-analyses to run with truncation optimisation first ...'
-                 .format(sbAnlysNeedOpt.sum(), len(dfExplOptAnlysSpecs)))
-    logger.info1('... implying possibly up to {} auto-analyses in the background if only full "auto" specs'
-                 .format(sbAnlysNeedOpt.sum() * pars.defCoreMaxIters * pars.defSubmitTimes))
-    if args.verbose and not args.realRun:
-        dfExplOptAnlysSpecs.to_excel(optAnlysExplSpecFilePath, index=False)
-
-    if any(col not in dfExplOptAnlysSpecs.columns for col in pars.optAnalysisSpecCustCols):
-        logger.error('Missing custom (pass-through) column(s) in opt-analysis specs: {}'
-                     .format(', '.join(col for col in pars.optAnalysisSpecCustCols
-                                       if col not in dfExplOptAnlysSpecs.columns)))
-        sys.exit(2)
-
-    # c. Check if recovery possible (not 100% reliable), if specified
-    if args.recoverOpts:
-        if not list(workDir.glob('optr-resbak-*.pickle.xz')):
-            logger.error('No optimisation backup file found, can\'t recover ; you must start from scratch')
-            sys.exit(2)
-        else:
-            logger.info('Backup files are there, recovery is very likely possible: let\'s try !')
-
-    # d. Run opt-analyses.
-    if args.realRun:
-        optResults = optAnlysr.run(implParamSpecs=optAnalysisSpecFile,
-                                   threads=args.threads, recoverOptims=args.recoverOpts)
-        dfExplOptAnlysSpecs.to_excel(optAnlysExplSpecFilePath, index=False)
-    optAnlysr.shutdown()
-
-    # e. Save results to disk.
-    if args.realRun:
-        optResults.toExcel(optAnlysResFilePath)
-
-    logger.closeOperation(oprText)
-
-# 14. Generate opt-analysis reports (if specified to).
-if args.optReports:
-
-    # a. Load opt-analysis results if not just computed
-    if not args.optAnalyses:
-
-        if not optAnlysResFilePath.exists():
-            logger.error(f'Cannot generate opt-analysis reports: results file not found {optAnlysResFilePath.as_posix()}')
-            sys.exit(2)
-
-        logger.info(f'Loading opt-analysis results from {optAnlysResFilePath.as_posix()}')
-
-        optAnlysr = OptAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
-                                effortConstVal=pars.passEffort, effortCol=pars.effortCol,
-                                transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
-                                sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
-                                sampleDistCol=pars.distanceCol, anlysSpecCustCols=pars.optAnalysisSpecCustCols,
-                                abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
-                                anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
-                                distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
-                                surveyType=pars.surveyType, distanceType=pars.distanceType,
-                                clustering=pars.clustering,
-                                resultsHeadCols=pars.optResultsHeadCols)
-
-        optResults = optAnlysr.setupResults()
-        optAnlysr.shutdown()  # Not really needed actually.
-
-        optResults.fromFile(optAnlysResFilePath)
-
-    # b. Generate specified reports
-    assert isinstance(pars.filsorReportSortAscend, bool) \
-           or len(pars.filsorReportSortCols) == len(pars.filsorReportSortAscend)
-
-    oprText = 'generation of {} opt-analysis report(s)'.format(','.join(args.optReports))
-    logger.openOperation(oprText)
-
-    optRepPrfx = f'{pars.studyName}{pars.subStudyName}-optanalyses-{reportWord}'
-
-    # * Auto-filtered reports.
-    if ('excel' in args.optReports and 'full' not in args.optReports['excel'] and 'filsorReportSchemes' in dir()) \
-       or ('html' in args.optReports and 'filsorOptAnlysReportSchemes' in dir()):
-
-        assert isinstance(pars.filsorReportSortAscend, bool) \
-               or len(pars.filsorReportSortCols) == len(pars.filsorReportSortAscend)
-
-        filsorOptReport = FilSorReport(resultsSet=optResults, lang=pars.studyLang,
-                                       title=pars.optAnlysFilsorReportStudyTitle,
-                                       subTitle=pars.optAnlysFilsorReportStudySubTitle,
-                                       anlysSubTitle=pars.optAnlysFilsorReportAnlysSubTitle,
-                                       description=pars.optAnlysFilsorReportStudyDescr,
-                                       keywords=pars.optAnlysFilsorReportStudyKeywords, pySources=paramFiles,
-                                       sampleCols=pars.filsorReportSampleCols, paramCols=pars.filsorReportParamCols,
-                                       resultCols=pars.filsorReportResultCols, synthCols=pars.filsorReportSynthCols,
-                                       sortCols=pars.filsorReportSortCols, sortAscend=pars.filsorReportSortAscend,
-                                       filSorSchemes=pars.filsorReportSchemes,
-                                       tgtFolder=workDir, tgtPrefix=optRepPrfx,
-                                       **pars.filsorReportPlotParams)
-
-        if 'excel' in args.optReports and 'full' not in args.optReports['excel'] and 'filsorReportSchemes' in dir():
-            logger.info1('* Auto-filtered Excel opt-analysis report to be generated: all schemes')
-            if args.realRun:
-                filsorOptReport.toExcel(rebuild=pars.filsorReportRebuild)
-
-        if 'html' in args.optReports and 'filsorOptAnlysReportSchemes' in dir():
-            logger.info1('* Auto-filtered HTML opt-analysis report(s) to be generated: {}'
-                         .format(', '.join(filsorOptAnlysReportSchemes.keys())))
-            if args.realRun:
-                for scheme in filsorOptAnlysReportSchemes.values():
-                    filsorOptReport.toHtml(filSorScheme=scheme, rebuild=pars.filsorReportRebuild)
-
-    # * Full reports.
-    if ('excel' in args.optReports and ('full' in args.optReports['excel'] or 'filsorReportSchemes' not in dir())) \
-       or ('html' in args.optReports and 'full' in args.optReports['html']):
-
-        assert isinstance(pars.fullReportSortAscend, bool) \
-               or len(pars.fullReportSortCols) == len(pars.fullReportSortAscend)
-
-        fullOptReport = FullReport(resultsSet=optResults, lang=pars.studyLang,
-                                   title=pars.optAnlysFullReportStudyTitle,
-                                   subTitle=pars.optAnlysFullReportStudySubTitle,
-                                   anlysSubTitle=pars.optAnlysFullReportAnlysSubTitle,
-                                   description=pars.optAnlysFullReportStudyDescr,
-                                   keywords=pars.optAnlysFullReportStudyKeywords, pySources=paramFiles,
-                                   sampleCols=pars.fullReportSampleCols, paramCols=pars.fullReportParamCols,
-                                   resultCols=pars.fullReportResultCols, synthCols=pars.fullReportSynthCols,
-                                   sortCols=pars.fullReportSortCols, sortAscend=pars.fullReportSortAscend,
-                                   tgtFolder=workDir, tgtPrefix=optRepPrfx,
-                                   **pars.fullReportPlotParams)
-
-        if 'excel' in args.optReports \
-           and ('full' in args.optReports['excel'] or 'filsorReportSchemes' not in dir()):
-            logger.info1('* Full Excel opt-analysis report to be generated')
-            if args.realRun:
-                fullOptReport.toExcel(rebuild=pars.fullReportRebuild)
-
-        if 'html' in args.optReports and 'full' in args.optReports['html']:
-            logger.info1('* Full HTML opt-analysis report to be generated')
-            if args.realRun:
-                fullOptReport.toHtml(rebuild=pars.fullReportRebuild,
-                                     generators=1 if args.threads is None else args.threads)
-
-    # if not any(rep in dir() for rep in ['fullOptReport', 'filsorOptReport']):  # This doesn't work ... !!??
-    if not any(['fullOptReport' in dir(), 'filsorOptReport' in dir()]):
-        logger.error('Neither full nor filter & sort opt-analysis report to be generated (whatever format)')
-        sys.exit(2)
-
-    logger.closeOperation(oprText)
-
-if not args.realRun and not emptyRun:
-    logger.info('Checks done, seems you can now really run this, through -u / --realrun :-)')
-
-# 15. Done.
-sys.exit(0)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Package main script, for when pyaudisam is invoked through "python -m"
+import re
+import sys
+import atexit
+import tempfile
+import pathlib as pl
+import argparse
+
+import pandas as pd
+
+from . import log, runtime
+from .utils import loadPythonData
+from .analyser import MCDSAnalyser, MCDSPreAnalyser, FilterSortSchemeIdManager
+from .optanalyser import MCDSTruncationOptanalyser
+from .report import MCDSResultsPreReport, MCDSResultsFullReport, MCDSResultsFilterSortReport
+
+
+class Logger:
+
+    """Local logger, taking care of output log file at exit time"""
+
+    DefLogNamePrefix = 'pyaudisam-main'
+
+    def __init__(self, runTimestamp, level=log.INFO):
+
+        self.runTimestamp = runTimestamp
+        self.openOpr = 'Checking'
+        self.dOprStart = dict()
+
+        # Log to sys.stdout, and also to a temporary log file (unique folder).
+        self.runLogFileName = f'{self.DefLogNamePrefix}.{runTimestamp}.log'
+        self.runLogFileName = pl.Path(tempfile.mkdtemp(prefix='pyaudisam')) / self.runLogFileName
+        log.configure(handlers=[sys.stdout, self.runLogFileName], reset=True,
+                      loggers=[dict(name='matplotlib', level=log.WARNING),
+                               dict(name='ads', level=log.INFO2),
+                               dict(name='ads.eng', level=log.INFO),
+                               dict(name='ads.exr', level=log.INFO),
+                               # dict(name='ads.dat', level=log.DEBUG),
+                               # dict(name='ads.rep', level=log.DEBUG),
+                               dict(name='ads.anr', level=log.INFO1),
+                               dict(name='main', level=level)])
+
+        # Fallback final log file path-name as long as it is not specified : current folder, generic (timestamped) name.
+        self.finalLogFileName = pl.Path('.') / self.runLogFileName.name
+
+        # Set up an atexit handler to move and rename the session log file in a user-friendly place if possible.
+        atexit.register(self.giveBackLogFile)
+
+        # Add logging methods to this logger
+        self.logger = log.logger(name='main')
+        for meth in dir(self.logger):
+            if any(meth.startswith(prefix) for prefix in ['critical', 'error', 'warning', 'info', 'debug']):
+                setattr(self, meth, getattr(self.logger, meth))
+
+        # Done.
+        self.info1(f'Logging session to temporary ' + self.runLogFileName.as_posix())
+        self.info('Current folder: ' + pl.Path().absolute().as_posix())
+        self.info('Command line: {} {}'.format(pl.Path(sys.argv[0]).as_posix(), ' '.join(sys.argv[1:])))
+
+    def setFinalLogPrefix(self, prefix=None):
+
+        self.finalLogFileName = None if prefix is None else pl.Path(prefix + f'.{self.runTimestamp}.log')
+
+    def giveBackLogFile(self):
+
+        if any(arg in sys.argv for arg in ['-h', '--help']):
+            self.finalLogFileName = None  # No need for a log file here !
+
+        if self.finalLogFileName is not None:
+            self.info1(f'Giving back session log to {self.finalLogFileName}')
+
+        # Release the log file
+        log.configure(handlers=[sys.stdout], reset=True, loggers=[dict(name='main', level=log.INFO)])
+
+        # Actually move and rename the log file if it is needed, or delete it if not.
+        if self.finalLogFileName is not None:
+            self.finalLogFileName.parent.mkdir(parents=True, exist_ok=True)
+            self.runLogFileName.rename(self.finalLogFileName)
+        else:
+            self.runLogFileName.unlink()
+
+        # Remove initial parent folder, now empty (was specially created for).
+        self.runLogFileName.parent.rmdir()
+
+    def setRealRun(self, realRun=True):
+
+        if realRun:
+            self.openOpr = 'Running'
+            self.info('This is a real run: requested operation will be actually run !')
+        else:
+            self.openOpr = 'Checking'
+            self.warning('Not a real run, only checking requested operations ...')
+
+    def openOperation(self, oprText):
+
+        self.info(f'{self.openOpr} {oprText} ...')
+        self.dOprStart[oprText] = pd.Timestamp.now()
+
+    def closeOperation(self, oprText):
+
+        elapsed = str(pd.Timestamp.now() - self.dOprStart[oprText]).replace('0 days ', '')
+        self.info(f'Done {self.openOpr.lower()} {oprText} ({elapsed}).')
+        del self.dOprStart[oprText]
+
+
+def decodeReportArg(repArg, repName='report'):
+
+    """Decode report argument value
+
+    Mini-language examples: none ; excel ; excel,html ; html:full ; excel,html:mqua-r925,html:mqua-r950
+
+    :returns: dict(format: list(type)) with format in {'none', 'excel', 'html'}, 'none' excluding any other,
+              and list(type) empty for 'none', empty or ['full'] for 'excel' format,
+              and containing at least 1 of {full, <filter-sort method regex search string>*} for 'html' format.
+    """
+
+    # Separate format[:type] items
+    repItems = [item.lower() for item in repArg.split(',')]
+
+    # List types for each format
+    repSpecs = dict()
+    for repItem in repItems:
+        fmt, typ = repItem.split(':') + ([None] if ':' not in repItem else [])
+        if fmt not in repSpecs:
+            repSpecs[fmt] = list()
+        if typ not in repSpecs[fmt] and typ is not None:
+            repSpecs[fmt].append(typ)
+
+    # 'none' format kept iff alone
+    repSpecs = {fmt: typs for fmt, typs in repSpecs.items() if fmt != 'none' or len(repSpecs) == 1}
+    if 'none' in repSpecs:
+        repSpecs.clear()
+
+    # Check formats
+    unsupRepFmts = [fmt for fmt in repSpecs if fmt not in ['none', 'html', 'excel']]
+    if unsupRepFmts:
+        logger.error('Unsupported {} format(s) {}'.format(repName, ', '.join(unsupRepFmts)))
+        sys.exit(2)
+
+    logger.debug1(f'{repName}: {repSpecs}')
+
+    return repSpecs
+
+
+# 0. Configure logging.
+runTimestamp = pd.Timestamp.now().strftime('%y%m%d-%H%M%S')  # Date+time of the run (for log file, ... etc).
+
+logger = Logger(runTimestamp, level=log.DEBUG2 if '-v' in sys.argv or '--verbose' in sys.argv else log.INFO1)
+
+logger.info('Computation platform:')
+for k, v in runtime.items():
+    logger.info(f'* {k}: {v}')
+
+# 1. Parse, check and post-process command line args.
+argser = argparse.ArgumentParser(prog='pyaudisam',  # usage='python -m pyaudisam',
+                                 description='Prepare or run (and / or generate reports for) many distance'
+                                             ' sampling (pre-)analyses using a DS engine from Distance software',
+                                 epilog='Exit codes:'
+                                        ' 0 if OK,'
+                                        ' 2 if any command line argument issue,'
+                                        ' 1 if any other (unexpected) issue.')
+
+argser.add_argument('-u', '--run', dest='realRun', action='store_true', default=False,
+                    help='Actually run specified operation (not only run diagnosis of)'
+                         ' => as long as -u/--run is not there, you can try any option,'
+                         ' it wont start or write anything (or may be slightly when -v, but anyway)'
+                         ' ... feel free, you are safe :-)')
+argser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,
+                    help='Display more infos about the work to be done and export sample / (opt-)analysis'
+                         ' spec. files when relevant in the current directory ;')
+argser.add_argument('-p', '--params', dest='paramFile', type=str, required=True,
+                    help='Path-name of python file (.py assumed if no extension / suffix given) specifying'
+                         ' export / (opt)analysis / report parameters')
+argser.add_argument('-s', '--speparams', dest='speParams', type=str, default='',
+                    help='Comma-separated key=value items specifying "special" parameters'
+                         'defined before the parameter file is loaded, just as overridable built-in variables'
+                         r' (syntax and limitations: string-only values, with no space or ,;\'"$&! inside'
+                         ' => use it only for few and simple args like switches and simple names')
+argser.add_argument('-w', '--workdir', dest='workDir', type=str, default='.',
+                    help='Work folder = where to store DS analyses sub-folders and output files'
+                         ' (Note: a timestamp sub-folder YYMMDD-HHMMSS is auto-appended, if not already such)')
+argser.add_argument('-n', '--notimestamp', dest='noTimestamp', action='store_true', default=False,
+                    help='Inhibit auto-timestamped work sub-folder creation (under work folder)')
+argser.add_argument('-x', '--distexport', dest='distExport', action='store_true', default=False,
+                    help='Export one Distance input file for each specified sample of the survey data'
+                         ' (Note: a sample spec. file and a survey data file must be also specified, through -p)')
+argser.add_argument('-e', '--preanalyses', dest='preAnalyses', action='store_true', default=False,
+                    help='Run pre-analyses for the specified samples of the survey data'
+                         ' (Note: a sample spec. file and a survey data file must be also specified, through -p)')
+argser.add_argument('-a', '--analyses', dest='analyses', action='store_true', default=False,
+                    help='Run analyses for the specified samples of the survey data'
+                         ' (Note: an analysis spec. file and a survey data file must be also specified, through -p)')
+argser.add_argument('-o', '--optanalyses', dest='optAnalyses', action='store_true', default=False,
+                    help='Run opt-analyses for the specified samples of the survey data'
+                         ' (Note: an opt-analysis spec. file and a survey data file must be also specified,'
+                         ' through -p)')
+argser.add_argument('-c', '--recoveropts', dest='recoverOpts', action='store_true', default=False,
+                    help='Restart optimisations at the point they were when interrupted (for any reason), from the'
+                         ' last usable recovery file found in the work folder (use in conjonction with -w & -n)'
+                         ' (Note: if no usable recovery file found, the restart will fail: no automatic restart from'
+                         ' scratch ; remove -c to do so)')
+argser.add_argument('-t', '--prereports', dest='preReports', type=str, default='none',
+                    help='Which reports to generate from pre-analyses results, through comma-separated keywords'
+                         ' among {excel, html, none} (case does not matter, none ignored if not alone)')
+argser.add_argument('-r', '--reports', dest='reports', type=str, default='none',
+                    help='Which reports to generate from analyses results, through comma-separated format:type'
+                         ' case-insensitive items with format among {excel, html, none}, "none" ignored if not alone ;'
+                         ' type among {full, <empty>} for excel format (use full to prevent default filter-sort report'
+                         " when filter-sort methods are available in the '-p' / '--params' parameters),"
+                         ' and among {full, <filter-sort method regex search string>*} (at least one of) for html ;'
+                         ' note: available filter-sort methods are automatically listed when auto-filtering parameters'
+                         ' are specified, so run command without -u first !'
+                         ' examples: none ; excel ; excel:full,html:full ; html:mqua92,html:full,excel,html:mqua950')
+argser.add_argument('-f', '--optreports', dest='optReports', type=str, default='none',
+                    help='Which reports to generate from opt-analyses results (same mini-language as for -r)')
+argser.add_argument('-l', '--logprefix', dest='logPrefix', type=str, default=None,
+                    help='Target log file path-name prefix (will be postfixed by .<YYMMDD-HHMMSS timestamp>.log)'
+                         f" (Default: <work folder>/{logger.DefLogNamePrefix} if -u/--run, else 'none' ;"
+                         "if special value 'none', no log saved)")
+argser.add_argument('-m', '--threads', dest='threads', type=int, default=0,
+                    help='Number of parallel threads to use for (pre/opt-)analyses / report generation'
+                         ' (default: 0 => auto-determined actual number of parallel threads from CPU specs ;'
+                         ' 1 for no parallelism ; for any other choice, first check your actual CPU specs)')
+argser.add_argument('-g', '--engine', dest='engineType', type=str, default='MCDS',
+                    choices=['MCDS'],
+                    help='The Distance engine to use, among MCDS, ... and no other for the moment'
+                         ' (insensitive to case)')
+
+args = argser.parse_args()
+logger.debug(f'Arguments: {args}')
+
+logger.setRealRun(args.realRun)
+
+if args.threads == 1:
+    args.threads = None  # No need for asynchronism: enforce sequential run.
+
+args.preReports = decodeReportArg(args.preReports, repName='pre-analysis report')
+args.reports = decodeReportArg(args.reports, repName='analysis report')
+args.optReports = decodeReportArg(args.optReports, repName='opt-analysis report')
+
+logger.info1('Arguments:')
+for k, v in vars(args).items():
+    logger.info1(f'* {k}: {v}')
+
+# 2. Load parameter python file, passing "special" parameters if any.
+speParamItems = args.speParams.split(',') if args.speParams else list()
+if any(item.count('=') != 1 for item in speParamItems):
+    logger.error(f'Syntax error in pre-parameters: "{args.speParams}"'
+                 ' (should be "name1=value1,name2=value2,...")')
+    sys.exit(2)
+
+speParams = dict([item.split('=') for item in speParamItems])
+paramFile, pars = loadPythonData(path=args.paramFile, **speParams)
+if not pars:
+    logger.error(f'Failed to load parameter file {paramFile.as_posix()}')
+    sys.exit(2)
+logger.debug1('Parameters: ' + ', '.join(vars(pars)))
+
+paramFiles = [paramFile.as_posix()]
+if 'paramFiles' in vars(pars):
+    paramFiles += pars.paramFiles
+
+# 3. More checks on args and parameters.
+# a. Check filter and sort report args
+if 'filsorReportSchemes' in vars(pars):
+    filsorSchemeIdMgr = FilterSortSchemeIdManager()
+    filsorReportSchemes = {filsorSchemeIdMgr.schemeId(sch): sch for sch in pars.filsorReportSchemes}
+    logger.info('Available filter & sort report schemes: ' + ', '.join(filsorReportSchemes.keys()))
+
+if 'html' in args.reports and not args.reports['html']:
+    logger.error('HTML analysis report: MUST specify type / filter & sort method')
+    sys.exit(2)
+
+if 'html' in args.reports and ('full' not in args.reports['html'] or len(args.reports['html']) > 1):
+
+    filsorMatches = {reSchId: [schId for schId in filsorReportSchemes if re.search(reSchId, schId, flags=re.I)]
+                     for reSchId in args.reports['html'] if reSchId != 'full'}
+    if any(len(matches) != 1 for matches in filsorMatches.values()) \
+       or len(filsorMatches) != len(set(match for matches in filsorMatches.values() for match in matches)):
+        logger.error('HTML analysis report: Bad or ambiguous filter & sort method specification(s) {}'
+                     .format(' ; '.join('{} => [{}]'.format(reSchId, ', '.join(matchSchemes))
+                                        for reSchId, matchSchemes in filsorMatches.items())))
+        sys.exit(2)
+
+    filsorAnlysReportSchemes = {schId: schValue for schId, schValue in filsorReportSchemes.items()
+                                if schId in set(match for matches in filsorMatches.values() for match in matches)}
+    if not filsorAnlysReportSchemes:
+        logger.error('HTML analysis report: No filter & sort method specified')
+        sys.exit(2)
+
+if 'html' in args.optReports and not args.optReports['html']:
+    logger.error('HTML opt-analysis report: MUST specify type / filter & sort method')
+    sys.exit(2)
+
+if 'html' in args.optReports and ('full' not in args.optReports['html'] or len(args.optReports['html']) > 1):
+
+    filsorMatches = {reSchId: [schId for schId in filsorReportSchemes if re.search(reSchId, schId, flags=re.I)]
+                     for reSchId in args.optReports['html'] if reSchId != 'full'}
+    if any(len(matches) != 1 for matches in filsorMatches.values()) \
+       or len(filsorMatches) != len(set(match for matches in filsorMatches.values() for match in matches)):
+        logger.error('HTML opt-analysis report: Bad or ambiguous filter & sort method specification(s) {}'
+                     .format(' ; '.join('{} => [{}]'.format(reSchId, ', '.join(matchSchemes))
+                                        for reSchId, matchSchemes in filsorMatches.items())))
+        sys.exit(2)
+
+    filsorOptAnlysReportSchemes = {schId: schValue for schId, schValue in filsorReportSchemes.items()
+                                   if schId in set(match for matches in filsorMatches.values() for match in matches)}
+    if not filsorOptAnlysReportSchemes:
+        logger.error('HTML opt-analysis report: No filter & sort method specified')
+        sys.exit(2)
+
+# 4. Output folder for results, reports ... etc.
+#    (post-fixed with the run timestamp, if not already specified)
+workDir = args.workDir if 'workDir' in vars(args) else pars.workDir if 'workDir' in vars(pars) else '.'
+workDir = pl.Path(workDir)
+if not(args.noTimestamp or re.match('.*[0-9]{6}-[0-9]{4,6}$', workDir.name)):
+    workDir = workDir / runTimestamp
+logger.info(f'Work folder: {workDir.as_posix()}')
+
+# 5. Now we can set up the final session log file path-name prefix !
+if args.logPrefix is None:
+    if args.realRun:  # Default
+        args.logPrefix = workDir.as_posix() + f'/{pars.studyName}{pars.subStudyName}'
+elif args.logPrefix.lower() == 'none':
+    args.logPrefix = None
+logger.setFinalLogPrefix(args.logPrefix)
+
+# 6. Really something to do ?
+emptyRun = not any([args.distExport, args.preAnalyses, args.preReports,
+                    args.analyses, args.reports, args.optAnalyses, args.optReports])
+if emptyRun:
+    logger.warning('No operation specified: nothing to do actually !')
+
+if args.realRun and not workDir.parent.exists():
+    workDir.parent.mkdir()  # pyaudisam create sub-dirs, but not parent
+
+# 7. Load input data if needed:
+# a. Survey data
+# * individualised data with distance from observer to observed "object",
+# * point transect definition
+if not emptyRun:
+
+    surveyDataFile = pars.surveyDataFile if 'surveyDataFile' in vars(pars) else None
+    if surveyDataFile:
+        surveyDataFile = pl.Path(surveyDataFile)
+        if surveyDataFile.exists():
+            indivDistSheet = pars.indivDistDataSheet if 'indivDistDataSheet' in vars(pars) else 0
+            transectSheet = pars.transectsDataSheet if 'transectsDataSheet' in vars(pars) else 1
+            logger.info1(f'Loading survey data and transects infos from file {surveyDataFile.as_posix()} ...')
+            with pd.ExcelFile(surveyDataFile) as xlInFile:
+                dfMonoCatObs = pd.read_excel(xlInFile, sheet_name=indivDistSheet)
+                dfTransects = pd.read_excel(xlInFile, sheet_name=transectSheet)
+            logger.info1(f'... found {len(dfMonoCatObs)} mono-category sightings and {len(dfTransects)} transects')
+        else:
+            logger.error(f'Could not find survey data file {surveyDataFile.as_posix()}')
+            sys.exit(2)
+    else:
+        logger.error('No survey data file specified, can\'t export Distance file or run any type of analysis')
+        sys.exit(2)
+
+# 7.b. Sample specs
+if args.distExport or args.preAnalyses:
+
+    sampleSpecFile = pars.sampleSpecFile if 'sampleSpecFile' in vars(pars) else None
+    if sampleSpecFile:
+        sampleSpecFile = pl.Path(sampleSpecFile)
+        if not sampleSpecFile.exists():
+            logger.error(f'Could not find sample spec. file {sampleSpecFile.as_posix()}')
+            sys.exit(2)
+    else:
+        logger.error('No sample spec. file specified, can\'t export Distance file or run pre-analyses')
+        sys.exit(2)
+
+# 8. Export input files for Distance software, for manual analyses (if specified to).
+sampExplSpecFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-samples-explispecs.xlsx'
+if sampExplSpecFilePath.exists():
+    logger.info('Found sample explicit specs file ' + sampExplSpecFilePath.as_posix())
+
+if args.distExport:
+
+    oprText = 'export of input data files for Distance'
+    logger.openOperation(oprText)
+
+    # a. Create PreAnalyser object.
+    preAnlysr = MCDSPreAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                                effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                                transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                                sampleSelCols=pars.sampleSelCols,
+                                sampleDecCols=[pars.effortCol, pars.distanceCol],
+                                sampleIndCol=pars.sampleIndCol, sampleSpecCustCols=pars.sampleSpecCustCols,
+                                abbrevCol=pars.sampleAbbrevCol, abbrevBuilder=pars.sampleAbbrev,
+                                distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                                surveyType=pars.surveyType, distanceType=pars.distanceType,
+                                clustering=pars.clustering,
+                                workDir=workDir)
+
+    # b. Check sample specs.
+    dfExplSampleSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
+        preAnlysr.explicitParamSpecs(implParamSpecs=sampleSpecFile, dropDupes=True, check=True)
+
+    assert userParamSpecCols == []  # No analysis params here (auto. generated by PreAnalyser)
+    assert intParamSpecCols == []  # Idem
+    assert verdict
+    assert not reasons
+
+    logger.info2(f'Explicit sample specs:\n{dfExplSampleSpecs.to_string()}')
+    logger.info1(f'From sample specs, {len(dfExplSampleSpecs)} samples to export')
+    if args.verbose and not args.realRun:
+        while not sampExplSpecFilePath.parent.exists():
+            sampExplSpecFilePath = sampExplSpecFilePath.parent.parent / sampExplSpecFilePath.name
+        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
+
+    # b. Export 1 Distance input data file for each specified sample.
+    if args.realRun:
+        preAnlysr.exportDSInputData(implSampleSpecs=sampleSpecFile, format='Distance')
+        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
+    preAnlysr.shutdown()  # Not really needed, actually.
+
+    logger.closeOperation(oprText)
+
+# 9. Run pre-analyses (if specified to).
+PreAnalyser = MCDSPreAnalyser
+resultsWord = 'resultats' if pars.studyLang == 'fr' else 'results'
+preAnlysResFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-preanalyses-{resultsWord}.xlsx'
+if preAnlysResFilePath.exists():
+    logger.info('Found pre-analyses results file ' + preAnlysResFilePath.as_posix())
+
+if args.preAnalyses:
+
+    oprText = 'pre-analyses'
+    logger.openOperation(oprText)
+
+    # a. Create PreAnalyser object.
+    preAnlysr = PreAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                            effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                            transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                            sampleSelCols=pars.sampleSelCols,
+                            sampleDecCols=[pars.effortCol, pars.distanceCol],
+                            sampleIndCol=pars.sampleIndCol, sampleSpecCustCols=pars.sampleSpecCustCols,
+                            abbrevCol=pars.sampleAbbrevCol, abbrevBuilder=pars.sampleAbbrev,
+                            distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                            surveyType=pars.surveyType, distanceType=pars.distanceType,
+                            clustering=pars.clustering,
+                            resultsHeadCols=pars.preResultsHeadCols,
+                            workDir=workDir,
+                            runMethod=pars.runPreAnalysisMethod, runTimeOut=pars.runPreAnalysisTimeOut,
+                            logData=pars.logPreAnalysisData, logProgressEvery=pars.logPreAnalysisProgressEvery)
+
+    # b. Check sample specs.
+    dfExplSampleSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
+        preAnlysr.explicitParamSpecs(implParamSpecs=sampleSpecFile, dropDupes=True, check=True)
+
+    assert userParamSpecCols == []  # No analysis params here (auto. generated by PreAnalyser)
+    assert verdict
+    assert not reasons
+
+    logger.info2(f'Explicit sample specs:\n{dfExplSampleSpecs.to_string()}')
+    logger.info2(f'Pre-analysis model fallback strategy:\n{pd.DataFrame(pars.modelPreStrategy).to_string()}')
+    logger.info1(f'From sample specs, {len(dfExplSampleSpecs)} samples to pre-analyse')
+    if args.verbose and not args.realRun:
+        while not sampExplSpecFilePath.parent.exists():
+            sampExplSpecFilePath = sampExplSpecFilePath.parent.parent / sampExplSpecFilePath.name
+        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
+
+    if any(col not in dfExplSampleSpecs.columns for col in pars.sampleSpecCustCols):
+        logger.error('Missing custom (pass-through) column(s) in sample specs: {}'
+                     .format(', '.join(col for col in pars.sampleSpecCustCols
+                                       if col not in dfExplSampleSpecs.columns)))
+        sys.exit(2)
+
+    # c. Run pre-analyses.
+    if args.realRun:
+        preResults = preAnlysr.run(implSampleSpecs=sampleSpecFile, dModelStrategy=pars.modelPreStrategy,
+                                   threads=args.threads)
+        dfExplSampleSpecs.to_excel(sampExplSpecFilePath, index=False)
+    preAnlysr.shutdown()
+
+    # d. Save results to disk.
+    # if 'dfSampleStats' in dir(): # Qq specs supplémentaires
+    #    preResults.updateSpecs(sampleStats=dfSampleStats)
+    if args.realRun:
+        preResults.toExcel(preAnlysResFilePath)
+
+    logger.closeOperation(oprText)
+
+# 10. Generate pre-analysis reports (if specified to).
+PreReport = MCDSResultsPreReport
+reportWord = 'rapport' if pars.studyLang == 'fr' else 'report'
+if args.preReports:
+
+    # a. Load pre-analyses results if not just computed
+    if not args.preAnalyses:
+
+        if not preAnlysResFilePath.exists():
+            logger.error('Cannot generate pre-analysis reports:'
+                         f' results file not found {preAnlysResFilePath.as_posix()}')
+            sys.exit(2)
+
+        logger.info(f'Loading pre-analysis results from {preAnlysResFilePath.as_posix()}')
+
+        preAnlysr = PreAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                                effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                                transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                                sampleSelCols=pars.sampleSelCols,
+                                sampleDecCols=[pars.effortCol, pars.distanceCol],
+                                sampleIndCol=pars.sampleIndCol, sampleSpecCustCols=pars.sampleSpecCustCols,
+                                abbrevCol=pars.sampleAbbrevCol, abbrevBuilder=pars.sampleAbbrev,
+                                distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                                surveyType=pars.surveyType, distanceType=pars.distanceType,
+                                clustering=pars.clustering,
+                                resultsHeadCols=pars.preResultsHeadCols)
+
+        preResults = preAnlysr.setupResults()
+        preAnlysr.shutdown()  # Not really needed, actually.
+
+        preResults.fromFile(preAnlysResFilePath)
+
+    # b. Check report generation parameters
+    assert isinstance(pars.preReportSortAscend, bool) or len(pars.preReportSortCols) == len(pars.preReportSortAscend)
+
+    # c. Generate specified reports
+    oprText = 'generation of {} pre-analysis report(s)'.format(','.join(args.preReports))
+    logger.openOperation(oprText)
+
+    preRepPrfx = f'{pars.studyName}{pars.subStudyName}-preanalyses-{reportWord}'
+    preReport = PreReport(resultsSet=preResults, lang=pars.studyLang,
+                          title=pars.preReportStudyTitle, subTitle=pars.preReportStudySubTitle,
+                          anlysSubTitle=pars.preReportAnlysSubTitle, description=pars.preReportStudyDescr,
+                          keywords=pars.preReportStudyKeywords, pySources=paramFiles,
+                          sampleCols=pars.preReportSampleCols, paramCols=pars.preReportParamCols,
+                          resultCols=pars.preReportResultCols, synthCols=pars.preReportSynthCols,
+                          sortCols=pars.preReportSortCols, sortAscend=pars.preReportSortAscend,
+                          tgtFolder=workDir, tgtPrefix=preRepPrfx,
+                          **pars.preReportPlotParams)
+
+    if 'excel' in args.preReports:
+        logger.info1('* Excel pre-analysis report to be generated')
+        if args.realRun:
+            preReport.toExcel(rebuild=pars.preReportRebuild)
+
+    if 'html' in args.preReports:
+        logger.info1('* HTML pre-analysis report to be generated')
+        if args.realRun:
+            preReport.toHtml(rebuild=pars.preReportRebuild,
+                             generators=1 if args.threads is None else args.threads)
+
+    logger.closeOperation(oprText)
+
+# 11. Run analyses (if specified to).
+Analyser = MCDSAnalyser
+anlysExplSpecFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-analyses-explispecs.xlsx'
+if anlysExplSpecFilePath.exists():
+    logger.info('Found analyses explicit spec. file ' + anlysExplSpecFilePath.as_posix())
+anlysResFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-analyses-{resultsWord}.xlsx'
+if anlysResFilePath.exists():
+    logger.info('Found analyses results file ' + anlysResFilePath.as_posix())
+
+if args.analyses:
+
+    # Check analysis spec. file
+    analysisSpecFile = pars.analysisSpecFile if 'analysisSpecFile' in vars(pars) else None
+    if analysisSpecFile:
+        analysisSpecFile = pl.Path(analysisSpecFile)
+        if not analysisSpecFile.exists():
+            logger.error(f'Could not find analysis spec. file {analysisSpecFile.as_posix()}')
+            sys.exit(2)
+    else:
+        logger.error('No analysis spec. file specified, can\'t run analyses')
+        sys.exit(2)
+
+    oprText = 'analyses'
+    logger.openOperation(oprText)
+
+    # a. Create Analyser object.
+    anlysr = Analyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                      effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                      transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                      sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
+                      anlysSpecCustCols=pars.analysisSpecCustCols,
+                      abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
+                      anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
+                      distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                      surveyType=pars.surveyType, distanceType=pars.distanceType,
+                      clustering=pars.clustering,
+                      resultsHeadCols=pars.resultsHeadCols,
+                      ldTruncIntrvSpecs=pars.ldTruncIntrvSpecs, truncIntrvEpsilon=pars.truncIntrvEpsilon,
+                      workDir=workDir, runMethod=pars.runAnalysisMethod, runTimeOut=pars.runAnalysisTimeOut,
+                      logData=pars.logAnalysisData, logProgressEvery=pars.logAnalysisProgressEvery,
+                      defEstimKeyFn=pars.defEstimKeyFn, defEstimAdjustFn=pars.defEstimAdjustFn,
+                      defEstimCriterion=pars.defEstimCriterion, defCVInterval=pars.defCVInterval,
+                      defMinDist=pars.defMinDist, defMaxDist=pars.defMaxDist,
+                      defFitDistCuts=pars.defFitDistCuts, defDiscrDistCuts=pars.defDiscrDistCuts)
+
+    # b. Check analysis specs.
+    dfExplAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
+        anlysr.explicitParamSpecs(implParamSpecs=analysisSpecFile, dropDupes=True, check=True)
+
+    assert userParamSpecCols == pars.analysisParamCols
+    assert verdict
+    assert not reasons
+
+    logger.info2(f'Explicit analysis specs:\n{dfExplAnlysSpecs.to_string()}')
+    logger.info1(f'From analysis specs, {len(dfExplAnlysSpecs)} analyses to run')
+    if args.verbose and not args.realRun:
+        while not anlysExplSpecFilePath.parent.exists():
+            anlysExplSpecFilePath = anlysExplSpecFilePath.parent.parent / anlysExplSpecFilePath.name
+        dfExplAnlysSpecs.to_excel(anlysExplSpecFilePath, index=False)
+
+    if any(col not in dfExplAnlysSpecs.columns for col in pars.analysisSpecCustCols):
+        logger.error('Missing custom (pass-through) column(s) in analysis specs: {}'
+                     .format(', '.join(col for col in pars.analysisSpecCustCols
+                                       if col not in dfExplAnlysSpecs.columns)))
+        sys.exit(2)
+
+    # c. Run analyses.
+    if args.realRun:
+        results = anlysr.run(implParamSpecs=analysisSpecFile, threads=args.threads)
+        dfExplAnlysSpecs.to_excel(anlysExplSpecFilePath, index=False)
+    anlysr.shutdown()
+
+    # d. Save results to disk.
+    if args.realRun:
+        results.toExcel(anlysResFilePath)
+
+    logger.closeOperation(oprText)
+
+# 12. Generate analysis reports (if specified to).
+FullReport = MCDSResultsFullReport
+FilSorReport = MCDSResultsFilterSortReport
+
+if args.reports:
+
+    # a. Load analysis results if not just computed
+    if not args.analyses:
+
+        if not anlysResFilePath.exists():
+            logger.error(f'Cannot generate analysis reports: results file not found {anlysResFilePath.as_posix()}')
+            sys.exit(2)
+
+        logger.info(f'Loading analysis results from {anlysResFilePath.as_posix()}')
+
+        anlysr = Analyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                          effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                          transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                          sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
+                          anlysSpecCustCols=pars.analysisSpecCustCols,
+                          abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
+                          anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
+                          distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                          surveyType=pars.surveyType, distanceType=pars.distanceType,
+                          clustering=pars.clustering,
+                          resultsHeadCols=pars.resultsHeadCols)
+
+        results = anlysr.setupResults()
+        anlysr.shutdown()  # Not really needed, actually.
+
+        results.fromFile(anlysResFilePath)
+
+    # b. Generate specified reports
+    oprText = 'generation of {} analysis report(s)'.format(','.join(args.reports))
+    logger.openOperation(oprText)
+
+    repPrfx = f'{pars.studyName}{pars.subStudyName}-analyses-{reportWord}'
+
+    # * Auto-filtered reports.
+    if ('excel' in args.reports and 'full' not in args.reports['excel'] and 'filsorReportSchemes' in dir()) \
+       or ('html' in args.reports and 'full' not in args.reports['html'] and 'filsorAnlysReportSchemes' in dir()):
+
+        assert isinstance(pars.filsorReportSortAscend, bool) \
+               or len(pars.filsorReportSortCols) == len(pars.filsorReportSortAscend)
+
+        filsorReport = FilSorReport(resultsSet=results, lang=pars.studyLang,
+                                    title=pars.anlysFilsorReportStudyTitle,
+                                    subTitle=pars.anlysFilsorReportStudySubTitle,
+                                    anlysSubTitle=pars.anlysFilsorReportAnlysSubTitle,
+                                    description=pars.anlysFilsorReportStudyDescr,
+                                    keywords=pars.anlysFilsorReportStudyKeywords, pySources=paramFiles,
+                                    sampleCols=pars.filsorReportSampleCols, paramCols=pars.filsorReportParamCols,
+                                    resultCols=pars.filsorReportResultCols, synthCols=pars.filsorReportSynthCols,
+                                    sortCols=pars.filsorReportSortCols, sortAscend=pars.filsorReportSortAscend,
+                                    filSorSchemes=pars.filsorReportSchemes,
+                                    tgtFolder=workDir, tgtPrefix=repPrfx,
+                                    **pars.filsorReportPlotParams)
+
+        if 'excel' in args.reports and 'full' not in args.reports['excel'] and 'filsorReportSchemes' in dir():
+            logger.info1('* Auto-filtered Excel analysis report to be generated: all schemes')
+            if args.realRun:
+                filsorReport.toExcel(rebuild=pars.filsorReportRebuild)
+
+        if 'html' in args.reports and 'full' not in args.reports['html'] and 'filsorAnlysReportSchemes' in dir():
+            logger.info1('* Auto-filtered HTML analysis report(s) to be generated: {}'
+                         .format(', '.join(filsorAnlysReportSchemes.keys())))
+            if args.realRun:
+                for scheme in filsorAnlysReportSchemes.values():
+                    filsorReport.toHtml(filSorScheme=scheme, rebuild=pars.filsorReportRebuild)
+
+    # * Full reports.
+    if ('excel' in args.reports and ('full' in args.reports['excel'] or 'filsorReportSchemes' not in dir())) \
+       or ('html' in args.reports and 'full' in args.reports['html']):
+
+        assert isinstance(pars.fullReportSortAscend, bool) \
+               or len(pars.fullReportSortCols) == len(pars.fullReportSortAscend)
+
+        fullReport = FullReport(resultsSet=results, lang=pars.studyLang,
+                                title=pars.anlysFullReportStudyTitle,
+                                subTitle=pars.anlysFullReportStudySubTitle,
+                                anlysSubTitle=pars.anlysFullReportAnlysSubTitle,
+                                description=pars.anlysFullReportStudyDescr,
+                                keywords=pars.anlysFullReportStudyKeywords, pySources=paramFiles,
+                                sampleCols=pars.fullReportSampleCols, paramCols=pars.fullReportParamCols,
+                                resultCols=pars.fullReportResultCols, synthCols=pars.fullReportSynthCols,
+                                sortCols=pars.fullReportSortCols, sortAscend=pars.fullReportSortAscend,
+                                tgtFolder=workDir, tgtPrefix=repPrfx,
+                                **pars.fullReportPlotParams)
+
+        if 'excel' in args.reports and ('full' in args.reports['excel'] or 'filsorReportSchemes' not in dir()):
+            logger.info1('* Full Excel analysis report to be generated')
+            if args.realRun:
+                fullReport.toExcel(rebuild=pars.fullReportRebuild)
+
+        if 'html' in args.reports and 'full' in args.reports['html']:
+            logger.info1('* Full HTML analysis report to be generated')
+            if args.realRun:
+                fullReport.toHtml(rebuild=pars.fullReportRebuild,
+                                  generators=1 if args.threads is None else args.threads)
+
+    # if not any(rep in dir() for rep in ['fullReport', 'filsorReport']):  # This doesn't work ... !!??
+    if not any(['fullReport' in dir(), 'filsorReport' in dir()]):
+        logger.error('Neither full nor filter & sort analysis report to be generated (whatever format)')
+        sys.exit(2)
+
+    logger.closeOperation(oprText)
+
+# 13. Run opt-analyses (if specified to).
+OptAnalyser = MCDSTruncationOptanalyser
+optAnlysExplSpecFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-optanalyses-explispecs.xlsx'
+if optAnlysExplSpecFilePath.exists():
+    logger.info('Found opt-analyses explicit spec. file ' + optAnlysExplSpecFilePath.as_posix())
+optAnlysResFilePath = workDir / f'{pars.studyName}{pars.subStudyName}-optanalyses-{resultsWord}.xlsx'
+if optAnlysResFilePath.exists():
+    logger.info('Found opt-analyses results file ' + optAnlysResFilePath.as_posix())
+if args.optAnalyses:
+
+    # Check analysis spec. file
+    optAnalysisSpecFile = pars.optAnalysisSpecFile if 'optAnalysisSpecFile' in vars(pars) else None
+    if optAnalysisSpecFile:
+        optAnalysisSpecFile = pl.Path(optAnalysisSpecFile)
+        if not optAnalysisSpecFile.exists():
+            logger.error(f'Could not find opt-analysis spec. file {optAnalysisSpecFile.as_posix()}')
+            sys.exit(2)
+    else:
+        logger.error('No opt-analysis spec. file specified, can\'t run opt-analyses')
+        sys.exit(2)
+
+    oprText = 'opt-analyses'
+    logger.openOperation(oprText)
+
+    # a. Create OptAnalyser object.
+    optAnlysr = OptAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                            effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                            transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                            sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
+                            sampleDistCol=pars.distanceCol, anlysSpecCustCols=pars.optAnalysisSpecCustCols,
+                            abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
+                            anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
+                            distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                            surveyType=pars.surveyType, distanceType=pars.distanceType,
+                            clustering=pars.clustering,
+                            resultsHeadCols=pars.optResultsHeadCols,
+                            ldTruncIntrvSpecs=pars.ldTruncIntrvSpecs, truncIntrvEpsilon=pars.truncIntrvEpsilon,
+                            workDir=workDir, logData=pars.logOptAnalysisData,
+                            runMethod=pars.runOptAnalysisMethod, runTimeOut=pars.runOptAnalysisTimeOut,
+                            logAnlysProgressEvery=pars.logOptAnalysisProgressEvery,
+                            logOptimProgressEvery=pars.logOptimisationProgressEvery,
+                            backupOptimEvery=pars.backupOptimisationsEvery,
+                            defEstimKeyFn=pars.defEstimKeyFn, defEstimAdjustFn=pars.defEstimAdjustFn,
+                            defEstimCriterion=pars.defEstimCriterion, defCVInterval=pars.defCVInterval,
+                            defExpr2Optimise=pars.defExpr2Optimise, defMinimiseExpr=pars.defMinimiseExpr,
+                            defOutliersMethod=pars.defOutliersMethod,
+                            defOutliersQuantCutPct=pars.defOutliersQuantCutPct,
+                            defFitDistCutsFctr=pars.defFitDistCutsFctr,
+                            defDiscrDistCutsFctr=pars.defDiscrDistCutsFctr,
+                            defSubmitTimes=pars.defSubmitTimes, defSubmitOnlyBest=pars.defSubmitOnlyBest,
+                            dDefSubmitOtherParams=pars.dDefSubmitOtherParams,
+                            dDefOptimCoreParams=dict(core=pars.defCoreEngine, maxIters=pars.defCoreMaxIters,
+                                                     termExprValue=pars.defCoreTermExprValue,
+                                                     algorithm=pars.defCoreAlgorithm,
+                                                     maxRetries=pars.defCoreMaxRetries))
+
+    # b. Check opt-analysis specs.
+    dfExplOptAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \
+        optAnlysr.explicitParamSpecs(implParamSpecs=optAnalysisSpecFile, dropDupes=True, check=True)
+
+    assert userParamSpecCols == pars.optAnalysisParamCols
+    assert verdict
+    assert not reasons
+
+    logger.info2(f'Explicit opt-analysis specs:\n{dfExplOptAnlysSpecs.to_string()}')
+    optUserParamSpecCols = optAnlysr.zoptr4Specs.optimisationParamSpecUserNames(userParamSpecCols, intParamSpecCols)
+    sbAnlysNeedOpt = dfExplOptAnlysSpecs[optUserParamSpecCols].apply(optAnlysr.analysisNeedsOptimisationFirst,
+                                                                     axis='columns')
+    logger.info1('From opt-analysis specs, {} / {} opt-analyses to run with truncation optimisation first ...'
+                 .format(sbAnlysNeedOpt.sum(), len(dfExplOptAnlysSpecs)))
+    logger.info1('... implying possibly up to {} auto-analyses in the background if only full "auto" specs'
+                 .format(sbAnlysNeedOpt.sum() * pars.defCoreMaxIters * pars.defSubmitTimes))
+    if args.verbose and not args.realRun:
+        while not optAnlysExplSpecFilePath.parent.exists():
+            optAnlysExplSpecFilePath = optAnlysExplSpecFilePath.parent.parent / optAnlysExplSpecFilePath.name
+        dfExplOptAnlysSpecs.to_excel(optAnlysExplSpecFilePath, index=False)
+
+    if any(col not in dfExplOptAnlysSpecs.columns for col in pars.optAnalysisSpecCustCols):
+        logger.error('Missing custom (pass-through) column(s) in opt-analysis specs: {}'
+                     .format(', '.join(col for col in pars.optAnalysisSpecCustCols
+                                       if col not in dfExplOptAnlysSpecs.columns)))
+        sys.exit(2)
+
+    # c. Check if recovery possible (not 100% reliable), if specified
+    if args.recoverOpts:
+        if not list(workDir.glob('optr-resbak-*.pickle.xz')):
+            logger.error('No optimisation backup file found, can\'t recover ; you must start from scratch')
+            sys.exit(2)
+        else:
+            logger.info('Backup files are there, recovery is very likely possible: let\'s try !')
+
+    # d. Run opt-analyses.
+    if args.realRun:
+        optResults = optAnlysr.run(implParamSpecs=optAnalysisSpecFile,
+                                   threads=args.threads, recoverOptims=args.recoverOpts)
+        dfExplOptAnlysSpecs.to_excel(optAnlysExplSpecFilePath, index=False)
+    optAnlysr.shutdown()
+
+    # e. Save results to disk.
+    if args.realRun:
+        optResults.toExcel(optAnlysResFilePath)
+
+    logger.closeOperation(oprText)
+
+# 14. Generate opt-analysis reports (if specified to).
+if args.optReports:
+
+    # a. Load opt-analysis results if not just computed
+    if not args.optAnalyses:
+
+        if not optAnlysResFilePath.exists():
+            logger.error('Cannot generate opt-analysis reports:'
+                         f' results file not found {optAnlysResFilePath.as_posix()}')
+            sys.exit(2)
+
+        logger.info(f'Loading opt-analysis results from {optAnlysResFilePath.as_posix()}')
+
+        optAnlysr = OptAnalyser(dfMonoCatObs, dfTransects=dfTransects, dSurveyArea=pars.studyAreaSpecs,
+                                effortConstVal=pars.passEffort, effortCol=pars.effortCol,
+                                transectPlaceCols=pars.transectPlaceCols, passIdCol=pars.passIdCol,
+                                sampleSelCols=pars.sampleSelCols, sampleDecCols=[pars.effortCol, pars.distanceCol],
+                                sampleDistCol=pars.distanceCol, anlysSpecCustCols=pars.optAnalysisSpecCustCols,
+                                abbrevCol=pars.analysisAbbrevCol, abbrevBuilder=pars.analysisAbbrev,
+                                anlysIndCol=pars.analysisIndCol, sampleIndCol=pars.sampleIndCol,
+                                distanceUnit=pars.distanceUnit, areaUnit=pars.areaUnit,
+                                surveyType=pars.surveyType, distanceType=pars.distanceType,
+                                clustering=pars.clustering,
+                                resultsHeadCols=pars.optResultsHeadCols)
+
+        optResults = optAnlysr.setupResults()
+        optAnlysr.shutdown()  # Not really needed, actually.
+
+        optResults.fromFile(optAnlysResFilePath)
+
+    # b. Generate specified reports
+    assert isinstance(pars.filsorReportSortAscend, bool) \
+           or len(pars.filsorReportSortCols) == len(pars.filsorReportSortAscend)
+
+    oprText = 'generation of {} opt-analysis report(s)'.format(','.join(args.optReports))
+    logger.openOperation(oprText)
+
+    optRepPrfx = f'{pars.studyName}{pars.subStudyName}-optanalyses-{reportWord}'
+
+    # * Auto-filtered reports.
+    if ('excel' in args.optReports and 'full' not in args.optReports['excel'] and 'filsorReportSchemes' in dir()) \
+       or ('html' in args.optReports and 'filsorOptAnlysReportSchemes' in dir()):
+
+        assert isinstance(pars.filsorReportSortAscend, bool) \
+               or len(pars.filsorReportSortCols) == len(pars.filsorReportSortAscend)
+
+        filsorOptReport = FilSorReport(resultsSet=optResults, lang=pars.studyLang,
+                                       title=pars.optAnlysFilsorReportStudyTitle,
+                                       subTitle=pars.optAnlysFilsorReportStudySubTitle,
+                                       anlysSubTitle=pars.optAnlysFilsorReportAnlysSubTitle,
+                                       description=pars.optAnlysFilsorReportStudyDescr,
+                                       keywords=pars.optAnlysFilsorReportStudyKeywords, pySources=paramFiles,
+                                       sampleCols=pars.filsorReportSampleCols, paramCols=pars.filsorReportParamCols,
+                                       resultCols=pars.filsorReportResultCols, synthCols=pars.filsorReportSynthCols,
+                                       sortCols=pars.filsorReportSortCols, sortAscend=pars.filsorReportSortAscend,
+                                       filSorSchemes=pars.filsorReportSchemes,
+                                       tgtFolder=workDir, tgtPrefix=optRepPrfx,
+                                       **pars.filsorReportPlotParams)
+
+        if 'excel' in args.optReports and 'full' not in args.optReports['excel'] and 'filsorReportSchemes' in dir():
+            logger.info1('* Auto-filtered Excel opt-analysis report to be generated: all schemes')
+            if args.realRun:
+                filsorOptReport.toExcel(rebuild=pars.filsorReportRebuild)
+
+        if 'html' in args.optReports and 'filsorOptAnlysReportSchemes' in dir():
+            logger.info1('* Auto-filtered HTML opt-analysis report(s) to be generated: {}'
+                         .format(', '.join(filsorOptAnlysReportSchemes.keys())))
+            if args.realRun:
+                for scheme in filsorOptAnlysReportSchemes.values():
+                    filsorOptReport.toHtml(filSorScheme=scheme, rebuild=pars.filsorReportRebuild)
+
+    # * Full reports.
+    if ('excel' in args.optReports and ('full' in args.optReports['excel'] or 'filsorReportSchemes' not in dir())) \
+       or ('html' in args.optReports and 'full' in args.optReports['html']):
+
+        assert isinstance(pars.fullReportSortAscend, bool) \
+               or len(pars.fullReportSortCols) == len(pars.fullReportSortAscend)
+
+        fullOptReport = FullReport(resultsSet=optResults, lang=pars.studyLang,
+                                   title=pars.optAnlysFullReportStudyTitle,
+                                   subTitle=pars.optAnlysFullReportStudySubTitle,
+                                   anlysSubTitle=pars.optAnlysFullReportAnlysSubTitle,
+                                   description=pars.optAnlysFullReportStudyDescr,
+                                   keywords=pars.optAnlysFullReportStudyKeywords, pySources=paramFiles,
+                                   sampleCols=pars.fullReportSampleCols, paramCols=pars.fullReportParamCols,
+                                   resultCols=pars.fullReportResultCols, synthCols=pars.fullReportSynthCols,
+                                   sortCols=pars.fullReportSortCols, sortAscend=pars.fullReportSortAscend,
+                                   tgtFolder=workDir, tgtPrefix=optRepPrfx,
+                                   **pars.fullReportPlotParams)
+
+        if 'excel' in args.optReports \
+           and ('full' in args.optReports['excel'] or 'filsorReportSchemes' not in dir()):
+            logger.info1('* Full Excel opt-analysis report to be generated')
+            if args.realRun:
+                fullOptReport.toExcel(rebuild=pars.fullReportRebuild)
+
+        if 'html' in args.optReports and 'full' in args.optReports['html']:
+            logger.info1('* Full HTML opt-analysis report to be generated')
+            if args.realRun:
+                fullOptReport.toHtml(rebuild=pars.fullReportRebuild,
+                                     generators=1 if args.threads is None else args.threads)
+
+    # if not any(rep in dir() for rep in ['fullOptReport', 'filsorOptReport']):  # This doesn't work ... !!??
+    if not any(['fullOptReport' in dir(), 'filsorOptReport' in dir()]):
+        logger.error('Neither full nor filter & sort opt-analysis report to be generated (whatever format)')
+        sys.exit(2)
+
+    logger.closeOperation(oprText)
+
+if not args.realRun and not emptyRun:
+    logger.info('Checks done, seems you can now really run this, through -u / --realrun :-)')
+
+# 15. Done.
+sys.exit(0)
```

### Comparing `pyaudisam-0.9.3/pyaudisam/analyser.py` & `pyaudisam-1.0.1/pyaudisam/analyser.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,2820 +1,2831 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "analyser": Run a bunch of DS analyses according to a user-friendly set of analysis specs
-
-import copy
-import re
-import pathlib as pl
-from packaging import version
-
-import numpy as np
-import pandas as pd
-
-from . import log, runtime
-from .data import MonoCategoryDataSet, ResultsSet
-from .executor import Executor
-from .engine import MCDSEngine
-from .analysis import DSAnalysis, MCDSAnalysis, MCDSPreAnalysis
-
-logger = log.logger('ads.anr')
-
-
-class AnalysisResultsSet(ResultsSet):
-    
-    """
-    A result set for multiple analyses from the same engine.
-    
-    Internal columns index is a 3-level multi-index.
-    """
-    
-    def __init__(self, analysisClass, miCustomCols=None, dfCustomColTrans=None,
-                 dComputedCols=None, dfComputedColTrans=None, sortCols=[], sortAscend=[]):
-        
-        assert issubclass(analysisClass, DSAnalysis), 'analysisClass must derive from DSAnalysis'
-        assert miCustomCols is None \
-               or (isinstance(miCustomCols, pd.MultiIndex) and len(miCustomCols.levels) == 3), \
-               'customCols must be None or a 3 level pd.MultiIndex'
-        
-        self.analysisClass = analysisClass
-        self.engineClass = analysisClass.EngineClass
-
-        # 3-level multi-index columns (module, statistic, figure) for analyses output
-        miCols = \
-            self.engineClass.statSampCols().append(analysisClass.MIRunColumns).append(self.engineClass.statModCols())
-        
-        # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
-        dfColTrans = pd.concat([self.engineClass.statSampColTrans(), analysisClass.DfRunColumnTrans,
-                                self.engineClass.statModColTrans()])
-        
-        super().__init__(miCols=miCols, dfColTrans=dfColTrans,
-                         miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
-                         dComputedCols=dComputedCols, dfComputedColTrans=dfComputedColTrans,
-                         sortCols=sortCols, sortAscend=sortAscend)
-    
-    def copy(self, withData=True):
-    
-        """Clone function (shallow), with optional (deep) data copy"""
-    
-        # 1. Call ctor without computed columns stuff (we no more have initial data)
-        clone = AnalysisResultsSet(analysisClass=self.analysisClass,
-                                   miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
-                                   sortCols=[], sortAscend=[])
-    
-        # 2. Complete clone initialisation.
-        # 3-level multi-index columns (module, statistic, figure)
-        clone.miCols = self.miCols
-        clone.computedCols = self.computedCols
-        
-        # DataFrames for translating 3-level multi-index columns to 1 level lang-translated columns
-        clone.dfColTrans = self.dfColTrans
-        
-        # Copy data if needed.
-        if withData:
-            clone._dfData = self._dfData.copy()
-            clone.rightColOrder = self.rightColOrder
-            clone.postComputed = self.postComputed
-
-        return clone
-                
-
-class Analyser(object):
-
-    """Tools for building analysis variants specifications and explicitating them.
-    
-    Abstract base class for DS analysers.
-    """
-
-    def __init__(self):
-
-        # Computation specifications, for traceability only.
-        # For gathering copies of computations default parameter values, and stuff like that.
-        self.specs = dict()
-
-    def updateSpecs(self, reset=False, overwrite=False, **specs):
-
-        if reset:
-            self.specs.clear()
-
-        if not overwrite:
-            assert all(name not in self.specs for name in specs), \
-                   "Won't overwrite already present specs {}" \
-                   .format(', '.join(name for name in specs if name in self.specs))
-
-        self.specs.update(specs)
-
-    def flatSpecs(self):
-
-        # Flatten "in-line" 2nd level dicts if any (with 1st level name prefixing).
-        dFlatSpecs = dict()
-        for name, value in self.specs.items():
-            if isinstance(value, dict):
-                for n, v in value.items():
-                    dFlatSpecs[name + n[0].upper() + n[1:]] = v
-            else:
-                dFlatSpecs[name] = value
-
-        # Done.
-        return pd.Series(dFlatSpecs, name='Value')
-
-    # Generation of a table of implicit "partial variant" specification,
-    # from a list of possible data selection criteria for each variable.
-    # "Partial variant" because its only about a sub-set of variants
-    # * dVariants : { target columns name: list of possibles criteria for data selection }
-    @staticmethod
-    def implicitPartialVariantSpecs(dVariants):
-        
-        def fixedLengthList(toComplete, length):
-            return toComplete + [np.nan]*(length - len(toComplete))
-
-        nRows = max(len(crits) for crits in dVariants.values())
-
-        return pd.DataFrame({colName: fixedLengthList(variants, nRows) for colName, variants in dVariants.items()})
-
-    @staticmethod
-    def _dropCommentColumns(dfSpecs):
-    
-        """Drop (in-place) comment columns from a spec dataframe (implicit or explicit)
-        """
-        
-        cols2drop = [col for col in dfSpecs.columns if not col.strip() or col.startswith('Unnamed:')
-                                                       or any(col.strip().lower().startswith(start)
-                                                              for start in ['comm', 'rem', '#'])]
-
-        dfSpecs.drop(columns=cols2drop, inplace=True)        
-
-    @staticmethod
-    def explicitPartialVariantSpecs(implSpecs, convertCols=dict()):
-
-        """Generation of a table of explicit "partial variant" specifications, from an implicit one
-        (= generate all combinations of variants)
-        
-        Parameters:
-        :param:implSpecs: implicit partial specs object, as a DataFrame, taken as is,
-           or a dict, preprocessed through implicitPartialVariantSpecs
-        :param:convertCols Name and conversion method for explicit columns to convert 
-          (each column is converted through :
-           dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
-           for colName, convertCol in convertCols.items()) 
-        """
-        
-        # Convert spec from dict to DataFrame if needed.
-        if isinstance(implSpecs, dict):
-            dfImplSpecs = Analyser.implicitPartialVariantSpecs(implSpecs)
-        else:
-            assert isinstance(implSpecs, pd.DataFrame)
-            dfImplSpecs = implSpecs.copy()
-        
-        # Drop any comment / no header ... = useless column
-        Analyser._dropCommentColumns(dfImplSpecs)
-        
-        # First columns kept (nearly) as is (actually an explicit one !) :
-        # keep 1 heading NaN if any, drop trailing ones, and drop duplicates if any.
-        def cleanup(s):
-            return s if s.empty else s.iloc[0:1].append(s.iloc[1:].dropna()).drop_duplicates()
-        dfExplSpecs = cleanup(dfImplSpecs[dfImplSpecs.columns[0]]).to_frame()
-
-        # For each implicit specs column (but the first)
-        for col in dfImplSpecs.columns[1:]:
-
-            # Get variants :
-            # keep 1 heading NaN if any, drop trailing ones, and drop duplicates if any.
-            sVariants = cleanup(dfImplSpecs[col])
-            
-            # Duplicate current explicit table as much as variants are many
-            dfExplSpecs = dfExplSpecs.loc[np.repeat(dfExplSpecs.index.to_numpy(), len(sVariants))]
-            
-            # Add the new columns by tiling the variants along the whole index range
-            dfExplSpecs[col] = np.tile(sVariants.to_numpy(), len(dfExplSpecs) // len(sVariants))
-            
-            # Reset index for easy next duplication
-            dfExplSpecs.reset_index(inplace=True, drop=True)
-
-        # Convert specified columns if any
-        assert all(colName in dfExplSpecs.columns for colName in convertCols), \
-               'Could not find some column to convert of {} in explicit table columns {}' \
-               .format(list(convertCols.keys()), list(dfExplSpecs.columns))
-        for colName, convertCol in convertCols.items():
-            dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
-        
-        # Done.
-        return dfExplSpecs
-
-    SupportedFileExts = \
-        ['.xlsx'] + (['.ods'] if version.parse(pd.__version__).release >= (0, 25) else [])
-    
-    @classmethod
-    def _loadPartSpecsFromFile(cls, sourceFpn):
-
-        """Load partial specs for a workbook file (Excel or ODF format)
-
-        :returns: dict(partial specs dataframes)"""
-
-        if isinstance(sourceFpn, str):
-            sourceFpn = pl.Path(sourceFpn)
-    
-        assert sourceFpn.exists(), 'Source file for partial analysis specs not found : {}'.format(sourceFpn)
-
-        ext = sourceFpn.suffix.lower()
-        assert ext in cls.SupportedFileExts, \
-               'Unsupported source file type {}: not from {{{}}}' \
-               .format(ext, ','.join(cls.SupportedFileExts))
-        if ext in ['.xlsx']:
-            ddfData = pd.read_excel(sourceFpn, sheet_name=None)
-        elif ext in ['.ods']:
-            ddfData = pd.read_excel(sourceFpn, sheet_name=None, engine='odf' if ext in ['.ods'] else 'openpyxml')
-        else:
-            raise NotImplementedError(f'Unsupported file extension {ext} for input partial specs file')
-
-        logger.debug('Loaded partial specs: {}\n{}'
-                     .format(', '.join(ddfData.keys()), '\n'.join(str(df) for df in ddfData.values())))
-
-        return ddfData
-    
-    @classmethod
-    def explicitVariantSpecs(cls, partSpecs, keep=None, ignore=None,
-                             varIndCol=None, convertCols=dict(), computedCols=dict()):
-        
-        """Generation of a table of explicit variant specifications,
-        from a set of implicit and explicit partial variant specs objects
-           
-        Parameters
-        :param partSpecs: The (ordered) dict of name => partial specs objects,
-           each as a DataFrame, taken as is, or a dict, preprocessed through implicitPartialVariantSpecs.
-           Or: pathname of an Excel (.xlsx) or Open Doc. (.ods) worksheet file (1 sheet per specs table)
-           Warning: implicit tables are only found by their name containing "_impl"
-        :param keep: List of names of specs to consider from partSpecs (default None => consider all)
-        :param ignore: List of names of specs to ignore from partSpecs (default None => ignore none) ;
-                       Warning: names in keep and ignore are ... ignored.
-        :param varIndCol: Name of the autogenerated variant index column (defaut: None = no such column added)
-        :param convertCols: Name and conversion method for explicit columns to convert 
-          (each column is converted through :
-           dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
-           for colName, convertCol in convertCols.items()) 
-        :param computedCols: Name and computing method for explicit columns to add (after appending :param varIndCol)
-          (each column to add is computed through :
-           dfExplSpecs[colName] = dfExplSpecs.apply(computeCol, axis='columns')
-           for colName, computeCol in computedCols.items()) 
-
-        TODO: Translate to english
-        
-        partSpecs est donc un dictionnaire ordonné de tables de specs partielles.
-        * chaque table donne un sous-ensemble (ou la totalité) des colonnes de variantes d'analyses,
-        * chaque table peut être implicite ou explicite :
-            * explicite : toutes les combinaisons désirées sont données pour les colonnes de la table,
-            * implicite : dans chaque colonne, verticalement, on donne la liste des variantes possibles
-              (chaque colonne n'aura donc pas la même longueur) ; l'explicitation consistera à générer
-              automatiquement la totalité des combinaisons possibles des valeurs données dans les colonnes,
-            * le type des tables est déterminé par leur nom : implicite s'il contient "_impl",
-              explicite sinon.
-        * plusieurs tables peuvent avoir les mêmes colonnes :
-            * chacune peut être soit implicite, soit explicite, peu importe,
-            * une fois explicitées, elles doivent cependant fournir des lignes ne se recouvrant pas,
-            * cela permet de spécifier facilement des combinaisons différentes pour des sous-ensembles
-              distincts de valeurs d'un sous-ensemble donné de colonnes,
-            * avant de les combiner avec les autres tables, on les concatène verticalement en 1 seule
-              après explicitation individuelle,
-        * les tables qui n'ont aucune colonne en commun avec les autres produiront
-          l'ensemble des combinaisons possibles avec celles obtenues par explicitation des colonnes la précédant,
-        * celles qui ont certaines (mais pas toutes) colonnes en communs avec celles qui la précèdent
-          permettent de produire des variantes spécifiques pour ces colonnes : elles feront l'objet
-          d'une jointure à gauche avec ces tables précédentes,
-        * car l'algorithme d'explicitation - fusion des tables suit leur ordre dans le classeur,
-          une fois faite l'explicitation - concaténation verticale des tables de même jeux de colonnes.
-          
-        N.B. Pas prévu, mais ça marche : pour imposer des valeurs de paramètres vides,
-             il suffit de fournit une table vide, avec les entêtes correspondants (exemple avec ACDC).
-        """
-        
-        # Load partial variant specs from source (trivial if given as a dict).
-        assert isinstance(partSpecs, (dict, str, pl.Path)), 'Can explicit only worksheet files or alike dicts'
-        dPartSpecs = partSpecs if isinstance(partSpecs, dict) else cls._loadPartSpecsFromFile(partSpecs)
-        assert len(dPartSpecs) > 0, "Can't explicit variants with no partial variant"
-        
-        # Setup filters
-        keep = keep or list(dPartSpecs.keys())
-        ignore = ignore or []
-        
-        # Filter specs as requested and convert any implicit partial variant spec
-        # from dict to DataFrame if needed.
-        ddfPartSpecs = dict()
-        for psName, psValues in dPartSpecs.items():
-            if psName in keep and psName not in ignore:
-                if isinstance(psValues, dict):
-                    ddfPartSpecs[psName] = cls.implicitPartialVariantSpecs(psValues)
-                else:
-                    assert isinstance(psValues, pd.DataFrame)
-                    ddfPartSpecs[psName] = psValues.copy()
-                    # Drop any comment / no header ... = useless column
-                    cls._dropCommentColumns(ddfPartSpecs[psName])
-                
-        # Group partial specs tables with same column sets (according to column names)
-        dSameColsPsNames = dict()  # { cols: [table names] }
-        
-        for psName, dfPsValues in ddfPartSpecs.items():
-            
-            colSetId = ':'.join(sorted(dfPsValues.columns))
-            if colSetId not in dSameColsPsNames:
-                dSameColsPsNames[colSetId] = list()
-                
-            dSameColsPsNames[colSetId].append(psName)
-
-        # For each group, concat. tables into one, after expliciting if needed
-        ldfExplPartSpecs = list()
-
-        for lPsNames in dSameColsPsNames.values():
-
-            ldfSameColsPartSpecs = list()
-            for psName in lPsNames:
-
-                dfPartSpecs = ddfPartSpecs[psName]
-
-                # Implicit specs case:
-                if '_impl' in psName:
-
-                    logger.debug1(f'Explicitating {psName}')
-                    dfPartSpecs = cls.explicitPartialVariantSpecs(dfPartSpecs)
-
-                # Now, specs are explicit.
-                ldfSameColsPartSpecs.append(dfPartSpecs)
-
-            # Concat groups of same columns set explicit specs
-            ldfExplPartSpecs.append(pd.concat(ldfSameColsPartSpecs, ignore_index=True))
-        
-        # Combine explicit specs (following in order)
-        dfExplSpecs = ldfExplPartSpecs[0]
-
-        for dfExplPartSpecs in ldfExplPartSpecs[1:]:
-
-            commCols = [col for col in dfExplSpecs.columns if col in dfExplPartSpecs.columns]
-
-            if commCols:  # Any column in common : left join each left row to each matching right row
-
-                dfExplSpecs = dfExplSpecs.join(dfExplPartSpecs.set_index(commCols), on=commCols)
-
-            else:  # No columns in common : combine each left row with all right rows
-
-                nInitSpecs = len(dfExplSpecs)
-
-                dfExplSpecs = dfExplSpecs.loc[np.repeat(dfExplSpecs.index, len(dfExplPartSpecs))]
-                dfExplSpecs.reset_index(drop=True, inplace=True)
-
-                dfExplPartSpecs = pd.DataFrame(data=np.tile(dfExplPartSpecs, [nInitSpecs, 1]),
-                                               columns=dfExplPartSpecs.columns)
-
-                dfExplSpecs = pd.concat([dfExplSpecs, dfExplPartSpecs], axis='columns')
-
-            dfExplSpecs.reset_index(drop=True, inplace=True)
-        
-        # Convert specified columns if any
-        assert all(colName in dfExplSpecs.columns for colName in convertCols), \
-               'Could not find some column to convert of {} in explicit table columns {}' \
-               .format(list(convertCols.keys()), list(dfExplSpecs.columns))
-        for colName, convertCol in convertCols.items():
-            dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
-        
-        # Generate explicit variant index column if specified
-        if varIndCol:
-            dfExplSpecs.reset_index(drop=False, inplace=True)
-            dfExplSpecs.rename(columns=dict(index=varIndCol), inplace=True)
-
-        logger.debug(f'Explicit specs (before computing columns):\n{dfExplSpecs}')
-
-        # Compute and add supplementary columns if any
-        for colName, computeCol in computedCols.items():
-            dfExplSpecs[colName] = dfExplSpecs.apply(computeCol, axis='columns')
-                
-        logger.debug(f'Explicit specs (after computing columns):\n{dfExplSpecs}')
-
-        # Done.
-        return dfExplSpecs
-
-
-class DSAnalyser(Analyser):
-
-    """Run a bunch of DS analyses on samples extracted from an individualised sightings data set,
-    according to a user-friendly set of analysis specs,
-    + Tools for building analysis variants specifications and explicitating them.
-    
-    Abstract class.
-    """
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
-                 sampleDecCols=['Effort', 'Distance'], anlysSpecCustCols=[],
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'],
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 abbrevCol='AnlysAbbrev', abbrevBuilder=None,
-                 anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
-                 workDir='.'):
-                       
-        """Ctor
-        
-        Parameters:
-        :param pd.DataFrame dfMonoCatObs: mono-category sighting from FieldDataSet.monoCategorise() or individualise()
-        :param pd.DataFrame dfTransects: Transects infos with columns : transectPlaceCols (n), passIdCol (1),
-            effortCol (1) ; if None, auto generated from input sightings
-        :param effortConstVal: if dfTransects is None and effortCol not in source table, use this constant value
-        :param dSurveyArea: 
-        :param transectPlaceCols: 
-        :param passIdCol: 
-        :param effortCol: 
-        :param sampleSelCols: sample identification = selection columns
-        :param sampleDecCols: Decimal columns among sighting columns
-        :param anlysSpecCustCols: Special columns from analysis specs to simply pass through in results
-        :param distanceUnit: see MCDSEngine
-        :param areaUnit: see MCDSEngine
-        :param resultsHeadCols: dict of list of column names (from dfMonoCatObs) to use in order
-            to build results (right) header columns ; 'sample' columns are sample selection columns ;
-            sampleIndCol is added to resultsHeadCols['before'] if not elswehere in resultsHeadCols ;
-            same for anlysIndCol, right before sampleIndCol
-        :param abbrevCol: Name of column to generate for abbreviating analyses params, not sure really useful ...
-        :param abbrevBuilder: Function of explicit analysis params (as a Series) to generate abbreviated name
-        :param anlysIndCol: Name of column to generate for identifying analyses, unless already there in input data.
-        :param sampleIndCol: Name of column to generate for identifying samples, unless already there in input data.
-        :param workDir: Folder where to generate analysis and results files
-        """
-
-        assert all(col in resultsHeadCols for col in ['before', 'sample', 'after'])
-        assert sampleIndCol
-
-        super().__init__()
-
-        self.dfMonoCatObs = dfMonoCatObs
-
-        self.resultsHeadCols = resultsHeadCols.copy()
-        self.abbrevCol = abbrevCol
-        self.abbrevBuilder = abbrevBuilder
-        self.anlysIndCol = anlysIndCol
-        self.sampleSelCols = sampleSelCols
-        self.sampleIndCol = sampleIndCol
-        self.anlysSpecCustCols = anlysSpecCustCols
-        
-        # sampleIndCol is added to resultsHeadCols['before'] if not already somewhere in resultsHeadCols
-        self.sampIndResHChap = None
-        for chap, cols in self.resultsHeadCols.items():
-            for col in cols:
-                if col == sampleIndCol:
-                    self.sampIndResHChap = chap
-                    break
-        if not self.sampIndResHChap:
-            self.sampIndResHChap = 'before'
-            self.resultsHeadCols[self.sampIndResHChap] = [sampleIndCol] + self.resultsHeadCols[self.sampIndResHChap]
-
-        # anlysIndCol is added to resultsHeadCols['before'], right at the beginning, if not None
-        # and not already somewhere in resultsHeadCols
-        if anlysIndCol and not any(anlysIndCol in cols for cols in self.resultsHeadCols.values()):
-            self.resultsHeadCols['before'] = [anlysIndCol] + self.resultsHeadCols['before']
-        
-        self.workDir = workDir
-
-        self.distanceUnit = distanceUnit
-        self.areaUnit = areaUnit
-        
-        # Individualised data (all samples)
-        self._mcDataSet = \
-            MonoCategoryDataSet(dfMonoCatObs, dfTransects=dfTransects, effortConstVal=effortConstVal,
-                                dSurveyArea=dSurveyArea, transectPlaceCols=transectPlaceCols,
-                                passIdCol=passIdCol, effortCol=effortCol, sampleDecFields=sampleDecCols)
-                                
-        # Analysis engine and executor.
-        self._executor = None
-        self._engine = None
-        
-        # Results.
-        self.results = None
-
-        # Specs.
-        self.updateSpecs(**dSurveyArea)
-        self.updateSpecs(**{name: getattr(self, name) for name in ['distanceUnit', 'areaUnit']})
-
-    # Possible regexps (values) for auto-detection of analyser _internal_ parameter spec names (keys)
-    # from explicit _user_ spec columns
-    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
-    #  and case is ignored during searching).
-    Int2UserSpecREs = dict()
-
-    @staticmethod
-    def userSpec2ParamNames(userSpecCols, int2UserSpecREs, strict=True):
-
-        """
-        Retrieve the internal param. names matching with the "user specified" ones
-        according to the given regexp dictionary.
-        
-        Parameters:
-        :param userSpecCols: list of user spec. columns
-        :param int2UserSpecREs: Possible regexps for internal param. names
-        :param strict: if True, raise KeyError if any name in userSpecCols cannot be matched ;
-                       if False, will return None for each unmatched internal param. name
-
-        Return: List of internal param. names, same order as userSpecCols,
-                with Nones when not found (if not strict)
-        """
-        logger.debug('Matching user spec. columns:')
-
-        parNames = list()
-        for specName in userSpecCols:
-            try:
-                parName = next(iter(parName for parName in int2UserSpecREs
-                                    if any(re.search(pat, specName, flags=re.IGNORECASE)
-                                           for pat in int2UserSpecREs[parName])))
-                logger.debug1(f' * "{specName}" => {parName}')
-                parNames.append(parName)
-            except StopIteration:
-                if strict:
-                    raise KeyError('Could not match user spec. column "{}" in sample data set columns [{}]'
-                                   .format(specName, ', '.join(int2UserSpecREs.keys())))
-                else:
-                    logger.debug1(f' * "{specName}" => Not found')
-                    parNames.append(None)
-
-        logger.debug('... success{}.'.format('' if strict else ' but {} mismatches'.format(parNames.count(None))))
-
-        return parNames
-
-    @staticmethod
-    def _explicitParamSpecs(implParamSpecs=None, dfExplParamSpecs=None, int2UserSpecREs=dict(),
-                            sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
-                            abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum',
-                            sampleIndCol='SampleNum', anlysSpecCustCols=[], dropDupes=True):
-                           
-        """Explicitate analysis param. specs if not already done, and complete columns if needed ;
-        also automatically extract (regexps) columns which are really analysis parameters,
-        with their analyser-internal name, and also their "user" name.
-        
-        Parameters:
-        :param implParamSpecs: Implicit analysis param specs, suitable for explicitation
-           through explicitVariantSpecs()
-        :param dfExplParamSpecs: Explicit analysis param specs, as a DataFrame
-           (generated through explicitVariantSpecs, as an example)
-        :param int2UserSpecREs: Possible regexps for internal param. names
-        :param sampleSelCols: sample identification = selection columns
-        :param abbrevCol: Name of column to generate for abbreviating analyses params, not sure really useful ...
-        :param abbrevBuilder: Function of explicit analysis params (as a Series) to generate abbreviated name
-        :param anlysIndCol: Name of column to generate for identifying analyses, unless already there in input data.
-        :param sampleIndCol: Name of column to generate for identifying samples, unless already there in input data.
-        :param anlysSpecCustCols: special columns from analysis specs to simply pass through and ignore
-        :param dropDupes: if True, drop duplicates (keep first) in the final explicit DataFrame
-           
-        :return: Explicit specs as a DataFrame (input dfExplParamSpecs not modified : a new one is returned),
-                 list of matched analysis param. columns user names,
-                 list of matched analysis param. columns internal names,
-                 list of unmatched analysis param. columns user names.
-        """
-    
-        # Explicitate analysis specs if needed (and add computed columns if any and not already there).
-        assert dfExplParamSpecs is None or implParamSpecs is None, \
-               'Only one of dfExplParamSpecs and paramSpecCols can be specified'
-        
-        dCompdCols = {abbrevCol: abbrevBuilder} if abbrevCol and abbrevBuilder else {}
-        if implParamSpecs is not None:
-            dfExplParamSpecs = \
-                Analyser.explicitVariantSpecs(implParamSpecs, varIndCol=anlysIndCol,
-                                              computedCols=dCompdCols)
-        else:
-            dfExplParamSpecs = dfExplParamSpecs.copy()
-            for colName, computeCol in dCompdCols.items():
-                if colName not in dfExplParamSpecs.columns:
-                    dfExplParamSpecs[colName] = dfExplParamSpecs.apply(computeCol, axis='columns')
-            if anlysIndCol and anlysIndCol not in dfExplParamSpecs.columns:
-                dfExplParamSpecs[anlysIndCol] = np.arange(len(dfExplParamSpecs))
-
-        # Add sample index column if requested and not already there
-        if sampleIndCol and sampleIndCol not in dfExplParamSpecs.columns:
-            # Drop all-NaN sample selection columns (sometimes, it happens) for a working groupby()
-            dfSampInd = dfExplParamSpecs[sampleSelCols].dropna(axis='columns', how='all')
-            dfExplParamSpecs[sampleIndCol] = \
-                dfSampInd.groupby(list(dfSampInd.columns), sort=False).ngroup()
-
-        # Check for columns duplicates : a killer (to avoid weird error later).
-        iCols = dfExplParamSpecs.columns
-        assert not iCols.duplicated().any(), \
-               'Some duplicate column(s) in parameter specs: ' + ' ,'.join(iCols[iCols.duplicated()])
-
-        # Convert explicit. analysis spec. columns to the internal parameter names,
-        # and extract the real analysis parameters (None entries when not matched).
-        intParamSpecCols = \
-            DSAnalyser.userSpec2ParamNames(dfExplParamSpecs.columns, int2UserSpecREs, strict=False)
-
-        # Get back to associated column user names
-        # a. matched with internal param. names
-        userParamSpecCols = [usp for inp, usp in zip(intParamSpecCols, dfExplParamSpecs.columns) if inp]
-        
-        # b. unmatched with internal param. names, and assumed to be real user analysis params.
-        ignUserParamSpeCols = sampleSelCols.copy()
-        if abbrevCol:
-            ignUserParamSpeCols += [abbrevCol]
-        if sampleIndCol:
-            ignUserParamSpeCols += [sampleIndCol]
-        if anlysIndCol:
-            ignUserParamSpeCols += [anlysIndCol]
-        if anlysSpecCustCols:
-            ignUserParamSpeCols += anlysSpecCustCols
-        unmUserParamSpecCols = [usp for inp, usp in zip(intParamSpecCols, dfExplParamSpecs.columns)
-                                if not inp and usp not in ignUserParamSpeCols]
-
-        # Cleanup implicit name list from Nones (strict=False)
-        intParamSpecCols = [inp for inp in intParamSpecCols if inp]
-
-        # Drop duplicate specs if specified.
-        if dropDupes:
-            nBefore = len(dfExplParamSpecs)
-            dupDetCols = sampleSelCols + userParamSpecCols
-            dfExplParamSpecs.drop_duplicates(subset=dupDetCols, inplace=True)
-            dfExplParamSpecs.reset_index(drop=True, inplace=True)
-            logger.info('Dropped {} last duplicate specs of {}, on [{}] columns'
-                        .format(nBefore - len(dfExplParamSpecs), nBefore, ', '.join(dupDetCols)))
-
-        # Done.
-        return dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols
-
-    def explicitParamSpecs(self, implParamSpecs=None, dfExplParamSpecs=None, dropDupes=True, check=False):
-    
-        """Explicitate analysis param. specs if not already done, and complete columns if needed ;
-        also automatically extract (regexps) columns which are really analysis parameters,
-        with their analyser-internal name, and also their "user" name.
-        
-        Can moreover check params specs for usability, if check is True :
-        * use it before calling analyser.run(implParamSpecs=..., dfExplParamSpecs=..., ...)
-          to check that everythings OK,
-        * or be sure that run() will fail at startup (because it itself will do it).
-        
-        Parameters:
-        :param implParamSpecs: Implicit analysis param specs, suitable for explicitation
-           through explicitVariantSpecs()
-        :param dfExplParamSpecs: Explicit analysis param specs, as a DataFrame
-           (generated through explicitVariantSpecs, as an example)
-        :param dropDupes: if True, drop duplicates (keep first)
-        :param check: if True, checks params for usability by run(),
-           and return a bool verdict and a list of strings explaining the negative (False) verdict
-
-        :return: a 3 or 5-item tuple :
-           * explicit specs as a DataFrame (input dfExplParamSpecs not modified: a new updated one is returned),
-           * list of analysis param. columns internal names,
-           * list of analysis param. columns user names,
-           if ckeck, 2 more items in the return tuple :
-           * check verdict : True if everything went well, False otherwise,
-             * some columns from paramSpecCols could not be found in dfExplParamSpecs columns,
-             * some user columns could not be matched with some of the expected internal parameter names,
-             * some rows are not suitable for DS analysis (empty sample identification columns, ...).
-           * check failure reasons : list of strings explaining things that went bad.
-        """
-        
-        # Explicitate and complete
-        tplRslt = self._explicitParamSpecs(implParamSpecs, dfExplParamSpecs, self.Int2UserSpecREs,
-                                           sampleSelCols=self.sampleSelCols, abbrevCol=self.abbrevCol,
-                                           abbrevBuilder=self.abbrevBuilder, anlysIndCol=self.anlysIndCol,
-                                           sampleIndCol=self.sampleIndCol, anlysSpecCustCols=self.anlysSpecCustCols,
-                                           dropDupes=dropDupes)
-        
-        # Check if requested
-        if check:
-        
-            verdict = True
-            reasons = []
-     
-            dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = tplRslt
-            
-            # Check that an internal column name was found for every user spec column.
-            if len(unmUserParamSpecCols):
-                verdict = False
-                reasons.append('Failed to match some user spec. names with internal ones: {}'
-                               .format(', '.join(unmUserParamSpecCols)))
-
-            # Check that all rows are suitable for DS analysis (non empty sample identification columns, ...).
-            if dfExplParamSpecs[self.sampleSelCols].isnull().all(axis='columns').any():
-                verdict = False
-                reasons.append('Some rows have some null sample selection columns')
-
-            # Check done.
-            tplRslt += verdict, reasons
-
-        return tplRslt
-
-    def shutdown(self):
-    
-        """Shutdown engine and executor (only usefull if run() raises an exception and so fails to do it),
-        but keep the remainder of the object state as is.
-        """
-
-        if self._engine:
-            self._engine.shutdown()
-            self._engine = None
-        if self._executor:
-            self._executor.shutdown()
-            self._executor = None
-        
-#    def __del__(self):
-#    
-#        self.shutdown()
-
-
-class _FilterSortSteps(object):
-
-    """Log = history of filter and sort steps for a given scheme"""
-
-    def __init__(self, filSorSchId, resultsSet, lang):
-        self.schemeId = filSorSchId
-        self.lang = lang
-        self.results = resultsSet  # only for resultsSet.transColumn(...)
-        self.steps = list()  # of [stepName, propName, propValue]
-
-    def copy(self):
-        clone = _FilterSortSteps(filSorSchId=self.schemeId, resultsSet=self.results, lang=self.lang)
-        clone.steps = copy.deepcopy(self.steps)
-        return clone
-    
-    def transColumns(self, columns):
-        if isinstance(columns, (list, dict)):
-            return ', '.join(self.results.transColumn(col, self.lang) for col in columns)
-        else:
-            return self.results.transColumn(columns, self.lang)
-
-    def append(self, stepName, propName, propValue, transColumns=False):
-        """Append a (step, property, value) record in the filter & sort operation log
-        (translate value as columns label(s) if specified: transColumns=True)"""
-        logger.debug2(f'* {stepName}: {propName} = {propValue}')
-        if transColumns:
-            propValue = self.transColumns(propValue)
-        self.steps.append([stepName, propName, propValue])
-
-    def toList(self):
-        return [[self.schemeId] + step for step in self.steps]
-
-
-class _FilterSortCache(object):
-
-    def __init__(self):
-        self.dResults = dict()
-
-    def copy(self):
-        clone = _FilterSortCache()
-        clone.dResults = {schId: (iFilSor.copy(), filSorSteps.copy())
-                          for schId, (iFilSor, filSorSteps) in self.dResults.items()}
-        return clone
-
-    def clear(self):
-        self.dResults.clear()
-
-    def update(self, schemeId, iFilSor, filSorSteps):
-        # Add / update a detached copy
-        self.dResults[schemeId] = iFilSor.copy(), filSorSteps.copy()
-
-    def get(self, schemeId):
-        if schemeId in self.dResults:
-            # return a detached copy.
-            iFilSor, filSorSteps = self.dResults[schemeId]
-            logger.info1(f'Filter and sort scheme "{schemeId}" found in cache.')
-            return iFilSor.copy(), filSorSteps.copy()
-        return None, None
-
-
-class FilterSortSchemeIdManager(object):
-
-    """Filter and sort scheme Id generator"""
-
-    # Constants for schemeId()
-    MainSchSpecNames = ['method', 'deduplicate', 'filterSort',
-                        'preselCols', 'preselAscs', 'preselThrhs', 'preselNum']
-
-    def __init__(self):
-
-        """Ctor"""
-
-        # Constants for schemeId() (can't define as class variables: would need forward ref to MCDSAnalysisResultsSet)
-        RS = MCDSAnalysisResultsSet
-        self.FinalQuaShortNames = {RS.CLChi2: 'k2', RS.CLKS: 'ks', RS.CLCvMUw: 'cu', RS.CLCvMCw: 'cw',
-                                   RS.CLDCv: 'dc', RS.CLSightRate: 'sr',
-                                   RS.CLCmbQuaBal1: 'q1', RS.CLCmbQuaBal2: 'q2', RS.CLCmbQuaBal3: 'q3',
-                                   RS.CLCmbQuaChi2: 'qk2', RS.CLCmbQuaKS: 'qks', RS.CLCmbQuaDCv: 'qdc'}
-
-        # Short (but unique) Ids for already seen "filter and sort" schemes,
-        # based on the scheme name and an additional int suffix when needed ;
-        # Definition: 2 equal schemes (dict ==) have the same Id
-        self.dFilSorSchemes = dict()  # Unique Id => value
-
-    def copy(self):
-
-        clone = FilterSortSchemeIdManager()
-        clone.dFilSorSchemes = copy.deepcopy(self.dFilSorSchemes)
-
-    def clear(self):
-
-        self.dFilSorSchemes.clear()
-
-    def schemeId(self, scheme):
-
-        """Human readable but unique identification of a filter and sort scheme
-
-        Built on the scheme name format and an additional int suffix when needed.
-
-        Definition: 2 equal schemes (dict ==) have the same Id
-
-        Parameters:
-        :param scheme: the scheme to identify
-                       as a dict(method= filterSortOnXXX method to use,
-                                 deduplicate= dict(dupSubset=, dDupRounds=) of deduplication params
-                                     (if not or partially given, see filterSortOnXXX defaults)
-                                 filterSort= dict of other <method> params (see filterSortOnXXX methods),
-                                 preselCols= target columns for generating auto-preselection ones,
-                                             containing [1, preselNum] ranks ; default: []
-                                 preselAscs= Rank direction to use for each column (list),
-                                             or all (single bool) ; default: True
-                                             (True means that lower values are "better" ones)
-                                 preselThrhs= Eliminating threshold for each column (list),
-                                              or all (single number) ; default: 0.2
-                                              (eliminated above if preselAscs True, below otherwise)
-                                 preselNum= number of (best) pre-selections to keep for each sample) ;
-                                            default: 5
-
-        :return: the unique Id.
-        """
-
-        # Check scheme specification (1st level properties: presence of mandatory ones, authorised list, ...)
-        props = scheme.keys()
-        assert all(prop in self.MainSchSpecNames for prop in props), \
-               'Unknown filter and sort scheme property/ies: {}' \
-                .format(', '.join(prop for prop in props if prop not in self.MainSchSpecNames))
-        mandProps = ['method']
-        assert all(prop in props for prop in mandProps), \
-               'Missing filter and sort scheme mandatory property/ies: {}' \
-                .format(', '.join(prop for prop in mandProps if prop not in props))
-        method = scheme['method']
-        assert callable(method), 'Filter and sort scheme method must be callable'
-        assert method is MCDSAnalysisResultsSet.filterSortOnExecCode \
-               or method is MCDSAnalysisResultsSet.filterSortOnExCAicMulQua, \
-               'Unsupported filter and sort scheme method: ' + str(method)
-
-        # Compute the heading "name" part of the Id
-        schemeId = 'ExCode' if method is MCDSAnalysisResultsSet.filterSortOnExecCode else 'ExAicMQua'
-        methArgs = scheme.get('filterSort', {})
-        if 'sightRate' in methArgs:
-            schemeId += '-r{:.1f}'.format(methArgs['sightRate']).replace('.', '')
-        if 'whichBestQua' in methArgs:
-            schemeId += 'm{}'.format(len(methArgs['whichBestQua']))
-        if 'whichFinalQua' in methArgs and method is not MCDSAnalysisResultsSet.filterSortOnExecCode:
-            assert methArgs['whichFinalQua'] in self.FinalQuaShortNames, \
-                'Unsupported quality indicator for filtering: ' + str(methArgs['whichFinalQua'])
-            schemeId += self.FinalQuaShortNames[methArgs['whichFinalQua']]
-        if 'nFinalRes' in methArgs:
-            schemeId += 'd{}'.format(methArgs['nFinalRes'])
-
-        # The Id must be unique: enforce it through a uniqueness suffix only if needed
-        if schemeId in self.dFilSorSchemes:  # Seems there's a possible collision ...
-            closeSchemes = {schId: schVal for schId, schVal in self.dFilSorSchemes.items()
-                            if schId.startswith(schemeId)}
-            for schId, schVal in closeSchemes.items():  # Check if not simply an exact match
-                if scheme == schVal:
-                    return schId  # Bingo !
-            schemeId += '-' + str(len(closeSchemes) + 1)  # No exact match => new unused Id !
-
-        # Register new scheme and Id.
-        self.dFilSorSchemes[schemeId] = scheme
-
-        return schemeId
-
-
-class MCDSAnalysisResultsSet(AnalysisResultsSet):
-
-    """A specialized results set for MCDS analyses, with extra. post-computed columns : Delta AIC, Chi2 P"""
-    
-    # Shortcut to already existing and useful columns names.
-    CLRunStatus = MCDSAnalysis.CLRunStatus
-    CLRunStartTime = MCDSAnalysis.CLRunStartTime
-    CLRunElapsedTime = MCDSAnalysis.CLRunElapsedTime
-    CLRunFolder = MCDSAnalysis.CLRunFolder
-    CLParEstKeyFn = MCDSAnalysis.CLParEstKeyFn
-    CLParEstAdjSer = MCDSAnalysis.CLParEstAdjSer
-    CLParEstSelCrit = MCDSAnalysis.CLParEstSelCrit
-    CLParEstCVInt = MCDSAnalysis.CLParEstCVInt
-    CLParTruncLeft = MCDSAnalysis.CLParTruncLeft
-    CLParTruncRight = MCDSAnalysis.CLParTruncRight
-    CLParModFitDistCuts = MCDSAnalysis.CLParModFitDistCuts
-    CLParModDiscrDistCuts = MCDSAnalysis.CLParModDiscrDistCuts
-
-    CLEffort = ('encounter rate', 'effort (L or K or T)', 'Value')
-    CLPDetec = ('detection probability', 'probability of detection (Pw)', 'Value')
-    CLPDetecMin = ('detection probability', 'probability of detection (Pw)', 'Lcl')
-    CLPDetecMax = ('detection probability', 'probability of detection (Pw)', 'Ucl')
-    CLPDetecCv = ('detection probability', 'probability of detection (Pw)', 'Cv')
-    CLPDetecDf = ('detection probability', 'probability of detection (Pw)', 'Df')
-    CLEswEdr = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Value')
-    CLEswEdrMin = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Lcl')
-    CLEswEdrMax = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Ucl')
-    CLEswEdrCv = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Cv')
-    CLEswEdrDf = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Df')
-    CLDensity = ('density/abundance', 'density of animals', 'Value')
-    CLDensityMin = ('density/abundance', 'density of animals', 'Lcl')
-    CLDensityMax = ('density/abundance', 'density of animals', 'Ucl')
-    CLDensityCv = ('density/abundance', 'density of animals', 'Cv')
-    CLDensityDf = ('density/abundance', 'density of animals', 'Df')
-    CLNumber = ('density/abundance', 'number of animals, if survey area is specified', 'Value')
-    CLNumberMin = ('density/abundance', 'number of animals, if survey area is specified', 'Lcl')
-    CLNumberMax = ('density/abundance', 'number of animals, if survey area is specified', 'Ucl')
-    CLNumberCv = ('density/abundance', 'number of animals, if survey area is specified', 'Cv')
-    CLNumberDf = ('density/abundance', 'number of animals, if survey area is specified', 'Df')
-
-    DCLParTruncDist = dict(left=CLParTruncLeft, right=CLParTruncRight)
-
-    # Computed Column Labels
-    # a. Chi2 determined, Delta AIC, delta DCv
-    CLChi2 = ('detection probability', 'chi-square test probability determined', 'Value')
-    CLDeltaAic = ('detection probability', 'Delta AIC', 'Value')
-    CLDeltaDCv = ('density/abundance', 'density of animals', 'Delta Cv')
-
-    # b. Observation rate and combined quality indicators
-    CLSightRate = ('encounter rate', 'observation rate', 'Value')
-    CLCmbQuaBal1 = ('combined quality', 'balanced 1', 'Value')
-    CLCmbQuaBal2 = ('combined quality', 'balanced 2', 'Value')
-    CLCmbQuaBal3 = ('combined quality', 'balanced 3', 'Value')
-    CLCmbQuaChi2 = ('combined quality', 'more Chi2', 'Value')
-    CLCmbQuaKS = ('combined quality', 'more KS', 'Value')
-    CLCmbQuaDCv = ('combined quality', 'more DCv', 'Value')
-
-    # c. Automated filtering and grouping + sorting
-    CLCAutoFilSor = 'auto filter sort'  # Label "Chapter" (1st level)
-    CLTTruncGroup = 'Group'  # Label "Type" (3rd level)
-    CLTSortOrder = 'Order'  # Label "Type" (3rd level)
-    CLTPreSelection = 'Pre-selection'  # Label "Type" (3rd level)
-
-    #   i. Close truncation group identification
-    CLGroupTruncLeft = (CLCAutoFilSor, CLParTruncLeft[1], CLTTruncGroup)
-    CLGroupTruncRight = (CLCAutoFilSor, CLParTruncRight[1], CLTTruncGroup)
-
-    #   ii. Order inside groups with same = identical truncation parameters (distances and model cut points)
-    CLGrpOrdSmTrAic = (CLCAutoFilSor, 'AIC (same trunc)', CLTSortOrder)
-
-    #   iii. Order inside groups of close truncation distances
-    CLGrpOrdClTrChi2KSDCv = (CLCAutoFilSor, 'Chi2 KS DCv (close trunc)', CLTSortOrder)
-    # CLGrpOrdClTrChi2 = (CLCAutoFilSor, 'Chi2 (close trunc)', CLTSortOrder)
-    CLGrpOrdClTrDCv = (CLCAutoFilSor, 'DCv (close trunc)', CLTSortOrder)
-    
-    CLGrpOrdClTrQuaBal1 = (CLCAutoFilSor, 'Bal. quality 1 (close trunc)', CLTSortOrder)
-    CLGrpOrdClTrQuaBal2 = (CLCAutoFilSor, 'Bal. quality 2 (close trunc)', CLTSortOrder)
-    CLGrpOrdClTrQuaBal3 = (CLCAutoFilSor, 'Bal. quality 3 (close trunc)', CLTSortOrder)
-    CLGrpOrdClTrQuaChi2 = (CLCAutoFilSor, 'Bal. quality Chi2+ (close trunc)', CLTSortOrder)
-    CLGrpOrdClTrQuaKS = (CLCAutoFilSor, 'Bal. quality KS+ (close trunc)', CLTSortOrder)
-    CLGrpOrdClTrQuaDCv = (CLCAutoFilSor, 'Bal. quality DCv+ (close trunc)', CLTSortOrder)
-    
-    #   iv. Global order
-    CLGblOrdChi2KSDCv = (CLCAutoFilSor, 'Chi2 KS DCv (global)', CLTSortOrder)
-
-    CLGblOrdQuaBal1 = (CLCAutoFilSor, 'Bal. quality 1 (global)', CLTSortOrder)
-    CLGblOrdQuaBal2 = (CLCAutoFilSor, 'Bal. quality 2 (global)', CLTSortOrder)
-    CLGblOrdQuaBal3 = (CLCAutoFilSor, 'Bal. quality 3 (global)', CLTSortOrder)
-    CLGblOrdQuaChi2 = (CLCAutoFilSor, 'Bal. quality Chi2+ (global)', CLTSortOrder)
-    CLGblOrdQuaKS = (CLCAutoFilSor, 'Bal. quality KS+ (global)', CLTSortOrder)
-    CLGblOrdQuaDCv = (CLCAutoFilSor, 'Bal. quality DCv+ (global)', CLTSortOrder)
-
-    CLGblOrdDAicChi2KSDCv = (CLCAutoFilSor, 'DeltaAIC Chi2 KS DCv (global)', CLTSortOrder)
-
-    # Computed columns specs (name translation + position).
-    _firstResColInd = len(MCDSEngine.statSampCols()) + len(MCDSAnalysis.MIRunColumns)
-    DComputedCols = {CLSightRate: _firstResColInd + 10,  # After Encounter Rate / Left|Right Trunc. Dist.
-                     CLDeltaAic: _firstResColInd + 12,  # Before AIC
-                     CLChi2: _firstResColInd + 14,  # Before all Chi2 tests
-                     CLDeltaDCv: _firstResColInd + 72,  # Before Density of animals / Cv
-                     # And, at the end ...
-                     **{cl: -1 for cl in [CLCmbQuaBal1, CLCmbQuaBal2, CLCmbQuaBal3,
-                                          CLCmbQuaChi2, CLCmbQuaKS, CLCmbQuaDCv,
-                                          CLGroupTruncLeft, CLGroupTruncRight,
-                                          CLGrpOrdSmTrAic,
-                                          CLGrpOrdClTrChi2KSDCv,  # CLGrpOrdClTrChi2,
-                                          CLGrpOrdClTrDCv,
-                                          CLGrpOrdClTrQuaBal1, CLGrpOrdClTrQuaBal2, CLGrpOrdClTrQuaBal3,
-                                          CLGrpOrdClTrQuaChi2, CLGrpOrdClTrQuaKS, CLGrpOrdClTrQuaDCv,
-                                          CLGblOrdChi2KSDCv,
-                                          CLGblOrdQuaBal1, CLGblOrdQuaBal2, CLGblOrdQuaBal3,
-                                          CLGblOrdQuaChi2, CLGblOrdQuaKS, CLGblOrdQuaDCv,
-                                          CLGblOrdDAicChi2KSDCv]}}
-
-    DfComputedColTrans = \
-        pd.DataFrame(index=DComputedCols.keys(),
-                     data=dict(en=['Obs Rate', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Density',
-                                   'Qual Bal 1', 'Qual Bal 2', 'Qual Bal 3',
-                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+',
-                                   'Group Left Trunc', 'Group Right Trunc',
-                                   'Order Same Trunc AIC',
-                                   'Order Close Trunc Chi2 KS DCv',  # 'Order Close Trunc Chi2',
-                                   'Order Close Trunc DCv', 'Order Close Trunc Bal 1 Qual',
-                                   'Order Close Trunc Bal 2 Qual', 'Order Close Trunc Bal 3 Qual',
-                                   'Order Close Trunc Bal Chi2+ Qual', 'Order Close Trunc Bal KS+ Qual',
-                                   'Order Close Trunc Bal DCv+ Qual',
-                                   'Order Global Chi2 KS DCv', 'Order Global Bal 1 Qual',
-                                   'Order Global Bal 2 Qual', 'Order Global Bal 3 Qual',
-                                   'Order Global Bal Chi2+ Qual', 'Order Global Bal KS+ Qual',
-                                   'Order Global Bal DCv+ Qual',
-                                   'Order Global DeltaAIC Chi2 KS DCv'],
-                               fr=['Taux Obs', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Densité',
-                                   'Qual Equi 1', 'Qual Equi 2', 'Qual Equi 3',
-                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+',
-                                   'Groupe Tronc Gche', 'Groupe Tronc Drte',
-                                   'Ordre Tronc Ident AIC',
-                                   'Ordre Tronc Proch Chi2 KS DCv',  # 'Ordre Tronc Proch Chi2',
-                                   'Ordre Tronc Proch DCv', 'Ordre Tronc Proch Qual Equi 1',
-                                   'Ordre Tronc Proch Qual Equi 2', 'Ordre Tronc Proch Qual Equi 3',
-                                   'Ordre Tronc Proch Qual Equi Chi2+', 'Ordre Tronc Proch Qual Equi KS+',
-                                   'Ordre Tronc Proch Qual Equi DCv+',
-                                   'Ordre Global Chi2 KS DCv', 'Ordre Global Qual Equi 1',
-                                   'Ordre Global Qual Equi 2', 'Ordre Global Qual Equi 3',
-                                   'Ordre Global Qual Equi Chi2+', 'Ordre Global Qual Equi KS+',
-                                   'Ordre Global Qual Equi DCv+',
-                                   'Ordre Global DeltaAIC Chi2 KS DCv']))
-
-    # Final-selection column label (empty, for user decision)
-    CLFinalSelection = (CLCAutoFilSor, 'Final selection', 'Value')
-    DFinalSelColTrans = dict(fr='Sélection finale', en='Final selection')
-
-    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,
-                 sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
-                                    dict(col='right', minDist=25.0, maxLen=25.0)],
-                 truncIntrvEpsilon=1e-6, ldFilSorKeySchemes=None):
-        
-        """Ctor
-
-        Parameters:
-        :param miSampleCols: columns to use for grouping by sample ; defaults to miCustomCols if None
-        :param sampleIndCol: multi-column index for the sample Id column ; no default, must be there !
-        :param ldFilSorKeySchemes: Replacement for predefined filter-sort key schemes
-                                   None => use predefined ones AutoFilSorKeySchemes.
-        """
-
-        assert all(len(self.DfComputedColTrans[lang].dropna()) == len(self.DComputedCols)
-                   for lang in self.DfComputedColTrans.columns)
-
-        assert sampleIndCol is not None
-
-        assert all(spec['col'] in self.DCLParTruncDist for spec in ldTruncIntrvSpecs)
-
-        # Initialise base.
-        super().__init__(MCDSAnalysis, miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
-                         dComputedCols=self.DComputedCols, dfComputedColTrans=self.DfComputedColTrans,
-                         sortCols=sortCols, sortAscend=sortAscend)
-        
-        if self.CLFinalSelection:
-            self.addColumnsTrans({self.CLFinalSelection: self.DFinalSelColTrans})
-
-        # Sample columns
-        self.miSampleCols = miSampleCols if miSampleCols is not None else self.miCustomCols
-        self.sampleIndCol = sampleIndCol
-
-        # Sample table (auto computed, see listSamples below)
-        self.dfSamples = None
-    
-        # Descriptive parameters, not used in computations actually.
-        self.distanceUnit = distanceUnit
-        self.areaUnit = areaUnit
-        self.surveyType = surveyType
-        self.distanceType = distanceType
-        self.clustering = clustering
-
-        # Parameters for truncation group intervals post-computations
-        self.ldTruncIntrvSpecs = ldTruncIntrvSpecs
-        self.truncIntrvEpsilon = truncIntrvEpsilon
-
-        # Parameters for filter and sort key generation schemes
-        self.ldFilSorKeySchemes = ldFilSorKeySchemes
-
-        # Ids manager for "filter and sort" schemes
-        self.filSorIdMgr = FilterSortSchemeIdManager()
-
-        # Cache for results of filter and sort schemes
-        self.filSorCache = _FilterSortCache()
-
-    def copy(self, withData=True):
-    
-        """Clone function, with optional data copy"""
-    
-        # Create new instance with same ctor params.
-        clone = MCDSAnalysisResultsSet(miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
-                                       miSampleCols=self.miSampleCols, sampleIndCol=self.sampleIndCol,
-                                       sortCols=self.sortCols, sortAscend=self.sortAscend,
-                                       distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                       surveyType=self.surveyType, distanceType=self.distanceType,
-                                       clustering=self.clustering,
-                                       ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
-                                       truncIntrvEpsilon=self.truncIntrvEpsilon)
-
-        # Copy data if needed.
-        if withData:
-            clone._dfData = self._dfData.copy()
-            clone.rightColOrder = self.rightColOrder
-            clone.postComputed = self.postComputed
-            clone.dfSamples = None if self.dfSamples is None else self.dfSamples.copy()
-            clone.filSorIdMgr = self.filSorIdMgr.copy()
-            clone.filSorCache = self.filSorCache.copy()
-
-        return clone
-    
-    def onDataChanged(self):
-
-        """React to results data (_dfData) changes that invalidate calculated data,
-        but only when the calculus is coded in this class ; other calculi impacts taken care in base classes"""
-
-        self.dfSamples = None
-        self.filSorIdMgr.clear()
-        self.filSorCache.clear()
-
-    def dropRows(self, sbSelRows):
-    
-        super().dropRows(sbSelRows)
-
-        self.onDataChanged()
-        
-    def setData(self, dfData, postComputed=False, acceptNewCols=False):
-        
-        super().setData(dfData, postComputed=postComputed, acceptNewCols=acceptNewCols)
-
-        self.onDataChanged()
-
-    # Get translate names of custom columns
-    def transSampleColumns(self, lang):
-        
-        return self.dfCustomColTrans.loc[self.miSampleCols, lang].to_list()
-
-    # Post-computations : Actual Chi2 value, from multiple tests done.
-    MaxChi2Tests = 3  # TODO: Really a constant, or actually depends on some analysis params ?
-    CLsChi2All = [('detection probability', 'chi-square test probability (distance set {})'.format(i), 'Value')
-                  for i in range(MaxChi2Tests, 0, -1)]
-    
-    @staticmethod
-    def _determineChi2Value(sChi2AllDists):
-        for chi2 in sChi2AllDists:
-            if not np.isnan(chi2):
-                return chi2
-        return np.nan
-
-    def _postComputeChi2(self):
-        
-        logger.info2(f'Post-computing actual Chi2: {self.CLChi2}')
-        
-        # Last value of all the tests done.
-        chi2AllColLbls = [col for col in self.CLsChi2All if col in self._dfData.columns]
-        if chi2AllColLbls:
-            self._dfData[self.CLChi2] = self._dfData[chi2AllColLbls].apply(self._determineChi2Value, axis='columns')
-
-    # Post-computations : Delta AIC/DCv per sampleCols + truncation param. cols group => AIC - min(group).
-    CLAic = ('detection probability', 'AIC value', 'Value')
-    CLDCv = CLDensityCv
-    CLsTruncDist = [('encounter rate', 'left truncation distance', 'Value'),
-                    ('encounter rate', 'right truncation distance (w)', 'Value')]
-
-    def _postComputeDeltaAicDCv(self):
-        
-        logger.info2(f'Post-computing Delta AIC/DCv: {self.CLDeltaAic} / {self.CLDeltaDCv}')
-        
-        # a. Minimum AIC & DCv per group
-        #    (drop all-NaN sample selection columns (sometimes, it happens) for a working groupby())
-        groupColLbls = self.miSampleCols.append(pd.MultiIndex.from_tuples(self.CLsTruncDist))
-        groupColLbls = [col for col in groupColLbls
-                        if col in self._dfData.columns and not self._dfData[col].isna().all()]
-        df2Join = self._dfData.groupby(groupColLbls, dropna=False)[[self.CLAic, self.CLDCv]].min()
-        
-        # b. Rename computed columns to target 'Delta XXX'
-        df2Join.columns = pd.MultiIndex.from_tuples([self.CLDeltaAic, self.CLDeltaDCv])
-
-        # c. Join the column to the target data-frame
-        self._dfData = self._dfData.join(df2Join, on=groupColLbls)
-
-        # d. Compute delta-AIC & DCv in-place
-        self._dfData[self.CLDeltaAic] = self._dfData[self.CLAic] - self._dfData[self.CLDeltaAic]
-        self._dfData[self.CLDeltaDCv] = self._dfData[self.CLDCv] - self._dfData[self.CLDeltaDCv]
-
-    # Post computations : Useful columns for quality indicators.
-    CLNObs = ('encounter rate', 'number of observations (n)', 'Value')
-    CLNTotObs = MCDSEngine.MIStatSampCols[0]
-    CLMinObsDist = MCDSEngine.MIStatSampCols[1]
-    CLMaxObsDist = MCDSEngine.MIStatSampCols[2]
-    CLKeyFn = ('detection probability', 'key function type', 'Value')
-    CLNTotPars = ('detection probability', 'total number of parameters (m)', 'Value')
-    CLNAdjPars = ('detection probability', 'number of adjustment term parameters (NAP)', 'Value')
-    CLKS = ('detection probability', 'Kolmogorov-Smirnov test probability', 'Value')
-    CLCvMUw = ('detection probability', 'Cramér-von Mises (uniform weighting) test probability', 'Value')
-    CLCvMCw = ('detection probability', 'Cramér-von Mises (cosine weighting) test probability', 'Value')
-
-    # Post computations : Quality indicators.
-    CLsQuaIndicSources = [CLKeyFn, CLNAdjPars, CLNTotPars, CLNObs, CLNTotObs, CLChi2, CLKS, CLCvMUw, CLCvMCw, CLDCv]
-
-    CIKeyFn = CLsQuaIndicSources.index(CLKeyFn)
-    CINAdjPars = CLsQuaIndicSources.index(CLNAdjPars)
-    CINTotPars = CLsQuaIndicSources.index(CLNTotPars)
-    CINObs = CLsQuaIndicSources.index(CLNObs)
-    CINTotObs = CLsQuaIndicSources.index(CLNTotObs)
-    CIChi2 = CLsQuaIndicSources.index(CLChi2)
-    CIKS = CLsQuaIndicSources.index(CLKS)
-    CICvMUw = CLsQuaIndicSources.index(CLCvMUw)
-    CICvMCw = CLsQuaIndicSources.index(CLCvMCw)
-    CIDCv = CLsQuaIndicSources.index(CLDCv)
-
-    @classmethod
-    def _combinedQualityBalanced1(cls, aRes):
-
-        """Historical QualBal1 indicator (March 2021), optimized through numpy
-
-        Returns:
-             np.array (shape: aRes rows, 1 column)
-        """
-
-        chi2KsCvMs = aRes[:, cls.CIChi2:cls.CICvMCw + 1].prod(axis=1)
-        normNObs = aRes[:, cls.CINObs].astype(float) / aRes[:, cls.CINTotObs].astype(float)
-        normNTotPars = 1 / (0.2 * np.maximum(2, aRes[:, cls.CINTotPars].astype(float)) + 0.6)
-        normCVDens = np.exp(-12 * np.square(aRes[:, cls.CIDCv].astype(float)))
-
-        return np.power(chi2KsCvMs * normNObs * normNTotPars * normCVDens, 1 / 7.0)  # shape: aRes rows, 1 column
-
-    # Raw key function indicator and relevant universal numpy function.
-    # DNormKeyFn = dict(HNORMAL=1.00, UNIFORM=0.75, HAZARD=0.5, NEXPON=0.1)  # Not better
-    _ufnNormKeyFn = \
-        np.frompyfunc(lambda keyFn: dict(HNORMAL=1.0, UNIFORM=0.9, HAZARD=0.6, NEXPON=0.1).get(keyFn, 0.0), 1, 1)
-
-    CLsNewQuaIndics = [CLCmbQuaBal2, CLCmbQuaBal3, CLCmbQuaChi2, CLCmbQuaKS, CLCmbQuaDCv]
-
-    @classmethod
-    def _combinedQualityAll(cls, aRes):
-
-        """New quality indicators (August, 2021 and later), optimized through numpy
-        (QualBal2, QualBal3, QualMoreChi2, QualMoreKS, QualMoreDCv)
-
-        Returns:
-             tuple(np.array) (shape: aRes rows, 1 column each, order of cls.CLsNewQuaIndics)
-        """
-
-        # Common computations (all times)
-        chi2 = aRes[:, cls.CIChi2].astype(float)
-        ks = aRes[:, cls.CIKS].astype(float)
-        dcv = aRes[:, cls.CIDCv].astype(float)
-        chi2KsCvMs = aRes[:, cls.CIChi2:cls.CICvMCw + 1].astype(float).prod(axis=1)
-        normNObs = aRes[:, cls.CINObs].astype(float) / aRes[:, cls.CINTotObs].astype(float)
-
-        # Common computations (October 2021)
-        nAdjPars2 = np.square(aRes[:, cls.CINAdjPars]).astype(float)
-        normKeyFn = cls._ufnNormKeyFn(aRes[:, cls.CIKeyFn])
-        normChi2KsCvMsNObsKFn = chi2KsCvMs * normNObs * normKeyFn
-
-        # QualBal2 (August 2021)
-        # normNTotPars2 = 1 / (0.2 * np.maximum(1, aRes[:, cls.CINTotPars].astype(float)) + 0.8)
-        # normCVDens2 = np.exp(-16 * np.square(dcv))
-        # normChi2KsCvMsNObsTotPDcv2 = chi2KsCvMs * normNObs * normNTotPars2 * normCVDens2
-        # quaBal2 = np.power(normChi2KsCvMsNObsTotPDcv2, 1 / 7.0)
-
-        # QualBal2 (October 2021): A more devaluating version for CVDens, using KeyFn, replacing NTotPars by NAdjPars
-        normNAdjPars2 = np.exp(-0.15 * nAdjPars2)
-        normCVDens2 = np.exp(-20 * np.square(dcv))
-        quaBal2 = np.power(normChi2KsCvMsNObsKFn * normNAdjPars2 * normCVDens2, 1 / 8.0)
-
-        # QualBal3 (August 2021)
-        # normNTotPars3 = 1 / (0.3 * np.maximum(1, aRes[:, cls.CINTotPars].astype(float)) + 0.7)
-        # normCVDens3 = np.exp(-20 * np.square(dcv))
-        # normChi2KsCvMsNObsTotPDcv3 = chi2KsCvMs * normNObs * normNTotPars3 * normCVDens3
-        # quaBal3 = np.power(normChi2KsCvMsNObsTotPDcv3, 1 / 7.0)
-
-        # QualBal3 (October 2021): Same as QualBal2, but even more devaluating for CVDens
-        normNAdjPars3 = np.exp(-0.17 * nAdjPars2)
-        normCVDens3 = np.exp(-63 * np.power(dcv, 2.8))
-        normChi2KsCvMsNObsKFnAdjPDcv3 = normChi2KsCvMsNObsKFn * normNAdjPars3 * normCVDens3
-        quaBal3 = np.power(normChi2KsCvMsNObsKFnAdjPDcv3, 1 / 8.0)
-
-        # QualMoreX (March 2021)
-        # normNTotParsM = 1 / (0.2 * np.maximum(2, aRes[:, cls.CINTotPars].astype(float)) + 0.6)
-        # normCVDensM = np.exp(-12 * np.square(dcv))
-        # normChi2KsCvMsNObsTotPDCvM = chi2KsCvMs * normNObs * normNTotParsM * normCVDensM
-        # moreChi2 = np.power(normChi2KsCvMsNObsTotPDCvM * chi2, 1 / 8.0)
-        # moreKS = np.power(normChi2KsCvMsNObsTotPDCvM * ks, 1 / 8.0)
-        # moreDCv = np.power(normChi2KsCvMsNObsTotPDCvM * normCVDensM, 1 / 8.0)
-
-        # QualMoreX (October 2021): Follow QualBal3 update (were based on historical QualBal1)
-        moreChi2 = np.power(normChi2KsCvMsNObsKFnAdjPDcv3 * chi2, 1 / 9.0)
-        moreKS = np.power(normChi2KsCvMsNObsKFnAdjPDcv3 * ks, 1 / 9.0)
-        moreDCv = np.power(normChi2KsCvMsNObsKFnAdjPDcv3 * normCVDens3, 1 / 9.0)
-
-        return quaBal2, quaBal3, moreChi2, moreKS, moreDCv
-
-    # Killer values for base MCDS indicators
-    KilrNObs = 0
-    KilrStaTest = 0
-    KilrDensCv = 1e5
-    KilrNPars = 1e3
-    KilrNTotObs = 1e9
-    KilrBalQua = 0
-
-    def _postComputeQualityIndicators(self):
-
-        cls = self
-
-        logger.info2('Post-computing Quality Indicators')
-
-        # Sighting rate (not 100% due to truncations).
-        self._dfData[cls.CLSightRate] = 100 * self._dfData[cls.CLNObs] / self._dfData[cls.CLNTotObs]  # [0,1] => %
-
-        # Prepare data for computations
-        logger.info3('* Pre-processing source data')
-
-        # a. extract the useful columns, after adding them if not present
-        for miCol in cls.CLsQuaIndicSources:
-            if miCol not in self._dfData.columns:
-                self._dfData[miCol] = np.nan
-        dfCompData = self._dfData[cls.CLsQuaIndicSources].copy()
-
-        # b. historical bal quality  indicator 1
-        logger.info3('* Balanced quality 1')
-        self._dfData[cls.CLCmbQuaBal1] = cls._combinedQualityBalanced1(dfCompData.values)
-
-        # c. newer quality indicators
-        #    (NaN value MUST kill down these indicators to compute => we have to enforce this)
-        dfCompData.fillna({cls.CLNObs: cls.KilrNObs,
-                           cls.CLChi2: cls.KilrStaTest, cls.CLKS: cls.KilrStaTest,
-                           cls.CLCvMUw: cls.KilrStaTest, cls.CLCvMCw: cls.KilrStaTest,
-                           cls.CLDCv: cls.KilrDensCv,  # Usually considered good under 0.3
-                           cls.CLNTotObs: cls.KilrNTotObs,  # Should slap down _normObs whatever NObs
-                           cls.CLNAdjPars: cls.KilrNPars,  # Should slap down _normNAdjPars whatever NObs
-                           cls.CLNTotPars: cls.KilrNPars},
-                          inplace=True)
-
-        logger.info3('* Balanced quality 2, 3, Chi2+, KS+, DCv+')
-        for miCol, aIndic in zip(cls.CLsNewQuaIndics, cls._combinedQualityAll(dfCompData.values)):
-            self._dfData[miCol] = aIndic
-
-        # For some unknown reason, the theorically better code below raises some odd exceptions like (depends):
-        # * index-join on non unique index not implemented
-        # * KeyError: None of <items of cls.CLsNewQuaIndics> exists in index
-        # whereas the same code works in devarchive2.ipynb/Development : Optimise _postComputeQualityIndicators).
-        # self._dfData[cls.CLsNewQuaIndics] = np.stack(cls._combinedQualityAll(dfCompData.values), axis=1)
-
-    # Post computations : Truncations groups.
-    @staticmethod
-    def _groupingIntervals(sValues, minDist, maxLen, epsilon=1e-6):
-
-        """Build a list of value grouping intervals from a series of values
-        taking care of min distance and max length constraints
-
-        Parameters:
-        :param sValues: pd.Series of values to group
-        :param minDist: minimal distance between consecutive intervals (left.max - right.min > minIntrvDist)
-        :param maxLen: max length of intervals (but ... see below)
-
-        TODO: Fix current implementation that actually does not produce intervals of max length maxLen, but 1.5*maxLen !
-
-        :return: pd.DataFrame of left-closed and right-open resulting intervals (columns = vmin, vsup)
-        """
-
-        # Cleanup and sort (ascending) distance series first
-        # (+ for some reason, need for enforcing float dtype ... otherwise dtype='O' !?)
-        sValues = sValues.dropna().astype(float).sort_values()
-
-        # If not a single cleaned up distance to examine, stop here.
-        if sValues.empty:
-            return pd.DataFrame()
-
-        # List non-null differences between consecutive sorted distances
-        dfIntrvs = pd.DataFrame(dict(v=sValues.values))
-        dfIntrvs['vdelta'] = dfIntrvs.v.diff()
-        dfIntrvs.loc[dfIntrvs.v.idxmin(), 'vdelta'] = np.inf
-        dfIntrvs.dropna(inplace=True)
-        dfIntrvs = dfIntrvs[dfIntrvs.vdelta > 0].copy()
-
-        # Deduce start (min) and end (sup) for each such interval (left-closed, right-open)
-        dfIntrvs['vmin'] = dfIntrvs.loc[dfIntrvs.vdelta > minDist, 'v']
-        dfIntrvs['vsup'] = dfIntrvs.loc[dfIntrvs.vdelta > minDist, 'v'].shift(-1).dropna()
-        dfIntrvs.loc[dfIntrvs['vmin'].idxmax(), 'vsup'] = np.inf
-        dfIntrvs.dropna(inplace=True)
-        dfIntrvs['vsup'] = dfIntrvs['vsup'].apply(lambda vs: sValues[sValues < vs].max() + epsilon)
-        dfIntrvs = dfIntrvs[['vmin', 'vsup']].reset_index(drop=True)
-
-        # If these intervals are two wide, cut them up in equal sub-intervals and make them new intervals
-        lsNewIntrvs = list()
-        for _, sIntrv in dfIntrvs.iterrows():
-
-            if sIntrv.vsup - sIntrv.vmin > maxLen:
-                # TODO: Well, this actually does not produce intervals of max length maxLen, but 1.5*maxLen !
-                nSubIntrvs = round((sIntrv.vsup - sIntrv.vmin) / maxLen)
-                subIntrvLen = (sIntrv.vsup - sIntrv.vmin) / nSubIntrvs
-                lsNewIntrvs += [pd.Series(dict(vmin=sIntrv.vmin + nInd * subIntrvLen,
-                                               vsup=min(sIntrv.vmin + (nInd + 1) * subIntrvLen,
-                                                        sIntrv.vsup)))
-                                for nInd in range(nSubIntrvs)]
-            else:
-                lsNewIntrvs.append(sIntrv)
-
-        dfIntrvs = pd.DataFrame(lsNewIntrvs).reset_index(drop=True)
-        dfIntrvs.sort_values(by='vmin', inplace=True)
-
-        return dfIntrvs
-
-    @staticmethod
-    def _intervalIndex(value, dfIntervals):
-        """Compute the index of the interval to which a value belongs, if any
-        :return: 0 for NaN values, -1 for values that belong to no interval, from-1 interval index otherwise
-        """
-        if pd.isnull(value):
-            return 0
-        dfWhere = dfIntervals[(dfIntervals.vmin <= value) & (dfIntervals.vsup > value)]
-        if dfWhere.empty:
-            return -1
-        return 1 + dfWhere.index[0]
-
-    @classmethod
-    def _sampleDistTruncGroups(cls, dfSampRes, ldIntrvSpecs, intrvEpsilon=1e-6):
-
-        """Compute distance truncation groups for 1 sample, for each target distance truncation column"""
-
-        # For each truncation "method" (left or right)
-        dTruncGroups = dict()
-        for dIntrvSpecs in ldIntrvSpecs:
-
-            truncCol = cls.DCLParTruncDist[dIntrvSpecs['col']]
-            logger.info5(f'  - {truncCol[1]}')
-
-            # Compute distance grouping intervals
-            dfIntrvs = cls._groupingIntervals(sValues=dfSampRes[truncCol], minDist=dIntrvSpecs['minDist'],
-                                              maxLen=dIntrvSpecs['maxLen'], epsilon=intrvEpsilon)
-
-            # Deduce index of belonging interval for each distance
-            # (special case when no truncation: num=0 if NaN truncation distance)
-            dTruncGroups[dIntrvSpecs['col']] = dfSampRes[truncCol].apply(cls._intervalIndex, dfIntervals=dfIntrvs)
-
-        return dTruncGroups
-
-    def listSamples(self, rebuild=False):
-
-        """List result samples (if really needed of specified)"""
-
-        if rebuild or self.dfSamples is None:
-
-            miSampleCols = self.miSampleCols
-            if self.sampleIndCol not in miSampleCols:
-                miSampleCols = pd.MultiIndex.from_tuples([self.sampleIndCol]).append(miSampleCols)
-            self.dfSamples = self._dfData[miSampleCols]
-            self.dfSamples = self.dfSamples.drop_duplicates()
-            self.dfSamples.set_index(self.sampleIndCol, inplace=True)
-            self.dfSamples.sort_index(inplace=True)
-            assert len(self.dfSamples) == self.dfSamples.index.nunique()
-
-        return self.dfSamples
-
-    def _distTruncGroups(self):
-
-        """Compute distance truncation groups for all samples, for each target distance truncation column"""
-
-        # For each sample,
-        dTruncGroups = dict()  # key=ldIntrvSpecs[*]['col'], value=list(Series of group nums)
-        for lblSamp, sSamp in self.listSamples().iterrows():
-
-            # Select sample rows
-            dfSampRes = self._dfData.loc[self._dfData[self.sampleIndCol] == lblSamp]
-            logger.info3('#{} {} : {} rows'
-                          .format(lblSamp, ', '.join([f'{k[1]}={v}' for k, v in sSamp.items()]), len(dfSampRes)))
-
-            # Compute truncation groups for this sample
-            dSampTruncGroups = self._sampleDistTruncGroups(dfSampRes, ldIntrvSpecs=self.ldTruncIntrvSpecs,
-                                                           intrvEpsilon=self.truncIntrvEpsilon)
-
-            # Store them for later concatenation
-            for colAlias, sGrpNums in dSampTruncGroups.items():
-                if colAlias not in dTruncGroups:
-                    dTruncGroups[colAlias] = list()
-                dTruncGroups[colAlias].append(sGrpNums)
-
-        # Concat series of computed group nums (opt or not) for each target distance column to group
-        return {colAlias: pd.concat(lsGrpNums) for colAlias, lsGrpNums in dTruncGroups.items()}
-
-    DCLGroupTruncDist = dict(left=CLGroupTruncLeft, right=CLGroupTruncRight)
-
-    def _postComputeTruncationGroups(self):
-
-        """Compute and add truncation group columns for later filtering and sorting"""
-
-        logger.info2('Post-computing Truncation Groups')
-
-        # Compute distance truncation groups for all samples, for each target distance truncation column
-        # and update result table.
-        for colAlias, sGrpNums in self._distTruncGroups().items():
-            self._dfData[self.DCLGroupTruncDist[colAlias]] = sGrpNums
-
-    # Post computations : filtering and sorting.
-    # a. Column Labels for computed group and sort orders
-    #    N.B. "close truncations" means "identical = same truncations" here (but see MCDSOptanalyserResultsSet)
-    # See above
-
-    # b. Schemes for computing filtering and sorting keys (see inherited _postComputeFilterSortKeys).
-    AutoFilSorKeySchemes = \
-        [  # Orders inside groups with identical truncation params.
-         dict(key=CLGrpOrdSmTrAic,  # Best AIC, for same left and right truncations (but variable nb of cut points)
-              sort=[CLParTruncLeft, CLParTruncRight, CLDeltaAic, CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
-              ascend=[True, True, True, False, False, True, False, True],
-              group=[CLParTruncLeft, CLParTruncRight, CLParModFitDistCuts]),
-
-         # Orders inside groups of close truncation params.
-         # dict(key=CLGrpOrdClTrChi2,  # Best Chi2 inside groups of close truncation params
-         #      sort=[CLGroupTruncLeft, CLGroupTruncRight,
-         #            CLChi2],
-         #      ascend=[True, True, False],
-         #      group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         # dict(key=CLGrpOrdClTrKS,  # Best KS inside groups of close truncation params
-         #      sort=[CLGroupTruncLeft, CLGroupTruncRight,
-         #            CLKS],
-         #      ascend=[True, True, False],
-         #      group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrDCv,  # Best DCv inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLDCv],
-              ascend=[True, True, True],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrChi2KSDCv,  # Best Chi2 & KS & DCv inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight, CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
-              ascend=[True, True, False, False, True, False, True],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-
-         dict(key=CLGrpOrdClTrQuaBal1,  # Best Combined Quality 1 inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLCmbQuaBal1],
-              ascend=[True, True, False],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrQuaBal2,  # Best Combined Quality 2 inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLCmbQuaBal2],
-              ascend=[True, True, False],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrQuaBal3,  # Best Combined Quality 3 inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLCmbQuaBal3],
-              ascend=[True, True, False],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrQuaChi2,  # Best Qualité combinée Chi2+ inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLCmbQuaChi2],
-              ascend=[True, True, False],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrQuaKS,  # Best Combined Quality KS+ inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLCmbQuaKS],
-              ascend=[True, True, False],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-         dict(key=CLGrpOrdClTrQuaDCv,  # Best Combined Quality DCv+ inside groups of close truncation params
-              sort=[CLGroupTruncLeft, CLGroupTruncRight,
-                    CLCmbQuaDCv],
-              ascend=[True, True, False],
-              group=[CLGroupTruncLeft, CLGroupTruncRight]),
-
-         # Global orders (no grouping by close or identical truncations)
-         dict(key=CLGblOrdChi2KSDCv,
-              sort=[CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
-              ascend=[False, False, True, False, True]),
-         dict(key=CLGblOrdQuaBal1,
-              sort=[CLCmbQuaBal1], ascend=False),
-         dict(key=CLGblOrdQuaBal2,
-              sort=[CLCmbQuaBal2], ascend=False),
-         dict(key=CLGblOrdQuaBal3,
-              sort=[CLCmbQuaBal3], ascend=False),
-         dict(key=CLGblOrdQuaChi2,
-              sort=[CLCmbQuaChi2], ascend=False),
-         dict(key=CLGblOrdQuaKS,
-              sort=[CLCmbQuaKS], ascend=False),
-         dict(key=CLGblOrdQuaDCv,
-              sort=[CLCmbQuaDCv], ascend=False),
-
-         dict(key=CLGblOrdDAicChi2KSDCv,
-              sort=[CLParTruncLeft, CLParTruncRight, CLParModFitDistCuts,
-                    CLDeltaAic, CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
-              ascend=[True, True, True, True, False, False, True, False, True], napos='first')]
-
-    # Enforce uniqueness of keys in filter and sort key specs.
-    assert len(AutoFilSorKeySchemes) == len(set(scheme['key'] for scheme in AutoFilSorKeySchemes)), \
-           'Duplicated scheme key in MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
-
-    # Enforce uniqueness of sort and group column in filter and sort key specs.
-    assert all(len(scheme['sort']) == len(set(scheme['sort'])) for scheme in AutoFilSorKeySchemes), \
-           'Duplicated sort column spec in some scheme of MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
-    assert all(len(scheme.get('group', [])) == len(set(scheme.get('group', []))) for scheme in AutoFilSorKeySchemes), \
-           'Duplicated group column spec in some scheme of MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
-
-    # Check sort vs ascend list lengths in filter and sort key specs.
-    assert all(isinstance(scheme['ascend'], bool) or len(scheme['ascend']) == len(scheme['sort'])
-               for scheme in AutoFilSorKeySchemes), \
-           'Inconsistent ascend vs sort specs in some scheme of MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
-
-    # c. Computation of filter and sort keys
-
-    # Make results cell values hashable (needed for sorting in _postComputeFilterSortKeys)
-    @staticmethod
-    def _toSortable(value):
-
-        if isinstance(value, list):
-            return len(value)
-        elif isinstance(value, (int, float, str)) or pd.isnull(value):
-            return value
-        else:
-            raise NotImplementedError
-
-    DCLUnsortableCols = \
-        {CLParModFitDistCuts: (CLParModFitDistCuts[0], CLParModFitDistCuts[1], 'Sortable'),
-         CLParModDiscrDistCuts: (CLParModDiscrDistCuts[0], CLParModDiscrDistCuts[1], 'Sortable')}
-
-    # Make results cell values hashable (needed for grouping in _postComputeFilterSortKeys)
-    @staticmethod
-    def _toHashable(value):
-    
-        if isinstance(value, list):
-            return ','.join(str(v) for v in value)
-        elif isinstance(value, (int, float, str)) or pd.isnull(value):
-            return value
-        else:
-            return str(value)
-
-    DCLUnhashableCols = \
-        {CLParModFitDistCuts: (CLParModFitDistCuts[0], CLParModFitDistCuts[1], 'Hashable'),
-         CLParModDiscrDistCuts: (CLParModDiscrDistCuts[0], CLParModDiscrDistCuts[1], 'Hashable')}
-
-    @classmethod
-    def _sampleFilterSortKeys(cls, dfSampRes, ldFilSorKeySchemes):
-
-        """Compute filter and sort keys for 1 sample, for each pre-defined scheme
-
-        Parameters:
-        :param ldFilSorKeySchemes: Filter-sort key generation schemes to use (ex. cls.AutoFilSorKeySchemes).
-        """
-
-        # Make a copy to avoid modifying source data.
-        dfSampRes = dfSampRes.copy()
-
-        # For each filter and sort scheme
-        dFilSorKeys = dict()
-        for scheme in ldFilSorKeySchemes:
-
-            logger.info4('* scheme {}'.format(scheme))
-
-            # Workaround to-be-sorted problematic columns.
-            sortCols = list()
-            for col in scheme['sort']:
-                if col in cls.DCLUnsortableCols:
-                    wkrndSortCol = cls.DCLUnsortableCols[col]
-                    logger.info5('{} => {}'.format(col, wkrndSortCol))
-                    dfSampRes[wkrndSortCol] = dfSampRes[col].apply(cls._toSortable)
-                    col = wkrndSortCol  # Will rather sort with this one !
-                sortCols.append(col)
-
-            # Sort results
-            dfSampRes.sort_values(by=sortCols, ascending=scheme['ascend'],
-                                  na_position=scheme.get('napos', 'last'), inplace=True)
-
-            # Compute order (target series is indexed like dfSampRes).
-            if 'group' in scheme:  # Partial = 'group' order.
-
-                # Workaround for to-be-grouped problematic columns.
-                groupCols = list()
-                for col in scheme['group']:
-                    if col in cls.DCLUnhashableCols:
-                        wkrndGroupCol = cls.DCLUnhashableCols[col]
-                        logger.info5('{} => {}'.format(col, wkrndGroupCol))
-                        dfSampRes[wkrndGroupCol] = dfSampRes[col].apply(cls._toHashable)
-                        col = wkrndGroupCol  # Will rather group with this one !
-                    groupCols.append(col)
-
-                sSampOrder = dfSampRes.groupby(groupCols, dropna=False).cumcount()
-
-            else:  # Global order.
-                sSampOrder = pd.Series(data=range(len(dfSampRes)), index=dfSampRes.index)
-
-            # Done: next scheme.
-            dFilSorKeys[scheme['key']] = sSampOrder
-
-        return dFilSorKeys
-
-    def _filterSortKeySchemes(self):
-        """Select filter and sort keys schemes to apply (predefined if not overriden at ctor time)"""
-        return self.ldFilSorKeySchemes or self.AutoFilSorKeySchemes
-
-    def _filterSortKeys(self):
-
-        """Compute filter and sort keys for all samples, for each scheme target key column"""
-
-        # Retrieve filter and sort keys schemes to apply.
-        ldFilSorKeySchemes = self._filterSortKeySchemes()
-
-        # For each sample,
-        dFilSorKeys = dict()  # key=<target column for key>, value=<Series of key values>)
-        for lblSamp, sSamp in self.listSamples().iterrows():
-
-            # Select sample rows
-            dfSampRes = self._dfData.loc[self._dfData[self.sampleIndCol] == lblSamp]
-            logger.info3('#{} {} : {} rows'
-                         .format(lblSamp, ', '.join([f'{k[1]}={v}' for k, v in sSamp.items()]), len(dfSampRes)))
-
-            # Compute key values for all schemes for this sample
-            dSampKeys = self._sampleFilterSortKeys(dfSampRes, ldFilSorKeySchemes)
-
-            # Store them for later concatenation
-            for colLbl, sFSKeys in dSampKeys.items():
-                if colLbl not in dFilSorKeys:
-                    dFilSorKeys[colLbl] = list()  # list(Series)
-                dFilSorKeys[colLbl].append(sFSKeys)
-
-        # Concat series of computed group nums (opt or not) for each target distance column to group
-        return {colLbl: pd.concat(lsFSKeys) for colLbl, lsFSKeys in dFilSorKeys.items()}
-
-    def _postComputeFilterSortKeys(self):
-
-        """Compute and add partial or global order columns for later filtering and sorting"""
-
-        logger.info2('Post-computing Filter and Sort keys')
-
-        # Compute keys for each sample and add relevant columns to results.
-        for colLbl, sFSKeys in self._filterSortKeys().items():
-            self._dfData[colLbl] = sFSKeys
-
-    # Post-computations : All of them.
-    def postComputeColumns(self):
-        
-        self._postComputeChi2()
-        self._postComputeDeltaAicDCv()
-        self._postComputeQualityIndicators()
-        self._postComputeTruncationGroups()
-        self._postComputeFilterSortKeys()
-
-    # Tools for actually filtering results
-    @classmethod
-    def _indexOfDuplicates(cls, dfRes, keep='first', subset=list(), round2decs=dict()):
-
-        """Compute the indices of duplicates to remove in a data-frame,
-        keep=first means that the first item of an "equality" set is the one kept at the end
-        Warning: No sorting done here => do it before to get the right first !
-        """
-        if round2decs:
-            # dfRes = dfRes.round(round2decs)  # Buggy (pandas 1.0.x up to 1.1.2): forgets columns !?!?!?
-            if len(subset) > 0:
-                pass  # TODO: Optimise = only copy subset cols
-            dfRes = dfRes.copy()
-            for col, dec in round2decs.items():
-                if len(subset) == 0 or col in subset:
-                    dfRes[col] = dfRes[col].apply(lambda x: x if pd.isnull(x) else round(x, ndigits=dec))
-
-        return dfRes[dfRes.duplicated(keep=keep, subset=subset)].index
-
-    @classmethod
-    def _indexOfWorstOneCriterion(cls, dfRes, sampleIds, sampleIdCol, critCol, ascendCrit=True, nTgtRes=10):
-
-        """Filtering function enforcing a target number of best output results based on 1 given criterion.
-        ascendCrit=True means that the best criterion values are the smallest ones
-        """
-
-        i2Drop = pd.Index([], dtype=dfRes.index.dtype)
-
-        # For each sample to filter ...
-        for sampId in sampleIds:
-            # Extract sample results, and sort them  based on the criterion.
-            dfSampRes = dfRes[dfRes[sampleIdCol] == sampId].sort_values(by=critCol, ascending=ascendCrit)
-
-            # Done for this sample.
-            i2Drop = i2Drop.append(dfSampRes.index[nTgtRes:])  # Will drop indices after the N best's ones
-
-        return i2Drop
-
-    @classmethod
-    def _indexOfWorstMultiOrderCriteria(cls, dfRes, critCols=list(), supCrit=1):
-
-        """Filtering function enforcing a max(sup) order for output results based on multiple given criterion
-        (results will be dropped if not in the supCrit best ones for at least 1 of the order criteria)
-        """
-
-        sb2keep = pd.Series(data=False, index=dfRes.index)
-        for critCol in critCols:
-            sb2keep |= (dfRes[critCol] < supCrit)
-
-        return dfRes[~sb2keep].index
-
-    def filSorSchemeId(self, scheme):
-
-        return self.filSorIdMgr.schemeId(scheme)
-
-    @classmethod
-    def _filterOnExecCode(cls, dfFilSorRes, filSorSteps, sampleIndCol, dupSubset, dDupRounds):
-
-        """Inplace filter out results based on exec code and truncation params duplicates
-
-        Details:
-        1. drop results with error ExecCode >= 3
-        2. drop results with identical truncation distances (keep best exec codes)
-
-        Note: This doesn't actually modifies a single bit of the results set, but returns the resulting
-              filtered and sorted index, suitable for indexing on self.dfData / dfTransData ...
-
-        Parameters:
-        :param dfFilSorRes: results table to update
-        :param filSorSteps: filter and sort step list to update
-        :param sampleIndCol: sample index = identification column
-        :param dupSubset: Subset of (3-level multi-index) columns for detecting duplicates (as a list of tuples)
-                          Warning: self.sampleIndCol is automatically prepended to this list if not already inside
-        :param dDupRounds: {col: nb decimals} => number of decimals to keep (after rounding)
-                           for a sub-set or all of dupSubset columns
-        """
-
-        # 1. Filter-out results obtained with some computation error (whatever sample).
-        dfFilSorRes.drop(dfFilSorRes[dfFilSorRes[cls.CLRunStatus] > MCDSEngine.RCWarnings].index, inplace=True)
-        stepId = 'run status'
-        filSorSteps.append(stepId, 'column', cls.CLRunStatus, transColumns=True)
-        filSorSteps.append(stepId, 'max value', MCDSEngine.RCWarnings)
-        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
-
-        # 2. Filter-out results which are duplicates with respect to sample and truncation distances,
-        # keeping best run status code first.
-        dfFilSorRes.sort_values(by=[sampleIndCol, cls.CLParTruncLeft, cls.CLParTruncRight, cls.CLRunStatus],
-                                ascending=True, na_position='first', inplace=True)
-        if sampleIndCol not in dupSubset:
-            dupSubset = [sampleIndCol] + dupSubset
-        dfFilSorRes.drop(cls._indexOfDuplicates(dfFilSorRes, keep='first', subset=dupSubset, round2decs=dDupRounds),
-                         inplace=True)
-        stepId = 'duplicates on params'
-        filSorSteps.append(stepId, 'param. names', dupSubset, transColumns=True)
-        filSorSteps.append(stepId, 'param. precisions', ', '.join(filSorSteps.transColumns(miCol) + f': {nDec}'
-                                                                  for miCol, nDec in dDupRounds.items()))
-        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
-
-    LDupSubsetDef = [CLNObs, CLEffort, CLDeltaAic, CLChi2, CLKS, CLCvMUw, CLCvMCw, CLDCv,
-                     CLPDetec, CLPDetecMin, CLPDetecMax, CLDensity, CLDensityMin, CLDensityMax]
-    DDupRoundsDef = {CLDeltaAic: 1, CLChi2: 2, CLKS: 2, CLCvMUw: 2, CLCvMCw: 2, CLDCv: 2,
-                     CLPDetec: 3, CLPDetecMin: 3, CLPDetecMax: 3, CLDensity: 2, CLDensityMin: 2, CLDensityMax: 2}
-
-    @classmethod
-    def _sortOnQua(cls, dfFilSorRes, filSorSteps, sampleIndCol, whichQua=CLCmbQuaBal3, ascendQua=False):
-
-        """Inplace sort results per sample based on truncation distances and a customisable quality indicator
-
-        Parameters:
-        :param dfFilSorRes: results table to update
-        :param filSorSteps: filter and sort step list to update
-        :param sampleIndCol: sample index = identification column
-        :param whichQua: Quality indicator (not order of) column to use
-        :param ascendQua: if True, lower whichQua indicator values are better
-        """
-
-        sortCols = [cls.CLParTruncLeft, cls.CLParTruncRight, whichQua]
-        dfFilSorRes.sort_values(by=[sampleIndCol] + sortCols, ascending=[True, True, True, ascendQua],
-                                na_position='first', inplace=True)
-        stepId = 'final sorting'
-        filSorSteps.append(stepId, 'columns', sortCols, transColumns=True)
-        filSorSteps.append(stepId, 'lower last column is better ?', ascendQua)
-
-    def filterSortOnExecCode(self, schemeId, lang, whichFinalQua=CLCmbQuaBal3, ascFinalQua=False,
-                             dupSubset=LDupSubsetDef, dDupRounds=DDupRoundsDef):
-
-        """Minimal filter and sort scheme
-
-        Details:
-        1. drop results with error ExecCode >= 3
-        2. drop results with identical truncation distances (keep best exec codes)
-        3. sort on truncation distances and specified indicator
-
-        Note: This doesn't actually modifies a single bit of the results set, but returns the resulting
-              filtered and sorted index, suitable for indexing on self.dfData / dfTransData ...
-
-        Parameters:
-        :param schemeId: Scheme identification, for traceability
-        :param lang: Translation language, for traceability
-        :param whichFinalQua: Quality indicator (not order of) column to use for final sorting
-        :param ascFinalQua: if True, lower whichFinalQua indicator values are better
-        :param dupSubset: Subset of (3-level multi-index) columns for detecting duplicates (as a list of tuples)
-                          Warning: self.sampleIndCol is automatically prepended to this list if not already inside
-        :param dDupRounds: {col: nb decimals} => number of decimals to keep (after rounding)
-                           for a sub-set or all of dupSubset columns
-
-        :return: tuple(index of selected and sorted results, log of filter & sort steps accomplished)
-        """
-
-        cls = self
-
-        logger.debug(f'Filter and sort scheme "{schemeId}": Applying.')
-
-        filSorSteps = _FilterSortSteps(schemeId, resultsSet=self, lang=lang)
-
-        # 0. Retrieve results to filter and sort.
-        dfFilSorRes = self.getData(copy=True)
-        filSorSteps.append('before', 'datetime', pd.Timestamp.now())
-        filSorSteps.append('before', 'results', len(dfFilSorRes))
-
-        # 1&2. Filter-out results with some computation error, and also duplicates based on same truncation params.
-        self._filterOnExecCode(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
-                               dupSubset=dupSubset, dDupRounds=dDupRounds)
-
-        # 3. Final sorting : increasing order for truncation distances (and final quality indicator if needed).
-        self._sortOnQua(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
-                        whichQua=whichFinalQua, ascendQua=ascFinalQua)
-
-        # Done.
-        filSorSteps.append('after', 'results', len(dfFilSorRes))
-
-        return dfFilSorRes.index, filSorSteps
-
-    @classmethod
-    def _filterOnAicMultiQua(cls, dfFilSorRes, filSorSteps, sampleIndCol,
-                             minSightRate=90, nBestAicOrd=2,
-                             nBestMQuaOrd=1, whichBestMQuaOrd=[CLGrpOrdClTrChi2KSDCv, CLGrpOrdClTrQuaBal3],
-                             nFinalQua=10, whichFinalQua=CLCmbQuaBal3, ascFinalQua=False):
-
-        """Inplace filter out results based on a mostly customisable selection of quality indicators
-
-        Details:
-        1. Per sample and group of IDENTICAL left and right truncation distances,
-           keep only the results with CLGrpOrdSmTrAic < nBestAicOrd ;
-           Note: this is generally close to: the nBestAicOrd best AIC values
-            (and if equal, the best Chi2, KS, DCv, NObs, CodEx ... etc)
-           but may actually give more than nBestAicOrd rows per sample and group of ...
-        2. Per sample and group of close truncation distances (see _postComputeTruncationGroups),
-           keep only the results with a least one of the whichBestMQuaOrd indicator orders < nBestMQuaOrd ;
-           Note: this will probably give more than nBestMQuaOrd rows par sample and group of ...
-        3. Eliminate sighting rates < minSightRate,
-        4. Keep only the nFinalQua best results, with respect to whichFinalQua indicator
-           (ascendFinalQua=True meaning that lower values are better)
-
-        Parameters:
-        :param dfFilSorRes: results table to update
-        :param filSorSteps: filter and sort step list to update
-        :param sampleIndCol: sample index = identification column
-        :param minSightRate: Minimal observation rate (ratio of NTot Obs / NObs, seldom=1 because of dist. truncations)
-        :param nBestAicOrd: Upper limit (excluded) for CLGrpOrdSmTrAic quality indicator order
-                            (per sample and IDENTICAL truncation parameters)
-        :param nBestMQuaOrd: Upper limit (excluded) for whichBestMQuaOrd for quality indicator orders
-                             (keep results with at least 1 of them under the limit)
-        :param whichBestMQua: Quality indicator order columns to use for filtering best results per sample
-                              and close truncation distances
-        :param nFinalQua: Final nb of best whichFinalQua indicator (not order of) results to keep per sample
-        :param whichFinalQua: Quality indicator (not order of) column to use
-                              for final "best results per sample" selection
-        :param ascFinalQua: if True, lower whichFinalQua indicator values are better
-        """
-
-        # 1. Filter-out results with poorest AIC, per groups of same sample and IDENTICAL truncation distances.
-        stepId = 'best AIC'
-        filSorSteps.append(stepId, 'column', cls.CLGrpOrdSmTrAic, transColumns=True)
-        filSorSteps.append(stepId, 'upper limit (excluded)', nBestAicOrd)
-        dfFilSorRes.drop(dfFilSorRes[dfFilSorRes[cls.CLGrpOrdSmTrAic] >= nBestAicOrd].index,
-                         inplace=True)
-        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
-
-        # 2. Filter-out results not in N best ones for at least 1 in a specified set of quality indicator,
-        # per groups of same sample and CLOSE truncation distances.
-        stepId = 'best results for >= 1 indicator'
-        filSorSteps.append(stepId, 'selected indicator orders', whichBestMQuaOrd, transColumns=True)
-        filSorSteps.append(stepId, 'order limit (excluded) / indicator', nBestMQuaOrd)
-        i2Drop = cls._indexOfWorstMultiOrderCriteria(dfFilSorRes, critCols=whichBestMQuaOrd, supCrit=nBestMQuaOrd)
-        dfFilSorRes.drop(labels=i2Drop, inplace=True)
-        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
-
-        # 3. Filter-out results with insufficient considered sightings rate
-        #    (due to a small sample or truncations params).
-        stepId = 'non-outlier sightings'
-        filSorSteps.append(stepId, 'min sighting rate', minSightRate)
-        dfFilSorRes.drop(dfFilSorRes[dfFilSorRes[cls.CLSightRate] < minSightRate].index, inplace=True)
-        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
-
-        # 4. Filter-out eventually too numerous results, keeping only the N best ones
-        #    with respect to the specified quality indicator (real indicator, not relevant close trunc order).
-        stepId = 'final best results'
-        filSorSteps.append(stepId, 'column', whichFinalQua, transColumns=True)
-        filSorSteps.append(stepId, 'lower is better ?', ascFinalQua)
-        filSorSteps.append(stepId, 'max results', nFinalQua)
-        i2Drop = cls._indexOfWorstOneCriterion(dfFilSorRes, sampleIds=dfFilSorRes[sampleIndCol].unique(),
-                                               sampleIdCol=sampleIndCol, nTgtRes=nFinalQua,
-                                               critCol=whichFinalQua, ascendCrit=ascFinalQua)
-        dfFilSorRes.drop(labels=i2Drop, inplace=True)
-        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
-
-    def filterSortOnExCAicMulQua(self, schemeId, lang, sightRate=95, nBestAIC=2, nBestQua=1,
-                                 whichBestQua=[CLGrpOrdClTrChi2KSDCv, CLGrpOrdClTrDCv, CLGrpOrdClTrQuaBal1,
-                                               CLGrpOrdClTrQuaChi2, CLGrpOrdClTrQuaKS, CLGrpOrdClTrQuaDCv],
-                                 nFinalRes=10, whichFinalQua=CLCmbQuaBal3, ascFinalQua=False,
-                                 dupSubset=LDupSubsetDef, dDupRounds=DDupRoundsDef):
-
-        """Filter and sort scheme for selecting best results with respect to a set of quality indicators,
-        all taken at the same priority, except for one used to limit results at the end,
-        all of this after same filtering on exec code and truncation param duplicates as in filterSortOnExecCode
-
-        Details:
-        1. Same filtering as filterSortOnExecCode
-        2. Per sample and group of IDENTICAL left and right truncation distances,
-           keep only the results with CLGrpOrdSmTrAic < nBestAIC ;
-           Note: this is generally close to: the nBestAIC best AIC values
-            (and if equal, the best Chi2, KS, DCv, NObs, CodEx ... etc)
-           but may actually give more than nBestAIC rows per sample and group of ...
-        3. Per sample and group of close truncation distances (see _postComputeTruncationGroups),
-           keep only the results with a least one of the whichBestQua indicator orders < nBestQua ;
-           Note: this will probably give more than nBestQua rows par sample and group of ...
-        4. Eliminate sighting rates < sightRate,
-        5. Keep only the nFinalRes best results, with respect to whichFinalQua indicator
-           (ascendFinalQua=True meaning that lower values are better)
-        6. Finally, sort by truncation distances (no truncation first, shorter distances first, ... simpler first)
-           and by best whichFinalQua indicator values
-
-        Note: This doesn't actually modifies a single bit of the results set, but returns the resulting
-              filtered and sorted index, suitable for indexing on self.dfData / dfTransData ...
-
-        Parameters:
-        :param schemeId: Scheme identification, for traceability
-        :param lang: Translation language, for traceability
-        :param sightRate: Minimal observation rate (ratio of NTot Obs / NObs, not 1 because of dist. truncations)
-        :param nBestAIC: Nb of best AIC results to keep per sample and IDENTICAL truncation parameters
-        :param nBestQua: Nb of best results to keep per sample with respect to each quality indicator specified
-                         through its related order column name in whichBestQua
-        :param whichBestQua: Quality indicator order columns to use for filtering best results per sample
-                             (at most nBestQua best results are kept for each related indicator ;
-                              to be retained, a result MUST be among the nBestQua best ones for ALL
-                              the specified indicators)
-        :param nFinalRes: Final nb of best whichFinalQua results to keep per sample
-        :param whichFinalQua: Quality indicator order column to use for final "best results per sample" selection
-        :param ascFinalQua: if True, lower whichFinalQua indicator values are better
-        :param dupSubset: Subset of (3-level multi-index) columns for detecting duplicates (as a list of tuples)
-                          Warning: self.sampleIndCol is automatically prepended to this list if not already inside
-        :param dDupRounds: {col: nb decimals} => number of decimals to keep (after rounding)
-                           for a sub-set or all of dupSubset columns
-
-        :return: tuple(index of selected and sorted results, log of filter & sort steps accomplished)
-        """
-
-        logger.debug(f'Filter and sort scheme "{schemeId}": Applying.')
-
-        filSorSteps = _FilterSortSteps(schemeId, resultsSet=self, lang=lang)
-
-        # 0. Retrieve results to filter and sort.
-        dfFilSorRes = self.getData(copy=True)
-        filSorSteps.append('before', 'datetime', pd.Timestamp.now())
-        filSorSteps.append('before', 'results', len(dfFilSorRes))
-
-        # 1. Filter-out results with some computation error, and also duplicates based on same truncation params.
-        self._filterOnExecCode(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
-                               dupSubset=dupSubset, dDupRounds=dDupRounds)
-
-        # 2. Filter-out results with poorest AIC, per groups of same sample and IDENTICAL truncation distances.
-        # 3. Filter-out results with poorest values of specified quality indicators,
-        #    per groups of same sample and CLOSE truncation distances.
-        # 4. Filter-out results with insufficient considered sightings rate
-        #    (due to a small sample or truncations params).
-        # 5. Filter-out eventually too numerous results, keeping only the N best ones
-        #    with respect to the specified quality indicator.
-        self._filterOnAicMultiQua(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
-                                  minSightRate=sightRate, nBestAicOrd=nBestAIC,
-                                  nBestMQuaOrd=nBestQua, whichBestMQuaOrd=whichBestQua,
-                                  nFinalQua=nFinalRes, whichFinalQua=whichFinalQua, ascFinalQua=ascFinalQua)
-
-        # 6. Final sorting : increasing order for truncation distances (and final quality indicator if needed).
-        self._sortOnQua(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
-                        whichQua=whichFinalQua, ascendQua=ascFinalQua)
-
-        # Done.
-        filSorSteps.append('after', 'results', len(dfFilSorRes))
-
-        return dfFilSorRes.index, filSorSteps
-
-    def _preselColumn(self, srcCol):
-        """Pre-selection column label (from source column) and translation"""
-        return ((srcCol[0], srcCol[1], self.CLTPreSelection),
-                dict(fr='Pré-sélection ' + self.transColumn(srcCol, 'fr'),
-                     en='Pre-selection ' + self.transColumn(srcCol, 'en')))
-
-    def _addPreselColumns(self, dfFilSorRes, filSorSteps,
-                          preselCols=[CLCmbQuaBal1], preselAscend=True, preselThresh=[0.2], nSamplePreSels=5):
-
-        """Add (in-place) a pre-selection column to a filtered and sorted translated results table
-
-        Parameters:
-        :param dfFilSorRes: the filtered and sorted table to update
-        :param filSorSteps: the filtered and sorted step log to update
-        :param nSamplePreSels: Max number of generated pre-selections per sample
-        :param preselCols: Results columns to use for generating auto-preselection indices (in [1, nSamplePreSels])
-        :param preselAscend: Order to use for each column (list[bool]), or all (single bool) ;
-                             True means that lower values are "better" ones, and the other way round
-        :param preselThresh: Value above (ascend=True) or below () which no preselection is proposed
-                             (=> nan in target column), for each column (list[number]), or all (single number)
-
-        """
-
-        cls = self
-
-        if isinstance(preselAscend, bool):
-            preselAscend = [preselAscend] * len(preselCols)
-        assert len(preselCols) == len(preselAscend), \
-               'preselAscend must be a single bool or a list(bool) with len(preselCols)'
-
-        if isinstance(preselThresh, (int, float)):
-            preselThresh = [preselThresh] * len(preselCols)
-        assert len(preselCols) == len(preselThresh), \
-               'preselAscend must be a single number or a list(number) with len(preselCols)'
-
-        filSorSteps.append('auto-preselection', 'Nb of pre-selections', nSamplePreSels)
-
-        # Create each pre-selection column: rank per sample in preselCol/ascending (or not) order
-        # up to nSamplePreSels (but no preselection under / over threshold).
-        for srcCol, srcColAscend, srcColThresh in zip(preselCols, preselAscend, preselThresh):
-
-            # Determine label and translation.
-            tgtPreSelCol, dTgtPreSelColTrans = self._preselColumn(srcCol)
-            self.addColumnsTrans({tgtPreSelCol: dTgtPreSelColTrans})
-
-            filSorSteps.append('auto-preselection', 'pre-selection column', srcCol, transColumns=True)
-            filSorSteps.append('auto-preselection', 'lower is better ?', srcColAscend)
-            filSorSteps.append('auto-preselection', 'eliminating threshold', srcColThresh)
-
-            # Compute contents and add to table
-            # a. Rank all results
-            dfFilSorRes.insert(dfFilSorRes.columns.get_loc(srcCol), tgtPreSelCol,
-                               dfFilSorRes.groupby(self.miSampleCols.to_list())[[srcCol]]
-                                          .transform(lambda s: s.rank(ascending=srcColAscend,
-                                                                      method='dense', na_option='keep'))[srcCol])
-
-            # b. Nullify too big ranks and "under" threshold values
-            sbKillOnThresh = dfFilSorRes[srcCol] > srcColThresh if srcColAscend else dfFilSorRes[srcCol] < srcColThresh
-            sbKillOnNumber = dfFilSorRes[tgtPreSelCol] > nSamplePreSels
-            dfFilSorRes.loc[sbKillOnThresh | sbKillOnNumber, tgtPreSelCol] = np.nan
-
-        # Create final empty selection column (for the user to self-decide at the end)
-        # (right before the first added pre-selection column, no choice)
-        if len(preselCols) > 0:
-            dfFilSorRes.insert(dfFilSorRes.columns.get_loc(preselCols[0]) - 1, cls.CLFinalSelection, np.nan)
-
-        return dfFilSorRes
-
-    def dfFilSorData(self, scheme=dict(method=filterSortOnExecCode,
-                                       filterSort=dict(whichFinalQua=CLCmbQuaBal3, ascFinalQua=False),
-                                       preselCols=[CLCmbQuaBal3], preselAscs=False, preselThrhs=[0.2],
-                                       preselNum=5),
-                     columns=None, lang=None, rebuild=False):
-
-        """Extract filtered and sorted data following the given scheme
-
-        Note: Let R be MCDSAnalysisResultsSet, or a subclass (needed below).
-        
-        Parameters:
-        :param scheme: filter and sort scheme to apply
-                 as a dict(method= ResClass.filterSortOnXXX method to use,
-                           deduplicate= dict(dupSubset=, dDupRounds=) of deduplication params
-                               (if not or partially given, see RCLS.filterSortOnXXX defaults)
-                           filterSort= dict of other <method> params,
-                           preselCols= target columns for generating auto-preselection ones,
-                                       containing [1, preselNum] ranks ; default: []
-                           preselAscs= Rank direction to use for each column (list),
-                                       or all (single bool) ; default: True
-                                       (True means that lower values are "better" ones)
-                           preselThrhs= Eliminating threshold for each column (list),
-                                        or all (single number) ; default: 0.2
-                                        (eliminated above if preselAscs True, below otherwise)
-                           preselNum= number of (best) pre-selections to keep for each sample) ;
-                                      default: 5)
-                 examples: dict(method=R.filterSortOnExecCode,
-                                filterSort=dict(whichFinalQua=CLCmbQuaBal3, ascFinalQua=False),
-                                preselCols=[R.CLCmbQuaBal1, R.CLCmbQuaBal2], preselAscs=False,
-                                preselThrhs=0.2, preselNum=5),
-                           dict(method=R.filterSortOnExCAicMulQua,
-                                deduplicate=dict(dupSubset=[R.CLNObs, R.CLEffort, R.CLDeltaAic, R.CLChi2,
-                                                            R.CLKS, R.CLCvMUw, R.CLCvMCw, R.CLDCv]),
-                                                 dDupRounds={R.CLDeltaAic: 1, R.CLChi2: 2, R.CLKS: 2,
-                                                             R.CLCvMUw: 2, R.CLCvMCw: 2, R.CLDCv: 2})
-                                filterSort=dict(sightRate=92.5, nBestAIC=3, nBestQua=1, 
-                                                whichBestQua=[R.CLGrpOrdClTrChi2KSDCv, R.CLGrpOrdClTrDCv,
-                                                              R.CLGrpOrdClTrQuaBal1, R.CLGrpOrdClTrQuaChi2,
-                                                              R.CLGrpOrdClTrQuaKS, R.CLGrpOrdClTrQuaDCv],
-                                                nFinalRes=12, whichFinalQua=R.CLCmbQuaBal1, ascFinalQua=False),
-                                preselCols=[R.CLCmbQuaBal1, R.CLDCv], preselAscs=[False, True],
-                                preselThrhs=[0.2, 0.5], preselNum=3)
-        :param columns: Subset and order of columns to keep at the end (before translation) (None = [] = all)
-                        Warning: No need to specify here pre-selection and final selection columns,
-                                 as they'll be added automatically, and relocated at a non-customisable place.
-        :param lang: Target language for column name translation (if None, no translation => keep original names)
-        :param rebuild: If True, rebuild filtered and sorted table ; otherwise, simply reuse cached data
-               if the results set didn't change enough meanwhile.
-
-        :return: tuple(scheme id, result data-frame, log of completed filter and sort steps
-                       as a list of [schemeId, step name, property name, property value])
-        """
-
-        # Check (1/2) if need for applying scheme : needed if rebuild requested or post-computation needed.
-        isApplySchemeNeeded = rebuild or not self.postComputed
-        filSorSchId = self.filSorIdMgr.schemeId(scheme)
-
-        # If not needed, check (2/2) also if same scheme have been applied yet
-        if not isApplySchemeNeeded:
-            iFilSor, filSorSteps = self.filSorCache.get(filSorSchId)
-            isApplySchemeNeeded = iFilSor is None  # Not already applied => to apply now !
-
-        # Apply scheme if needed.
-        if isApplySchemeNeeded:
-
-            # Do it => index of filtered and sorted results + log of steps
-            iFilSor, filSorSteps = \
-                scheme['method'](self, schemeId=filSorSchId, lang=lang or 'en',
-                                 **scheme.get('filterSort', {}), **scheme.get('deduplicate', {}))
-
-            # Update cache
-            self.filSorCache.update(filSorSchId, iFilSor, filSorSteps)
-
-        # Actually extract filtered and sorted rows and selected columns.
-        dfFilSorRes = self.dfSubData(index=iFilSor, columns=columns, copy=True)
-
-        # Add the preselection column (and update filter and sort log)
-        if not dfFilSorRes.empty:
-            dfFilSorRes = self._addPreselColumns(dfFilSorRes, filSorSteps,
-                                                 nSamplePreSels=scheme.get('preselNum', 5),
-                                                 preselCols=scheme.get('preselCols', []),
-                                                 preselAscend=scheme.get('preselAscs', True),
-                                                 preselThresh=scheme.get('preselThrhs', 0.2))
-
-        # Final translation if specified.
-        if lang:
-            dfFilSorRes.columns = self.transColumns(dfFilSorRes.columns, lang)
-
-        # Done.
-        return filSorSchId, dfFilSorRes, filSorSteps.toList()
-
-
-class MCDSAnalyser(DSAnalyser):
-
-    """Run a bunch of MCDS analyses
-    """
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
-                 sampleDecCols=['Effort', 'Distance'], anlysSpecCustCols=[],
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], 
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
-                                    dict(col='right', minDist=25.0, maxLen=25.0)], truncIntrvEpsilon=1e-6,
-                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
-                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False, logProgressEvery=50,
-                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
-                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
-                 defMinDist=MCDSEngine.DistMinDef, defMaxDist=MCDSEngine.DistMaxDef, 
-                 defFitDistCuts=MCDSEngine.DistFitCutsDef, defDiscrDistCuts=MCDSEngine.DistDiscrCutsDef):
-
-        """Ctor
-
-        Parameters:
-        :param anlysSpecCustCols: Special columns from analysis specs to simply pass through in results
-        :param runTimeOut: time-out for every analysis run (s) (None => no limit)
-        :param runMethod: for calling MCDS engine executable : 'os.system' or 'subprocess.run'
-        :param ldTruncIntrvSpecs: separation and length specs for truncation group intervals computation
-                                  (postComputations for automated results filtering and sorting)
-        :param truncIntrvEpsilon: epsilon for truncation group intervals computation (idem)
-        """
-
-        assert distanceUnit == 'Meter', 'Not implemented: Only "Meter" distance unit supported for the moment'
-
-        super().__init__(dfMonoCatObs=dfMonoCatObs, dfTransects=dfTransects, 
-                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
-                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol,
-                         effortCol=effortCol, sampleSelCols=sampleSelCols,
-                         sampleDecCols=sampleDecCols, anlysSpecCustCols=anlysSpecCustCols,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         resultsHeadCols=resultsHeadCols, abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
-                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol, workDir=workDir)
-                         
-        assert logProgressEvery > 0, 'logProgressEvery must be positive'
-
-        self.surveyType = surveyType
-        self.distanceType = distanceType
-        self.clustering = clustering
-        
-        self.runMethod = runMethod
-        self.runTimeOut = runTimeOut
-        self.logData = logData
-        self.logProgressEvery = logProgressEvery
-
-        self.ldTruncIntrvSpecs = ldTruncIntrvSpecs
-        self.truncIntrvEpsilon = truncIntrvEpsilon
-        
-        self.defEstimKeyFn = defEstimKeyFn
-        self.defEstimAdjustFn = defEstimAdjustFn
-        self.defEstimCriterion = defEstimCriterion
-        self.defCVInterval = defCVInterval
-        self.defMinDist = defMinDist
-        self.defMaxDist = defMaxDist
-        self.defFitDistCuts = defFitDistCuts
-        self.defDiscrDistCuts = defDiscrDistCuts
-                         
-        # Specs.
-        self.updateSpecs(**{name: getattr(self, name)
-                            for name in ['runMethod', 'runTimeOut', 'surveyType', 'distanceType', 'clustering',
-                                         'defEstimKeyFn', 'defEstimAdjustFn', 'defEstimCriterion', 'defCVInterval',
-                                         'defMinDist', 'defMaxDist', 'defFitDistCuts', 'defDiscrDistCuts']})
-
-    # Analyser internal parameter spec names, for which a match should be found (when one is needed)
-    # with user explicit optimisation specs used in run() calls.
-    IntSpecEstimKeyFn = 'EstimKeyFn'
-    IntSpecEstimAdjustFn = 'EstimAdjustFn'
-    IntSpecEstimCriterion = 'EstimCriterion'
-    IntSpecCVInterval = 'CvInterval'
-    IntSpecMinDist = 'MinDist'  # Left truncation distance
-    IntSpecMaxDist = 'MaxDist'  # Right truncation distance
-    IntSpecFitDistCuts = 'FitDistCuts'
-    IntSpecDiscrDistCuts = 'DiscrDistCuts'
-
-    # Possible regexps (values) for auto-detection of analyser _internal_ parameter spec names (keys)
-    # from explicit _user_ spec columns
-    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
-    #  and case is ignored during searching).
-    Int2UserSpecREs = \
-        {IntSpecEstimKeyFn:     ['ke[a-z]*[\.\-_ ]*f', 'f[o]?n[a-z]*[\.\-_ ]*cl'],
-         IntSpecEstimAdjustFn:  ['ad[a-z]*[\.\-_ ]*s', 's[éa-z]*[\.\-_ ]*aj'],
-         IntSpecEstimCriterion: ['crit[èa-z]*[\.\-_ ]*'],
-         IntSpecCVInterval:     ['conf[a-z]*[\.\-_ ]*[a-z]*[\.\-_ ]*int',
-                                 'int[a-z]*[\.\-_ ]*conf'],
-         IntSpecMinDist:        ['min[a-z]*[\.\-_ ]*d', 'd[a-z]*[\.\-_ ]*min',
-                                 'tr[a-z]*[\.\-_ ]*g[ca]', 'le[a-z]*[\.\-_ ]*tr'],
-         IntSpecMaxDist:        ['max[a-z]*[\.\-_ ]*d', 'd[a-z]*[\.\-_ ]*max',
-                                 'tr[a-z]*[\.\-_ ]*d[rt]', 'le[a-z]*[\.\-_ ]*tr'],
-         IntSpecFitDistCuts:    ['fit[a-z]*[\.\-_ ]*d', 'tr[a-z]*[\.\-_ ]*[a-z]*[\.\-_ ]*mod'],
-         IntSpecDiscrDistCuts:  ['disc[a-z]*[\.\-_ ]*d', 'tr[a-z]*[\.\-_ ]*[a-z]*[\.\-_ ]*disc']}
-
-    # Analysis object ctor parameter names (MUST match exactly: check in analysis submodule !).
-    ParmEstimKeyFn = 'estimKeyFn'
-    ParmEstimAdjustFn = 'estimAdjustFn'
-    ParmEstimCriterion = 'estimCriterion'
-    ParmCVInterval = 'cvInterval'
-    ParmMinDist = 'minDist'
-    ParmMaxDist = 'maxDist'
-    ParmFitDistCuts = 'fitDistCuts'
-    ParmDiscrDistCuts = 'discrDistCuts'
-
-    def _getAnalysisParams(self, sAnIntSpec):
-    
-        """Retrieve analysis parameters, from user specs and default values
-        
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-        
-        :return: dict(estimKeyFn=, estimAdjustFn=, estimCriterion=, cvInterval=,
-                      minDist=, maxDist=, fitDistCuts=, discrDistCuts=)
-        """
-        return {self.ParmEstimKeyFn: sAnIntSpec.get(self.IntSpecEstimKeyFn, self.defEstimKeyFn),
-                self.ParmEstimAdjustFn: sAnIntSpec.get(self.IntSpecEstimAdjustFn, self.defEstimAdjustFn),
-                self.ParmEstimCriterion: sAnIntSpec.get(self.IntSpecEstimCriterion, self.defEstimCriterion),
-                self.ParmCVInterval: sAnIntSpec.get(self.IntSpecCVInterval, self.defCVInterval),
-                self.ParmMinDist: sAnIntSpec.get(self.IntSpecMinDist, self.defMinDist),
-                self.ParmMaxDist: sAnIntSpec.get(self.IntSpecMaxDist, self.defMaxDist),
-                self.ParmFitDistCuts: sAnIntSpec.get(self.IntSpecFitDistCuts, self.defFitDistCuts),
-                self.ParmDiscrDistCuts: sAnIntSpec.get(self.IntSpecDiscrDistCuts, self.defDiscrDistCuts)}
-
-    DAnlr2ResChapName = dict(before='header (head)', sample='header (sample)', after='header (tail)')
-
-    def prepareResultsColumns(self):
-        
-        # a. Sample multi-index columns
-        sampleSelCols = self.resultsHeadCols['sample']
-        sampMCols = [(self.DAnlr2ResChapName['sample'], col, 'Value') for col in sampleSelCols]
-        miSampCols = pd.MultiIndex.from_tuples(sampMCols)
-
-        # b. Full custom multi-index columns to append and prepend to raw analysis results
-        beforeCols = self.resultsHeadCols['before']
-        custMCols = [(self.DAnlr2ResChapName['before'], col, 'Value') for col in beforeCols]
-        custMCols += sampMCols
-        
-        afterCols = self.resultsHeadCols['after']
-        custMCols += [(self.DAnlr2ResChapName['after'], col, 'Value') for col in afterCols]
-
-        customCols = beforeCols + sampleSelCols + afterCols
-        miCustCols = pd.MultiIndex.from_tuples(custMCols)
-
-        # c. Translation for it (well, no translation actually ... only one language forced for all !)
-        dfCustColTrans = pd.DataFrame(index=miCustCols, data={lang: customCols for lang in ['fr', 'en']})
-
-        # d. The 3-columns index for the sample index column
-        sampIndMCol = (self.DAnlr2ResChapName[self.sampIndResHChap], self.sampleIndCol, 'Value')
-
-        # e. And finally, the result object (sorted at the end by the analysis or else sample index column)
-        if self.anlysIndCol or self.sampleIndCol:
-            sortCols = [next(mCol for mCol in custMCols if mCol[1] == self.anlysIndCol or self.sampleIndCol)]
-        else:
-            sortCols = []
-        sortAscend = [True] * len(sortCols)
-
-        return miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend
-
-    def setupResults(self, ldFilSorKeySchemes=None):
-    
-        """Build an empty results objects.
-
-        Parameters:
-        :param ldFilSorKeySchemes: Replacement for MCDSAnalysisResultsSet predefined filter-sort key schemes
-                                   None => use predefined ones MCDSAnalysisResultsSet.AutoFilSorKeySchemes.
-        """
-    
-        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \
-            self.prepareResultsColumns()
-        
-        return MCDSAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,
-                                      miSampleCols=miSampCols, sampleIndCol=sampIndMCol,
-                                      sortCols=sortCols, sortAscend=sortAscend,
-                                      distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                      surveyType=self.surveyType, distanceType=self.distanceType,
-                                      clustering=self.clustering,
-                                      ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
-                                      truncIntrvEpsilon=self.truncIntrvEpsilon,
-                                      ldFilSorKeySchemes=ldFilSorKeySchemes)
-    
-    def _getResults(self, dAnlyses):
-    
-        """Wait for and gather dAnalyses (MCDSAnalysis futures) results into a MCDSAnalysisResultsSet 
-        """
-    
-        # Start of elapsed time measurement (yes, starting the analyses may take some time, but it is
-        # negligible when compared to analysis time ; and better here for evaluating mean per analysis).
-        anlysStart = pd.Timestamp.now()
-        
-        # Create results container.
-        results = self.setupResults()
-
-        # For each analysis as it gets completed (first completed => first yielded)
-        nDone = 0
-        for anlysFut in self._executor.asCompleted(dAnlyses):
-            
-            # Retrieve analysis object from its associated future object
-            anlys = dAnlyses[anlysFut]
-            
-            # Get analysis results
-            sResult = anlys.getResults()
-
-            # Get custom header values, and set target index (= columns) for results
-            sCustomHead = anlys.customData
-            sCustomHead.index = results.miCustomCols
-
-            # Save results
-            results.append(sResult, sCustomHead=sCustomHead)
-
-            # Report elapsed time and number of analyses completed until now
-            # (once per self.logProgressEvery analyses though).
-            nDone += 1
-            if nDone % self.logProgressEvery == 0 or nDone == len(dAnlyses):
-                now = pd.Timestamp.now()
-                elapsedTilNow = now - anlysStart
-                if nDone < len(dAnlyses):
-                    expectedEnd = \
-                        now + pd.Timedelta(elapsedTilNow.value * (len(dAnlyses) - nDone) / nDone)
-                    expectedEnd = expectedEnd.strftime('%Y-%m-%d %H:%M:%S').replace(now.strftime('%Y-%m-%d '), '')
-                    endOfMsg = 'should end around ' + expectedEnd
-                else:
-                    endOfMsg = 'done'
-                logger.info1('{}/{} analyses in {} (mean {:.3f}s): {}.'
-                             .format(nDone, len(dAnlyses), str(elapsedTilNow.round('S')).replace('0 days ', ''),
-                                     elapsedTilNow.total_seconds() / nDone, endOfMsg))
-
-        # Terminate analysis executor
-        self._executor.shutdown()
-
-        # Terminate analysis engine
-        self._engine.shutdown()
-
-        return results
-
-    def run(self, dfExplParamSpecs=None, implParamSpecs=None, threads=None):
-    
-        """Run specified analyses
-        
-        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
-        
-        Parameters:
-        :param dfExplParamSpecs: Explicit MCDS analysis param specs, as a DataFrame
-          (generated through explicitVariantSpecs, as an example),
-        :param implParamSpecs: Implicit MCDS analysis param specs, suitable for explicitation
-          through explicitVariantSpecs
-        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
-        """
-    
-        # Executor (parallel or sequential).
-        self._executor = Executor(threads=threads)
-
-        # MCDS analysis engine
-        self._engine = MCDSEngine(workDir=self.workDir, executor=self._executor,
-                                  runMethod=self.runMethod, timeOut=self.runTimeOut,
-                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                  surveyType=self.surveyType, distanceType=self.distanceType,
-                                  clustering=self.clustering)
-
-        # Custom columns for results.
-        customCols = self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
-        
-        # Explicitate and complete analysis specs, and check for usability
-        # (should be also done before calling run, to avoid failure).
-        dfExplParamSpecs, userParamSpecCols, intParamSpecCols, _, checkVerdict, checkErrors = \
-            self.explicitParamSpecs(implParamSpecs, dfExplParamSpecs, dropDupes=True, check=True)
-        assert checkVerdict, 'Analysis params check failed: {}'.format('; '.join(checkErrors))
-        
-        # For each analysis to run :
-        exptdWorkers = self._executor.expectedWorkers()
-        runHow = 'in sequence' if exptdWorkers <= 1 else f'at most {exptdWorkers} parallel threads'
-        logger.info('Running {} MCDS analyses ({}) ...'.format(len(dfExplParamSpecs), runHow))
-        dAnlyses = dict()
-        for anInd, sAnSpec in dfExplParamSpecs.iterrows():
-            
-            logger.info1(f'#{anInd+1}/{len(dfExplParamSpecs)} : {sAnSpec[self.abbrevCol]}')
-
-            # Select data sample to process
-            sds = self._mcDataSet.sampleDataSet(sAnSpec[self.sampleSelCols])
-            if not sds:
-                continue
-
-            # Build analysis params specs series with parameters internal names.
-            sAnIntSpec = sAnSpec[userParamSpecCols].set_axis(intParamSpecCols, inplace=False)
-            
-            # Get analysis parameters from user specs and default values.
-            dAnlysParams = self._getAnalysisParams(sAnIntSpec)
-            
-            # Analysis object
-            logger.debug('Anlys params: {}'.format(', '.join(f'{k}:{v}' for k, v in dAnlysParams.items())))
-            anlys = MCDSAnalysis(engine=self._engine, sampleDataSet=sds, name=sAnSpec[self.abbrevCol],
-                                 customData=sAnSpec[customCols].copy(), logData=self.logData, **dAnlysParams)
-
-            # Start running pre-analysis in parallel, but don't wait for it's finished, go on
-            anlysFut = anlys.submit()
-            
-            # Store pre-analysis object and associated "future" for later use (should be running soon or later).
-            dAnlyses[anlysFut] = anlys
-            
-            # Next analysis (loop).
-
-        logger.info('All analyses started ; now waiting for their end, and results ...')
-
-        # Wait for and gather results of all analyses.
-        self.results = self._getResults(dAnlyses)
-
-        # Set results specs for traceability.
-        self.results.updateSpecs(analyses=dfExplParamSpecs, analyser=self.flatSpecs(),
-                                 runtime=pd.Series(runtime, name='Version'))
-        
-        # Done.
-        logger.info(f'Analyses completed ({len(self.results)} results).')
-
-        return self.results
-
-
-class MCDSPreAnalysisResultsSet(MCDSAnalysisResultsSet):
-
-    """A specialized results set for MCDS pre-analyses
-    (simpler post-computations that base class MCDSAnalysisResultsSet)
-
-    TODO: Should obviously rather be the base class for MCDSAnalysisResultsSet !
-    """
-    
-    # Computed columns specs (name translation + position).
-    Super = MCDSAnalysisResultsSet
-    _firstResColInd = len(MCDSEngine.statSampCols()) + len(MCDSAnalysis.MIRunColumns)
-    DComputedCols = {Super.CLSightRate: _firstResColInd + 10,  # After Encounter Rate / Left|Right Trunc. Dist.
-                     Super.CLDeltaAic: _firstResColInd + 12,  # Before AIC
-                     Super.CLChi2: _firstResColInd + 14,  # Before all Chi2 tests
-                     Super.CLDeltaDCv: _firstResColInd + 72,  # Before Density of animals / Cv
-                     # And, at the end ...
-                     **{cl: -1 for cl in [Super.CLCmbQuaBal1, Super.CLCmbQuaBal2, Super.CLCmbQuaBal3,
-                                          Super.CLCmbQuaChi2, Super.CLCmbQuaKS, Super.CLCmbQuaDCv]}}
-
-    DfComputedColTrans = \
-        pd.DataFrame(index=DComputedCols.keys(),
-                     data=dict(en=['Obs Rate', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Density',
-                                   'Qual Bal 1', 'Qual Bal 2', 'Qual Bal 3',
-                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+'],
-                               fr=['Taux Obs', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Densité',
-                                   'Qual Equi 1', 'Qual Equi 2', 'Qual Equi 3',
-                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+']))
-
-    # Needed presence in base class, but use inhibited.
-    CLFinalSelection = None
-
-    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,
-                 sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False):
-        
-        """
-        Parameters:
-        :param miSampleCols: columns to use for grouping by sample ; defaults to miCustomCols if None
-        :param sampleIndCol: multi-column index for the sample Id column ; no default, must be there !
-        """
-
-        # Initialise base.
-        super().__init__(miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
-                         miSampleCols=miSampleCols, sampleIndCol=sampleIndCol,
-                         sortCols=sortCols, sortAscend=sortAscend,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         surveyType=surveyType, distanceType=distanceType, clustering=clustering)
-
-    def copy(self, withData=True):
-        """Clone function, with optional data copy"""
-
-        # Create new instance with same ctor params.
-        clone = MCDSPreAnalysisResultsSet(miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
-                                          miSampleCols=self.miSampleCols, sampleIndCol=self.sampleIndCol,
-                                          sortCols=self.sortCols, sortAscend=self.sortAscend,
-                                          distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                          surveyType=self.surveyType, distanceType=self.distanceType,
-                                          clustering=self.clustering)
-
-        # Copy data if needed.
-        if withData:
-            clone._dfData = self._dfData.copy()
-            clone.rightColOrder = self.rightColOrder
-            clone.postComputed = self.postComputed
-            clone.dfSamples = None if self.dfSamples is None else self.dfSamples.copy()
-            clone.filSorIdMgr = self.filSorIdMgr.copy()
-            clone.filSorCache = self.filSorCache.copy()
-
-        return clone
-
-    def postComputeColumns(self):
-        """Post-computation: no need for filter and sort stuff"""
-        self._postComputeChi2()
-        self._postComputeDeltaAicDCv()
-        self._postComputeQualityIndicators()
-
-
-# Default strategy for pre-analyses model choice sequence (if one fails, take next in order, and so on)
-ModelEstimCritDef = 'AIC'
-ModelCVIntervalDef = 95
-ModelStrategyDef = [dict(keyFn=kf, adjSr='COSINE', estCrit=ModelEstimCritDef, cvInt=ModelCVIntervalDef)
-                    for kf in ['HNORMAL', 'HAZARD', 'UNIFORM', 'NEXPON']]
-
-
-class MCDSPreAnalyser(MCDSAnalyser):
-
-    """MCDSPreAnalyser: Run a bunch of MCDS pre-analyses
-    """
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(),
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], sampleDecCols=['Effort', 'Distance'],
-                 sampleSpecCustCols=[], abbrevCol='SampAbbrev', abbrevBuilder=None, sampleIndCol='SampleNum',
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 resultsHeadCols=dict(before=['SampleNum'], after=['SampleAbbrev'], 
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False, logProgressEvery=5):
-
-        super().__init__(dfMonoCatObs=dfMonoCatObs, dfTransects=dfTransects,
-                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
-                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
-                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
-                         anlysSpecCustCols=sampleSpecCustCols,
-                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder, sampleIndCol=sampleIndCol,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
-                         resultsHeadCols=resultsHeadCols, anlysIndCol=None, 
-                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut, logData=logData,
-                         logProgressEvery=logProgressEvery)
-
-        assert runTimeOut is None or runMethod != 'os.system', \
-               f"Can't care about {runTimeOut}s execution time limit with os.system run method (not implemented)"
-
-    def setupResults(self):
-    
-        """Build an empty results objects.
-        """
-    
-        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \
-            self.prepareResultsColumns()
-        
-        return MCDSPreAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,
-                                         miSampleCols=miSampCols, sampleIndCol=sampIndMCol,
-                                         sortCols=sortCols, sortAscend=sortAscend,
-                                         distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                         surveyType=self.surveyType, distanceType=self.distanceType,
-                                         clustering=self.clustering)
-    
-    def run(self, dfExplSampleSpecs=None, implSampleSpecs=None, dModelStrategy=ModelStrategyDef, threads=None):
-    
-        """Run specified analyses
-        
-        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
-
-        Parameters:
-        :param dfExplSampleSpecs: Explicit sample specs, as a DataFrame
-          (generated through explicitVariantSpecs, as an example),
-        :param implSampleSpecs: Implicit sample specs, suitable for explicitation
-          through explicitVariantSpecs
-        :param dModelStrategy: Sequence of fallback models to use when analyses fails.
-        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
-        """
-    
-        # Executor (parallel or sequential).
-        self._executor = Executor(threads=threads)
-
-        # MCDS analysis engine (a sequential one: 'cause MCDSPreAnalysis does the parallel stuff itself).
-        # Failed try: Seems we can't stack ThreadPoolExecutors, as pre-analyses get run sequentially
-        #             when using an Executor(threads=1) (means async) for self._engine ... 
-        # engineExor = None if self.runMethod != 'os.system' or self.runTimeOut is None else Executor(threads=1)
-        self._engine = MCDSEngine(workDir=self.workDir,  # executor=engineExor,
-                                  runMethod=self.runMethod, timeOut=self.runTimeOut,
-                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                  surveyType=self.surveyType, distanceType=self.distanceType,
-                                  clustering=self.clustering)
-
-        # Custom columns for results.
-        customCols = \
-            self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
-        
-        # Explicitate and complete analysis specs, and check for usability
-        # (should be also done before calling run, to avoid failure).
-        dfExplSampleSpecs, _, _, _, checkVerdict, checkErrors = \
-            self.explicitParamSpecs(implSampleSpecs, dfExplSampleSpecs, dropDupes=True, check=True)
-        assert checkVerdict, 'Pre-analysis params check failed: {}'.format('; '.join(checkErrors))
-        
-        # For each sample to analyse :
-        exptdWorkers = self._executor.expectedWorkers()
-        runHow = 'in sequence' if exptdWorkers <= 1 else f'at most {exptdWorkers} parallel threads'
-        logger.info('Running {} MCDS pre-analyses ({}) ...'.format(len(dfExplSampleSpecs), runHow))
-        
-        dAnlyses = dict()
-        for sampInd, sSampSpec in dfExplSampleSpecs.iterrows():
-            
-            logger.info1(f'#{sampInd+1}/{len(dfExplSampleSpecs)} : {sSampSpec[self.abbrevCol]}')
-
-            # Select data sample to process
-            sds = self._mcDataSet.sampleDataSet(sSampSpec[self.sampleSelCols])
-            if not sds:
-                continue  # No data => no analysis.
-
-            # Pre-analysis object
-            anlys = MCDSPreAnalysis(engine=self._engine, executor=self._executor,
-                                    sampleDataSet=sds, name=sSampSpec[self.abbrevCol],
-                                    customData=sSampSpec[customCols].copy(),
-                                    logData=False, modelStrategy=dModelStrategy)
-
-            # Start running pre-analysis in parallel, but don't wait for it's finished, go on
-            anlysFut = anlys.submit()
-            
-            # Store pre-analysis object and associated "future" for later use (should be running soon or later).
-            dAnlyses[anlysFut] = anlys
-            
-            # Next analysis (loop).
-
-        logger.info('All analyses started ; now waiting for their end, and results ...')
-
-        # Wait for and gather results of all analyses.
-        self.results = self._getResults(dAnlyses)
-
-        # Set results specs for traceability.
-        self.results.updateSpecs(samples=dfExplSampleSpecs, models=pd.DataFrame(dModelStrategy),
-                                 analyser=self.flatSpecs(), runtime=pd.Series(runtime, name='Version'))
-        
-        # Done.
-        logger.info('Analyses completed.')
-
-        return self.results
-
-    def exportDSInputData(self, dfExplSampleSpecs=None, implSampleSpecs=None, format='Distance'):
-    
-        """Export specified data samples to the specified DS input format, for "manual" DS analyses
-        
-        Parameters:
-        :param dfExplSampleSpecs: Explicit sample specs, as a DataFrame
-          (generated through explicitVariantSpecs, as an example),
-        :param implSampleSpecs: Implicit sample specs, suitable for explicitation
-          through explicitVariantSpecs
-        :param format: output files format, only 'Distance' supported for the moment.
-        """
-    
-        assert format == 'Distance', 'Only Distance format supported for the moment'
-    
-        # MCDS analysis engine
-        self._engine = MCDSEngine(workDir=self.workDir, runMethod=self.runMethod, timeOut=self.runTimeOut,
-                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                  surveyType=self.surveyType, distanceType=self.distanceType,
-                                  clustering=self.clustering)
-
-        # Explicitate and complete analysis specs, and check for usability
-        # (should be also done before calling run, to avoid failure).
-        dfExplSampleSpecs, _, _, _, checkVerdict, checkErrors = \
-            self.explicitParamSpecs(implSampleSpecs, dfExplSampleSpecs, dropDupes=True, check=True)
-        assert checkVerdict, 'Sample specs check failed: {}'.format('; '.join(checkErrors))
-        
-        # For each sample to export:
-        logger.info('Exporting {} samples to {} format ...'.format(len(dfExplSampleSpecs), format))
-        logger.debug(dfExplSampleSpecs)
-
-        for sampInd, sSampSpec in dfExplSampleSpecs.iterrows():
-            
-            # Selection des données
-            sds = self._mcDataSet.sampleDataSet(sSampSpec[self.sampleSelCols])
-            if not sds:
-                logger.warning('#{}/{} => No data in {} sample, no file exported'
-                               .format(sampInd+1, len(dfExplSampleSpecs), sSampSpec[self.abbrevCol]))
-                continue
-
-            # Export to Distance
-            fpn = pl.Path(self.workDir) / '{}-dist.txt'.format(sSampSpec[self.abbrevCol])
-            fpn = self._engine.buildDistanceDataFile(sds, tgtFilePathName=fpn)
-
-            logger.info1('#{}/{} => {}'.format(sampInd+1, len(dfExplSampleSpecs), fpn.name))
-
-        # Done.
-        self._engine.shutdown()
-
-        logger.info(f'Done exporting.')
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "analyser": Run a bunch of DS analyses according to a user-friendly set of analysis specs
+
+import copy
+import re
+import pathlib as pl
+from packaging import version
+
+import numpy as np
+import pandas as pd
+
+from . import log, runtime
+from .data import MonoCategoryDataSet, ResultsSet
+from .executor import Executor
+from .engine import MCDSEngine
+from .analysis import DSAnalysis, MCDSAnalysis, MCDSPreAnalysis
+
+logger = log.logger('ads.anr')
+
+
+class AnalysisResultsSet(ResultsSet):
+    
+    """
+    A result set for multiple analyses from the same engine.
+    
+    Internal columns index is a 3-level multi-index.
+    """
+    
+    def __init__(self, analysisClass, miCustomCols=None, dfCustomColTrans=None,
+                 dComputedCols=None, dfComputedColTrans=None, sortCols=[], sortAscend=[]):
+        
+        assert issubclass(analysisClass, DSAnalysis), 'analysisClass must derive from DSAnalysis'
+        assert miCustomCols is None \
+               or (isinstance(miCustomCols, pd.MultiIndex) and len(miCustomCols.levels) == 3), \
+               'customCols must be None or a 3 level pd.MultiIndex'
+        
+        self.analysisClass = analysisClass
+        self.engineClass = analysisClass.EngineClass
+
+        # 3-level multi-index columns (module, statistic, figure) for analyses output
+        miCols = \
+            self.engineClass.statSampCols().append(analysisClass.MIRunColumns).append(self.engineClass.statModCols())
+        
+        # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
+        dfColTrans = pd.concat([self.engineClass.statSampColTrans(), analysisClass.DfRunColumnTrans,
+                                self.engineClass.statModColTrans()])
+        
+        super().__init__(miCols=miCols, dfColTrans=dfColTrans,
+                         miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
+                         dComputedCols=dComputedCols, dfComputedColTrans=dfComputedColTrans,
+                         sortCols=sortCols, sortAscend=sortAscend)
+    
+    def copy(self, withData=True):
+    
+        """Clone function (shallow), with optional (deep) data copy"""
+    
+        # 1. Call ctor without computed columns stuff (we no more have initial data)
+        clone = AnalysisResultsSet(analysisClass=self.analysisClass,
+                                   miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
+                                   sortCols=[], sortAscend=[])
+    
+        # 2. Complete clone initialisation.
+        # 3-level multi-index columns (module, statistic, figure)
+        clone.miCols = self.miCols
+        clone.computedCols = self.computedCols
+        
+        # DataFrames for translating 3-level multi-index columns to 1 level lang-translated columns
+        clone.dfColTrans = self.dfColTrans
+        
+        # Copy data if needed.
+        if withData:
+            clone._dfData = self._dfData.copy()
+            clone.rightColOrder = self.rightColOrder
+            clone.postComputed = self.postComputed
+
+        return clone
+                
+
+class Analyser(object):
+
+    """Tools for building analysis variants specifications and explicitating them.
+    
+    Abstract base class for DS analysers.
+    """
+
+    def __init__(self):
+
+        # Computation specifications, for traceability only.
+        # For gathering copies of computations default parameter values, and stuff like that.
+        self.specs = dict()
+
+    def updateSpecs(self, reset=False, overwrite=False, **specs):
+
+        if reset:
+            self.specs.clear()
+
+        if not overwrite:
+            assert all(name not in self.specs for name in specs), \
+                   "Won't overwrite already present specs {}" \
+                   .format(', '.join(name for name in specs if name in self.specs))
+
+        self.specs.update(specs)
+
+    def flatSpecs(self):
+
+        # Flatten "in-line" 2nd level dicts if any (with 1st level name prefixing).
+        dFlatSpecs = dict()
+        for name, value in self.specs.items():
+            if isinstance(value, dict):
+                for n, v in value.items():
+                    dFlatSpecs[name + n[0].upper() + n[1:]] = v
+            else:
+                dFlatSpecs[name] = value
+
+        # Done.
+        return pd.Series(dFlatSpecs, name='Value')
+
+    # Generation of a table of implicit "partial variant" specification,
+    # from a list of possible data selection criteria for each variable.
+    # "Partial variant" because it's only about a sub-set of variants
+    # * dVariants : { target columns name: list of possibles criteria for data selection }
+    @staticmethod
+    def implicitPartialVariantSpecs(dVariants):
+        
+        def fixedLengthList(toComplete, length):
+            return toComplete + [np.nan]*(length - len(toComplete))
+
+        nRows = max(len(crits) for crits in dVariants.values())
+
+        return pd.DataFrame({colName: fixedLengthList(variants, nRows) for colName, variants in dVariants.items()})
+
+    @staticmethod
+    def _dropCommentColumns(dfSpecs):
+    
+        """Drop (in-place) comment columns from a spec dataframe (implicit or explicit)
+        """
+        
+        cols2drop = [col for col in dfSpecs.columns if not col.strip() or col.startswith('Unnamed:')
+                                                       or any(col.strip().lower().startswith(start)
+                                                              for start in ['comm', 'rem', '#'])]
+
+        dfSpecs.drop(columns=cols2drop, inplace=True)        
+
+    @staticmethod
+    def explicitPartialVariantSpecs(implSpecs, convertCols=dict()):
+
+        """Generation of a table of explicit "partial variant" specifications, from an implicit one
+        (= generate all combinations of variants)
+        
+        Parameters:
+        :param:implSpecs: implicit partial specs object, as a DataFrame, taken as is,
+           or a dict, preprocessed through implicitPartialVariantSpecs
+        :param:convertCols Name and conversion method for explicit columns to convert 
+          (each column is converted through :
+           dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
+           for colName, convertCol in convertCols.items())
+
+        Return: The explicitated partial variant specs, as a DataFrame
+        """
+        
+        # Convert spec from dict to DataFrame if needed.
+        if isinstance(implSpecs, dict):
+            dfImplSpecs = Analyser.implicitPartialVariantSpecs(implSpecs)
+        else:
+            assert isinstance(implSpecs, pd.DataFrame)
+            dfImplSpecs = implSpecs.copy()
+        
+        # Drop any comment / no header ... = useless column
+        Analyser._dropCommentColumns(dfImplSpecs)
+        
+        # First columns kept (nearly) as is (actually an explicit one !) :
+        # keep 1 heading NaN if any, drop trailing ones, and drop duplicates if any.
+        def cleanup(s):
+            return s if s.empty else s.iloc[0:1].append(s.iloc[1:].dropna()).drop_duplicates()
+        dfExplSpecs = cleanup(dfImplSpecs[dfImplSpecs.columns[0]]).to_frame()
+
+        # For each implicit specs column (but the first)
+        for col in dfImplSpecs.columns[1:]:
+
+            # Get variants :
+            # keep 1 heading NaN if any, drop trailing ones, and drop duplicates if any.
+            sVariants = cleanup(dfImplSpecs[col])
+            
+            # Duplicate current explicit table as much as variants are many
+            dfExplSpecs = dfExplSpecs.loc[np.repeat(dfExplSpecs.index.to_numpy(), len(sVariants))]
+            
+            # Add the new columns by tiling the variants along the whole index range
+            dfExplSpecs[col] = np.tile(sVariants.to_numpy(), len(dfExplSpecs) // len(sVariants))
+            
+            # Reset index for easy next duplication
+            dfExplSpecs.reset_index(inplace=True, drop=True)
+
+        # Convert specified columns if any
+        assert all(colName in dfExplSpecs.columns for colName in convertCols), \
+               'Could not find some column to convert of {} in explicit table columns {}' \
+               .format(list(convertCols.keys()), list(dfExplSpecs.columns))
+        for colName, convertCol in convertCols.items():
+            dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
+        
+        # Done.
+        return dfExplSpecs
+
+    SupportedFileExts = \
+        ['.xlsx'] + (['.ods'] if version.parse(pd.__version__).release >= (0, 25) else [])
+    
+    @classmethod
+    def _loadPartSpecsFromFile(cls, sourceFpn):
+
+        """Load partial specs for a workbook file (Excel or ODF format)
+
+        :returns: dict(partial specs dataframes)"""
+
+        if isinstance(sourceFpn, str):
+            sourceFpn = pl.Path(sourceFpn)
+    
+        assert sourceFpn.exists(), 'Source file for partial analysis specs not found : {}'.format(sourceFpn)
+
+        ext = sourceFpn.suffix.lower()
+        assert ext in cls.SupportedFileExts, \
+               'Unsupported source file type {}: not from {{{}}}' \
+               .format(ext, ','.join(cls.SupportedFileExts))
+        if ext in ['.xlsx']:
+            ddfData = pd.read_excel(sourceFpn, sheet_name=None)
+        elif ext in ['.ods']:
+            ddfData = pd.read_excel(sourceFpn, sheet_name=None, engine='odf' if ext in ['.ods'] else 'openpyxml')
+        else:
+            raise NotImplementedError(f'Unsupported file extension {ext} for input partial specs file')
+
+        logger.debug('Loaded partial specs: {}\n{}'
+                     .format(', '.join(ddfData.keys()), '\n'.join(str(df) for df in ddfData.values())))
+
+        return ddfData
+    
+    @classmethod
+    def explicitVariantSpecs(cls, partSpecs, keep=None, ignore=None,
+                             varIndCol=None, convertCols=dict(), computedCols=dict()):
+        
+        """Generation of a table of explicit variant specifications,
+        from a set of implicit and explicit partial variant specs objects
+
+        Let this set (partSpecs) be actually an ordered dictionary of partial variant specs tables.
+        * Each table gives a subset (or all) of the columns of the analysis variants,
+        * Each table can be of an "implicit" or "explicit" type, having thus 2 possible different forms:
+            * explicit: all desired combinations are given for all the columns of the table
+              (each row is actually 1 desired combination),
+            * implicit: in each column, vertically, the list of possible variants is given
+              (all columns will then not have the same length);
+              the explicitation will consist of automatically generating all the possible
+              combinations of the values in the table, from those listed vertically in the columns.
+            * warning: the type of the tables is determined by their name: implicit if it contains "_impl",
+              explicit otherwise (this  can't be automated, as the implicit type may sometimes
+              look like it is of the explict one).
+        * several tables can have the same columns:
+            * each can be either of implicit or explicit type, it doesn't matter,
+            * once explicitated, they must still provide non-overlapping rows,
+            * this makes it easy to specify different combinations for different subsets of values in a given
+              subset of columns ;
+            * before combining them with the other tables, they are concatenated vertically into 1 table
+              after individual explicitation,
+        * the tables which have no columns in common with the others will produce the set of all possible
+          combinations with those obtained by explicitating the columns preceding it,
+        * those which have some (but not all) columns in common with those preceding it
+          allow to produce specific variants for these columns: they will be the object of a left join
+          with these preceding tables,
+        * This is because the algorithm for explicitating and merging the tables follows their order in the workbook,
+          once the vertical concatenation of tables with the same set of columns is done.
+
+        N.B. This was not planned, but it works: to impose empty parameter values,
+             you just have to provide an empty table, with the corresponding column headers.
+
+        Parameters
+        :param partSpecs: The dict of name => partial specs objects (Note: the order is meaningful),
+           each as a DataFrame, taken as is, or a dict, preprocessed through implicitPartialVariantSpecs.
+           Or: pathname of an Excel (.xlsx) or Open Doc. (.ods) worksheet file (1 sheet per specs table)
+           Warning: implicit tables are only found by their name containing "_impl"
+        :param keep: List of names of specs to consider from partSpecs (default None => consider all)
+        :param ignore: List of names of specs to ignore from partSpecs (default None => ignore none) ;
+                       Warning: names in keep and ignore are ... ignored.
+        :param varIndCol: Name of the autogenerated variant index column (default: None = no such column added)
+        :param convertCols: Name and conversion method for explicit columns to convert 
+          (each column is converted through :
+           dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
+           for colName, convertCol in convertCols.items()) 
+        :param computedCols: Name and computing method for explicit columns to add (after appending :param varIndCol)
+          (each column to add is computed through :
+           dfExplSpecs[colName] = dfExplSpecs.apply(computeCol, axis='columns')
+           for colName, computeCol in computedCols.items()) 
+
+        Return: The explicitated variant specs, as a DataFrame listing explicitely all the specified variants (rows)
+        """
+        
+        # Load partial variant specs from source (trivial if given as a dict).
+        assert isinstance(partSpecs, (dict, str, pl.Path)), 'Can explicit only worksheet files or alike dicts'
+        dPartSpecs = partSpecs if isinstance(partSpecs, dict) else cls._loadPartSpecsFromFile(partSpecs)
+        assert len(dPartSpecs) > 0, "Can't explicit variants with no partial variant"
+        
+        # Setup filters
+        keep = keep or list(dPartSpecs.keys())
+        ignore = ignore or []
+        
+        # Filter specs as requested and convert any implicit partial variant spec
+        # from dict to DataFrame if needed.
+        ddfPartSpecs = dict()
+        for psName, psValues in dPartSpecs.items():
+            if psName in keep and psName not in ignore:
+                if isinstance(psValues, dict):
+                    ddfPartSpecs[psName] = cls.implicitPartialVariantSpecs(psValues)
+                else:
+                    assert isinstance(psValues, pd.DataFrame)
+                    ddfPartSpecs[psName] = psValues.copy()
+                    # Drop any comment / no header ... = useless column
+                    cls._dropCommentColumns(ddfPartSpecs[psName])
+                
+        # Group partial specs tables with same column sets (according to column names)
+        dSameColsPsNames = dict()  # { cols: [table names] }
+        
+        for psName, dfPsValues in ddfPartSpecs.items():
+            
+            colSetId = ':'.join(sorted(dfPsValues.columns))
+            if colSetId not in dSameColsPsNames:
+                dSameColsPsNames[colSetId] = list()
+                
+            dSameColsPsNames[colSetId].append(psName)
+
+        # For each group, concat. tables into one, after expliciting if needed
+        ldfExplPartSpecs = list()
+
+        for lPsNames in dSameColsPsNames.values():
+
+            ldfSameColsPartSpecs = list()
+            for psName in lPsNames:
+
+                dfPartSpecs = ddfPartSpecs[psName]
+
+                # Implicit specs case:
+                if '_impl' in psName:
+
+                    logger.debug1(f'Explicitating {psName}')
+                    dfPartSpecs = cls.explicitPartialVariantSpecs(dfPartSpecs)
+
+                # Now, specs are explicit.
+                ldfSameColsPartSpecs.append(dfPartSpecs)
+
+            # Concat groups of same columns set explicit specs
+            ldfExplPartSpecs.append(pd.concat(ldfSameColsPartSpecs, ignore_index=True))
+        
+        # Combine explicit specs (following in order)
+        dfExplSpecs = ldfExplPartSpecs[0]
+
+        for dfExplPartSpecs in ldfExplPartSpecs[1:]:
+
+            commCols = [col for col in dfExplSpecs.columns if col in dfExplPartSpecs.columns]
+
+            if commCols:  # Any column in common : left join each left row to each matching right row
+
+                dfExplSpecs = dfExplSpecs.join(dfExplPartSpecs.set_index(commCols), on=commCols)
+
+            else:  # No columns in common : combine each left row with all right rows
+
+                nInitSpecs = len(dfExplSpecs)
+
+                dfExplSpecs = dfExplSpecs.loc[np.repeat(dfExplSpecs.index, len(dfExplPartSpecs))]
+                dfExplSpecs.reset_index(drop=True, inplace=True)
+
+                dfExplPartSpecs = pd.DataFrame(data=np.tile(dfExplPartSpecs, [nInitSpecs, 1]),
+                                               columns=dfExplPartSpecs.columns)
+
+                dfExplSpecs = pd.concat([dfExplSpecs, dfExplPartSpecs], axis='columns')
+
+            dfExplSpecs.reset_index(drop=True, inplace=True)
+        
+        # Convert specified columns if any
+        assert all(colName in dfExplSpecs.columns for colName in convertCols), \
+               'Could not find some column to convert of {} in explicit table columns {}' \
+               .format(list(convertCols.keys()), list(dfExplSpecs.columns))
+        for colName, convertCol in convertCols.items():
+            dfExplSpecs[colName] = dfExplSpecs[colName].apply(convertCol)
+        
+        # Generate explicit variant index column if specified
+        if varIndCol:
+            dfExplSpecs.reset_index(drop=False, inplace=True)
+            dfExplSpecs.rename(columns=dict(index=varIndCol), inplace=True)
+
+        logger.debug(f'Explicit specs (before computing columns):\n{dfExplSpecs}')
+
+        # Compute and add supplementary columns if any
+        for colName, computeCol in computedCols.items():
+            dfExplSpecs[colName] = dfExplSpecs.apply(computeCol, axis='columns')
+                
+        logger.debug(f'Explicit specs (after computing columns):\n{dfExplSpecs}')
+
+        # Done.
+        return dfExplSpecs
+
+
+class DSAnalyser(Analyser):
+
+    """Run a bunch of DS analyses on samples extracted from an individualised sightings data set,
+    according to a user-friendly set of analysis specs,
+    + Tools for building analysis variants specifications and explicitating them.
+    
+    Abstract class.
+    """
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
+                 sampleDecCols=['Effort', 'Distance'], anlysSpecCustCols=[],
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'],
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 abbrevCol='AnlysAbbrev', abbrevBuilder=None,
+                 anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
+                 workDir='.'):
+                       
+        """Ctor
+        
+        Parameters:
+        :param pd.DataFrame dfMonoCatObs: mono-category sighting from FieldDataSet.monoCategorise() or individualise()
+        :param pd.DataFrame dfTransects: Transects infos with columns : transectPlaceCols (n), passIdCol (1),
+            effortCol (1) ; if None, auto generated from input sightings
+        :param effortConstVal: if dfTransects is None and effortCol not in source table, use this constant value
+        :param dSurveyArea: 
+        :param transectPlaceCols: 
+        :param passIdCol: 
+        :param effortCol: 
+        :param sampleSelCols: sample identification = selection columns
+        :param sampleDecCols: Decimal columns among sighting columns
+        :param anlysSpecCustCols: Special columns from analysis specs to simply pass through in results
+        :param distanceUnit: see MCDSEngine
+        :param areaUnit: see MCDSEngine
+        :param resultsHeadCols: dict of list of column names (from dfMonoCatObs) to use in order
+            to build results (right) header columns ; 'sample' columns are sample selection columns ;
+            sampleIndCol is added to resultsHeadCols['before'] if not elswehere in resultsHeadCols ;
+            same for anlysIndCol, right before sampleIndCol
+        :param abbrevCol: Name of column to generate for abbreviating analyses params, not sure really useful ...
+        :param abbrevBuilder: Function of explicit analysis params (as a Series) to generate abbreviated name
+        :param anlysIndCol: Name of column to generate for identifying analyses, unless already there in input data.
+        :param sampleIndCol: Name of column to generate for identifying samples, unless already there in input data.
+        :param workDir: Folder where to generate analysis and results files
+        """
+
+        assert all(col in resultsHeadCols for col in ['before', 'sample', 'after'])
+        assert sampleIndCol
+
+        super().__init__()
+
+        self.dfMonoCatObs = dfMonoCatObs
+
+        self.resultsHeadCols = resultsHeadCols.copy()
+        self.abbrevCol = abbrevCol
+        self.abbrevBuilder = abbrevBuilder
+        self.anlysIndCol = anlysIndCol
+        self.sampleSelCols = sampleSelCols
+        self.sampleIndCol = sampleIndCol
+        self.anlysSpecCustCols = anlysSpecCustCols
+        
+        # sampleIndCol is added to resultsHeadCols['before'] if not already somewhere in resultsHeadCols
+        self.sampIndResHChap = None
+        for chap, cols in self.resultsHeadCols.items():
+            for col in cols:
+                if col == sampleIndCol:
+                    self.sampIndResHChap = chap
+                    break
+        if not self.sampIndResHChap:
+            self.sampIndResHChap = 'before'
+            self.resultsHeadCols[self.sampIndResHChap] = [sampleIndCol] + self.resultsHeadCols[self.sampIndResHChap]
+
+        # anlysIndCol is added to resultsHeadCols['before'], right at the beginning, if not None
+        # and not already somewhere in resultsHeadCols
+        if anlysIndCol and not any(anlysIndCol in cols for cols in self.resultsHeadCols.values()):
+            self.resultsHeadCols['before'] = [anlysIndCol] + self.resultsHeadCols['before']
+        
+        self.workDir = workDir
+
+        self.distanceUnit = distanceUnit
+        self.areaUnit = areaUnit
+        
+        # Individualised data (all samples)
+        self._mcDataSet = \
+            MonoCategoryDataSet(dfMonoCatObs, dfTransects=dfTransects, effortConstVal=effortConstVal,
+                                dSurveyArea=dSurveyArea, transectPlaceCols=transectPlaceCols,
+                                passIdCol=passIdCol, effortCol=effortCol, sampleDecFields=sampleDecCols)
+                                
+        # Analysis engine and executor.
+        self._executor = None
+        self._engine = None
+        
+        # Results.
+        self.results = None
+
+        # Specs.
+        self.updateSpecs(**dSurveyArea)
+        self.updateSpecs(**{name: getattr(self, name) for name in ['distanceUnit', 'areaUnit']})
+
+    # Possible regexps (values) for auto-detection of analyser _internal_ parameter spec names (keys)
+    # from explicit _user_ spec columns
+    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
+    #  and case is ignored during searching).
+    Int2UserSpecREs = dict()
+
+    @staticmethod
+    def userSpec2ParamNames(userSpecCols, int2UserSpecREs, strict=True):
+
+        """
+        Retrieve the internal param. names matching with the "user specified" ones
+        according to the given regexp dictionary.
+        
+        Parameters:
+        :param userSpecCols: list of user spec. columns
+        :param int2UserSpecREs: Possible regexps for internal param. names
+        :param strict: if True, raise KeyError if any name in userSpecCols cannot be matched ;
+                       if False, will return None for each unmatched internal param. name
+
+        Return: List of internal param. names, same order as userSpecCols,
+                with Nones when not found (if not strict)
+        """
+        logger.debug('Matching user spec. columns:')
+
+        parNames = list()
+        for specName in userSpecCols:
+            try:
+                parName = next(iter(parName for parName in int2UserSpecREs
+                                    if any(re.search(pat, specName, flags=re.IGNORECASE)
+                                           for pat in int2UserSpecREs[parName])))
+                logger.debug1(f' * "{specName}" => {parName}')
+                parNames.append(parName)
+            except StopIteration:
+                if strict:
+                    raise KeyError('Could not match user spec. column "{}" in sample data set columns [{}]'
+                                   .format(specName, ', '.join(int2UserSpecREs.keys())))
+                else:
+                    logger.debug1(f' * "{specName}" => Not found')
+                    parNames.append(None)
+
+        logger.debug('... success{}.'.format('' if strict else ' but {} mismatches'.format(parNames.count(None))))
+
+        return parNames
+
+    @staticmethod
+    def _explicitParamSpecs(implParamSpecs=None, dfExplParamSpecs=None, int2UserSpecREs=dict(),
+                            sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
+                            abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum',
+                            sampleIndCol='SampleNum', anlysSpecCustCols=[], dropDupes=True):
+                           
+        """Explicitate analysis param. specs if not already done, and complete columns if needed ;
+        also automatically extract (regexps) columns which are really analysis parameters,
+        with their analyser-internal name, and also their "user" name.
+        
+        Parameters:
+        :param implParamSpecs: Implicit analysis param specs, suitable for explicitation
+           through explicitVariantSpecs()
+        :param dfExplParamSpecs: Explicit analysis param specs, as a DataFrame
+           (generated through explicitVariantSpecs, as an example)
+        :param int2UserSpecREs: Possible regexps for internal param. names
+        :param sampleSelCols: sample identification = selection columns
+        :param abbrevCol: Name of column to generate for abbreviating analyses params, not sure really useful ...
+        :param abbrevBuilder: Function of explicit analysis params (as a Series) to generate abbreviated name
+        :param anlysIndCol: Name of column to generate for identifying analyses, unless already there in input data
+        :param sampleIndCol: Name of column to generate for identifying samples, unless already there in input data
+        :param anlysSpecCustCols: special columns from analysis specs to simply pass through and ignore
+        :param dropDupes: if True, drop duplicates (keep first) in the final explicit DataFrame
+           
+        Return: Explicit specs as a DataFrame (input dfExplParamSpecs not modified : a new one is returned),
+                list of matched analysis param. columns user names,
+                list of matched analysis param. columns internal names,
+                list of unmatched analysis param. columns user names.
+        """
+    
+        # Explicitate analysis specs if needed (and add computed columns if any and not already there).
+        assert dfExplParamSpecs is None or implParamSpecs is None, \
+               'Only one of dfExplParamSpecs and paramSpecCols can be specified'
+        
+        dCompdCols = {abbrevCol: abbrevBuilder} if abbrevCol and abbrevBuilder else {}
+        if implParamSpecs is not None:
+            dfExplParamSpecs = \
+                Analyser.explicitVariantSpecs(implParamSpecs, varIndCol=anlysIndCol,
+                                              computedCols=dCompdCols)
+        else:
+            dfExplParamSpecs = dfExplParamSpecs.copy()
+            for colName, computeCol in dCompdCols.items():
+                if colName not in dfExplParamSpecs.columns:
+                    dfExplParamSpecs[colName] = dfExplParamSpecs.apply(computeCol, axis='columns')
+            if anlysIndCol and anlysIndCol not in dfExplParamSpecs.columns:
+                dfExplParamSpecs[anlysIndCol] = np.arange(len(dfExplParamSpecs))
+
+        # Add sample index column if requested and not already there
+        if sampleIndCol and sampleIndCol not in dfExplParamSpecs.columns:
+            # Drop all-NaN sample selection columns (sometimes, it happens) for a working groupby()
+            dfSampInd = dfExplParamSpecs[sampleSelCols].dropna(axis='columns', how='all')
+            dfExplParamSpecs[sampleIndCol] = \
+                dfSampInd.groupby(list(dfSampInd.columns), sort=False).ngroup()
+
+        # Check for columns duplicates : a killer (to avoid weird error later).
+        iCols = dfExplParamSpecs.columns
+        assert not iCols.duplicated().any(), \
+               'Some duplicate column(s) in parameter specs: ' + ' ,'.join(iCols[iCols.duplicated()])
+
+        # Convert explicit. analysis spec. columns to the internal parameter names,
+        # and extract the real analysis parameters (None entries when not matched).
+        intParamSpecCols = \
+            DSAnalyser.userSpec2ParamNames(dfExplParamSpecs.columns, int2UserSpecREs, strict=False)
+
+        # Get back to associated column user names
+        # a. matched with internal param. names
+        userParamSpecCols = [usp for inp, usp in zip(intParamSpecCols, dfExplParamSpecs.columns) if inp]
+        
+        # b. unmatched with internal param. names, and assumed to be real user analysis params.
+        ignUserParamSpeCols = sampleSelCols.copy()
+        if abbrevCol:
+            ignUserParamSpeCols += [abbrevCol]
+        if sampleIndCol:
+            ignUserParamSpeCols += [sampleIndCol]
+        if anlysIndCol:
+            ignUserParamSpeCols += [anlysIndCol]
+        if anlysSpecCustCols:
+            ignUserParamSpeCols += anlysSpecCustCols
+        unmUserParamSpecCols = [usp for inp, usp in zip(intParamSpecCols, dfExplParamSpecs.columns)
+                                if not inp and usp not in ignUserParamSpeCols]
+
+        # Cleanup implicit name list from Nones (strict=False)
+        intParamSpecCols = [inp for inp in intParamSpecCols if inp]
+
+        # Drop duplicate specs if specified.
+        if dropDupes:
+            nBefore = len(dfExplParamSpecs)
+            dupDetCols = sampleSelCols + userParamSpecCols
+            dfExplParamSpecs.drop_duplicates(subset=dupDetCols, inplace=True)
+            dfExplParamSpecs.reset_index(drop=True, inplace=True)
+            logger.info('Dropped {} last duplicate specs of {}, on [{}] columns'
+                        .format(nBefore - len(dfExplParamSpecs), nBefore, ', '.join(dupDetCols)))
+
+        # Done.
+        return dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols
+
+    def explicitParamSpecs(self, implParamSpecs=None, dfExplParamSpecs=None, dropDupes=True, check=False):
+    
+        """Explicitate analysis param. specs if not already done, and complete columns if needed ;
+        also automatically extract (regexps) columns which are really analysis parameters,
+        with their analyser-internal name, and also their "user" name.
+        
+        Can moreover check params specs for usability, if check is True :
+        * use it before calling analyser.run(implParamSpecs=..., dfExplParamSpecs=..., ...)
+          to check that everythings OK,
+        * or be sure that run() will fail at startup (because it itself will do it).
+        
+        Parameters:
+        :param implParamSpecs: Implicit analysis param specs, suitable for explicitation
+           through explicitVariantSpecs()
+        :param dfExplParamSpecs: Explicit analysis param specs, as a DataFrame
+           (generated through explicitVariantSpecs, as an example)
+        :param dropDupes: if True, drop duplicates (keep first)
+        :param check: if True, checks params for usability by run(),
+           and return a bool verdict and a list of strings explaining the negative (False) verdict
+
+        Return: a 3 or 5-item tuple :
+           * explicit specs as a DataFrame (input dfExplParamSpecs not modified: a new updated one is returned),
+           * list of analysis param. columns internal names,
+           * list of analysis param. columns user names,
+           if ckeck, 2 more items in the return tuple :
+           * check verdict : True if everything went well, False otherwise,
+             * some columns from paramSpecCols could not be found in dfExplParamSpecs columns,
+             * some user columns could not be matched with some of the expected internal parameter names,
+             * some rows are not suitable for DS analysis (empty sample identification columns, ...).
+           * check failure reasons : list of strings explaining things that went bad.
+        """
+        
+        # Explicitate and complete
+        tplRslt = self._explicitParamSpecs(implParamSpecs, dfExplParamSpecs, self.Int2UserSpecREs,
+                                           sampleSelCols=self.sampleSelCols, abbrevCol=self.abbrevCol,
+                                           abbrevBuilder=self.abbrevBuilder, anlysIndCol=self.anlysIndCol,
+                                           sampleIndCol=self.sampleIndCol, anlysSpecCustCols=self.anlysSpecCustCols,
+                                           dropDupes=dropDupes)
+        
+        # Check if requested
+        if check:
+        
+            verdict = True
+            reasons = []
+     
+            dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = tplRslt
+            
+            # Check that an internal column name was found for every user spec column.
+            if len(unmUserParamSpecCols):
+                verdict = False
+                reasons.append('Failed to match some user spec. names with internal ones: {}'
+                               .format(', '.join(unmUserParamSpecCols)))
+
+            # Check that all rows are suitable for DS analysis (non-empty sample identification columns, ...).
+            if dfExplParamSpecs[self.sampleSelCols].isnull().all(axis='columns').any():
+                verdict = False
+                reasons.append('Some rows have some null sample selection columns')
+
+            # Check done.
+            tplRslt += verdict, reasons
+
+        return tplRslt
+
+    def shutdown(self):
+    
+        """Shutdown engine and executor (only usefull if run() raises an exception and so fails to do it),
+        but keep the remainder of the object state as is.
+        """
+
+        if self._engine:
+            self._engine.shutdown()
+            self._engine = None
+        if self._executor:
+            self._executor.shutdown()
+            self._executor = None
+        
+#    def __del__(self):
+#    
+#        self.shutdown()
+
+
+class _FilterSortSteps(object):
+
+    """Log = history of filter and sort steps for a given scheme"""
+
+    def __init__(self, filSorSchId, resultsSet, lang):
+        self.schemeId = filSorSchId
+        self.lang = lang
+        self.results = resultsSet  # only for resultsSet.transColumn(...)
+        self.steps = list()  # of [stepName, propName, propValue]
+
+    def copy(self):
+        clone = _FilterSortSteps(filSorSchId=self.schemeId, resultsSet=self.results, lang=self.lang)
+        clone.steps = copy.deepcopy(self.steps)
+        return clone
+    
+    def transColumns(self, columns):
+        if isinstance(columns, (list, dict)):
+            return ', '.join(self.results.transColumn(col, self.lang) for col in columns)
+        else:
+            return self.results.transColumn(columns, self.lang)
+
+    def append(self, stepName, propName, propValue, transColumns=False):
+        """Append a (step, property, value) record in the filter & sort operation log
+        (translate value as columns label(s) if specified: transColumns=True)"""
+        logger.debug2(f'* {stepName}: {propName} = {propValue}')
+        if transColumns:
+            propValue = self.transColumns(propValue)
+        self.steps.append([stepName, propName, propValue])
+
+    def toList(self):
+        return [[self.schemeId] + step for step in self.steps]
+
+
+class _FilterSortCache(object):
+
+    def __init__(self):
+        self.dResults = dict()
+
+    def copy(self):
+        clone = _FilterSortCache()
+        clone.dResults = {schId: (iFilSor.copy(), filSorSteps.copy())
+                          for schId, (iFilSor, filSorSteps) in self.dResults.items()}
+        return clone
+
+    def clear(self):
+        self.dResults.clear()
+
+    def update(self, schemeId, iFilSor, filSorSteps):
+        # Add / update a detached copy
+        self.dResults[schemeId] = iFilSor.copy(), filSorSteps.copy()
+
+    def get(self, schemeId):
+        if schemeId in self.dResults:
+            # return a detached copy.
+            iFilSor, filSorSteps = self.dResults[schemeId]
+            logger.info1(f'Filter and sort scheme "{schemeId}" found in cache.')
+            return iFilSor.copy(), filSorSteps.copy()
+        return None, None
+
+
+class FilterSortSchemeIdManager(object):
+
+    """Filter and sort scheme Id generator"""
+
+    # Constants for schemeId()
+    MainSchSpecNames = ['method', 'deduplicate', 'filterSort',
+                        'preselCols', 'preselAscs', 'preselThrhs', 'preselNum']
+
+    def __init__(self):
+
+        """Ctor"""
+
+        # Constants for schemeId() (can't define as class variables: would need forward ref to MCDSAnalysisResultsSet)
+        RS = MCDSAnalysisResultsSet
+        self.FinalQuaShortNames = {RS.CLChi2: 'k2', RS.CLKS: 'ks', RS.CLCvMUw: 'cu', RS.CLCvMCw: 'cw',
+                                   RS.CLDCv: 'dc', RS.CLSightRate: 'sr',
+                                   RS.CLCmbQuaBal1: 'q1', RS.CLCmbQuaBal2: 'q2', RS.CLCmbQuaBal3: 'q3',
+                                   RS.CLCmbQuaChi2: 'qk2', RS.CLCmbQuaKS: 'qks', RS.CLCmbQuaDCv: 'qdc'}
+
+        # Short (but unique) Ids for already seen "filter and sort" schemes,
+        # based on the scheme name and an additional int suffix when needed ;
+        # Definition: 2 equal schemes (dict ==) have the same Id
+        self.dFilSorSchemes = dict()  # Unique Id => value
+
+    def copy(self):
+
+        clone = FilterSortSchemeIdManager()
+        clone.dFilSorSchemes = copy.deepcopy(self.dFilSorSchemes)
+
+    def clear(self):
+
+        self.dFilSorSchemes.clear()
+
+    def schemeId(self, scheme):
+
+        """Human-readable but unique identification of a filter and sort scheme
+
+        Built on the scheme name format and an additional int suffix when needed.
+
+        Definition: 2 equal schemes (dict ==) have the same Id
+
+        Parameters:
+        :param scheme: the scheme to identify
+                       as a dict(method= filterSortOnXXX method to use,
+                                 deduplicate= dict(dupSubset=, dDupRounds=) of deduplication params
+                                     (if not or partially given, see filterSortOnXXX defaults)
+                                 filterSort= dict of other <method> params (see filterSortOnXXX methods),
+                                 preselCols= target columns for generating auto-preselection ones,
+                                             containing [1, preselNum] ranks ; default: []
+                                 preselAscs= Rank direction to use for each column (list),
+                                             or all (single bool) ; default: True
+                                             (True means that lower values are "better" ones)
+                                 preselThrhs= Eliminating threshold for each column (list),
+                                              or all (single number) ; default: 0.2
+                                              (eliminated above if preselAscs True, below otherwise)
+                                 preselNum= number of (best) pre-selections to keep for each sample) ;
+                                            default: 5
+
+        :return: the unique Id.
+        """
+
+        # Check scheme specification (1st level properties: presence of mandatory ones, authorised list, ...)
+        props = scheme.keys()
+        assert all(prop in self.MainSchSpecNames for prop in props), \
+               'Unknown filter and sort scheme property/ies: {}' \
+                .format(', '.join(prop for prop in props if prop not in self.MainSchSpecNames))
+        mandProps = ['method']
+        assert all(prop in props for prop in mandProps), \
+               'Missing filter and sort scheme mandatory property/ies: {}' \
+                .format(', '.join(prop for prop in mandProps if prop not in props))
+        method = scheme['method']
+        assert callable(method), 'Filter and sort scheme method must be callable'
+        assert method is MCDSAnalysisResultsSet.filterSortOnExecCode \
+               or method is MCDSAnalysisResultsSet.filterSortOnExCAicMulQua, \
+               'Unsupported filter and sort scheme method: ' + str(method)
+
+        # Compute the heading "name" part of the Id
+        schemeId = 'ExCode' if method is MCDSAnalysisResultsSet.filterSortOnExecCode else 'ExAicMQua'
+        methArgs = scheme.get('filterSort', {})
+        if 'sightRate' in methArgs:
+            schemeId += '-r{:.1f}'.format(methArgs['sightRate']).replace('.', '')
+        if 'whichBestQua' in methArgs:
+            schemeId += 'm{}'.format(len(methArgs['whichBestQua']))
+        if 'whichFinalQua' in methArgs and method is not MCDSAnalysisResultsSet.filterSortOnExecCode:
+            assert methArgs['whichFinalQua'] in self.FinalQuaShortNames, \
+                'Unsupported quality indicator for filtering: ' + str(methArgs['whichFinalQua'])
+            schemeId += self.FinalQuaShortNames[methArgs['whichFinalQua']]
+        if 'nFinalRes' in methArgs:
+            schemeId += 'd{}'.format(methArgs['nFinalRes'])
+
+        # The Id must be unique: enforce it through a uniqueness suffix only if needed
+        if schemeId in self.dFilSorSchemes:  # Seems there's a possible collision ...
+            closeSchemes = {schId: schVal for schId, schVal in self.dFilSorSchemes.items()
+                            if schId.startswith(schemeId)}
+            for schId, schVal in closeSchemes.items():  # Check if not simply an exact match
+                if scheme == schVal:
+                    return schId  # Bingo !
+            schemeId += '-' + str(len(closeSchemes) + 1)  # No exact match => new unused Id !
+
+        # Register new scheme and Id.
+        self.dFilSorSchemes[schemeId] = scheme
+
+        return schemeId
+
+
+class MCDSAnalysisResultsSet(AnalysisResultsSet):
+
+    """A specialized results set for MCDS analyses, with extra. post-computed columns : Delta AIC, Chi2 P"""
+    
+    # Shortcut to already existing and useful columns names.
+    CLRunStatus = MCDSAnalysis.CLRunStatus
+    CLRunStartTime = MCDSAnalysis.CLRunStartTime
+    CLRunElapsedTime = MCDSAnalysis.CLRunElapsedTime
+    CLRunFolder = MCDSAnalysis.CLRunFolder
+    CLParEstKeyFn = MCDSAnalysis.CLParEstKeyFn
+    CLParEstAdjSer = MCDSAnalysis.CLParEstAdjSer
+    CLParEstSelCrit = MCDSAnalysis.CLParEstSelCrit
+    CLParEstCVInt = MCDSAnalysis.CLParEstCVInt
+    CLParTruncLeft = MCDSAnalysis.CLParTruncLeft
+    CLParTruncRight = MCDSAnalysis.CLParTruncRight
+    CLParModFitDistCuts = MCDSAnalysis.CLParModFitDistCuts
+    CLParModDiscrDistCuts = MCDSAnalysis.CLParModDiscrDistCuts
+
+    CLEffort = ('encounter rate', 'effort (L or K or T)', 'Value')
+    CLPDetec = ('detection probability', 'probability of detection (Pw)', 'Value')
+    CLPDetecMin = ('detection probability', 'probability of detection (Pw)', 'Lcl')
+    CLPDetecMax = ('detection probability', 'probability of detection (Pw)', 'Ucl')
+    CLPDetecCv = ('detection probability', 'probability of detection (Pw)', 'Cv')
+    CLPDetecDf = ('detection probability', 'probability of detection (Pw)', 'Df')
+    CLEswEdr = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Value')
+    CLEswEdrMin = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Lcl')
+    CLEswEdrMax = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Ucl')
+    CLEswEdrCv = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Cv')
+    CLEswEdrDf = ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Df')
+    CLDensity = ('density/abundance', 'density of animals', 'Value')
+    CLDensityMin = ('density/abundance', 'density of animals', 'Lcl')
+    CLDensityMax = ('density/abundance', 'density of animals', 'Ucl')
+    CLDensityCv = ('density/abundance', 'density of animals', 'Cv')
+    CLDensityDf = ('density/abundance', 'density of animals', 'Df')
+    CLNumber = ('density/abundance', 'number of animals, if survey area is specified', 'Value')
+    CLNumberMin = ('density/abundance', 'number of animals, if survey area is specified', 'Lcl')
+    CLNumberMax = ('density/abundance', 'number of animals, if survey area is specified', 'Ucl')
+    CLNumberCv = ('density/abundance', 'number of animals, if survey area is specified', 'Cv')
+    CLNumberDf = ('density/abundance', 'number of animals, if survey area is specified', 'Df')
+
+    DCLParTruncDist = dict(left=CLParTruncLeft, right=CLParTruncRight)
+
+    # Computed Column Labels
+    # a. Chi2 determined, Delta AIC, delta DCv
+    CLChi2 = ('detection probability', 'chi-square test probability determined', 'Value')
+    CLDeltaAic = ('detection probability', 'Delta AIC', 'Value')
+    CLDeltaDCv = ('density/abundance', 'density of animals', 'Delta Cv')
+
+    # b. Observation rate and combined quality indicators
+    CLSightRate = ('encounter rate', 'observation rate', 'Value')
+    CLCmbQuaBal1 = ('combined quality', 'balanced 1', 'Value')
+    CLCmbQuaBal2 = ('combined quality', 'balanced 2', 'Value')
+    CLCmbQuaBal3 = ('combined quality', 'balanced 3', 'Value')
+    CLCmbQuaChi2 = ('combined quality', 'more Chi2', 'Value')
+    CLCmbQuaKS = ('combined quality', 'more KS', 'Value')
+    CLCmbQuaDCv = ('combined quality', 'more DCv', 'Value')
+
+    # c. Automated filtering and grouping + sorting
+    CLCAutoFilSor = 'auto filter sort'  # Label "Chapter" (1st level)
+    CLTTruncGroup = 'Group'  # Label "Type" (3rd level)
+    CLTSortOrder = 'Order'  # Label "Type" (3rd level)
+    CLTPreSelection = 'Pre-selection'  # Label "Type" (3rd level)
+
+    #   i. Close truncation group identification
+    CLGroupTruncLeft = (CLCAutoFilSor, CLParTruncLeft[1], CLTTruncGroup)
+    CLGroupTruncRight = (CLCAutoFilSor, CLParTruncRight[1], CLTTruncGroup)
+
+    #   ii. Order inside groups with same = identical truncation parameters (distances and model cut points)
+    CLGrpOrdSmTrAic = (CLCAutoFilSor, 'AIC (same trunc)', CLTSortOrder)
+
+    #   iii. Order inside groups of close truncation distances
+    CLGrpOrdClTrChi2KSDCv = (CLCAutoFilSor, 'Chi2 KS DCv (close trunc)', CLTSortOrder)
+    # CLGrpOrdClTrChi2 = (CLCAutoFilSor, 'Chi2 (close trunc)', CLTSortOrder)
+    CLGrpOrdClTrDCv = (CLCAutoFilSor, 'DCv (close trunc)', CLTSortOrder)
+    
+    CLGrpOrdClTrQuaBal1 = (CLCAutoFilSor, 'Bal. quality 1 (close trunc)', CLTSortOrder)
+    CLGrpOrdClTrQuaBal2 = (CLCAutoFilSor, 'Bal. quality 2 (close trunc)', CLTSortOrder)
+    CLGrpOrdClTrQuaBal3 = (CLCAutoFilSor, 'Bal. quality 3 (close trunc)', CLTSortOrder)
+    CLGrpOrdClTrQuaChi2 = (CLCAutoFilSor, 'Bal. quality Chi2+ (close trunc)', CLTSortOrder)
+    CLGrpOrdClTrQuaKS = (CLCAutoFilSor, 'Bal. quality KS+ (close trunc)', CLTSortOrder)
+    CLGrpOrdClTrQuaDCv = (CLCAutoFilSor, 'Bal. quality DCv+ (close trunc)', CLTSortOrder)
+    
+    #   iv. Global order
+    CLGblOrdChi2KSDCv = (CLCAutoFilSor, 'Chi2 KS DCv (global)', CLTSortOrder)
+
+    CLGblOrdQuaBal1 = (CLCAutoFilSor, 'Bal. quality 1 (global)', CLTSortOrder)
+    CLGblOrdQuaBal2 = (CLCAutoFilSor, 'Bal. quality 2 (global)', CLTSortOrder)
+    CLGblOrdQuaBal3 = (CLCAutoFilSor, 'Bal. quality 3 (global)', CLTSortOrder)
+    CLGblOrdQuaChi2 = (CLCAutoFilSor, 'Bal. quality Chi2+ (global)', CLTSortOrder)
+    CLGblOrdQuaKS = (CLCAutoFilSor, 'Bal. quality KS+ (global)', CLTSortOrder)
+    CLGblOrdQuaDCv = (CLCAutoFilSor, 'Bal. quality DCv+ (global)', CLTSortOrder)
+
+    CLGblOrdDAicChi2KSDCv = (CLCAutoFilSor, 'DeltaAIC Chi2 KS DCv (global)', CLTSortOrder)
+
+    # Computed columns specs (name translation + position).
+    _firstResColInd = len(MCDSEngine.statSampCols()) + len(MCDSAnalysis.MIRunColumns)
+    DComputedCols = {CLSightRate: _firstResColInd + 10,  # After Encounter Rate / Left|Right Trunc. Dist.
+                     CLDeltaAic: _firstResColInd + 12,  # Before AIC
+                     CLChi2: _firstResColInd + 14,  # Before all Chi2 tests
+                     CLDeltaDCv: _firstResColInd + 72,  # Before Density of animals / Cv
+                     # And, at the end ...
+                     **{cl: -1 for cl in [CLCmbQuaBal1, CLCmbQuaBal2, CLCmbQuaBal3,
+                                          CLCmbQuaChi2, CLCmbQuaKS, CLCmbQuaDCv,
+                                          CLGroupTruncLeft, CLGroupTruncRight,
+                                          CLGrpOrdSmTrAic,
+                                          CLGrpOrdClTrChi2KSDCv,  # CLGrpOrdClTrChi2,
+                                          CLGrpOrdClTrDCv,
+                                          CLGrpOrdClTrQuaBal1, CLGrpOrdClTrQuaBal2, CLGrpOrdClTrQuaBal3,
+                                          CLGrpOrdClTrQuaChi2, CLGrpOrdClTrQuaKS, CLGrpOrdClTrQuaDCv,
+                                          CLGblOrdChi2KSDCv,
+                                          CLGblOrdQuaBal1, CLGblOrdQuaBal2, CLGblOrdQuaBal3,
+                                          CLGblOrdQuaChi2, CLGblOrdQuaKS, CLGblOrdQuaDCv,
+                                          CLGblOrdDAicChi2KSDCv]}}
+
+    DfComputedColTrans = \
+        pd.DataFrame(index=DComputedCols.keys(),
+                     data=dict(en=['Obs Rate', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Density',
+                                   'Qual Bal 1', 'Qual Bal 2', 'Qual Bal 3',
+                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+',
+                                   'Group Left Trunc', 'Group Right Trunc',
+                                   'Order Same Trunc AIC',
+                                   'Order Close Trunc Chi2 KS DCv',  # 'Order Close Trunc Chi2',
+                                   'Order Close Trunc DCv', 'Order Close Trunc Bal 1 Qual',
+                                   'Order Close Trunc Bal 2 Qual', 'Order Close Trunc Bal 3 Qual',
+                                   'Order Close Trunc Bal Chi2+ Qual', 'Order Close Trunc Bal KS+ Qual',
+                                   'Order Close Trunc Bal DCv+ Qual',
+                                   'Order Global Chi2 KS DCv', 'Order Global Bal 1 Qual',
+                                   'Order Global Bal 2 Qual', 'Order Global Bal 3 Qual',
+                                   'Order Global Bal Chi2+ Qual', 'Order Global Bal KS+ Qual',
+                                   'Order Global Bal DCv+ Qual',
+                                   'Order Global DeltaAIC Chi2 KS DCv'],
+                               fr=['Taux Obs', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Densité',
+                                   'Qual Equi 1', 'Qual Equi 2', 'Qual Equi 3',
+                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+',
+                                   'Groupe Tronc Gche', 'Groupe Tronc Drte',
+                                   'Ordre Tronc Ident AIC',
+                                   'Ordre Tronc Proch Chi2 KS DCv',  # 'Ordre Tronc Proch Chi2',
+                                   'Ordre Tronc Proch DCv', 'Ordre Tronc Proch Qual Equi 1',
+                                   'Ordre Tronc Proch Qual Equi 2', 'Ordre Tronc Proch Qual Equi 3',
+                                   'Ordre Tronc Proch Qual Equi Chi2+', 'Ordre Tronc Proch Qual Equi KS+',
+                                   'Ordre Tronc Proch Qual Equi DCv+',
+                                   'Ordre Global Chi2 KS DCv', 'Ordre Global Qual Equi 1',
+                                   'Ordre Global Qual Equi 2', 'Ordre Global Qual Equi 3',
+                                   'Ordre Global Qual Equi Chi2+', 'Ordre Global Qual Equi KS+',
+                                   'Ordre Global Qual Equi DCv+',
+                                   'Ordre Global DeltaAIC Chi2 KS DCv']))
+
+    # Final-selection column label (empty, for user decision)
+    CLFinalSelection = (CLCAutoFilSor, 'Final selection', 'Value')
+    DFinalSelColTrans = dict(fr='Sélection finale', en='Final selection')
+
+    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,
+                 sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
+                                    dict(col='right', minDist=25.0, maxLen=25.0)],
+                 truncIntrvEpsilon=1e-6, ldFilSorKeySchemes=None):
+        
+        """Ctor
+
+        Parameters:
+        :param miSampleCols: columns to use for grouping by sample ; defaults to miCustomCols if None
+        :param sampleIndCol: multi-column index for the sample Id column ; no default, must be there !
+        :param ldFilSorKeySchemes: Replacement for predefined filter-sort key schemes
+                                   None => use predefined ones AutoFilSorKeySchemes
+        """
+
+        assert all(len(self.DfComputedColTrans[lang].dropna()) == len(self.DComputedCols)
+                   for lang in self.DfComputedColTrans.columns)
+
+        assert sampleIndCol is not None
+
+        assert all(spec['col'] in self.DCLParTruncDist for spec in ldTruncIntrvSpecs)
+
+        # Initialise base.
+        super().__init__(MCDSAnalysis, miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
+                         dComputedCols=self.DComputedCols, dfComputedColTrans=self.DfComputedColTrans,
+                         sortCols=sortCols, sortAscend=sortAscend)
+        
+        if self.CLFinalSelection:
+            self.addColumnsTrans({self.CLFinalSelection: self.DFinalSelColTrans})
+
+        # Sample columns
+        self.miSampleCols = miSampleCols if miSampleCols is not None else self.miCustomCols
+        self.sampleIndCol = sampleIndCol
+
+        # Sample table (auto computed, see listSamples below)
+        self.dfSamples = None
+    
+        # Descriptive parameters, not used in computations, actually.
+        self.distanceUnit = distanceUnit
+        self.areaUnit = areaUnit
+        self.surveyType = surveyType
+        self.distanceType = distanceType
+        self.clustering = clustering
+
+        # Parameters for truncation group intervals post-computations
+        self.ldTruncIntrvSpecs = ldTruncIntrvSpecs
+        self.truncIntrvEpsilon = truncIntrvEpsilon
+
+        # Parameters for filter and sort key generation schemes
+        self.ldFilSorKeySchemes = ldFilSorKeySchemes
+
+        # Ids manager for "filter and sort" schemes
+        self.filSorIdMgr = FilterSortSchemeIdManager()
+
+        # Cache for results of filter and sort schemes
+        self.filSorCache = _FilterSortCache()
+
+    def copy(self, withData=True):
+    
+        """Clone function, with optional data copy"""
+    
+        # Create new instance with same ctor params.
+        clone = MCDSAnalysisResultsSet(miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
+                                       miSampleCols=self.miSampleCols, sampleIndCol=self.sampleIndCol,
+                                       sortCols=self.sortCols, sortAscend=self.sortAscend,
+                                       distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                       surveyType=self.surveyType, distanceType=self.distanceType,
+                                       clustering=self.clustering,
+                                       ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
+                                       truncIntrvEpsilon=self.truncIntrvEpsilon)
+
+        # Copy data if needed.
+        if withData:
+            clone._dfData = self._dfData.copy()
+            clone.rightColOrder = self.rightColOrder
+            clone.postComputed = self.postComputed
+            clone.dfSamples = None if self.dfSamples is None else self.dfSamples.copy()
+            clone.filSorIdMgr = self.filSorIdMgr.copy()
+            clone.filSorCache = self.filSorCache.copy()
+
+        return clone
+    
+    def onDataChanged(self):
+
+        """React to results data (_dfData) changes that invalidate calculated data,
+        but only when the calculus is coded in this class ; other calculi impacts taken care in base classes"""
+
+        self.dfSamples = None
+        self.filSorIdMgr.clear()
+        self.filSorCache.clear()
+
+    def dropRows(self, sbSelRows):
+    
+        super().dropRows(sbSelRows)
+
+        self.onDataChanged()
+        
+    def setData(self, dfData, postComputed=False, acceptNewCols=False):
+        
+        super().setData(dfData, postComputed=postComputed, acceptNewCols=acceptNewCols)
+
+        self.onDataChanged()
+
+    # Get translate names of custom columns
+    def transSampleColumns(self, lang):
+        
+        return self.dfCustomColTrans.loc[self.miSampleCols, lang].to_list()
+
+    # Post-computations : Actual Chi2 value, from multiple tests done.
+    MaxChi2Tests = 3  # TODO: Really a constant, or actually depends on some analysis params ?
+    CLsChi2All = [('detection probability', 'chi-square test probability (distance set {})'.format(i), 'Value')
+                  for i in range(MaxChi2Tests, 0, -1)]
+    
+    @staticmethod
+    def _determineChi2Value(sChi2AllDists):
+        for chi2 in sChi2AllDists:
+            if not np.isnan(chi2):
+                return chi2
+        return np.nan
+
+    def _postComputeChi2(self):
+        
+        logger.info2(f'Post-computing actual Chi2: {self.CLChi2}')
+        
+        # Last value of all the tests done.
+        chi2AllColLbls = [col for col in self.CLsChi2All if col in self._dfData.columns]
+        if chi2AllColLbls:
+            self._dfData[self.CLChi2] = self._dfData[chi2AllColLbls].apply(self._determineChi2Value, axis='columns')
+
+    # Post-computations : Delta AIC/DCv per sampleCols + truncation param. cols group => AIC - min(group).
+    CLAic = ('detection probability', 'AIC value', 'Value')
+    CLDCv = CLDensityCv
+    CLsTruncDist = [('encounter rate', 'left truncation distance', 'Value'),
+                    ('encounter rate', 'right truncation distance (w)', 'Value')]
+
+    def _postComputeDeltaAicDCv(self):
+        
+        logger.info2(f'Post-computing Delta AIC/DCv: {self.CLDeltaAic} / {self.CLDeltaDCv}')
+        
+        # a. Minimum AIC & DCv per group
+        #    (drop all-NaN sample selection columns (sometimes, it happens) for a working groupby())
+        groupColLbls = self.miSampleCols.append(pd.MultiIndex.from_tuples(self.CLsTruncDist))
+        groupColLbls = [col for col in groupColLbls
+                        if col in self._dfData.columns and not self._dfData[col].isna().all()]
+        df2Join = self._dfData.groupby(groupColLbls, dropna=False)[[self.CLAic, self.CLDCv]].min()
+        
+        # b. Rename computed columns to target 'Delta XXX'
+        df2Join.columns = pd.MultiIndex.from_tuples([self.CLDeltaAic, self.CLDeltaDCv])
+
+        # c. Join the column to the target data-frame
+        self._dfData = self._dfData.join(df2Join, on=groupColLbls)
+
+        # d. Compute delta-AIC & DCv in-place
+        self._dfData[self.CLDeltaAic] = self._dfData[self.CLAic] - self._dfData[self.CLDeltaAic]
+        self._dfData[self.CLDeltaDCv] = self._dfData[self.CLDCv] - self._dfData[self.CLDeltaDCv]
+
+    # Post computations : Useful columns for quality indicators.
+    CLNObs = ('encounter rate', 'number of observations (n)', 'Value')
+    CLNTotObs = MCDSEngine.MIStatSampCols[0]
+    CLMinObsDist = MCDSEngine.MIStatSampCols[1]
+    CLMaxObsDist = MCDSEngine.MIStatSampCols[2]
+    CLKeyFn = ('detection probability', 'key function type', 'Value')
+    CLNTotPars = ('detection probability', 'total number of parameters (m)', 'Value')
+    CLNAdjPars = ('detection probability', 'number of adjustment term parameters (NAP)', 'Value')
+    CLKS = ('detection probability', 'Kolmogorov-Smirnov test probability', 'Value')
+    CLCvMUw = ('detection probability', 'Cramér-von Mises (uniform weighting) test probability', 'Value')
+    CLCvMCw = ('detection probability', 'Cramér-von Mises (cosine weighting) test probability', 'Value')
+
+    # Post computations : Quality indicators.
+    CLsQuaIndicSources = [CLKeyFn, CLNAdjPars, CLNTotPars, CLNObs, CLNTotObs, CLChi2, CLKS, CLCvMUw, CLCvMCw, CLDCv]
+
+    CIKeyFn = CLsQuaIndicSources.index(CLKeyFn)
+    CINAdjPars = CLsQuaIndicSources.index(CLNAdjPars)
+    CINTotPars = CLsQuaIndicSources.index(CLNTotPars)
+    CINObs = CLsQuaIndicSources.index(CLNObs)
+    CINTotObs = CLsQuaIndicSources.index(CLNTotObs)
+    CIChi2 = CLsQuaIndicSources.index(CLChi2)
+    CIKS = CLsQuaIndicSources.index(CLKS)
+    CICvMUw = CLsQuaIndicSources.index(CLCvMUw)
+    CICvMCw = CLsQuaIndicSources.index(CLCvMCw)
+    CIDCv = CLsQuaIndicSources.index(CLDCv)
+
+    @classmethod
+    def _combinedQualityBalanced1(cls, aRes):
+
+        """Historical QualBal1 indicator (March 2021), optimized through numpy
+
+        Returns:
+             np.array (shape: aRes rows, 1 column)
+        """
+
+        chi2KsCvMs = aRes[:, cls.CIChi2:cls.CICvMCw + 1].prod(axis=1)
+        normNObs = aRes[:, cls.CINObs].astype(float) / aRes[:, cls.CINTotObs].astype(float)
+        normNTotPars = 1 / (0.2 * np.maximum(2, aRes[:, cls.CINTotPars].astype(float)) + 0.6)
+        normCVDens = np.exp(-12 * np.square(aRes[:, cls.CIDCv].astype(float)))
+
+        return np.power(chi2KsCvMs * normNObs * normNTotPars * normCVDens, 1 / 7.0)  # shape: aRes rows, 1 column
+
+    # Raw key function indicator and relevant universal numpy function.
+    # DNormKeyFn = dict(HNORMAL=1.00, UNIFORM=0.75, HAZARD=0.5, NEXPON=0.1)  # Not better
+    _ufnNormKeyFn = \
+        np.frompyfunc(lambda keyFn: dict(HNORMAL=1.0, UNIFORM=0.9, HAZARD=0.6, NEXPON=0.1).get(keyFn, 0.0), 1, 1)
+
+    CLsNewQuaIndics = [CLCmbQuaBal2, CLCmbQuaBal3, CLCmbQuaChi2, CLCmbQuaKS, CLCmbQuaDCv]
+
+    @classmethod
+    def _combinedQualityAll(cls, aRes):
+
+        """New quality indicators (August, 2021 and later), optimized through numpy
+        (QualBal2, QualBal3, QualMoreChi2, QualMoreKS, QualMoreDCv)
+
+        Returns:
+             tuple(np.array) (shape: aRes rows, 1 column each, order of cls.CLsNewQuaIndics)
+        """
+
+        # Common computations (all times)
+        chi2 = aRes[:, cls.CIChi2].astype(float)
+        ks = aRes[:, cls.CIKS].astype(float)
+        dcv = aRes[:, cls.CIDCv].astype(float)
+        chi2KsCvMs = aRes[:, cls.CIChi2:cls.CICvMCw + 1].astype(float).prod(axis=1)
+        normNObs = aRes[:, cls.CINObs].astype(float) / aRes[:, cls.CINTotObs].astype(float)
+
+        # Common computations (October 2021)
+        nAdjPars2 = np.square(aRes[:, cls.CINAdjPars]).astype(float)
+        normKeyFn = cls._ufnNormKeyFn(aRes[:, cls.CIKeyFn])
+        normChi2KsCvMsNObsKFn = chi2KsCvMs * normNObs * normKeyFn
+
+        # QualBal2 (August 2021)
+        # normNTotPars2 = 1 / (0.2 * np.maximum(1, aRes[:, cls.CINTotPars].astype(float)) + 0.8)
+        # normCVDens2 = np.exp(-16 * np.square(dcv))
+        # normChi2KsCvMsNObsTotPDcv2 = chi2KsCvMs * normNObs * normNTotPars2 * normCVDens2
+        # quaBal2 = np.power(normChi2KsCvMsNObsTotPDcv2, 1 / 7.0)
+
+        # QualBal2 (October 2021): A more devaluating version for CVDens, using KeyFn, replacing NTotPars by NAdjPars
+        normNAdjPars2 = np.exp(-0.15 * nAdjPars2)
+        normCVDens2 = np.exp(-20 * np.square(dcv))
+        quaBal2 = np.power(normChi2KsCvMsNObsKFn * normNAdjPars2 * normCVDens2, 1 / 8.0)
+
+        # QualBal3 (August 2021)
+        # normNTotPars3 = 1 / (0.3 * np.maximum(1, aRes[:, cls.CINTotPars].astype(float)) + 0.7)
+        # normCVDens3 = np.exp(-20 * np.square(dcv))
+        # normChi2KsCvMsNObsTotPDcv3 = chi2KsCvMs * normNObs * normNTotPars3 * normCVDens3
+        # quaBal3 = np.power(normChi2KsCvMsNObsTotPDcv3, 1 / 7.0)
+
+        # QualBal3 (October 2021): Same as QualBal2, but even more devaluating for CVDens
+        normNAdjPars3 = np.exp(-0.17 * nAdjPars2)
+        normCVDens3 = np.exp(-63 * np.power(dcv, 2.8))
+        normChi2KsCvMsNObsKFnAdjPDcv3 = normChi2KsCvMsNObsKFn * normNAdjPars3 * normCVDens3
+        quaBal3 = np.power(normChi2KsCvMsNObsKFnAdjPDcv3, 1 / 8.0)
+
+        # QualMoreX (March 2021)
+        # normNTotParsM = 1 / (0.2 * np.maximum(2, aRes[:, cls.CINTotPars].astype(float)) + 0.6)
+        # normCVDensM = np.exp(-12 * np.square(dcv))
+        # normChi2KsCvMsNObsTotPDCvM = chi2KsCvMs * normNObs * normNTotParsM * normCVDensM
+        # moreChi2 = np.power(normChi2KsCvMsNObsTotPDCvM * chi2, 1 / 8.0)
+        # moreKS = np.power(normChi2KsCvMsNObsTotPDCvM * ks, 1 / 8.0)
+        # moreDCv = np.power(normChi2KsCvMsNObsTotPDCvM * normCVDensM, 1 / 8.0)
+
+        # QualMoreX (October 2021): Follow QualBal3 update (were based on historical QualBal1)
+        moreChi2 = np.power(normChi2KsCvMsNObsKFnAdjPDcv3 * chi2, 1 / 9.0)
+        moreKS = np.power(normChi2KsCvMsNObsKFnAdjPDcv3 * ks, 1 / 9.0)
+        moreDCv = np.power(normChi2KsCvMsNObsKFnAdjPDcv3 * normCVDens3, 1 / 9.0)
+
+        return quaBal2, quaBal3, moreChi2, moreKS, moreDCv
+
+    # Killer values for base MCDS indicators
+    KilrNObs = 0
+    KilrStaTest = 0
+    KilrDensCv = 1e5
+    KilrNPars = 1e3
+    KilrNTotObs = 1e9
+    KilrBalQua = 0
+
+    def _postComputeQualityIndicators(self):
+
+        cls = self
+
+        logger.info2('Post-computing Quality Indicators')
+
+        # Sighting rate (not 100% due to truncations).
+        self._dfData[cls.CLSightRate] = 100 * self._dfData[cls.CLNObs] / self._dfData[cls.CLNTotObs]  # [0,1] => %
+
+        # Prepare data for computations
+        logger.info3('* Pre-processing source data')
+
+        # a. extract the useful columns, after adding them if not present
+        for miCol in cls.CLsQuaIndicSources:
+            if miCol not in self._dfData.columns:
+                self._dfData[miCol] = np.nan
+        dfCompData = self._dfData[cls.CLsQuaIndicSources].copy()
+
+        # b. historical bal quality  indicator 1
+        logger.info3('* Balanced quality 1')
+        self._dfData[cls.CLCmbQuaBal1] = cls._combinedQualityBalanced1(dfCompData.values)
+
+        # c. newer quality indicators
+        #    (NaN value MUST kill down these indicators to compute => we have to enforce this)
+        dfCompData.fillna({cls.CLNObs: cls.KilrNObs,
+                           cls.CLChi2: cls.KilrStaTest, cls.CLKS: cls.KilrStaTest,
+                           cls.CLCvMUw: cls.KilrStaTest, cls.CLCvMCw: cls.KilrStaTest,
+                           cls.CLDCv: cls.KilrDensCv,  # Usually considered good under 0.3
+                           cls.CLNTotObs: cls.KilrNTotObs,  # Should slap down _normObs whatever NObs
+                           cls.CLNAdjPars: cls.KilrNPars,  # Should slap down _normNAdjPars whatever NObs
+                           cls.CLNTotPars: cls.KilrNPars},
+                          inplace=True)
+
+        logger.info3('* Balanced quality 2, 3, Chi2+, KS+, DCv+')
+        for miCol, aIndic in zip(cls.CLsNewQuaIndics, cls._combinedQualityAll(dfCompData.values)):
+            self._dfData[miCol] = aIndic
+
+        # For some unknown reason, the theorically better code below raises some odd exceptions like (depends):
+        # * index-join on non-unique index not implemented
+        # * KeyError: None of <items of cls.CLsNewQuaIndics> exists in index
+        # whereas the same code works in devarchive2.ipynb/Development : Optimise _postComputeQualityIndicators).
+        # self._dfData[cls.CLsNewQuaIndics] = np.stack(cls._combinedQualityAll(dfCompData.values), axis=1)
+
+    # Post computations : Truncations groups.
+    @staticmethod
+    def _groupingIntervals(sValues, minDist, maxLen, epsilon=1e-6):
+
+        """Build a list of value grouping intervals from a series of values
+        taking care of min distance and max length constraints
+
+        Parameters:
+        :param sValues: pd.Series of values to group
+        :param minDist: minimal distance between consecutive intervals (left.max - right.min > minIntrvDist)
+        :param maxLen: max length of intervals (but ... see below)
+
+        TODO: Fix current implementation that actually does not produce intervals of max length maxLen, but 1.5*maxLen !
+
+        :return: pd.DataFrame of left-closed and right-open resulting intervals (columns = vmin, vsup)
+        """
+
+        # Cleanup and sort (ascending) distance series first
+        # (+ for some reason, need for enforcing float dtype ... otherwise dtype='O' !?)
+        sValues = sValues.dropna().astype(float).sort_values()
+
+        # If not a single cleaned up distance to examine, stop here.
+        if sValues.empty:
+            return pd.DataFrame()
+
+        # List non-null differences between consecutive sorted distances
+        dfIntrvs = pd.DataFrame(dict(v=sValues.values))
+        dfIntrvs['vdelta'] = dfIntrvs.v.diff()
+        dfIntrvs.loc[dfIntrvs.v.idxmin(), 'vdelta'] = np.inf
+        dfIntrvs.dropna(inplace=True)
+        dfIntrvs = dfIntrvs[dfIntrvs.vdelta > 0].copy()
+
+        # Deduce start (min) and end (sup) for each such interval (left-closed, right-open)
+        dfIntrvs['vmin'] = dfIntrvs.loc[dfIntrvs.vdelta > minDist, 'v']
+        dfIntrvs['vsup'] = dfIntrvs.loc[dfIntrvs.vdelta > minDist, 'v'].shift(-1).dropna()
+        dfIntrvs.loc[dfIntrvs['vmin'].idxmax(), 'vsup'] = np.inf
+        dfIntrvs.dropna(inplace=True)
+        dfIntrvs['vsup'] = dfIntrvs['vsup'].apply(lambda vs: sValues[sValues < vs].max() + epsilon)
+        dfIntrvs = dfIntrvs[['vmin', 'vsup']].reset_index(drop=True)
+
+        # If these intervals are too wide, cut them up in equal sub-intervals and make them new intervals
+        lsNewIntrvs = list()
+        for _, sIntrv in dfIntrvs.iterrows():
+
+            if sIntrv.vsup - sIntrv.vmin > maxLen:
+                # TODO: Well, this actually does not produce intervals of max length maxLen, but 1.5*maxLen !
+                nSubIntrvs = round((sIntrv.vsup - sIntrv.vmin) / maxLen)
+                subIntrvLen = (sIntrv.vsup - sIntrv.vmin) / nSubIntrvs
+                lsNewIntrvs += [pd.Series(dict(vmin=sIntrv.vmin + nInd * subIntrvLen,
+                                               vsup=min(sIntrv.vmin + (nInd + 1) * subIntrvLen,
+                                                        sIntrv.vsup)))
+                                for nInd in range(nSubIntrvs)]
+            else:
+                lsNewIntrvs.append(sIntrv)
+
+        dfIntrvs = pd.DataFrame(lsNewIntrvs).reset_index(drop=True)
+        dfIntrvs.sort_values(by='vmin', inplace=True)
+
+        return dfIntrvs
+
+    @staticmethod
+    def _intervalIndex(value, dfIntervals):
+        """Compute the index of the interval to which a value belongs, if any
+        :return: 0 for NaN values, -1 for values that belong to no interval, from-1 interval index otherwise
+        """
+        if pd.isnull(value):
+            return 0
+        dfWhere = dfIntervals[(dfIntervals.vmin <= value) & (dfIntervals.vsup > value)]
+        if dfWhere.empty:
+            return -1
+        return 1 + dfWhere.index[0]
+
+    @classmethod
+    def _sampleDistTruncGroups(cls, dfSampRes, ldIntrvSpecs, intrvEpsilon=1e-6):
+
+        """Compute distance truncation groups for 1 sample, for each target distance truncation column"""
+
+        # For each truncation "method" (left or right)
+        dTruncGroups = dict()
+        for dIntrvSpecs in ldIntrvSpecs:
+
+            truncCol = cls.DCLParTruncDist[dIntrvSpecs['col']]
+            logger.info5(f'  - {truncCol[1]}')
+
+            # Compute distance grouping intervals
+            dfIntrvs = cls._groupingIntervals(sValues=dfSampRes[truncCol], minDist=dIntrvSpecs['minDist'],
+                                              maxLen=dIntrvSpecs['maxLen'], epsilon=intrvEpsilon)
+
+            # Deduce index of belonging interval for each distance
+            # (special case when no truncation: num=0 if NaN truncation distance)
+            dTruncGroups[dIntrvSpecs['col']] = dfSampRes[truncCol].apply(cls._intervalIndex, dfIntervals=dfIntrvs)
+
+        return dTruncGroups
+
+    def listSamples(self, rebuild=False):
+
+        """List result samples (if really needed of specified)"""
+
+        if rebuild or self.dfSamples is None:
+
+            miSampleCols = self.miSampleCols
+            if self.sampleIndCol not in miSampleCols:
+                miSampleCols = pd.MultiIndex.from_tuples([self.sampleIndCol]).append(miSampleCols)
+            self.dfSamples = self._dfData[miSampleCols]
+            self.dfSamples = self.dfSamples.drop_duplicates()
+            self.dfSamples.set_index(self.sampleIndCol, inplace=True)
+            self.dfSamples.sort_index(inplace=True)
+            assert len(self.dfSamples) == self.dfSamples.index.nunique()
+
+        return self.dfSamples
+
+    def _distTruncGroups(self):
+
+        """Compute distance truncation groups for all samples, for each target distance truncation column"""
+
+        # For each sample,
+        dTruncGroups = dict()  # key=ldIntrvSpecs[*]['col'], value=list(Series of group nums)
+        for lblSamp, sSamp in self.listSamples().iterrows():
+
+            # Select sample rows
+            dfSampRes = self._dfData.loc[self._dfData[self.sampleIndCol] == lblSamp]
+            logger.info3('#{} {} : {} rows'
+                          .format(lblSamp, ', '.join([f'{k[1]}={v}' for k, v in sSamp.items()]), len(dfSampRes)))
+
+            # Compute truncation groups for this sample
+            dSampTruncGroups = self._sampleDistTruncGroups(dfSampRes, ldIntrvSpecs=self.ldTruncIntrvSpecs,
+                                                           intrvEpsilon=self.truncIntrvEpsilon)
+
+            # Store them for later concatenation
+            for colAlias, sGrpNums in dSampTruncGroups.items():
+                if colAlias not in dTruncGroups:
+                    dTruncGroups[colAlias] = list()
+                dTruncGroups[colAlias].append(sGrpNums)
+
+        # Concat series of computed group nums (opt or not) for each target distance column to group
+        return {colAlias: pd.concat(lsGrpNums) for colAlias, lsGrpNums in dTruncGroups.items()}
+
+    DCLGroupTruncDist = dict(left=CLGroupTruncLeft, right=CLGroupTruncRight)
+
+    def _postComputeTruncationGroups(self):
+
+        """Compute and add truncation group columns for later filtering and sorting"""
+
+        logger.info2('Post-computing Truncation Groups')
+
+        # Compute distance truncation groups for all samples, for each target distance truncation column
+        # and update result table.
+        for colAlias, sGrpNums in self._distTruncGroups().items():
+            self._dfData[self.DCLGroupTruncDist[colAlias]] = sGrpNums
+
+    # Post computations : filtering and sorting.
+    # a. Column Labels for computed group and sort orders
+    #    N.B. "close truncations" means "identical = same truncations" here (but see MCDSOptanalyserResultsSet)
+    # See above
+
+    # b. Schemes for computing filtering and sorting keys (see inherited _postComputeFilterSortKeys).
+    AutoFilSorKeySchemes = \
+        [  # Orders inside groups with identical truncation params.
+         dict(key=CLGrpOrdSmTrAic,  # Best AIC, for same left and right truncations (but variable nb of cut points)
+              sort=[CLParTruncLeft, CLParTruncRight, CLDeltaAic, CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
+              ascend=[True, True, True, False, False, True, False, True],
+              group=[CLParTruncLeft, CLParTruncRight, CLParModFitDistCuts]),
+
+         # Orders inside groups of close truncation params.
+         # dict(key=CLGrpOrdClTrChi2,  # Best Chi2 inside groups of close truncation params
+         #      sort=[CLGroupTruncLeft, CLGroupTruncRight,
+         #            CLChi2],
+         #      ascend=[True, True, False],
+         #      group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         # dict(key=CLGrpOrdClTrKS,  # Best KS inside groups of close truncation params
+         #      sort=[CLGroupTruncLeft, CLGroupTruncRight,
+         #            CLKS],
+         #      ascend=[True, True, False],
+         #      group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrDCv,  # Best DCv inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLDCv],
+              ascend=[True, True, True],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrChi2KSDCv,  # Best Chi2 & KS & DCv inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight, CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
+              ascend=[True, True, False, False, True, False, True],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+
+         dict(key=CLGrpOrdClTrQuaBal1,  # Best Combined Quality 1 inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLCmbQuaBal1],
+              ascend=[True, True, False],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrQuaBal2,  # Best Combined Quality 2 inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLCmbQuaBal2],
+              ascend=[True, True, False],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrQuaBal3,  # Best Combined Quality 3 inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLCmbQuaBal3],
+              ascend=[True, True, False],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrQuaChi2,  # Best Qualité combinée Chi2+ inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLCmbQuaChi2],
+              ascend=[True, True, False],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrQuaKS,  # Best Combined Quality KS+ inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLCmbQuaKS],
+              ascend=[True, True, False],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+         dict(key=CLGrpOrdClTrQuaDCv,  # Best Combined Quality DCv+ inside groups of close truncation params
+              sort=[CLGroupTruncLeft, CLGroupTruncRight,
+                    CLCmbQuaDCv],
+              ascend=[True, True, False],
+              group=[CLGroupTruncLeft, CLGroupTruncRight]),
+
+         # Global orders (no grouping by close or identical truncations)
+         dict(key=CLGblOrdChi2KSDCv,
+              sort=[CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
+              ascend=[False, False, True, False, True]),
+         dict(key=CLGblOrdQuaBal1,
+              sort=[CLCmbQuaBal1], ascend=False),
+         dict(key=CLGblOrdQuaBal2,
+              sort=[CLCmbQuaBal2], ascend=False),
+         dict(key=CLGblOrdQuaBal3,
+              sort=[CLCmbQuaBal3], ascend=False),
+         dict(key=CLGblOrdQuaChi2,
+              sort=[CLCmbQuaChi2], ascend=False),
+         dict(key=CLGblOrdQuaKS,
+              sort=[CLCmbQuaKS], ascend=False),
+         dict(key=CLGblOrdQuaDCv,
+              sort=[CLCmbQuaDCv], ascend=False),
+
+         dict(key=CLGblOrdDAicChi2KSDCv,
+              sort=[CLParTruncLeft, CLParTruncRight, CLParModFitDistCuts,
+                    CLDeltaAic, CLChi2, CLKS, CLDCv, CLNObs, CLRunStatus],
+              ascend=[True, True, True, True, False, False, True, False, True], napos='first')]
+
+    # Enforce uniqueness of keys in filter and sort key specs.
+    assert len(AutoFilSorKeySchemes) == len(set(scheme['key'] for scheme in AutoFilSorKeySchemes)), \
+           'Duplicated scheme key in MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
+
+    # Enforce uniqueness of sort and group column in filter and sort key specs.
+    assert all(len(scheme['sort']) == len(set(scheme['sort'])) for scheme in AutoFilSorKeySchemes), \
+           'Duplicated sort column spec in some scheme of MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
+    assert all(len(scheme.get('group', [])) == len(set(scheme.get('group', []))) for scheme in AutoFilSorKeySchemes), \
+           'Duplicated group column spec in some scheme of MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
+
+    # Check sort vs ascend list lengths in filter and sort key specs.
+    assert all(isinstance(scheme['ascend'], bool) or len(scheme['ascend']) == len(scheme['sort'])
+               for scheme in AutoFilSorKeySchemes), \
+           'Inconsistent ascend vs sort specs in some scheme of MCDSAnalysisResultsSet.AutoFilSorKeySchemes'
+
+    # c. Computation of filter and sort keys
+
+    # Make results cell values hashable (needed for sorting in _postComputeFilterSortKeys)
+    @staticmethod
+    def _toSortable(value):
+
+        if isinstance(value, list):
+            return len(value)
+        elif isinstance(value, (int, float, str)) or pd.isnull(value):
+            return value
+        else:
+            raise NotImplementedError
+
+    DCLUnsortableCols = \
+        {CLParModFitDistCuts: (CLParModFitDistCuts[0], CLParModFitDistCuts[1], 'Sortable'),
+         CLParModDiscrDistCuts: (CLParModDiscrDistCuts[0], CLParModDiscrDistCuts[1], 'Sortable')}
+
+    # Make results cell values hashable (needed for grouping in _postComputeFilterSortKeys)
+    @staticmethod
+    def _toHashable(value):
+    
+        if isinstance(value, list):
+            return ','.join(str(v) for v in value)
+        elif isinstance(value, (int, float, str)) or pd.isnull(value):
+            return value
+        else:
+            return str(value)
+
+    DCLUnhashableCols = \
+        {CLParModFitDistCuts: (CLParModFitDistCuts[0], CLParModFitDistCuts[1], 'Hashable'),
+         CLParModDiscrDistCuts: (CLParModDiscrDistCuts[0], CLParModDiscrDistCuts[1], 'Hashable')}
+
+    @classmethod
+    def _sampleFilterSortKeys(cls, dfSampRes, ldFilSorKeySchemes):
+
+        """Compute filter and sort keys for 1 sample, for each pre-defined scheme
+
+        Parameters:
+        :param ldFilSorKeySchemes: Filter-sort key generation schemes to use (ex. cls.AutoFilSorKeySchemes).
+        """
+
+        # Make a copy to avoid modifying source data.
+        dfSampRes = dfSampRes.copy()
+
+        # For each filter and sort scheme
+        dFilSorKeys = dict()
+        for scheme in ldFilSorKeySchemes:
+
+            logger.info4('* scheme {}'.format(scheme))
+
+            # Workaround for to-be-sorted problematic columns.
+            sortCols = list()
+            sortAsc = list()
+            schSortAsc = [scheme['ascend']] * len(sortCols) if isinstance(scheme['ascend'], bool) else scheme['ascend']
+            for col, asc in zip(scheme['sort'], schSortAsc):
+                if col in dfSampRes.columns:  # Ignore missing cols
+                    if col in cls.DCLUnsortableCols:
+                        wkrndSortCol = cls.DCLUnsortableCols[col]
+                        logger.info5('{} => {}'.format(col, wkrndSortCol))
+                        dfSampRes[wkrndSortCol] = dfSampRes[col].apply(cls._toSortable)
+                        col = wkrndSortCol  # Will rather sort with this one !
+                    sortCols.append(col)
+                    sortAsc.append(asc)
+
+            # Sort results
+            if sortCols:
+                dfSampRes.sort_values(by=sortCols, ascending=sortAsc,
+                                      na_position=scheme.get('napos', 'last'), inplace=True)
+
+            # Compute order (target series is indexed like dfSampRes).
+            if 'group' in scheme:  # Partial = 'group' order.
+
+                # Workaround for to-be-grouped problematic columns.
+                groupCols = list()
+                for col in scheme['group']:
+                    if col in dfSampRes.columns:
+                        if col in cls.DCLUnhashableCols:
+                            wkrndGroupCol = cls.DCLUnhashableCols[col]
+                            logger.info5('{} => {}'.format(col, wkrndGroupCol))
+                            dfSampRes[wkrndGroupCol] = dfSampRes[col].apply(cls._toHashable)
+                            col = wkrndGroupCol  # Will rather group with this one !
+                        groupCols.append(col)
+
+                sSampOrder = dfSampRes.groupby(groupCols, dropna=False).cumcount()
+
+            else:  # Global order.
+                sSampOrder = pd.Series(data=range(len(dfSampRes)), index=dfSampRes.index)
+
+            # Done: next scheme.
+            dFilSorKeys[scheme['key']] = sSampOrder
+
+        return dFilSorKeys
+
+    def _filterSortKeySchemes(self):
+        """Select filter and sort keys schemes to apply (predefined if not overriden at ctor time)"""
+        return self.ldFilSorKeySchemes or self.AutoFilSorKeySchemes
+
+    def _filterSortKeys(self):
+
+        """Compute filter and sort keys for all samples, for each scheme target key column"""
+
+        # Retrieve filter and sort keys schemes to apply.
+        ldFilSorKeySchemes = self._filterSortKeySchemes()
+
+        # For each sample,
+        dFilSorKeys = dict()  # key=<target column for key>, value=<Series of key values>)
+        for lblSamp, sSamp in self.listSamples().iterrows():
+
+            # Select sample rows
+            dfSampRes = self._dfData.loc[self._dfData[self.sampleIndCol] == lblSamp]
+            logger.info3('#{} {} : {} rows'
+                         .format(lblSamp, ', '.join([f'{k[1]}={v}' for k, v in sSamp.items()]), len(dfSampRes)))
+
+            # Compute key values for all schemes for this sample
+            dSampKeys = self._sampleFilterSortKeys(dfSampRes, ldFilSorKeySchemes)
+
+            # Store them for later concatenation
+            for colLbl, sFSKeys in dSampKeys.items():
+                if colLbl not in dFilSorKeys:
+                    dFilSorKeys[colLbl] = list()  # list(Series)
+                dFilSorKeys[colLbl].append(sFSKeys)
+
+        # Concat series of computed group nums (opt or not) for each target distance column to group
+        return {colLbl: pd.concat(lsFSKeys) for colLbl, lsFSKeys in dFilSorKeys.items()}
+
+    def _postComputeFilterSortKeys(self):
+
+        """Compute and add partial or global order columns for later filtering and sorting"""
+
+        logger.info2('Post-computing Filter and Sort keys')
+
+        # Compute keys for each sample and add relevant columns to results.
+        for colLbl, sFSKeys in self._filterSortKeys().items():
+            self._dfData[colLbl] = sFSKeys
+
+    # Post-computations : All of them.
+    def postComputeColumns(self):
+        
+        self._postComputeChi2()
+        self._postComputeDeltaAicDCv()
+        self._postComputeQualityIndicators()
+        self._postComputeTruncationGroups()
+        self._postComputeFilterSortKeys()
+
+    # Tools for actually filtering results
+    @classmethod
+    def _indexOfDuplicates(cls, dfRes, keep='first', subset=list(), round2decs=dict()):
+
+        """Compute the indices of duplicates to remove in a data-frame,
+        keep=first means that the first item of an "equality" set is the one kept at the end
+        Warning: No sorting done here => do it before to get the right first !
+        """
+        if round2decs:
+            # dfRes = dfRes.round(round2decs)  # Buggy (pandas 1.0.x up to 1.1.2): forgets columns !?!?!?
+            if len(subset) > 0:
+                pass  # TODO: Optimise = only copy subset cols
+            dfRes = dfRes.copy()
+            for col, dec in round2decs.items():
+                if len(subset) == 0 or col in subset:
+                    dfRes[col] = dfRes[col].apply(lambda x: x if pd.isnull(x) else round(x, ndigits=dec))
+
+        return dfRes[dfRes.duplicated(keep=keep, subset=subset)].index
+
+    @classmethod
+    def _indexOfWorstOneCriterion(cls, dfRes, sampleIds, sampleIdCol, critCol, ascendCrit=True, nTgtRes=10):
+
+        """Filtering function enforcing a target number of best output results based on 1 given criterion.
+        ascendCrit=True means that the best criterion values are the smallest ones
+        """
+
+        i2Drop = pd.Index([], dtype=dfRes.index.dtype)
+
+        # For each sample to filter ...
+        for sampId in sampleIds:
+            # Extract sample results, and sort them  based on the criterion.
+            dfSampRes = dfRes[dfRes[sampleIdCol] == sampId].sort_values(by=critCol, ascending=ascendCrit)
+
+            # Done for this sample.
+            i2Drop = i2Drop.append(dfSampRes.index[nTgtRes:])  # Will drop indices after the N best's ones
+
+        return i2Drop
+
+    @classmethod
+    def _indexOfWorstMultiOrderCriteria(cls, dfRes, critCols=list(), supCrit=1):
+
+        """Filtering function enforcing a max(sup) order for output results based on multiple given criterion
+        (results will be dropped if not in the supCrit best ones for at least 1 of the order criteria)
+        """
+
+        sb2keep = pd.Series(data=False, index=dfRes.index)
+        for critCol in critCols:
+            sb2keep |= (dfRes[critCol] < supCrit)
+
+        return dfRes[~sb2keep].index
+
+    def filSorSchemeId(self, scheme):
+
+        return self.filSorIdMgr.schemeId(scheme)
+
+    @classmethod
+    def _filterOnExecCode(cls, dfFilSorRes, filSorSteps, sampleIndCol, dupSubset, dDupRounds):
+
+        """Inplace filter out results based on exec code and truncation params duplicates
+
+        Details:
+        1. drop results with error ExecCode >= 3
+        2. drop results with identical truncation distances (keep best exec codes)
+
+        Note: This doesn't actually modify a single bit of the results set, but returns the resulting
+              filtered and sorted index, suitable for indexing on self.dfData / dfTransData ...
+
+        Parameters:
+        :param dfFilSorRes: results table to update
+        :param filSorSteps: filter and sort step list to update
+        :param sampleIndCol: sample index = identification column
+        :param dupSubset: Subset of (3-level multi-index) columns for detecting duplicates (as a list of tuples)
+                          Warning: self.sampleIndCol is automatically prepended to this list if not already inside
+        :param dDupRounds: {col: nb decimals} => number of decimals to keep (after rounding)
+                           for a sub-set or all of dupSubset columns
+        """
+
+        # 1. Filter-out results obtained with some computation error (whatever sample).
+        dfFilSorRes.drop(dfFilSorRes[dfFilSorRes[cls.CLRunStatus] > MCDSEngine.RCWarnings].index, inplace=True)
+        stepId = 'run status'
+        filSorSteps.append(stepId, 'column', cls.CLRunStatus, transColumns=True)
+        filSorSteps.append(stepId, 'max value', MCDSEngine.RCWarnings)
+        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
+
+        # 2. Filter-out results which are duplicates with respect to sample and truncation distances,
+        # keeping best run status code first.
+        dfFilSorRes.sort_values(by=[sampleIndCol, cls.CLParTruncLeft, cls.CLParTruncRight, cls.CLRunStatus],
+                                ascending=True, na_position='first', inplace=True)
+        if sampleIndCol not in dupSubset:
+            dupSubset = [sampleIndCol] + dupSubset
+        dfFilSorRes.drop(cls._indexOfDuplicates(dfFilSorRes, keep='first', subset=dupSubset, round2decs=dDupRounds),
+                         inplace=True)
+        stepId = 'duplicates on params'
+        filSorSteps.append(stepId, 'param. names', dupSubset, transColumns=True)
+        filSorSteps.append(stepId, 'param. precisions', ', '.join(filSorSteps.transColumns(miCol) + f': {nDec}'
+                                                                  for miCol, nDec in dDupRounds.items()))
+        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
+
+    LDupSubsetDef = [CLNObs, CLEffort, CLDeltaAic, CLChi2, CLKS, CLCvMUw, CLCvMCw, CLDCv,
+                     CLPDetec, CLPDetecMin, CLPDetecMax, CLDensity, CLDensityMin, CLDensityMax]
+    DDupRoundsDef = {CLDeltaAic: 1, CLChi2: 2, CLKS: 2, CLCvMUw: 2, CLCvMCw: 2, CLDCv: 2,
+                     CLPDetec: 3, CLPDetecMin: 3, CLPDetecMax: 3, CLDensity: 2, CLDensityMin: 2, CLDensityMax: 2}
+
+    @classmethod
+    def _sortOnQua(cls, dfFilSorRes, filSorSteps, sampleIndCol, whichQua=CLCmbQuaBal3, ascendQua=False):
+
+        """Inplace sort results per sample based on truncation distances and a customisable quality indicator
+
+        Parameters:
+        :param dfFilSorRes: results table to update
+        :param filSorSteps: filter and sort step list to update
+        :param sampleIndCol: sample index = identification column
+        :param whichQua: Quality indicator (not order of) column to use
+        :param ascendQua: if True, lower whichQua indicator values are better
+        """
+
+        sortCols = [cls.CLParTruncLeft, cls.CLParTruncRight, whichQua]
+        dfFilSorRes.sort_values(by=[sampleIndCol] + sortCols, ascending=[True, True, True, ascendQua],
+                                na_position='first', inplace=True)
+        stepId = 'final sorting'
+        filSorSteps.append(stepId, 'columns', sortCols, transColumns=True)
+        filSorSteps.append(stepId, 'lower last column is better ?', ascendQua)
+
+    def filterSortOnExecCode(self, schemeId, lang, whichFinalQua=CLCmbQuaBal3, ascFinalQua=False,
+                             dupSubset=LDupSubsetDef, dDupRounds=DDupRoundsDef):
+
+        """Minimal filter and sort scheme
+
+        Details:
+        1. drop results with error ExecCode >= 3
+        2. drop results with identical truncation distances (keep best exec codes)
+        3. sort on truncation distances and specified indicator
+
+        Note: This doesn't actually modify a single bit of the results set, but returns the resulting
+              filtered and sorted index, suitable for indexing on self.dfData / dfTransData ...
+
+        Parameters:
+        :param schemeId: Scheme identification, for traceability
+        :param lang: Translation language, for traceability
+        :param whichFinalQua: Quality indicator (not order of) column to use for final sorting
+        :param ascFinalQua: if True, lower whichFinalQua indicator values are better
+        :param dupSubset: Subset of (3-level multi-index) columns for detecting duplicates (as a list of tuples)
+                          Warning: self.sampleIndCol is automatically prepended to this list if not already inside
+        :param dDupRounds: {col: nb decimals} => number of decimals to keep (after rounding)
+                           for a sub-set or all of dupSubset columns
+
+        :return: tuple(index of selected and sorted results, log of filter & sort steps accomplished)
+        """
+
+        cls = self
+
+        logger.debug(f'Filter and sort scheme "{schemeId}": Applying.')
+
+        filSorSteps = _FilterSortSteps(schemeId, resultsSet=self, lang=lang)
+
+        # 0. Retrieve results to filter and sort.
+        dfFilSorRes = self.getData(copy=True)
+        filSorSteps.append('before', 'datetime', pd.Timestamp.now())
+        filSorSteps.append('before', 'results', len(dfFilSorRes))
+
+        # 1&2. Filter-out results with some computation error, and also duplicates based on same truncation params.
+        self._filterOnExecCode(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
+                               dupSubset=dupSubset, dDupRounds=dDupRounds)
+
+        # 3. Final sorting : increasing order for truncation distances (and final quality indicator if needed).
+        self._sortOnQua(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
+                        whichQua=whichFinalQua, ascendQua=ascFinalQua)
+
+        # Done.
+        filSorSteps.append('after', 'results', len(dfFilSorRes))
+
+        return dfFilSorRes.index, filSorSteps
+
+    @classmethod
+    def _filterOnAicMultiQua(cls, dfFilSorRes, filSorSteps, sampleIndCol,
+                             minSightRate=90, nBestAicOrd=2,
+                             nBestMQuaOrd=1, whichBestMQuaOrd=[CLGrpOrdClTrChi2KSDCv, CLGrpOrdClTrQuaBal3],
+                             nFinalQua=10, whichFinalQua=CLCmbQuaBal3, ascFinalQua=False):
+
+        """Inplace filter out results based on a mostly customisable selection of quality indicators
+
+        Details:
+        1. Per sample and group of IDENTICAL left and right truncation distances,
+           keep only the results with CLGrpOrdSmTrAic < nBestAicOrd ;
+           Note: this is generally close to: the nBestAicOrd best AIC values
+            (and if equal, the best Chi2, KS, DCv, NObs, CodEx ... etc)
+           but may actually give more than nBestAicOrd rows per sample and group of ...
+        2. Per sample and group of close truncation distances (see _postComputeTruncationGroups),
+           keep only the results with at least one of the whichBestMQuaOrd indicator orders < nBestMQuaOrd ;
+           Note: this will probably give more than nBestMQuaOrd rows par sample and group of ...
+        3. Eliminate sighting rates < minSightRate,
+        4. Keep only the nFinalQua best results, with respect to whichFinalQua indicator
+           (ascendFinalQua=True meaning that lower values are better)
+
+        Parameters:
+        :param dfFilSorRes: results table to update
+        :param filSorSteps: filter and sort step list to update
+        :param sampleIndCol: sample index = identification column
+        :param minSightRate: Minimal observation rate (ratio of NTot Obs / NObs, seldom=1 because of dist. truncations)
+        :param nBestAicOrd: Upper limit (excluded) for CLGrpOrdSmTrAic quality indicator order
+                            (per sample and IDENTICAL truncation parameters)
+        :param nBestMQuaOrd: Upper limit (excluded) for whichBestMQuaOrd for quality indicator orders
+                             (keep results with at least 1 of them under the limit)
+        :param whichBestMQuaOrd: Quality indicator order columns to use for filtering best results per sample
+                              and close truncation distances
+        :param nFinalQua: Final nb of best whichFinalQua indicator (not order of) results to keep per sample
+        :param whichFinalQua: Quality indicator (not order of) column to use
+                              for final "best results per sample" selection
+        :param ascFinalQua: if True, lower whichFinalQua indicator values are better
+        """
+
+        # 1. Filter-out results with poorest AIC, per groups of same sample and IDENTICAL truncation distances.
+        stepId = 'best AIC'
+        filSorSteps.append(stepId, 'column', cls.CLGrpOrdSmTrAic, transColumns=True)
+        filSorSteps.append(stepId, 'upper limit (excluded)', nBestAicOrd)
+        dfFilSorRes.drop(dfFilSorRes[dfFilSorRes[cls.CLGrpOrdSmTrAic] >= nBestAicOrd].index,
+                         inplace=True)
+        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
+
+        # 2. Filter-out results not in N best ones for at least 1 in a specified set of quality indicator,
+        # per groups of same sample and CLOSE truncation distances.
+        stepId = 'best results for >= 1 indicator'
+        filSorSteps.append(stepId, 'selected indicator orders', whichBestMQuaOrd, transColumns=True)
+        filSorSteps.append(stepId, 'order limit (excluded) / indicator', nBestMQuaOrd)
+        i2Drop = cls._indexOfWorstMultiOrderCriteria(dfFilSorRes, critCols=whichBestMQuaOrd, supCrit=nBestMQuaOrd)
+        dfFilSorRes.drop(labels=i2Drop, inplace=True)
+        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
+
+        # 3. Filter-out results with insufficient considered sightings rate
+        #    (due to a small sample or truncations params).
+        stepId = 'non-outlier sightings'
+        filSorSteps.append(stepId, 'min sighting rate', minSightRate)
+        dfFilSorRes.drop(dfFilSorRes[dfFilSorRes[cls.CLSightRate] < minSightRate].index, inplace=True)
+        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
+
+        # 4. Filter-out eventually too numerous results, keeping only the N best ones
+        #    with respect to the specified quality indicator (real indicator, not relevant close trunc order).
+        stepId = 'final best results'
+        filSorSteps.append(stepId, 'column', whichFinalQua, transColumns=True)
+        filSorSteps.append(stepId, 'lower is better ?', ascFinalQua)
+        filSorSteps.append(stepId, 'max results', nFinalQua)
+        i2Drop = cls._indexOfWorstOneCriterion(dfFilSorRes, sampleIds=dfFilSorRes[sampleIndCol].unique(),
+                                               sampleIdCol=sampleIndCol, nTgtRes=nFinalQua,
+                                               critCol=whichFinalQua, ascendCrit=ascFinalQua)
+        dfFilSorRes.drop(labels=i2Drop, inplace=True)
+        filSorSteps.append(stepId, 'results', len(dfFilSorRes))
+
+    def filterSortOnExCAicMulQua(self, schemeId, lang, sightRate=95, nBestAIC=2, nBestQua=1,
+                                 whichBestQua=[CLGrpOrdClTrChi2KSDCv, CLGrpOrdClTrDCv, CLGrpOrdClTrQuaBal1,
+                                               CLGrpOrdClTrQuaChi2, CLGrpOrdClTrQuaKS, CLGrpOrdClTrQuaDCv],
+                                 nFinalRes=10, whichFinalQua=CLCmbQuaBal3, ascFinalQua=False,
+                                 dupSubset=LDupSubsetDef, dDupRounds=DDupRoundsDef):
+
+        """Filter and sort scheme for selecting best results with respect to a set of quality indicators,
+        all taken at the same priority, except for one used to limit results at the end,
+        all of this after same filtering on exec code and truncation param duplicates as in filterSortOnExecCode
+
+        Details:
+        1. Same filtering as filterSortOnExecCode
+        2. Per sample and group of IDENTICAL left and right truncation distances,
+           keep only the results with CLGrpOrdSmTrAic < nBestAIC ;
+           Note: this is generally close to: the nBestAIC best AIC values
+            (and if equal, the best Chi2, KS, DCv, NObs, CodEx ... etc)
+           but may actually give more than nBestAIC rows per sample and group of ...
+        3. Per sample and group of close truncation distances (see _postComputeTruncationGroups),
+           keep only the results with a least one of the whichBestQua indicator orders < nBestQua ;
+           Note: this will probably give more than nBestQua rows par sample and group of ...
+        4. Eliminate sighting rates < sightRate,
+        5. Keep only the nFinalRes best results, with respect to whichFinalQua indicator
+           (ascendFinalQua=True meaning that lower values are better)
+        6. Finally, sort by truncation distances (no truncation first, shorter distances first, ... simpler first)
+           and by best whichFinalQua indicator values
+
+        Note: This doesn't actually modifies a single bit of the results set, but returns the resulting
+              filtered and sorted index, suitable for indexing on self.dfData / dfTransData ...
+
+        Parameters:
+        :param schemeId: Scheme identification, for traceability
+        :param lang: Translation language, for traceability
+        :param sightRate: Minimal observation rate (ratio of NTot Obs / NObs, not 1 because of dist. truncations)
+        :param nBestAIC: Nb of best AIC results to keep per sample and IDENTICAL truncation parameters
+        :param nBestQua: Nb of best results to keep per sample with respect to each quality indicator specified
+                         through its related order column name in whichBestQua
+        :param whichBestQua: Quality indicator order columns to use for filtering best results per sample
+                             (at most nBestQua best results are kept for each related indicator ;
+                              to be retained, a result MUST be among the nBestQua best ones for ALL
+                              the specified indicators)
+        :param nFinalRes: Final nb of best whichFinalQua results to keep per sample
+        :param whichFinalQua: Quality indicator order column to use for final "best results per sample" selection
+        :param ascFinalQua: if True, lower whichFinalQua indicator values are better
+        :param dupSubset: Subset of (3-level multi-index) columns for detecting duplicates (as a list of tuples)
+                          Warning: self.sampleIndCol is automatically prepended to this list if not already inside
+        :param dDupRounds: {col: nb decimals} => number of decimals to keep (after rounding)
+                           for a sub-set or all of dupSubset columns
+
+        :return: tuple(index of selected and sorted results, log of filter & sort steps accomplished)
+        """
+
+        logger.debug(f'Filter and sort scheme "{schemeId}": Applying.')
+
+        filSorSteps = _FilterSortSteps(schemeId, resultsSet=self, lang=lang)
+
+        # 0. Retrieve results to filter and sort.
+        dfFilSorRes = self.getData(copy=True)
+        filSorSteps.append('before', 'datetime', pd.Timestamp.now())
+        filSorSteps.append('before', 'results', len(dfFilSorRes))
+
+        # 1. Filter-out results with some computation error, and also duplicates based on same truncation params.
+        self._filterOnExecCode(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
+                               dupSubset=dupSubset, dDupRounds=dDupRounds)
+
+        # 2. Filter-out results with poorest AIC, per groups of same sample and IDENTICAL truncation distances.
+        # 3. Filter-out results with poorest values of specified quality indicators,
+        #    per groups of same sample and CLOSE truncation distances.
+        # 4. Filter-out results with insufficient considered sightings rate
+        #    (due to a small sample or truncations params).
+        # 5. Filter-out eventually too numerous results, keeping only the N best ones
+        #    with respect to the specified quality indicator.
+        self._filterOnAicMultiQua(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
+                                  minSightRate=sightRate, nBestAicOrd=nBestAIC,
+                                  nBestMQuaOrd=nBestQua, whichBestMQuaOrd=whichBestQua,
+                                  nFinalQua=nFinalRes, whichFinalQua=whichFinalQua, ascFinalQua=ascFinalQua)
+
+        # 6. Final sorting : increasing order for truncation distances (and final quality indicator if needed).
+        self._sortOnQua(dfFilSorRes, filSorSteps, sampleIndCol=self.sampleIndCol,
+                        whichQua=whichFinalQua, ascendQua=ascFinalQua)
+
+        # Done.
+        filSorSteps.append('after', 'results', len(dfFilSorRes))
+
+        return dfFilSorRes.index, filSorSteps
+
+    def _preselColumn(self, srcCol):
+        """Pre-selection column label (from source column) and translation"""
+        return ((srcCol[0], srcCol[1], self.CLTPreSelection),
+                dict(fr='Pré-sélection ' + self.transColumn(srcCol, 'fr'),
+                     en='Pre-selection ' + self.transColumn(srcCol, 'en')))
+
+    def _addPreselColumns(self, dfFilSorRes, filSorSteps,
+                          preselCols=[CLCmbQuaBal1], preselAscend=True, preselThresh=[0.2], nSamplePreSels=5):
+
+        """Add (in-place) a pre-selection column to a filtered and sorted translated results table
+
+        Parameters:
+        :param dfFilSorRes: the filtered and sorted table to update
+        :param filSorSteps: the filtered and sorted step log to update
+        :param nSamplePreSels: Max number of generated pre-selections per sample
+        :param preselCols: Results columns to use for generating auto-preselection indices (in [1, nSamplePreSels])
+        :param preselAscend: Order to use for each column (list[bool]), or all (single bool) ;
+                             True means that lower values are "better" ones, and the other way round
+        :param preselThresh: Value above (ascend=True) or below () which no preselection is proposed
+                             (=> nan in target column), for each column (list[number]), or all (single number)
+
+        """
+
+        cls = self
+
+        if isinstance(preselAscend, bool):
+            preselAscend = [preselAscend] * len(preselCols)
+        assert len(preselCols) == len(preselAscend), \
+               'preselAscend must be a single bool or a list(bool) with len(preselCols)'
+
+        if isinstance(preselThresh, (int, float)):
+            preselThresh = [preselThresh] * len(preselCols)
+        assert len(preselCols) == len(preselThresh), \
+               'preselAscend must be a single number or a list(number) with len(preselCols)'
+
+        filSorSteps.append('auto-preselection', 'Nb of pre-selections', nSamplePreSels)
+
+        # Create each pre-selection column: rank per sample in preselCol/ascending (or not) order
+        # up to nSamplePreSels (but no preselection under / over threshold).
+        for srcCol, srcColAscend, srcColThresh in zip(preselCols, preselAscend, preselThresh):
+
+            # Determine label and translation.
+            tgtPreSelCol, dTgtPreSelColTrans = self._preselColumn(srcCol)
+            self.addColumnsTrans({tgtPreSelCol: dTgtPreSelColTrans})
+
+            filSorSteps.append('auto-preselection', 'pre-selection column', srcCol, transColumns=True)
+            filSorSteps.append('auto-preselection', 'lower is better ?', srcColAscend)
+            filSorSteps.append('auto-preselection', 'eliminating threshold', srcColThresh)
+
+            # Compute contents and add to table
+            # a. Rank all results
+            dfFilSorRes.insert(dfFilSorRes.columns.get_loc(srcCol), tgtPreSelCol,
+                               dfFilSorRes.groupby(self.miSampleCols.to_list())[[srcCol]]
+                                          .transform(lambda s: s.rank(ascending=srcColAscend,
+                                                                      method='dense', na_option='keep'))[srcCol])
+
+            # b. Nullify too big ranks and "under" threshold values
+            sbKillOnThresh = dfFilSorRes[srcCol] > srcColThresh if srcColAscend else dfFilSorRes[srcCol] < srcColThresh
+            sbKillOnNumber = dfFilSorRes[tgtPreSelCol] > nSamplePreSels
+            dfFilSorRes.loc[sbKillOnThresh | sbKillOnNumber, tgtPreSelCol] = np.nan
+
+        # Create final empty selection column (for the user to self-decide at the end)
+        # (right before the first added pre-selection column, no choice)
+        if len(preselCols) > 0:
+            dfFilSorRes.insert(dfFilSorRes.columns.get_loc(preselCols[0]) - 1, cls.CLFinalSelection, np.nan)
+
+        return dfFilSorRes
+
+    def dfFilSorData(self, scheme=dict(method=filterSortOnExecCode,
+                                       filterSort=dict(whichFinalQua=CLCmbQuaBal3, ascFinalQua=False),
+                                       preselCols=[CLCmbQuaBal3], preselAscs=False, preselThrhs=[0.2],
+                                       preselNum=5),
+                     columns=None, lang=None, rebuild=False):
+
+        """Extract filtered and sorted data following the given scheme
+
+        Note: Let R be MCDSAnalysisResultsSet, or a subclass (needed below).
+        
+        Parameters:
+        :param scheme: filter and sort scheme to apply
+                 as a dict(method= ResClass.filterSortOnXXX method to use,
+                           deduplicate= dict(dupSubset=, dDupRounds=) of deduplication params
+                               (if not or partially given, see RCLS.filterSortOnXXX defaults)
+                           filterSort= dict of other <method> params,
+                           preselCols= target columns for generating auto-preselection ones,
+                                       containing [1, preselNum] ranks ; default: []
+                           preselAscs= Rank direction to use for each column (list),
+                                       or all (single bool) ; default: True
+                                       (True means that lower values are "better" ones)
+                           preselThrhs= Eliminating threshold for each column (list),
+                                        or all (single number) ; default: 0.2
+                                        (eliminated above if preselAscs True, below otherwise)
+                           preselNum= number of (best) pre-selections to keep for each sample) ;
+                                      default: 5)
+                 examples: dict(method=R.filterSortOnExecCode,
+                                filterSort=dict(whichFinalQua=CLCmbQuaBal3, ascFinalQua=False),
+                                preselCols=[R.CLCmbQuaBal1, R.CLCmbQuaBal2], preselAscs=False,
+                                preselThrhs=0.2, preselNum=5),
+                           dict(method=R.filterSortOnExCAicMulQua,
+                                deduplicate=dict(dupSubset=[R.CLNObs, R.CLEffort, R.CLDeltaAic, R.CLChi2,
+                                                            R.CLKS, R.CLCvMUw, R.CLCvMCw, R.CLDCv]),
+                                                 dDupRounds={R.CLDeltaAic: 1, R.CLChi2: 2, R.CLKS: 2,
+                                                             R.CLCvMUw: 2, R.CLCvMCw: 2, R.CLDCv: 2})
+                                filterSort=dict(sightRate=92.5, nBestAIC=3, nBestQua=1, 
+                                                whichBestQua=[R.CLGrpOrdClTrChi2KSDCv, R.CLGrpOrdClTrDCv,
+                                                              R.CLGrpOrdClTrQuaBal1, R.CLGrpOrdClTrQuaChi2,
+                                                              R.CLGrpOrdClTrQuaKS, R.CLGrpOrdClTrQuaDCv],
+                                                nFinalRes=12, whichFinalQua=R.CLCmbQuaBal1, ascFinalQua=False),
+                                preselCols=[R.CLCmbQuaBal1, R.CLDCv], preselAscs=[False, True],
+                                preselThrhs=[0.2, 0.5], preselNum=3)
+        :param columns: Subset and order of columns to keep at the end (before translation) (None = [] = all)
+                        Warning: No need to specify here pre-selection and final selection columns,
+                                 as they'll be added automatically, and relocated at a non-customisable place.
+        :param lang: Target language for column name translation (if None, no translation => keep original names)
+        :param rebuild: If True, rebuild filtered and sorted table ; otherwise, simply reuse cached data
+               if the results set didn't change enough meanwhile.
+
+        :return: tuple(scheme id, result data-frame, log of completed filter and sort steps
+                       as a list of [schemeId, step name, property name, property value])
+        """
+
+        # Check (1/2) if need for applying scheme : needed if rebuild requested or post-computation needed.
+        isApplySchemeNeeded = rebuild or not self.postComputed
+        filSorSchId = self.filSorIdMgr.schemeId(scheme)
+
+        # If not needed, check (2/2) also if same scheme have been applied yet
+        if not isApplySchemeNeeded:
+            iFilSor, filSorSteps = self.filSorCache.get(filSorSchId)
+            isApplySchemeNeeded = iFilSor is None  # Not already applied => to apply now !
+
+        # Apply scheme if needed.
+        if isApplySchemeNeeded:
+
+            # Do it => index of filtered and sorted results + log of steps
+            iFilSor, filSorSteps = \
+                scheme['method'](self, schemeId=filSorSchId, lang=lang or 'en',
+                                 **scheme.get('filterSort', {}), **scheme.get('deduplicate', {}))
+
+            # Update cache
+            self.filSorCache.update(filSorSchId, iFilSor, filSorSteps)
+
+        # Actually extract filtered and sorted rows and selected columns.
+        dfFilSorRes = self.dfSubData(index=iFilSor, columns=columns, copy=True)
+
+        # Add the preselection column (and update filter and sort log)
+        if not dfFilSorRes.empty:
+            dfFilSorRes = self._addPreselColumns(dfFilSorRes, filSorSteps,
+                                                 nSamplePreSels=scheme.get('preselNum', 5),
+                                                 preselCols=scheme.get('preselCols', []),
+                                                 preselAscend=scheme.get('preselAscs', True),
+                                                 preselThresh=scheme.get('preselThrhs', 0.2))
+
+        # Final translation if specified.
+        if lang:
+            dfFilSorRes.columns = self.transColumns(dfFilSorRes.columns, lang)
+
+        # Done.
+        return filSorSchId, dfFilSorRes, filSorSteps.toList()
+
+
+class MCDSAnalyser(DSAnalyser):
+
+    """Run a bunch of MCDS analyses
+    """
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
+                 sampleDecCols=['Effort', 'Distance'], anlysSpecCustCols=[],
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], 
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
+                                    dict(col='right', minDist=25.0, maxLen=25.0)], truncIntrvEpsilon=1e-6,
+                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
+                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False, logProgressEvery=50,
+                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
+                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
+                 defMinDist=MCDSEngine.DistMinDef, defMaxDist=MCDSEngine.DistMaxDef, 
+                 defFitDistCuts=MCDSEngine.DistFitCutsDef, defDiscrDistCuts=MCDSEngine.DistDiscrCutsDef):
+
+        """Ctor
+
+        Parameters:
+        :param anlysSpecCustCols: Special columns from analysis specs to simply pass through in results
+        :param runTimeOut: time-out for every analysis run (s) (None => no limit)
+        :param runMethod: for calling MCDS engine executable : 'os.system' or 'subprocess.run'
+        :param ldTruncIntrvSpecs: separation and length specs for truncation group intervals computation
+                                  (postComputations for automated results filtering and sorting)
+        :param truncIntrvEpsilon: epsilon for truncation group intervals computation (idem)
+        """
+
+        assert distanceUnit == 'Meter', 'Not implemented: Only "Meter" distance unit supported for the moment'
+
+        super().__init__(dfMonoCatObs=dfMonoCatObs, dfTransects=dfTransects, 
+                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
+                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol,
+                         effortCol=effortCol, sampleSelCols=sampleSelCols,
+                         sampleDecCols=sampleDecCols, anlysSpecCustCols=anlysSpecCustCols,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         resultsHeadCols=resultsHeadCols, abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
+                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol, workDir=workDir)
+                         
+        assert logProgressEvery > 0, 'logProgressEvery must be positive'
+
+        self.surveyType = surveyType
+        self.distanceType = distanceType
+        self.clustering = clustering
+        
+        self.runMethod = runMethod
+        self.runTimeOut = runTimeOut
+        self.logData = logData
+        self.logProgressEvery = logProgressEvery
+
+        self.ldTruncIntrvSpecs = ldTruncIntrvSpecs
+        self.truncIntrvEpsilon = truncIntrvEpsilon
+        
+        self.defEstimKeyFn = defEstimKeyFn
+        self.defEstimAdjustFn = defEstimAdjustFn
+        self.defEstimCriterion = defEstimCriterion
+        self.defCVInterval = defCVInterval
+        self.defMinDist = defMinDist
+        self.defMaxDist = defMaxDist
+        self.defFitDistCuts = defFitDistCuts
+        self.defDiscrDistCuts = defDiscrDistCuts
+                         
+        # Specs.
+        self.updateSpecs(**{name: getattr(self, name)
+                            for name in ['runMethod', 'runTimeOut', 'surveyType', 'distanceType', 'clustering',
+                                         'defEstimKeyFn', 'defEstimAdjustFn', 'defEstimCriterion', 'defCVInterval',
+                                         'defMinDist', 'defMaxDist', 'defFitDistCuts', 'defDiscrDistCuts']})
+
+    # Analyser internal parameter spec names, for which a match should be found (when one is needed)
+    # with user explicit optimisation specs used in run() calls.
+    IntSpecEstimKeyFn = 'EstimKeyFn'
+    IntSpecEstimAdjustFn = 'EstimAdjustFn'
+    IntSpecEstimCriterion = 'EstimCriterion'
+    IntSpecCVInterval = 'CvInterval'
+    IntSpecMinDist = 'MinDist'  # Left truncation distance
+    IntSpecMaxDist = 'MaxDist'  # Right truncation distance
+    IntSpecFitDistCuts = 'FitDistCuts'
+    IntSpecDiscrDistCuts = 'DiscrDistCuts'
+
+    # Possible regexps (values) for auto-detection of analyser _internal_ parameter spec names (keys)
+    # from explicit _user_ spec columns
+    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
+    #  and case is ignored during searching).
+    Int2UserSpecREs = \
+        {IntSpecEstimKeyFn:     ['ke[a-z]*[\.\-_ ]*f', 'f[o]?n[a-z]*[\.\-_ ]*cl'],
+         IntSpecEstimAdjustFn:  ['ad[a-z]*[\.\-_ ]*s', 's[éa-z]*[\.\-_ ]*aj'],
+         IntSpecEstimCriterion: ['crit[èa-z]*[\.\-_ ]*'],
+         IntSpecCVInterval:     ['conf[a-z]*[\.\-_ ]*[a-z]*[\.\-_ ]*int',
+                                 'int[a-z]*[\.\-_ ]*conf'],
+         IntSpecMinDist:        ['min[a-z]*[\.\-_ ]*d', 'd[a-z]*[\.\-_ ]*min',
+                                 'tr[a-z]*[\.\-_ ]*g[ca]', 'le[a-z]*[\.\-_ ]*tr'],
+         IntSpecMaxDist:        ['max[a-z]*[\.\-_ ]*d', 'd[a-z]*[\.\-_ ]*max',
+                                 'tr[a-z]*[\.\-_ ]*d[rt]', 'le[a-z]*[\.\-_ ]*tr'],
+         IntSpecFitDistCuts:    ['fit[a-z]*[\.\-_ ]*d', 'tr[a-z]*[\.\-_ ]*[a-z]*[\.\-_ ]*mod'],
+         IntSpecDiscrDistCuts:  ['disc[a-z]*[\.\-_ ]*d', 'tr[a-z]*[\.\-_ ]*[a-z]*[\.\-_ ]*disc']}
+
+    # Analysis object ctor parameter names (MUST match exactly: check in analysis submodule !).
+    ParmEstimKeyFn = 'estimKeyFn'
+    ParmEstimAdjustFn = 'estimAdjustFn'
+    ParmEstimCriterion = 'estimCriterion'
+    ParmCVInterval = 'cvInterval'
+    ParmMinDist = 'minDist'
+    ParmMaxDist = 'maxDist'
+    ParmFitDistCuts = 'fitDistCuts'
+    ParmDiscrDistCuts = 'discrDistCuts'
+
+    def _getAnalysisParams(self, sAnIntSpec):
+    
+        """Retrieve analysis parameters, from user specs and default values
+        
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+        
+        :return: dict(estimKeyFn=, estimAdjustFn=, estimCriterion=, cvInterval=,
+                      minDist=, maxDist=, fitDistCuts=, discrDistCuts=)
+        """
+        return {self.ParmEstimKeyFn: sAnIntSpec.get(self.IntSpecEstimKeyFn, self.defEstimKeyFn),
+                self.ParmEstimAdjustFn: sAnIntSpec.get(self.IntSpecEstimAdjustFn, self.defEstimAdjustFn),
+                self.ParmEstimCriterion: sAnIntSpec.get(self.IntSpecEstimCriterion, self.defEstimCriterion),
+                self.ParmCVInterval: sAnIntSpec.get(self.IntSpecCVInterval, self.defCVInterval),
+                self.ParmMinDist: sAnIntSpec.get(self.IntSpecMinDist, self.defMinDist),
+                self.ParmMaxDist: sAnIntSpec.get(self.IntSpecMaxDist, self.defMaxDist),
+                self.ParmFitDistCuts: sAnIntSpec.get(self.IntSpecFitDistCuts, self.defFitDistCuts),
+                self.ParmDiscrDistCuts: sAnIntSpec.get(self.IntSpecDiscrDistCuts, self.defDiscrDistCuts)}
+
+    DAnlr2ResChapName = dict(before='header (head)', sample='header (sample)', after='header (tail)')
+
+    def prepareResultsColumns(self):
+        
+        # a. Sample multi-index columns
+        sampleSelCols = self.resultsHeadCols['sample']
+        sampMCols = [(self.DAnlr2ResChapName['sample'], col, 'Value') for col in sampleSelCols]
+        miSampCols = pd.MultiIndex.from_tuples(sampMCols)
+
+        # b. Full custom multi-index columns to append and prepend to raw analysis results
+        beforeCols = self.resultsHeadCols['before']
+        custMCols = [(self.DAnlr2ResChapName['before'], col, 'Value') for col in beforeCols]
+        custMCols += sampMCols
+        
+        afterCols = self.resultsHeadCols['after']
+        custMCols += [(self.DAnlr2ResChapName['after'], col, 'Value') for col in afterCols]
+
+        customCols = beforeCols + sampleSelCols + afterCols
+        miCustCols = pd.MultiIndex.from_tuples(custMCols)
+
+        # c. Translation for it (well, no translation actually ... only one language forced for all !)
+        dfCustColTrans = pd.DataFrame(index=miCustCols, data={lang: customCols for lang in ['fr', 'en']})
+
+        # d. The 3-columns index for the sample index column
+        sampIndMCol = (self.DAnlr2ResChapName[self.sampIndResHChap], self.sampleIndCol, 'Value')
+
+        # e. And finally, the result object (sorted at the end by the analysis or else sample index column)
+        if self.anlysIndCol or self.sampleIndCol:
+            sortCols = [next(mCol for mCol in custMCols if mCol[1] == self.anlysIndCol or self.sampleIndCol)]
+        else:
+            sortCols = []
+        sortAscend = [True] * len(sortCols)
+
+        return miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend
+
+    def setupResults(self, ldFilSorKeySchemes=None):
+    
+        """Build an empty results objects.
+
+        Parameters:
+        :param ldFilSorKeySchemes: Replacement for MCDSAnalysisResultsSet predefined filter-sort key schemes
+                                   None => use predefined ones MCDSAnalysisResultsSet.AutoFilSorKeySchemes.
+        """
+    
+        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \
+            self.prepareResultsColumns()
+        
+        return MCDSAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,
+                                      miSampleCols=miSampCols, sampleIndCol=sampIndMCol,
+                                      sortCols=sortCols, sortAscend=sortAscend,
+                                      distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                      surveyType=self.surveyType, distanceType=self.distanceType,
+                                      clustering=self.clustering,
+                                      ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
+                                      truncIntrvEpsilon=self.truncIntrvEpsilon,
+                                      ldFilSorKeySchemes=ldFilSorKeySchemes)
+    
+    def _getResults(self, dAnlyses):
+    
+        """Wait for and gather dAnalyses (MCDSAnalysis futures) results into a MCDSAnalysisResultsSet 
+        """
+    
+        # Start of elapsed time measurement (yes, starting the analyses may take some time, but it is
+        # negligible when compared to analysis time ; and better here for evaluating mean per analysis).
+        anlysStart = pd.Timestamp.now()
+        
+        # Create results container.
+        results = self.setupResults()
+
+        # For each analysis as it gets completed (first completed => first yielded)
+        nDone = 0
+        for anlysFut in self._executor.asCompleted(dAnlyses):
+            
+            # Retrieve analysis object from its associated future object
+            anlys = dAnlyses[anlysFut]
+            
+            # Get analysis results
+            sResult = anlys.getResults()
+
+            # Get custom header values, and set target index (= columns) for results
+            sCustomHead = anlys.customData
+            sCustomHead.index = results.miCustomCols
+
+            # Save results
+            results.append(sResult, sCustomHead=sCustomHead)
+
+            # Report elapsed time and number of analyses completed until now
+            # (once per self.logProgressEvery analyses though).
+            nDone += 1
+            if nDone % self.logProgressEvery == 0 or nDone == len(dAnlyses):
+                now = pd.Timestamp.now()
+                elapsedTilNow = now - anlysStart
+                if nDone < len(dAnlyses):
+                    expectedEnd = \
+                        now + pd.Timedelta(elapsedTilNow.value * (len(dAnlyses) - nDone) / nDone)
+                    expectedEnd = expectedEnd.strftime('%Y-%m-%d %H:%M:%S').replace(now.strftime('%Y-%m-%d '), '')
+                    endOfMsg = 'should end around ' + expectedEnd
+                else:
+                    endOfMsg = 'done'
+                logger.info1('{}/{} analyses in {} (mean {:.3f}s): {}.'
+                             .format(nDone, len(dAnlyses), str(elapsedTilNow.round('S')).replace('0 days ', ''),
+                                     elapsedTilNow.total_seconds() / nDone, endOfMsg))
+
+        # Terminate analysis executor
+        self._executor.shutdown()
+
+        # Terminate analysis engine
+        self._engine.shutdown()
+
+        return results
+
+    def run(self, dfExplParamSpecs=None, implParamSpecs=None, threads=None):
+    
+        """Run specified analyses
+        
+        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
+        
+        Parameters:
+        :param dfExplParamSpecs: Explicit MCDS analysis param specs, as a DataFrame
+          (generated through explicitVariantSpecs, as an example),
+        :param implParamSpecs: Implicit MCDS analysis param specs, suitable for explicitation
+          through explicitVariantSpecs
+        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
+        """
+    
+        # Executor (parallel or sequential).
+        self._executor = Executor(threads=threads)
+
+        # MCDS analysis engine
+        self._engine = MCDSEngine(workDir=self.workDir, executor=self._executor,
+                                  runMethod=self.runMethod, timeOut=self.runTimeOut,
+                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                  surveyType=self.surveyType, distanceType=self.distanceType,
+                                  clustering=self.clustering)
+
+        # Custom columns for results.
+        customCols = self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
+        
+        # Explicitate and complete analysis specs, and check for usability
+        # (should be also done before calling run, to avoid failure).
+        dfExplParamSpecs, userParamSpecCols, intParamSpecCols, _, checkVerdict, checkErrors = \
+            self.explicitParamSpecs(implParamSpecs, dfExplParamSpecs, dropDupes=True, check=True)
+        assert checkVerdict, 'Analysis params check failed: {}'.format('; '.join(checkErrors))
+        
+        # For each analysis to run :
+        exptdWorkers = self._executor.expectedWorkers()
+        runHow = 'in sequence' if exptdWorkers <= 1 else f'at most {exptdWorkers} parallel threads'
+        logger.info('Running {} MCDS analyses ({}) ...'.format(len(dfExplParamSpecs), runHow))
+        dAnlyses = dict()
+        for anInd, sAnSpec in dfExplParamSpecs.iterrows():
+            
+            logger.info1(f'#{anInd+1}/{len(dfExplParamSpecs)} : {sAnSpec[self.abbrevCol]}')
+
+            # Select data sample to process
+            sds = self._mcDataSet.sampleDataSet(sAnSpec[self.sampleSelCols])
+            if not sds:
+                continue
+
+            # Build analysis params specs series with parameters internal names.
+            sAnIntSpec = sAnSpec[userParamSpecCols].set_axis(intParamSpecCols, inplace=False)
+            
+            # Get analysis parameters from user specs and default values.
+            dAnlysParams = self._getAnalysisParams(sAnIntSpec)
+            
+            # Analysis object
+            logger.debug('Anlys params: {}'.format(', '.join(f'{k}:{v}' for k, v in dAnlysParams.items())))
+            anlys = MCDSAnalysis(engine=self._engine, sampleDataSet=sds, name=sAnSpec[self.abbrevCol],
+                                 customData=sAnSpec[customCols].copy(), logData=self.logData, **dAnlysParams)
+
+            # Start running pre-analysis in parallel, but don't wait for it's finished, go on
+            anlysFut = anlys.submit()
+            
+            # Store pre-analysis object and associated "future" for later use (should be running soon or later).
+            dAnlyses[anlysFut] = anlys
+            
+            # Next analysis (loop).
+
+        logger.info('All analyses started ; now waiting for their end, and results ...')
+
+        # Wait for and gather results of all analyses.
+        self.results = self._getResults(dAnlyses)
+
+        # Set results specs for traceability.
+        self.results.updateSpecs(analyses=dfExplParamSpecs, analyser=self.flatSpecs(),
+                                 runtime=pd.Series(runtime, name='Version'))
+        
+        # Done.
+        logger.info(f'Analyses completed ({len(self.results)} results).')
+
+        return self.results
+
+
+class MCDSPreAnalysisResultsSet(MCDSAnalysisResultsSet):
+
+    """A specialized results set for MCDS pre-analyses
+    (simpler post-computations that base class MCDSAnalysisResultsSet)
+
+    TODO: Should obviously rather be the base class for MCDSAnalysisResultsSet !
+    """
+    
+    # Computed columns specs (name translation + position).
+    Super = MCDSAnalysisResultsSet
+    _firstResColInd = len(MCDSEngine.statSampCols()) + len(MCDSAnalysis.MIRunColumns)
+    DComputedCols = {Super.CLSightRate: _firstResColInd + 10,  # After Encounter Rate / Left|Right Trunc. Dist.
+                     Super.CLDeltaAic: _firstResColInd + 12,  # Before AIC
+                     Super.CLChi2: _firstResColInd + 14,  # Before all Chi2 tests
+                     Super.CLDeltaDCv: _firstResColInd + 72,  # Before Density of animals / Cv
+                     # And, at the end ...
+                     **{cl: -1 for cl in [Super.CLCmbQuaBal1, Super.CLCmbQuaBal2, Super.CLCmbQuaBal3,
+                                          Super.CLCmbQuaChi2, Super.CLCmbQuaKS, Super.CLCmbQuaDCv]}}
+
+    DfComputedColTrans = \
+        pd.DataFrame(index=DComputedCols.keys(),
+                     data=dict(en=['Obs Rate', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Density',
+                                   'Qual Bal 1', 'Qual Bal 2', 'Qual Bal 3',
+                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+'],
+                               fr=['Taux Obs', 'Delta AIC', 'Chi2 P', 'Delta CoefVar Densité',
+                                   'Qual Equi 1', 'Qual Equi 2', 'Qual Equi 3',
+                                   'Qual Chi2+', 'Qual KS+', 'Qual DCv+']))
+
+    # Needed presence in base class, but use inhibited.
+    CLFinalSelection = None
+
+    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,
+                 sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False):
+        
+        """
+        Parameters:
+        :param miSampleCols: columns to use for grouping by sample ; defaults to miCustomCols if None
+        :param sampleIndCol: multi-column index for the sample Id column ; no default, must be there !
+        """
+
+        # Initialise base.
+        super().__init__(miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
+                         miSampleCols=miSampleCols, sampleIndCol=sampleIndCol,
+                         sortCols=sortCols, sortAscend=sortAscend,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         surveyType=surveyType, distanceType=distanceType, clustering=clustering)
+
+    def copy(self, withData=True):
+        """Clone function, with optional data copy"""
+
+        # Create new instance with same ctor params.
+        clone = MCDSPreAnalysisResultsSet(miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
+                                          miSampleCols=self.miSampleCols, sampleIndCol=self.sampleIndCol,
+                                          sortCols=self.sortCols, sortAscend=self.sortAscend,
+                                          distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                          surveyType=self.surveyType, distanceType=self.distanceType,
+                                          clustering=self.clustering)
+
+        # Copy data if needed.
+        if withData:
+            clone._dfData = self._dfData.copy()
+            clone.rightColOrder = self.rightColOrder
+            clone.postComputed = self.postComputed
+            clone.dfSamples = None if self.dfSamples is None else self.dfSamples.copy()
+            clone.filSorIdMgr = self.filSorIdMgr.copy()
+            clone.filSorCache = self.filSorCache.copy()
+
+        return clone
+
+    def postComputeColumns(self):
+        """Post-computation: no need for filter and sort stuff"""
+        self._postComputeChi2()
+        self._postComputeDeltaAicDCv()
+        self._postComputeQualityIndicators()
+
+
+# Default strategy for pre-analyses model choice sequence (if one fails, take next in order, and so on)
+ModelEstimCritDef = 'AIC'
+ModelCVIntervalDef = 95
+ModelStrategyDef = [dict(keyFn=kf, adjSr='COSINE', estCrit=ModelEstimCritDef, cvInt=ModelCVIntervalDef)
+                    for kf in ['HNORMAL', 'HAZARD', 'UNIFORM', 'NEXPON']]
+
+
+class MCDSPreAnalyser(MCDSAnalyser):
+
+    """MCDSPreAnalyser: Run a bunch of MCDS pre-analyses
+    """
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(),
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], sampleDecCols=['Effort', 'Distance'],
+                 sampleSpecCustCols=[], abbrevCol='SampAbbrev', abbrevBuilder=None, sampleIndCol='SampleNum',
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 resultsHeadCols=dict(before=['SampleNum'], after=['SampleAbbrev'], 
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False, logProgressEvery=5):
+
+        super().__init__(dfMonoCatObs=dfMonoCatObs, dfTransects=dfTransects,
+                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
+                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
+                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
+                         anlysSpecCustCols=sampleSpecCustCols,
+                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder, sampleIndCol=sampleIndCol,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
+                         resultsHeadCols=resultsHeadCols, anlysIndCol=None, 
+                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut, logData=logData,
+                         logProgressEvery=logProgressEvery)
+
+        assert runTimeOut is None or runMethod != 'os.system', \
+               f"Can't care about {runTimeOut}s execution time limit with os.system run method (not implemented)"
+
+    def setupResults(self):
+    
+        """Build an empty results objects.
+        """
+    
+        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \
+            self.prepareResultsColumns()
+        
+        return MCDSPreAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,
+                                         miSampleCols=miSampCols, sampleIndCol=sampIndMCol,
+                                         sortCols=sortCols, sortAscend=sortAscend,
+                                         distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                         surveyType=self.surveyType, distanceType=self.distanceType,
+                                         clustering=self.clustering)
+    
+    def run(self, dfExplSampleSpecs=None, implSampleSpecs=None, dModelStrategy=ModelStrategyDef, threads=None):
+    
+        """Run specified analyses
+        
+        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
+
+        Parameters:
+        :param dfExplSampleSpecs: Explicit sample specs, as a DataFrame
+          (generated through explicitVariantSpecs, as an example),
+        :param implSampleSpecs: Implicit sample specs, suitable for explicitation
+          through explicitVariantSpecs
+        :param dModelStrategy: Sequence of fallback models to use when analyses fails.
+        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
+        """
+    
+        # Executor (parallel or sequential).
+        self._executor = Executor(threads=threads)
+
+        # MCDS analysis engine (a sequential one: 'cause MCDSPreAnalysis does the parallel stuff itself).
+        # Failed try: Seems we can't stack ThreadPoolExecutors, as pre-analyses get run sequentially
+        #             when using an Executor(threads=1) (means async) for self._engine ... 
+        # engineExor = None if self.runMethod != 'os.system' or self.runTimeOut is None else Executor(threads=1)
+        self._engine = MCDSEngine(workDir=self.workDir,  # executor=engineExor,
+                                  runMethod=self.runMethod, timeOut=self.runTimeOut,
+                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                  surveyType=self.surveyType, distanceType=self.distanceType,
+                                  clustering=self.clustering)
+
+        # Custom columns for results.
+        customCols = \
+            self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
+        
+        # Explicitate and complete analysis specs, and check for usability
+        # (should be also done before calling run, to avoid failure).
+        dfExplSampleSpecs, _, _, _, checkVerdict, checkErrors = \
+            self.explicitParamSpecs(implSampleSpecs, dfExplSampleSpecs, dropDupes=True, check=True)
+        assert checkVerdict, 'Pre-analysis params check failed: {}'.format('; '.join(checkErrors))
+        
+        # For each sample to analyse :
+        exptdWorkers = self._executor.expectedWorkers()
+        runHow = 'in sequence' if exptdWorkers <= 1 else f'at most {exptdWorkers} parallel threads'
+        logger.info('Running {} MCDS pre-analyses ({}) ...'.format(len(dfExplSampleSpecs), runHow))
+        
+        dAnlyses = dict()
+        for sampInd, sSampSpec in dfExplSampleSpecs.iterrows():
+            
+            logger.info1(f'#{sampInd+1}/{len(dfExplSampleSpecs)} : {sSampSpec[self.abbrevCol]}')
+
+            # Select data sample to process
+            sds = self._mcDataSet.sampleDataSet(sSampSpec[self.sampleSelCols])
+            if not sds:
+                continue  # No data => no analysis.
+
+            # Pre-analysis object
+            anlys = MCDSPreAnalysis(engine=self._engine, executor=self._executor,
+                                    sampleDataSet=sds, name=sSampSpec[self.abbrevCol],
+                                    customData=sSampSpec[customCols].copy(),
+                                    logData=False, modelStrategy=dModelStrategy)
+
+            # Start running pre-analysis in parallel, but don't wait for it's finished, go on
+            anlysFut = anlys.submit()
+            
+            # Store pre-analysis object and associated "future" for later use (should be running soon or later).
+            dAnlyses[anlysFut] = anlys
+            
+            # Next analysis (loop).
+
+        logger.info('All analyses started ; now waiting for their end, and results ...')
+
+        # Wait for and gather results of all analyses.
+        self.results = self._getResults(dAnlyses)
+
+        # Set results specs for traceability.
+        self.results.updateSpecs(samples=dfExplSampleSpecs, models=pd.DataFrame(dModelStrategy),
+                                 analyser=self.flatSpecs(), runtime=pd.Series(runtime, name='Version'))
+        
+        # Done.
+        logger.info('Analyses completed.')
+
+        return self.results
+
+    def exportDSInputData(self, dfExplSampleSpecs=None, implSampleSpecs=None, format='Distance'):
+    
+        """Export specified data samples to the specified DS input format, for "manual" DS analyses
+        
+        Parameters:
+        :param dfExplSampleSpecs: Explicit sample specs, as a DataFrame
+          (generated through explicitVariantSpecs, as an example),
+        :param implSampleSpecs: Implicit sample specs, suitable for explicitation
+          through explicitVariantSpecs
+        :param format: output files format, only 'Distance' supported for the moment.
+        """
+    
+        assert format == 'Distance', 'Only Distance format supported for the moment'
+    
+        # MCDS analysis engine
+        self._engine = MCDSEngine(workDir=self.workDir, runMethod=self.runMethod, timeOut=self.runTimeOut,
+                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                  surveyType=self.surveyType, distanceType=self.distanceType,
+                                  clustering=self.clustering)
+
+        # Explicitate and complete analysis specs, and check for usability
+        # (should be also done before calling run, to avoid failure).
+        dfExplSampleSpecs, _, _, _, checkVerdict, checkErrors = \
+            self.explicitParamSpecs(implSampleSpecs, dfExplSampleSpecs, dropDupes=True, check=True)
+        assert checkVerdict, 'Sample specs check failed: {}'.format('; '.join(checkErrors))
+        
+        # For each sample to export:
+        logger.info('Exporting {} samples to {} format ...'.format(len(dfExplSampleSpecs), format))
+        logger.debug(dfExplSampleSpecs)
+
+        for sampInd, sSampSpec in dfExplSampleSpecs.iterrows():
+            
+            # Selection des données
+            sds = self._mcDataSet.sampleDataSet(sSampSpec[self.sampleSelCols])
+            if not sds:
+                logger.warning('#{}/{} => No data in {} sample, no file exported'
+                               .format(sampInd+1, len(dfExplSampleSpecs), sSampSpec[self.abbrevCol]))
+                continue
+
+            # Export to Distance
+            fpn = pl.Path(self.workDir) / '{}-dist.txt'.format(sSampSpec[self.abbrevCol])
+            fpn = self._engine.buildDistanceDataFile(sds, tgtFilePathName=fpn)
+
+            logger.info1('#{}/{} => {}'.format(sampInd+1, len(dfExplSampleSpecs), fpn.name))
+
+        # Done.
+        self._engine.shutdown()
+
+        logger.info(f'Done exporting.')
```

### Comparing `pyaudisam-0.9.3/pyaudisam/analysis.py` & `pyaudisam-1.0.1/pyaudisam/analysis.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,386 +1,386 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "analysis": One layer above engines, to run DS analyses from an imput data set, and get computation results
-
-import sys
-import pathlib as pl
-import shutil
-import argparse
-
-import numpy as np
-import pandas as pd
-
-import concurrent.futures as cofu
-
-from . import log
-from .engine import DSEngine, MCDSEngine
-from .executor import Executor
-
-logger = log.logger('ads.ans')
-
-
-# Analysis (abstract) : Gather input params, data set, results, debug and log files
-class DSAnalysis(object):
-    
-    EngineClass = DSEngine
-    
-    # Run columns for output : root engine output (3-level multi-index)
-    CLRunStatus = ('run output', 'run status', 'Value')
-    CLRunStartTime = ('run output', 'start time', 'Value')
-    CLRunElapsedTime = ('run output', 'elapsed time', 'Value')
-    CLRunFolder = ('run output', 'run folder', 'Value')
-
-    RunRunColumns = [CLRunStatus, CLRunStartTime, CLRunElapsedTime, CLRunFolder]
-    RunFolderColumn = next(iter(col for col in RunRunColumns if col[1] == 'run folder'))
-    
-    # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
-    DRunRunColumnTrans = dict(en=['ExCod', 'StartTime', 'ElapsedTime', 'RunFolder'],
-                              fr=['CodEx', 'HeureExec', 'DuréeExec', 'DossierExec'])
-    
-    # Ctor
-    # * :param: engine : DS engine to use
-    # * :param: sampleDataSet : data.SampleDataSet instance to use
-    # * :param: name : name (may be empty), used for prefixing run folders or so, only for user-friendliness
-    # * :param: customData : any custom data to be transported with the analysis object
-    #                        during run (left completely untouched)
-    def __init__(self, engine, sampleDataSet, name, customData=None):
-        
-        self.engine = engine
-        self.sampleDataSet = sampleDataSet
-        self.name = name
-        self.customData = customData
-        
-class MCDSAnalysis(DSAnalysis):
-    
-    EngineClass = MCDSEngine
-    
-    @staticmethod
-    def checkDistCuts(distCuts, minDist, maxDist):
-        if distCuts is None:
-            return  # OK !
-        if isinstance(distCuts, int) or isinstance(distCuts, float):
-            assert distCuts > 1, 'Invalid number of distance cuts {}; should be None or > 1'.format(distCuts)
-        elif isinstance(distCuts, list):
-            assert len(distCuts) > 0, 'Invalid distance cut list {}; should not be empty'.format(distCuts)
-            prevCut = -1e10 if minDist is None else minDist
-            for cut in distCuts:
-                assert cut > prevCut, 'Invalid distance cut list {}; should be made of strictly increasing values' \
-                                      ' in ]{}, {}['.format(minDist, maxDist, distCuts)
-                prevCut = cut
-            if maxDist is not None:
-                assert distCuts[-1] < maxDist, \
-                       'Invalid last distance cut {}; should be empty < {}'.format(distCuts[-1], maxDist)
-    
-    def __init__(self, engine, sampleDataSet, name=None, customData=None, logData=False,
-                 estimKeyFn=EngineClass.EstKeyFnDef, estimAdjustFn=EngineClass.EstAdjustFnDef, 
-                 estimCriterion=EngineClass.EstCriterionDef, cvInterval=EngineClass.EstCVIntervalDef,
-                 minDist=EngineClass.DistMinDef, maxDist=EngineClass.DistMaxDef, 
-                 fitDistCuts=EngineClass.DistFitCutsDef, discrDistCuts=EngineClass.DistDiscrCutsDef):
-        
-        """Ctor
-        
-        Parameters:
-        :param engine: DS engine to use
-        :param sampleDataSet: SampleDataSet instance to use
-        :param name: used for prefixing run folders (sure to be automatically unique anyway),
-            analysis names, and so on, only for user-friendliness and easier debugging ;
-            default: None => auto-generated from optimisation parameters
-        :param customData: custom data for the run to ship through
-        :param estimKeyFn: fitting estimator key function (see Distance documentation)
-        :param estimAdjustFn: fitting estimator adjustment series
-        :param estimCriterion: criterion for judging goodness of fit
-        :param cvInterval:  confidence value interval (%)
-        :param logData: if True, print input data in output log
-        :param minDist: left truncation distance ; None or NaN or >=0 
-        :param maxDist: right truncation distance ; None or NaN or > :param minDist:
-        :param fitDistCuts: number of distance intervals for _model_fitting_
-            None or NaN or int = number of equal sub-intervals of [:param minDist:, :param maxDist:] 
-            or list of distance values inside [:param minDist:, :param maxDist:]
-        :param discrDistCuts: number of distance intervals for _distance_values_discretisation_
-            None or NaN or int = number of equal sub-intervals of [:param minDist:, :param maxDist:] 
-            or list of distance values inside [:param minDist:, :param maxDist:]
-        """
-        
-        # Check engine
-        assert isinstance(engine, MCDSEngine), 'Engine must be an MCDSEngine'
-        
-        # Check analysis params
-        assert len(estimKeyFn) >= 2 and estimKeyFn in [kf[:len(estimKeyFn)] for kf in engine.EstKeyFns], \
-               'Invalid estimate key function {}: should be in {} or at least 2-char abbreviations' \
-               .format(estimKeyFn, engine.EstKeyFns)
-        assert len(estimAdjustFn) >= 2 and estimAdjustFn in [kf[:len(estimAdjustFn)] for kf in engine.EstAdjustFns], \
-               'Invalid estimate adjust function {}: should be in {} or at least 2-char abbreviations' \
-               .format(estimAdjustFn, engine.EstAdjustFns)
-        assert estimCriterion in engine.EstCriteria, \
-               'Invalid estimate criterion {}: should be in {}'.format(estimCriterion, engine.EstCriteria)
-        assert 0 < cvInterval < 100, \
-               'Invalid cvInterval {}% : should be in {}'.format(cvInterval, ']0%, 100%[')
-        if isinstance(minDist, float) and np.isnan(minDist):
-            minDist = None  # enforce minDist NaN => None for later
-        assert minDist is None or minDist >= 0, \
-               'Invalid left truncation distance {}: should be None/NaN or >= 0'.format(minDist)
-        if isinstance(maxDist, float) and np.isnan(maxDist):
-            maxDist = None  # enforce maxDist NaN => None for later
-        assert maxDist is None or minDist is None or minDist <= maxDist, \
-               'Invalid right truncation distance {}:' \
-               ' should be None/NaN or >= left truncation distance if specified, or >= 0'.format(maxDist)
-        if isinstance(fitDistCuts, float) and np.isnan(fitDistCuts):
-            fitDistCuts = None  # enforce fitDistCuts NaN => None for later
-        self.checkDistCuts(fitDistCuts, minDist, maxDist)
-        if isinstance(discrDistCuts, float) and np.isnan(discrDistCuts):
-            discrDistCuts = None  # enforce discrDistCuts NaN => None for later
-        self.checkDistCuts(discrDistCuts, minDist, maxDist)
-        
-        # Build name from main params if not specified
-        if name is None:
-            name = '-'.join(['mcds'] + [p[:3].lower() for p in [estimKeyFn, estimAdjustFn]])
-            if estimCriterion != self.EngineClass.EstCriterionDef:
-                name += '-' + estimCriterion.lower()
-            if cvInterval != self.EngineClass.EstCVIntervalDef:
-                name += '-' + str(cvInterval)
-
-        # Initialise base.
-        super().__init__(engine, sampleDataSet, name, customData)
-
-        # Compute sample stats.
-        self.sSampleStats = self.engine.computeSampleStats(self.sampleDataSet)
-
-        # Analysis run time-out implemented here if engine doesn't know how to do it
-        # (but then, MCDS exe are not killed, only abandoned in "space")
-        self.timeOut = engine.timeOut if engine.runMethod == 'os.system' else None
-        if self.timeOut is not None:
-            logger.info2(f"Will take care of {self.timeOut}s time limit because engine can't do this")
-
-        # Save params.
-        self.logData = logData
-        self.estimKeyFn = estimKeyFn
-        self.estimAdjustFn = estimAdjustFn
-        self.estimCriterion = estimCriterion
-        self.cvInterval = cvInterval
-        self.minDist = minDist
-        self.maxDist = maxDist
-        self.fitDistCuts = fitDistCuts
-        self.discrDistCuts = discrDistCuts
-    
-    # Run columns for output : analysis params + root engine output (3-level multi-index)
-    CLParEstKeyFn = ('parameters', 'estimator key function', 'Value')
-    CLParEstAdjSer = ('parameters', 'estimator adjustment series', 'Value')
-    CLParEstSelCrit = ('parameters', 'estimator selection criterion', 'Value')
-    CLParEstCVInt = ('parameters', 'CV interval', 'Value')
-    CLParTruncLeft = ('parameters', 'left truncation distance', 'Value')
-    CLParTruncRight = ('parameters', 'right truncation distance', 'Value')
-    CLParModFitDistCuts = ('parameters', 'model fitting distance cut points', 'Value')
-    CLParModDiscrDistCuts = ('parameters', 'distance discretisation cut points', 'Value')
-
-    MIRunColumns = pd.MultiIndex.from_tuples([CLParEstKeyFn, CLParEstAdjSer,
-                                              CLParEstSelCrit, CLParEstCVInt,
-                                              CLParTruncLeft, CLParTruncRight,
-                                              CLParModFitDistCuts, CLParModDiscrDistCuts]
-                                             + DSAnalysis.RunRunColumns)
-    
-    # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
-    DfRunColumnTrans = \
-        pd.DataFrame(index=MIRunColumns,
-                     data=dict(en=['Mod Key Fn', 'Mod Adj Ser', 'Mod Chc Crit', 'Conf Interv',
-                                   'Left Trunc Dist', 'Right Trunc Dist', 'Fit Dist Cuts', 'Discr Dist Cuts']
-                                  + DSAnalysis.DRunRunColumnTrans['en'],
-                               fr=['Fn Clé Mod', 'Sér Ajust Mod', 'Crit Chx Mod', 'Interv Conf',
-                                   'Dist Tronc Gche', 'Dist Tronc Drte', 'Tranch Dist Mod', 'Tranch Dist Discr']
-                                  + DSAnalysis.DRunRunColumnTrans['fr']))
-    
-    # Start running the analysis, and return immediately (the associated cofu.Future object) :
-    # this starts an async. run ; you'll need to call getResults to wait for the real end of execution.
-    def submit(self, realRun=True):
-        
-        # Ask the engine to start running the analysis
-        self.future = \
-            self.engine.submitAnalysis(sampleDataSet=self.sampleDataSet, runPrefix=self.name,
-                                       realRun=realRun, logData=self.logData,
-                                       estimKeyFn=self.estimKeyFn, estimAdjustFn=self.estimAdjustFn,
-                                       estimCriterion=self.estimCriterion, cvInterval=self.cvInterval,
-                                       minDist=self.minDist, maxDist=self.maxDist,
-                                       fitDistCuts=self.fitDistCuts, discrDistCuts=self.discrDistCuts)
-        
-        return self.future
-        
-    # Wait for the real end of analysis execution, and return its results.
-    # This indicates the end of an async. run when returning.
-    def _wait4Results(self):
-        
-        # Get analysis execution results, when the computation is finished (blocking)
-        try:
-            if self.timeOut is not None:
-                startTime = pd.Timestamp.now()  # In case of cofu.TimeoutError
-            self.runStatus, self.startTime, self.elapsedTime, self.runDir, self.sResults = \
-                self.future.result(timeout=self.timeOut)
-        except cofu.TimeoutError:
-            logger.error('MCDS Analysis run timed-out after {}s'.format(self.timeOut))
-            self.runStatus, self.startTime, self.elapsedTime, self.runDir, self.sResults = \
-                self.engine.RCTimedOut, startTime, self.timeOut, None, None
-        
-    # Wait for the real end of analysis execution, and return its results.
-    # This indicates the end of an async. run when returning.
-    def getResults(self, postCleanup=False):
-        
-        # Get analysis execution results, when the computation is finished (blocking)
-        self._wait4Results()
-        
-        # Build up run data (input parameters and run status) for output.
-        sRunData = pd.Series(data=[self.estimKeyFn, self.estimAdjustFn, self.estimCriterion,
-                                   self.cvInterval, self.minDist, self.maxDist, self.fitDistCuts, self.discrDistCuts,
-                                   self.runStatus, self.startTime, self.elapsedTime, self.runDir],
-                             index=self.MIRunColumns)
-        
-        # Append the run results (if any usable) to the analysis sample stats and run data for output.
-        sSampStatsRunData = self.sSampleStats.append(sRunData)
-        if self.engine.success(self.runStatus) or self.engine.warnings(self.runStatus):
-            self.sResults = sSampStatsRunData.append(self.sResults)
-        else:
-            self.sResults = sSampStatsRunData
-            
-        # Post cleanup if requested.
-        if postCleanup:
-            self.cleanup()
-        
-        # Done: Return a result, even if not run or MCDS crashed or ...
-        return self.sResults
-        
-    def cleanup(self):
-    
-        if 'runDir' in dir(self) and self.runDir is not None:
-        
-            runDir = pl.Path(self.runDir)
-            if runDir.is_dir():
-            
-                # Take extra precautions before rm -fr :-) (at least 14 files inside after a report generation)
-                if not runDir.is_symlink() and len(list(runDir.rglob('*'))) < 15:
-                    logger.info3('Removing run folder "{}"'.format(runDir.as_posix()))
-                    shutil.rmtree(runDir)
-                else:
-                    logger.warning('Cowardly refused to remove suspect analysis run folder "{}"'.format(runDir))
-        
-    def wasRun(self):
-    
-        self._wait4Results()  # First, wait for end of actual run !
-        
-        return self.engine.wasRun(self.runStatus)
-    
-    def success(self):
-   
-        self._wait4Results()  # First, wait for end of actual run !
-        
-        return self.engine.success(self.runStatus)
-    
-    def warnings(self):
-    
-        self._wait4Results()  # First, wait for end of actual run !
-        
-        return self.engine.warnings(self.runStatus)
-    
-    def errors(self):
-   
-        self._wait4Results()  # First, wait for end of actual run !
-        
-        return self.engine.errors(self.runStatus)
-
-
-class MCDSPreAnalysis(MCDSAnalysis):
-
-    """Note: Was implemented this strange way at the beginning, but made simpler later,
-    and thus making it be fruitfully replaceable by a simple MCDSAnalysis through a simple MCDSAnalyser.
-    """
-    
-    EngineClass = MCDSAnalysis.EngineClass
-        
-    # * modelStrategy: iterable of dict(keyFn=, adjSr=, estCrit=, cvInt=)
-    # * executor: Executor object to use for parallel execution of multiple pre-analyses instances
-    #             Note: Up to the caller to shut it down when no more needed (not owned).
-    def __init__(self, engine, sampleDataSet, name=None, customData=None, logData=False, executor=None,
-                 modelStrategy=[dict(keyFn='HNORMAL', adjSr='COSINE', estCrit='AIC', cvInt=95)],
-                 minDist=EngineClass.DistMinDef, maxDist=EngineClass.DistMaxDef, 
-                 fitDistCuts=EngineClass.DistFitCutsDef, discrDistCuts=EngineClass.DistDiscrCutsDef):
-
-        assert len(modelStrategy) > 0, 'MCDSPreAnalysis: Empty modelStrategy !'
-        
-        super().__init__(engine, sampleDataSet, name, customData, logData,
-                         minDist=minDist, maxDist=maxDist, fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts)
-
-        self.modelStrategy = modelStrategy
-        self.executor = executor if executor is not None else Executor()
-    
-    MIAicValue = ('detection probability', 'AIC value', 'Value')
-
-    @classmethod
-    def isAnalysisBetter(cls, left, right):
-
-        """Return True if left is better than right, otherwise False"""
-
-        if left.success() or left.warnings():
-            if right.success() or right.warnings():
-                answ = left.getResults().get(cls.MIAicValue, 1e9) < right.getResults().get(cls.MIAicValue, 1e9)
-            else:
-                answ = True
-        else:
-            answ = False
-
-        return answ
-
-    def _run(self):
-
-        # Run models as planned in modelStrategy for best results
-        bestAnlys = None
-        for model in self.modelStrategy:
-
-            modAbbrev = model['keyFn'][:3].lower() + '-' + model['adjSr'][:3].lower()
-
-            # Create and run analysis for the new model
-            anlys = MCDSAnalysis(engine=self.engine, sampleDataSet=self.sampleDataSet,
-                                 name=self.name + '-' + modAbbrev, logData=False,
-                                 estimKeyFn=model['keyFn'], estimAdjustFn=model['adjSr'],
-                                 estimCriterion=model['estCrit'], cvInterval=model['cvInt'])
-            anlys.submit()
-
-            # Save analysis if better or first + cleanup no more needed analysis.
-            if bestAnlys is None:
-                bestAnlys = anlys
-            elif self.isAnalysisBetter(anlys, bestAnlys):
-                bestAnlys.cleanup()
-                bestAnlys = anlys
-            else:
-                anlys.cleanup()
-
-        return bestAnlys
-    
-    def submit(self, realRun=True):
-
-        # Submit analysis work and return a Future object to ask from and wait for its results.
-        self.future = self.executor.submit(self._run)
-        
-        return self.future
-    
-    def getResults(self, postCleanup=False):
-        
-        # Get self result : the best analysis.
-        anlys = self.future.result()
-        
-        # Get execution results of this best analysis.
-        sResults = anlys.getResults()
-        
-        # Store best analysis other outputs ... as self ones
-        self.runStatus, self.startTime, self.elapsedTime, self.runDir = \
-            anlys.runStatus, anlys.startTime, anlys.elapsedTime, anlys.runDir
-        
-        return sResults
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "analysis": One layer above engines, to run DS analyses from an imput data set, and get computation results
+
+import sys
+import pathlib as pl
+import shutil
+import argparse
+
+import numpy as np
+import pandas as pd
+
+import concurrent.futures as cofu
+
+from . import log
+from .engine import DSEngine, MCDSEngine
+from .executor import Executor
+
+logger = log.logger('ads.ans')
+
+
+# Analysis (abstract) : Gather input params, data set, results, debug and log files
+class DSAnalysis(object):
+    
+    EngineClass = DSEngine
+    
+    # Run columns for output : root engine output (3-level multi-index)
+    CLRunStatus = ('run output', 'run status', 'Value')
+    CLRunStartTime = ('run output', 'start time', 'Value')
+    CLRunElapsedTime = ('run output', 'elapsed time', 'Value')
+    CLRunFolder = ('run output', 'run folder', 'Value')
+
+    RunRunColumns = [CLRunStatus, CLRunStartTime, CLRunElapsedTime, CLRunFolder]
+    RunFolderColumn = next(iter(col for col in RunRunColumns if col[1] == 'run folder'))
+    
+    # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
+    DRunRunColumnTrans = dict(en=['ExCod', 'StartTime', 'ElapsedTime', 'RunFolder'],
+                              fr=['CodEx', 'HeureExec', 'DuréeExec', 'DossierExec'])
+    
+    # Ctor
+    # * :param: engine : DS engine to use
+    # * :param: sampleDataSet : data.SampleDataSet instance to use
+    # * :param: name : name (may be empty), used for prefixing run folders or so, only for user-friendliness
+    # * :param: customData : any custom data to be transported with the analysis object
+    #                        during run (left completely untouched)
+    def __init__(self, engine, sampleDataSet, name, customData=None):
+        
+        self.engine = engine
+        self.sampleDataSet = sampleDataSet
+        self.name = name
+        self.customData = customData
+        
+class MCDSAnalysis(DSAnalysis):
+    
+    EngineClass = MCDSEngine
+    
+    @staticmethod
+    def checkDistCuts(distCuts, minDist, maxDist):
+        if distCuts is None:
+            return  # OK !
+        if isinstance(distCuts, int) or isinstance(distCuts, float):
+            assert distCuts > 1, 'Invalid number of distance cuts {}; should be None or > 1'.format(distCuts)
+        elif isinstance(distCuts, list):
+            assert len(distCuts) > 0, 'Invalid distance cut list {}; should not be empty'.format(distCuts)
+            prevCut = -1e10 if minDist is None else minDist
+            for cut in distCuts:
+                assert cut > prevCut, 'Invalid distance cut list {}; should be made of strictly increasing values' \
+                                      ' in ]{}, {}['.format(minDist, maxDist, distCuts)
+                prevCut = cut
+            if maxDist is not None:
+                assert distCuts[-1] < maxDist, \
+                       'Invalid last distance cut {}; should be empty < {}'.format(distCuts[-1], maxDist)
+    
+    def __init__(self, engine, sampleDataSet, name=None, customData=None, logData=False,
+                 estimKeyFn=EngineClass.EstKeyFnDef, estimAdjustFn=EngineClass.EstAdjustFnDef, 
+                 estimCriterion=EngineClass.EstCriterionDef, cvInterval=EngineClass.EstCVIntervalDef,
+                 minDist=EngineClass.DistMinDef, maxDist=EngineClass.DistMaxDef, 
+                 fitDistCuts=EngineClass.DistFitCutsDef, discrDistCuts=EngineClass.DistDiscrCutsDef):
+        
+        """Ctor
+        
+        Parameters:
+        :param engine: DS engine to use
+        :param sampleDataSet: SampleDataSet instance to use
+        :param name: used for prefixing run folders (sure to be automatically unique anyway),
+            analysis names, and so on, only for user-friendliness and easier debugging ;
+            default: None => auto-generated from optimisation parameters
+        :param customData: custom data for the run to ship through
+        :param estimKeyFn: fitting estimator key function (see Distance documentation)
+        :param estimAdjustFn: fitting estimator adjustment series
+        :param estimCriterion: criterion for judging goodness of fit
+        :param cvInterval:  confidence value interval (%)
+        :param logData: if True, print input data in output log
+        :param minDist: left truncation distance ; None or NaN or >=0 
+        :param maxDist: right truncation distance ; None or NaN or > :param minDist:
+        :param fitDistCuts: number of distance intervals for _model_fitting_
+            None or NaN or int = number of equal sub-intervals of [:param minDist:, :param maxDist:] 
+            or list of distance values inside [:param minDist:, :param maxDist:]
+        :param discrDistCuts: number of distance intervals for _distance_values_discretisation_
+            None or NaN or int = number of equal sub-intervals of [:param minDist:, :param maxDist:] 
+            or list of distance values inside [:param minDist:, :param maxDist:]
+        """
+        
+        # Check engine
+        assert isinstance(engine, MCDSEngine), 'Engine must be an MCDSEngine'
+        
+        # Check analysis params
+        assert len(estimKeyFn) >= 2 and estimKeyFn in [kf[:len(estimKeyFn)] for kf in engine.EstKeyFns], \
+               'Invalid estimate key function {}: should be in {} or at least 2-char abbreviations' \
+               .format(estimKeyFn, engine.EstKeyFns)
+        assert len(estimAdjustFn) >= 2 and estimAdjustFn in [kf[:len(estimAdjustFn)] for kf in engine.EstAdjustFns], \
+               'Invalid estimate adjust function {}: should be in {} or at least 2-char abbreviations' \
+               .format(estimAdjustFn, engine.EstAdjustFns)
+        assert estimCriterion in engine.EstCriteria, \
+               'Invalid estimate criterion {}: should be in {}'.format(estimCriterion, engine.EstCriteria)
+        assert 0 < cvInterval < 100, \
+               'Invalid cvInterval {}% : should be in {}'.format(cvInterval, ']0%, 100%[')
+        if isinstance(minDist, float) and np.isnan(minDist):
+            minDist = None  # enforce minDist NaN => None for later
+        assert minDist is None or minDist >= 0, \
+               'Invalid left truncation distance {}: should be None/NaN or >= 0'.format(minDist)
+        if isinstance(maxDist, float) and np.isnan(maxDist):
+            maxDist = None  # enforce maxDist NaN => None for later
+        assert maxDist is None or minDist is None or minDist <= maxDist, \
+               'Invalid right truncation distance {}:' \
+               ' should be None/NaN or >= left truncation distance if specified, or >= 0'.format(maxDist)
+        if isinstance(fitDistCuts, float) and np.isnan(fitDistCuts):
+            fitDistCuts = None  # enforce fitDistCuts NaN => None for later
+        self.checkDistCuts(fitDistCuts, minDist, maxDist)
+        if isinstance(discrDistCuts, float) and np.isnan(discrDistCuts):
+            discrDistCuts = None  # enforce discrDistCuts NaN => None for later
+        self.checkDistCuts(discrDistCuts, minDist, maxDist)
+        
+        # Build name from main params if not specified
+        if name is None:
+            name = '-'.join(['mcds'] + [p[:3].lower() for p in [estimKeyFn, estimAdjustFn]])
+            if estimCriterion != self.EngineClass.EstCriterionDef:
+                name += '-' + estimCriterion.lower()
+            if cvInterval != self.EngineClass.EstCVIntervalDef:
+                name += '-' + str(cvInterval)
+
+        # Initialise base.
+        super().__init__(engine, sampleDataSet, name, customData)
+
+        # Compute sample stats.
+        self.sSampleStats = self.engine.computeSampleStats(self.sampleDataSet)
+
+        # Analysis run time-out implemented here if engine doesn't know how to do it
+        # (but then, MCDS exe are not killed, only abandoned in "space")
+        self.timeOut = engine.timeOut if engine.runMethod == 'os.system' else None
+        if self.timeOut is not None:
+            logger.info2(f"Will take care of {self.timeOut}s time limit because engine can't do this")
+
+        # Save params.
+        self.logData = logData
+        self.estimKeyFn = estimKeyFn
+        self.estimAdjustFn = estimAdjustFn
+        self.estimCriterion = estimCriterion
+        self.cvInterval = cvInterval
+        self.minDist = minDist
+        self.maxDist = maxDist
+        self.fitDistCuts = fitDistCuts
+        self.discrDistCuts = discrDistCuts
+    
+    # Run columns for output : analysis params + root engine output (3-level multi-index)
+    CLParEstKeyFn = ('parameters', 'estimator key function', 'Value')
+    CLParEstAdjSer = ('parameters', 'estimator adjustment series', 'Value')
+    CLParEstSelCrit = ('parameters', 'estimator selection criterion', 'Value')
+    CLParEstCVInt = ('parameters', 'CV interval', 'Value')
+    CLParTruncLeft = ('parameters', 'left truncation distance', 'Value')
+    CLParTruncRight = ('parameters', 'right truncation distance', 'Value')
+    CLParModFitDistCuts = ('parameters', 'model fitting distance cut points', 'Value')
+    CLParModDiscrDistCuts = ('parameters', 'distance discretisation cut points', 'Value')
+
+    MIRunColumns = pd.MultiIndex.from_tuples([CLParEstKeyFn, CLParEstAdjSer,
+                                              CLParEstSelCrit, CLParEstCVInt,
+                                              CLParTruncLeft, CLParTruncRight,
+                                              CLParModFitDistCuts, CLParModDiscrDistCuts]
+                                             + DSAnalysis.RunRunColumns)
+    
+    # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
+    DfRunColumnTrans = \
+        pd.DataFrame(index=MIRunColumns,
+                     data=dict(en=['Mod Key Fn', 'Mod Adj Ser', 'Mod Chc Crit', 'Conf Interv',
+                                   'Left Trunc Dist', 'Right Trunc Dist', 'Fit Dist Cuts', 'Discr Dist Cuts']
+                                  + DSAnalysis.DRunRunColumnTrans['en'],
+                               fr=['Fn Clé Mod', 'Sér Ajust Mod', 'Crit Chx Mod', 'Interv Conf',
+                                   'Dist Tronc Gche', 'Dist Tronc Drte', 'Tranch Dist Mod', 'Tranch Dist Discr']
+                                  + DSAnalysis.DRunRunColumnTrans['fr']))
+    
+    # Start running the analysis, and return immediately (the associated cofu.Future object) :
+    # this starts an async. run ; you'll need to call getResults to wait for the real end of execution.
+    def submit(self, realRun=True):
+        
+        # Ask the engine to start running the analysis
+        self.future = \
+            self.engine.submitAnalysis(sampleDataSet=self.sampleDataSet, runPrefix=self.name,
+                                       realRun=realRun, logData=self.logData,
+                                       estimKeyFn=self.estimKeyFn, estimAdjustFn=self.estimAdjustFn,
+                                       estimCriterion=self.estimCriterion, cvInterval=self.cvInterval,
+                                       minDist=self.minDist, maxDist=self.maxDist,
+                                       fitDistCuts=self.fitDistCuts, discrDistCuts=self.discrDistCuts)
+        
+        return self.future
+        
+    # Wait for the real end of analysis execution, and return its results.
+    # This indicates the end of an async. run when returning.
+    def _wait4Results(self):
+        
+        # Get analysis execution results, when the computation is finished (blocking)
+        try:
+            if self.timeOut is not None:
+                startTime = pd.Timestamp.now()  # In case of cofu.TimeoutError
+            self.runStatus, self.startTime, self.elapsedTime, self.runDir, self.sResults = \
+                self.future.result(timeout=self.timeOut)
+        except cofu.TimeoutError:
+            logger.error('MCDS Analysis run timed-out after {}s'.format(self.timeOut))
+            self.runStatus, self.startTime, self.elapsedTime, self.runDir, self.sResults = \
+                self.engine.RCTimedOut, startTime, self.timeOut, None, None
+        
+    # Wait for the real end of analysis execution, and return its results.
+    # This indicates the end of an async. run when returning.
+    def getResults(self, postCleanup=False):
+        
+        # Get analysis execution results, when the computation is finished (blocking)
+        self._wait4Results()
+        
+        # Build up run data (input parameters and run status) for output.
+        sRunData = pd.Series(data=[self.estimKeyFn, self.estimAdjustFn, self.estimCriterion,
+                                   self.cvInterval, self.minDist, self.maxDist, self.fitDistCuts, self.discrDistCuts,
+                                   self.runStatus, self.startTime, self.elapsedTime, self.runDir],
+                             index=self.MIRunColumns)
+        
+        # Append the run results (if any usable) to the analysis sample stats and run data for output.
+        sSampStatsRunData = self.sSampleStats.append(sRunData)
+        if self.engine.success(self.runStatus) or self.engine.warnings(self.runStatus):
+            self.sResults = sSampStatsRunData.append(self.sResults)
+        else:
+            self.sResults = sSampStatsRunData
+            
+        # Post cleanup if requested.
+        if postCleanup:
+            self.cleanup()
+        
+        # Done: Return a result, even if not run or MCDS crashed or ...
+        return self.sResults
+        
+    def cleanup(self):
+    
+        if 'runDir' in dir(self) and self.runDir is not None:
+        
+            runDir = pl.Path(self.runDir)
+            if runDir.is_dir():
+            
+                # Take extra precautions before rm -fr :-) (at least 14 files inside after a report generation)
+                if not runDir.is_symlink() and len(list(runDir.rglob('*'))) < 15:
+                    logger.info3('Removing run folder "{}"'.format(runDir.as_posix()))
+                    shutil.rmtree(runDir)
+                else:
+                    logger.warning('Cowardly refused to remove suspect analysis run folder "{}"'.format(runDir))
+        
+    def wasRun(self):
+    
+        self._wait4Results()  # First, wait for end of actual run !
+        
+        return self.engine.wasRun(self.runStatus)
+    
+    def success(self):
+   
+        self._wait4Results()  # First, wait for end of actual run !
+        
+        return self.engine.success(self.runStatus)
+    
+    def warnings(self):
+    
+        self._wait4Results()  # First, wait for end of actual run !
+        
+        return self.engine.warnings(self.runStatus)
+    
+    def errors(self):
+   
+        self._wait4Results()  # First, wait for end of actual run !
+        
+        return self.engine.errors(self.runStatus)
+
+
+class MCDSPreAnalysis(MCDSAnalysis):
+
+    """Note: Was implemented this strange way at the beginning, but made simpler later,
+    and thus making it be fruitfully replaceable by a simple MCDSAnalysis through a simple MCDSAnalyser.
+    """
+    
+    EngineClass = MCDSAnalysis.EngineClass
+        
+    # * modelStrategy: iterable of dict(keyFn=, adjSr=, estCrit=, cvInt=)
+    # * executor: Executor object to use for parallel execution of multiple pre-analyses instances
+    #             Note: Up to the caller to shut it down when no more needed (not owned).
+    def __init__(self, engine, sampleDataSet, name=None, customData=None, logData=False, executor=None,
+                 modelStrategy=[dict(keyFn='HNORMAL', adjSr='COSINE', estCrit='AIC', cvInt=95)],
+                 minDist=EngineClass.DistMinDef, maxDist=EngineClass.DistMaxDef, 
+                 fitDistCuts=EngineClass.DistFitCutsDef, discrDistCuts=EngineClass.DistDiscrCutsDef):
+
+        assert len(modelStrategy) > 0, 'MCDSPreAnalysis: Empty modelStrategy !'
+        
+        super().__init__(engine, sampleDataSet, name, customData, logData,
+                         minDist=minDist, maxDist=maxDist, fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts)
+
+        self.modelStrategy = modelStrategy
+        self.executor = executor if executor is not None else Executor()
+    
+    MIAicValue = ('detection probability', 'AIC value', 'Value')
+
+    @classmethod
+    def isAnalysisBetter(cls, left, right):
+
+        """Return True if left is better than right, otherwise False"""
+
+        if left.success() or left.warnings():
+            if right.success() or right.warnings():
+                answ = left.getResults().get(cls.MIAicValue, 1e9) < right.getResults().get(cls.MIAicValue, 1e9)
+            else:
+                answ = True
+        else:
+            answ = False
+
+        return answ
+
+    def _run(self):
+
+        # Run models as planned in modelStrategy for best results
+        bestAnlys = None
+        for model in self.modelStrategy:
+
+            modAbbrev = model['keyFn'][:3].lower() + '-' + model['adjSr'][:3].lower()
+
+            # Create and run analysis for the new model
+            anlys = MCDSAnalysis(engine=self.engine, sampleDataSet=self.sampleDataSet,
+                                 name=self.name + '-' + modAbbrev, logData=False,
+                                 estimKeyFn=model['keyFn'], estimAdjustFn=model['adjSr'],
+                                 estimCriterion=model['estCrit'], cvInterval=model['cvInt'])
+            anlys.submit()
+
+            # Save analysis if better or first + cleanup no more needed analysis.
+            if bestAnlys is None:
+                bestAnlys = anlys
+            elif self.isAnalysisBetter(anlys, bestAnlys):
+                bestAnlys.cleanup()
+                bestAnlys = anlys
+            else:
+                anlys.cleanup()
+
+        return bestAnlys
+    
+    def submit(self, realRun=True):
+
+        # Submit analysis work and return a Future object to ask from and wait for its results.
+        self.future = self.executor.submit(self._run)
+        
+        return self.future
+    
+    def getResults(self, postCleanup=False):
+        
+        # Get self result : the best analysis.
+        anlys = self.future.result()
+        
+        # Get execution results of this best analysis.
+        sResults = anlys.getResults()
+        
+        # Store best analysis other outputs ... as self ones
+        self.runStatus, self.startTime, self.elapsedTime, self.runDir = \
+            anlys.runStatus, anlys.startTime, anlys.elapsedTime, anlys.runDir
+        
+        return sResults
```

### Comparing `pyaudisam-0.9.3/pyaudisam/data.py` & `pyaudisam-1.0.1/pyaudisam/data.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,1674 +1,1674 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "data": Input and output DS data manipulation tools
-
-import sys
-import pathlib as pl
-from packaging import version as pkgver
-
-import lzma
-import pickle
-
-import numpy as np
-import pandas as pd
-
-from . import log, runtime
-
-runtime.update(numpy=np.__version__, pandas=pd.__version__)
-
-logger = log.logger('ads.dat')
-
-
-class DataSet(object):
-
-    """"A tabular data set built by concatenating various-formatted source tables into one.
-    
-    Note: visionat module also defines this class: no reason it differs in any way => synchronise please !
-    Why ? Just to keep the 2 modules independent ; might change in the future"""
-    
-    def __init__(self, sources, dRenameCols={}, dComputeCols={}, importDecFields=[],
-                 sheet=None, skipRows=None, headerRows=0, indexCols=None, separator='\t', encoding='utf-8'):
-    
-        """Ctor
-        :param sources: data sources to read from ; input support provided for:
-             * pandas.DataFrame,
-             * Excel .xlsx (through 'openpyxl' or 'xlrd' module)
-               and .xls files (through 'openpyxl' module, no need for 'xlwt'),
-             * tab-separated .csv/.txt files,
-             * and even OpenDoc .ods file with pandas >= 0.25 (through 'odfpy' module) ;
-             when multiple source provided, sources are supposed to have compatible columns names,
-             and data rows from each source are appended 1 source after the previous.
-        :param dRenameCols: dict for renaming input columns right after loading data
-        :param dComputeCols: name and compute method for computed columns to be auto-added ;
-                             as a dict { new col. name => constant, or function to apply
-                             to each row to auto-compute the new sightings column } ;
-                             note: these columns can also be renamed, through dRenameCols
-        :param importDecFields: for smart ./, decimal character management in CSV sources (pandas is not smart on this)
-        :param sheet: name of the sheet to read from, for multi-sheet data files (like Excel or Open Doc. workbooks)
-        :param skipRows: list of indexes of initial rows to skip for file sources (before the column names row)
-        :param headerRows: index (or list of) of rows holding columns names (default 0 => 1st row)
-        :param indexCols: index (or list of) of first columns to use as (multi)index (default None => auto-gen index)
-        :param separator: columns separator for CSV sources
-        :param encoding: encoding for CSV sources
-        """
-    
-        if isinstance(sources, (str, pl.Path)):
-            self._dfData = self._fromDataFile(sources, sheet=sheet, decimalFields=importDecFields,
-                                              skipRows=skipRows, headerRows=headerRows, indexCols=indexCols,
-                                              separator=separator, encoding=encoding)
-        elif isinstance(sources, pd.DataFrame):
-            self._dfData = self._fromDataFrame(sources)
-        elif isinstance(sources, list):
-            ldfData = list()
-            for source in sources:
-                if isinstance(source, (str, pl.Path)):
-                    dfData = self._fromDataFile(source, sheet=sheet, decimalFields=importDecFields,
-                                                skipRows=skipRows, headerRows=headerRows, indexCols=indexCols,
-                                                separator=separator, encoding=encoding)
-                elif isinstance(source, pd.DataFrame):
-                    dfData = self._fromDataFrame(source)
-                    logger.info1('Loaded {} rows x {} columns from data frame'.format(len(dfData), len(dfData.columns)))
-                else:
-                    raise Exception('source for DataSet must be a pandas.DataFrame or an existing file')
-                ldfData.append(dfData)
-            self._dfData = pd.concat(ldfData, ignore_index=True)
-        else:
-            raise Exception('Source for DataSet must be a pandas.DataFrame or an existing file')
-
-        if self._dfData.empty:
-            logger.warning('No data in source data set')
-            return
-            
-        logger.info(f'Loaded {len(self)} x {len(self.columns)} total rows x columns in data set ...')
-        logger.info('... found columns: [{}]'.format('|'.join(str(c) for c in self.columns)))
-        
-        # Rename columns if requested.
-        if dRenameCols:
-            for key in dRenameCols.keys():
-                if key in dComputeCols:
-                    dComputeCols[dRenameCols[key]] = dComputeCols[key]
-                    dComputeCols.pop(key)            
-            self.renameColumns(dRenameCols)
-
-        # Add auto-computed columns if any.
-        if dComputeCols:
-
-            self.addColumns(dComputeCols)
-
-    # Wrapper around pd.read_csv for smart ./, decimal character management (pandas is not smart on this)
-    # TODO: Make this more efficient
-    @staticmethod
-    def _csv2df(fileName, decCols, skipRows=None, headerRows=0, indexCols=None, sep='\t', encoding='utf-8'):
-        df = pd.read_csv(fileName, sep=sep, skiprows=skipRows,
-                         header=headerRows, index_col=indexCols)
-        allRight = True
-        for col in decCols:
-            if df[col].dropna().apply(lambda v: isinstance(v, str)).any():
-                allRight = False
-                break
-        if not allRight:
-            df = pd.read_csv(fileName, sep=sep, skiprows=skipRows, encoding=encoding,
-                             header=headerRows, index_col=indexCols, decimal=',')
-
-        return df
-    
-    SupportedFileExts = \
-        ['.xlsx', '.xls', '.csv', '.txt'] + (['.ods'] if pkgver.parse(pd.__version__).release >= (0, 25) else [])
-    
-    @classmethod
-    def _fromDataFile(cls, sourceFpn, sheet=None, skipRows=None, headerRows=0, indexCols=None,
-                      decimalFields=[], separator='\t', encoding='utf-8'):
-        
-        if isinstance(sourceFpn, str):
-            sourceFpn = pl.Path(sourceFpn)
-    
-        assert sourceFpn.exists(), 'Source file for DataSet not found : {}'.format(sourceFpn)
-
-        ext = sourceFpn.suffix.lower()
-        assert ext in cls.SupportedFileExts, \
-               'Unsupported source file type {}: not from {{{}}}'.format(ext, ','.join(cls.SupportedFileExts))
-
-        logger.info1('Loading set from file {} ...'.format(sourceFpn.as_posix()))
-
-        if ext in ['.xlsx', '.xls', '.ods']:
-            dfData = pd.read_excel(sourceFpn, sheet_name=sheet or 0,
-                                   skiprows=skipRows, header=headerRows, index_col=indexCols)
-        elif ext in ['.csv', '.txt']:
-            dfData = cls._csv2df(sourceFpn, decCols=decimalFields, sep=separator, encoding=encoding,
-                                 skipRows=skipRows, headerRows=headerRows, indexCols=indexCols)
-        else:
-            raise NotImplementedError(f'Unsupported DataSet file input format {ext}')
-
-        logger.info1('... loaded {} rows x {} columns'.format(len(dfData), len(dfData.columns)))
-
-        return dfData
-    
-    @classmethod
-    def _fromDataFrame(cls, sourceDf):
-        
-        return sourceDf.copy()
-    
-    def __len__(self):
-        
-        return len(self._dfData)
-    
-    @property
-    def empty(self):
-        
-        return self._dfData.empty
-
-    @property
-    def columns(self):
-        
-        return self._dfData.columns
-
-    @property
-    def index(self):
-        
-        return self._dfData.index
-
-    @property
-    def dfData(self):
-        
-        return self._dfData
-
-    @dfData.setter
-    def dfData(self, dfData_):
-        
-        raise NotImplementedError('No change allowed to data ; create a new dataset !')
-
-    def dfSubData(self, index=None, columns=None, copy=False):
-    
-        """Get a subset of the all-results table rows and columns
-
-        Parameters:
-        :param index: rows to select, as an iterable of a subset of self.dfData.index values
-                      (anything suitable for pd.DataFrame.loc[...]) ; None = all rows.
-        :param columns: columns to select, as a list(string) or pd.Index when mono-indexed columns
-                       or as a list(tuple(string*)) or pd.MultiIndex when multi-indexed ; None = all columns.
-        :param copy: if True, return a full copy of the selected data, not a "reference" to the internal table
-        """
-        
-        assert columns is None or isinstance(columns, list) or isinstance(columns, (pd.Index, pd.MultiIndex)), \
-               'columns must be specified as None (all), or as a list of tuples, or as a pandas.MultiIndex'
-
-        # Make a copy of / extract selected columns of dfData.
-        if columns is None or len(columns) == 0:
-            dfSbData = self.dfData
-        else:
-            if isinstance(self._dfData.columns, pd.MultiIndex) and isinstance(columns, list):
-                iColumns = pd.MultiIndex.from_tuples(columns)
-            else:
-                iColumns = columns
-            dfSbData = self.dfData.reindex(columns=iColumns)
-        
-        if index is not None:
-            dfSbData = dfSbData.loc[index]
-
-        if copy:
-            dfSbData = dfSbData.copy()
-            
-        return dfSbData
-
-    def dropColumns(self, cols):
-    
-        self._dfData.drop(columns=cols, inplace=True)
-        
-    def dropRows(self, sbSelRows):
-        
-        self._dfData.drop(self._dfData[sbSelRows].index, inplace=True)
-        
-    @staticmethod
-    def _addComputedColumns(dfData, dComputeCols):
-    
-        """Add computed columns to a DataFrame
-        
-        :param dfData: the DataFrame to update
-        :param dComputeCols: dict new col. name => constant, or function to apply
-                          to each row to compute its value
-        """
-        
-        for colName, computeCol in dComputeCols.items():
-            if callable(computeCol):
-                dfData[colName] = dfData.apply(computeCol, axis='columns')
-            else:
-                dfData[colName] = computeCol
-               
-        return dfData  # Can be useful when chaining ...
-
-    def addColumns(self, dComputeCols):
-    
-        """Add computed columns to the sightings data set
-        
-        :param dComputeCols: dict new col. name => constant, or function to apply
-                          to each row to compute its value
-        """
-            
-        self._addComputedColumns(self._dfData, dComputeCols)
-
-    def renameColumns(self, dRenameCols):
-
-        self._dfData.rename(columns=dRenameCols, inplace=True)
-
-    def toPickle(self, fileName, subset=None, index=True):
-
-        """Save data table to a pickle file (XZ compressed format if requested).
-
-        Parameters:
-        :param fileName: target file pathname ; file is auto-compressed to XZ format
-                         through the lzma module if its extension is .xz or .lzma,
-                         or else not compressed.
-        :param subset: subset of columns to save
-        :param index: if True, save index column as is (otherwise reset&drop it to a range(size)-like)
-        """
-        
-        start = pd.Timestamp.now()
-
-        dfOutData = self.dfSubData(columns=subset)
-        if not index:
-            dfOutData = dfOutData.reset_index(drop=True)
-
-        compress = pl.Path(fileName).suffix in ['.xz', '.lzma']
-        with lzma.open(fileName, 'wb') if compress else open(fileName, 'wb') as file:
-            pickle.dump(dfOutData, file)
-
-        logger.info('{} results rows saved to {} in {:.3f}s'
-                    .format(len(self), fileName, (pd.Timestamp.now() - start).total_seconds()))
-
-    def toExcel(self, fileName, sheetName=None, subset=None, index=True, engine=None):
-        
-        """Save data table to a worksheet format (Excel, ODF, ...) :
-        * newer XLSX format for .xlsx extensions (through 'openpyxl' or 'xlrd' module)
-        * .xls (through 'xlwt').
-        * .ods (through 'odfpy'), if pandas >= 0.25.
-
-        Parameters:
-        :param fileName: target file pathname
-        :param sheetName: for results data only
-        :param subset: subset of columns to save
-        :param index: if True, save index column
-        :param engine: None => auto-selection from file extension ; otherwise, use xlrd, openpyxl or odf.
-        """
-
-        dfOutData = self.dfSubData(columns=subset)
-        
-        dfOutData.to_excel(fileName, sheet_name=sheetName or 'AllResults',
-                           index=index, engine=engine)
-
-    # Save data to an Open Document worksheet (through 'odfpy' module)
-    def toOpenDoc(self, fileName, sheetName=None, subset=None, index=True):
-
-        """Save data table to an Open Document worksheet, ODF format (through 'odfpy' module)
-
-        Warning: Needs pandas >= 0.25
-
-        Parameters:
-        :param fileName: target file pathname
-        :param sheetName: for results data only
-        :param subset: subset of columns to save
-        :param index: if True, save index column
-        """
-        
-        assert pkgver.parse(pd.__version__).release >= (1, 1), \
-               'Don\'t know how to write to OpenDoc format before Pandas 1.1'
-        
-        return self.toExcel(fileName, sheetName=sheetName, subset=subset,
-                            index=index, engine='odf')  # Force engine in case not a .ods.
-
-    @staticmethod
-    def _closeness(sLeftRight):
-
-        """
-        Relative closeness of 2 numbers : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)
-        = Compute the order of magnitude that separates the difference to the absolute max. of the two values.
-        
-        The greater it is, the lower the relative difference
-           Ex: 3 = 10**3 ratio between max absolute difference of the two,
-               +inf = NO difference at all,
-               0 = bad, one of the two is 0, and the other not.
-        """
-
-        x, y = sLeftRight.to_list()
-        
-        # Special cases with 1 NaN, or 1 or more inf => all different
-        if np.isnan(x):
-            if not np.isnan(y):
-                return 0  # All different
-        elif np.isnan(y):
-            return 0  # All different
-        
-        if np.isinf(x) or np.isinf(y):
-            return 0  # All different
-        
-        # Normal case
-        c = abs(x - y)
-        if not np.isnan(c) and c != 0:
-            c = c / max(abs(x), abs(y))
-        
-        return np.inf if c == 0 else round(-np.log10(c), 1)
-
-    # Make results cell values hashable (and especially analysis model params)
-    # * needed for use in indexes (hashability)
-    # * needed to cope with to_excel/read_excel Inconsistent None management
-    @staticmethod
-    def _toHashable(value):
-    
-        if isinstance(value, list):
-            hValue = str([float(v) for v in value])
-        elif pd.isnull(value):
-            hValue = 'None'
-        elif isinstance(value, (int, float)):
-            hValue = value
-        elif isinstance(value, str):
-            if ',' in value:  # Assumed already somewhat stringified list
-                hValue = str([float(v) for v in value.strip('[]').split(',')])
-            else:
-                hValue = str(value)
-        else:
-            hValue = str(value)
-        
-        return hValue
-    
-    @classmethod
-    def compareDataFrames(cls, dfLeft, dfRight, subsetCols=[], indexCols=[],
-                          dropCloser=np.inf, dropNans=True, dropCloserCols=False):
-    
-        """
-        Compare 2 DataFrames.
-
-        The resulting diagnosis DataFrame will have the same columns and merged index,
-        with a "closeness" value for each cell (see _closeness method) ;
-        rows where all cells have closeness > dropCloser (or eventually NaN) are yet dropped.
-        and columns where all cells have closeness > dropCloser (or eventually NaN) can also be dropped.
-        
-        Parameters:
-        :param dfLeft: Left DataFrame
-        :param dfRight: Right DataFrame
-        :param list subsetCols: on a subset of columns,
-        :param list indexCols: ignoring these columns, but keeping them as the index and sorting order,
-        :param float dropCloser: result will only include rows with all cell closeness > dropCloser
-                                 (default: np.inf => all cols and rows kept).
-        :param bool dropNans: smoother condition for dropCloser : if True, NaN values are also considered > dropCloser
-                              ('cause NaN != NaN :-( ).
-        :param bool dropCloserCols: if True, also drop all "> dropCloser (or eventually NaN)"-all-cell columns,
-                                    just as rows
-
-        :return: the diagnostic DataFrame.
-        """
-
-        if dfLeft.empty and dfRight.empty:
-            return pd.DataFrame()
-
-        # Make copies : we need to change the frames.
-        dfLeft = dfLeft.copy()
-        dfRight = dfRight.copy()
-        
-        # Check input columns
-        dColsSets = {'Subset column': subsetCols, 'Index column': indexCols}
-        for colsSetName, colsSet in dColsSets.items():
-            for col in colsSet:
-                if col not in dfLeft.columns:
-                    raise KeyError('{} {} not in left result set'.format(colsSetName, col))
-                if col not in dfRight.columns:
-                    raise KeyError('{} {} not in right result set'.format(colsSetName, col))
-        
-        # Set specified cols as the index (after making them hashable) and sort it.
-        dfLeft[indexCols] = dfLeft[indexCols].applymap(cls._toHashable)
-        dfLeft.set_index(indexCols, inplace=True)
-        dfLeft = dfLeft.sort_index()  # Not inplace: don't modify a copy/slice
-
-        dfRight[indexCols] = dfRight[indexCols].applymap(cls._toHashable)
-        dfRight.set_index(indexCols, inplace=True)
-        dfRight = dfRight.sort_index()  # Idem
-
-        # Filter data to compare (subset of columns).
-        if subsetCols:
-            dfLeft = dfLeft[subsetCols]
-            dfRight = dfRight[subsetCols]
-
-        # Append mutually missing rows to the 2 tables => a complete and identical index.
-        anyCol = dfLeft.columns[0]  # Need one, whichever.
-        dfLeft = dfLeft.join(dfRight[[anyCol]], rsuffix='_r', how='outer')
-        dfLeft.drop(columns=dfLeft.columns[-1], inplace=True)
-        dfRight = dfRight.join(dfLeft[[anyCol]], rsuffix='_l', how='outer')
-        dfRight.drop(columns=dfRight.columns[-1], inplace=True)
-
-        # Compare : Compute closeness 
-        nColLevels = dfLeft.columns.nlevels
-        KRightCol = 'tmp' if nColLevels == 1 else tuple('tmp{}'.format(i) for i in range(nColLevels))
-        dfRelDiff = dfLeft.copy()
-        exception = False
-        for leftCol in dfLeft.columns:
-            dfRelDiff[KRightCol] = dfRight[leftCol]
-            try:
-                dfRelDiff[leftCol] = dfRelDiff[[leftCol, KRightCol]].apply(cls._closeness, axis='columns')
-            except TypeError as exc:
-                logger.error(f'Column {leftCol} : {exc}')
-                exception = True
-            dfRelDiff.drop(columns=[KRightCol], inplace=True)
-            
-        if exception:
-            raise TypeError('Stopping: Some columns could not be compared (see errors logged above)')
-            
-        # Complete comparison : rows with index not in both frames forced to all-0 closeness
-        # (of course they should result so ... unless some NaNs here and there : fix this)
-        dfRelDiff.loc[dfLeft[~dfLeft.index.isin(dfRight.index)].index, :] = 0
-        dfRelDiff.loc[dfRight[~dfRight.index.isin(dfLeft.index)].index, :] = 0
-        
-        # Drop rows (and may be columns) with closeness over the threshold (or of NaN value if authorized)
-        dfCells2Drop = dfRelDiff.applymap(lambda v: v > dropCloser or (dropNans and pd.isnull(v)))
-        dfRelDiff.drop(dfRelDiff[dfCells2Drop.all(axis='columns')].index, inplace=True)
-        if dropCloserCols:
-            # dfRelDiff.drop(columns=dfRelDiff.T[dfCells2Drop.all(axis='index')].index, axis='index', inplace=True)
-            dfRelDiff.drop(columns=[col for col, drop in dfCells2Drop.all(axis='index').items() if drop],
-                           inplace=True)  # 40% faster.
-            
-        return dfRelDiff
-
-    def compare(self, other, subsetCols=[], indexCols=[], dropCloser=np.inf, dropNans=True, dropCloserCols=False):
-    
-        """
-        Compare 2 data sets.
-        
-        The resulting diagnosis DataFrame will have the same columns and merged index,
-        with a "closeness" value for each cell (see _closeness method) ;
-        rows where all cells have closeness > dropCloser (or eventually NaN) are yet dropped.
-        and columns where all cells have closeness > dropCloser (or eventually NaN) can also be dropped.
-        
-        Parameters:
-        :param other: Right data set or DataFrame object to compare
-        :param list subsetCols: on a subset of columns,
-        :param list indexCols: ignoring these columns, but keeping them as the index and sorting order,
-        :param float dropCloser: result will only include rows with all cell closeness > dropCloser
-                                 (default: np.inf => all cols and rows kept).
-        :param bool dropNans: smoother condition for dropCloser : if True, NaN values are also considered > dropCloser
-                              ('cause NaN != NaN :-( ).
-        :param bool dropCloserCols: if True, also drop all "> dropCloser (or eventually NaN)" columns-all-cell,
-                                    just as rows
-
-        :return: the diagnostic DataFrame.
-        """
-        
-        return self.compareDataFrames(dfLeft=self.dfData,
-                                      dfRight=other if isinstance(other, pd.DataFrame) else other.dfData,
-                                      subsetCols=subsetCols, indexCols=indexCols,
-                                      dropCloser=dropCloser, dropNans=dropNans, dropCloserCols=dropCloserCols)
-
-
-class FieldDataSet(DataSet):
-
-    """A tabular data set for producing mono-category or even individuals data sets from "raw sightings data",
-    aka "field data" (with possibly multiple category counts on each row)
-
-    Input support provided for pandas.DataFrame, Excel .xlsx file, tab-separated .csv/.txt files,
-      and even OpenDoc .ods file with pandas >= 0.25 (needs odfpy module)
-    """
-    def __init__(self, source, countCols, addMonoCatCols=dict(), importDecFields=[], sheet=None, separator='\t'):
-
-        """Ctor
-
-        Parameters:
-        :param source: the input field-data table
-        :param countCols: the category columns (each of them holds counts of individuals for the category)
-        :param addMonoCatCols: name and method of computing for columns to add after separating multi-category counts
-             (each column to add is computed through :
-                dfMonoCatSights[colName] = dfMonoCatSights[].apply(computeCol, axis='columns')
-                for colName, computeCol in addMonoCatCols.items())
-        :param importDecFields: for smart ./, decimal character management in CSV sources (pandas is not smart on this)
-        :param sheet: name of the sheet to read from, for multi-sheet data input file
-             (like Excel or Open Doc. workbooks)
-        :param separator: columns separator for CSV input file
-        """
-
-        super().__init__(sources=source, importDecFields=importDecFields, sheet=sheet, separator=separator)
-        
-        self.countCols = countCols
-        self.dCompdMonoCatColSpecs = addMonoCatCols
-        
-        self.dfIndivSights = None  # Not yet computed.
-        self.dfMonoCatSights = None  # Idem.
-        
-        logger.info(f'Field data : {len(self)} sightings')
-
-    @property
-    def dfData(self):
-        
-        return self._dfData
-
-    @dfData.setter
-    def dfData(self, dfData_):
-        
-        assert sorted(self._dfData.columns) == sorted(dfData_.columns), "Can't set data with different columns"
-        
-        self._dfData = dfData_
-        
-        self.dfIndivSights = None  # Not yet computed.
-        self.dfMonoCatSights = None  # Idem.
-    
-    # Transform a multi-category sightings set into an equivalent mono-category sightings set,
-    # that is where no sightings has more that one category with positive count (keeping the same total counts).
-    # Highly optimized version.
-    # Ex: A sightings set with 2 category count columns nMales and nFemales
-    #     * in the input set, you may have 1 sightings with nMales = 5 and nFemales = 2
-    #     * in the output set, this sightings have been separated in 2 distinct ones
-    #       (all other properties left untouched) :
-    #       the 1st with nMales = 5 and nFemales = 0, the 2nd with nMales = 0 and nFemales = 2.
-    @staticmethod
-    def _separateMultiCategoryCounts(dfInSights, countColumns):
-        
-        # For each count column
-        ldfMonoCat = list()
-        for col in countColumns:
-
-            # Select rows with some individuals in this column
-            dfOneCat = dfInSights[dfInSights[col] > 0].copy()
-
-            # Set all other count cols to 0.
-            otherCols = countColumns.copy()
-            otherCols.remove(col)
-            dfOneCat[otherCols] = 0
-
-            # Store into a list for later
-            ldfMonoCat.append(dfOneCat)
-
-        # Concat all data frames into one.
-        dfOutSights = pd.concat(ldfMonoCat)
-
-        # Sort to "initial" order (easier to read), and reset index (for unique labels).
-        dfOutSights.sort_index(inplace=True)
-        dfOutSights.reset_index(inplace=True, drop=True)
-
-        # Done.
-        return dfOutSights
-
-    # Transform a multi-individual mono-category sightings set into an equivalent mono-individual
-    # mono-category sightings set, that is where no sightings has more that one individual
-    # per category (keeping the same total counts).
-    # Highly optimized version.
-    # Ex: A sightings set with 2 mono-category count columns nMales and nFemales
-    #     * in tyhe input set, you may have 1 sightings with nMales = 3 and nFemales = 0
-    #       (but none with nMales and nFemales > 0)
-    #     * in the output set, this sightings have been separated in 3 distinct ones
-    #       (all other properties left untouched) : all with nMales = 1 and nFemales = 0.
-    @staticmethod
-    def _individualiseMonoCategoryCounts(dfInSights, countColumns):
-        
-        # For each category column
-        ldfIndiv = list()
-        for col in countColumns:
-            
-            # Select rows with some individuals in this column
-            dfOneCat = dfInSights[dfInSights[col] > 0]
-            
-            # Repeat each one by its count of individuals
-            dfIndiv = dfInSights.loc[np.repeat(dfOneCat.index.values, dfOneCat[col].astype(int).values)]
-
-            # Replace non-zero counts by 1.
-            dfIndiv.loc[dfIndiv[col] > 0, col] = 1
-            
-            # Store into a list for later
-            ldfIndiv.append(dfIndiv)
-            
-        # Concat all data frames into one.
-        dfOutSights = pd.concat(ldfIndiv)
-
-        # Sort to "initial" order (easier to read), and reset index (for unique labels).
-        dfOutSights.sort_index(inplace=True)
-        dfOutSights.reset_index(inplace=True, drop=True)
-        
-        # Done.
-        return dfOutSights
-        
-    # Access to the resulting mono-category data set
-    def monoCategorise(self, copy=True):
-    
-        # Compute only if not already done.
-        if self.dfMonoCatSights is None:
-        
-            # Separate multi-category counts
-            self.dfMonoCatSights = self._separateMultiCategoryCounts(self._dfData, self.countCols)
-            
-            # Compute and add supplementary mono-category columns
-            for colName, computeCol in self.dCompdMonoCatColSpecs.items():
-                self.dfMonoCatSights[colName] = self.dfMonoCatSights[self.countCols].apply(computeCol, axis='columns')
-        
-        return self.dfMonoCatSights if copy else self.dfMonoCatSights
-    
-    # Access to the resulting individuals data set 
-    # = a mono-category data set with individualised data : 1 individuals per row
-    def individualise(self, copy=True):
-    
-        # Compute only if not already done.
-        if self.dfIndivSights is None:
-        
-            # Individualise mono-category counts
-            self.dfIndivSights = \
-                self._individualiseMonoCategoryCounts(self.monoCategorise(copy=False), self.countCols)
-
-        return self.dfIndivSights.copy() if copy else self.dfIndivSights
-        
-
-# A tabular data set for producing on-demand sample data sets from "mono-category sightings data"
-# * Input support provided for pandas.DataFrame (from FieldDataSet.individualise/monoCategorise),
-#   Excel .xlsx file, tab-separated .csv/.txt files, and even OpenDoc .ods file with pandas >= 0.25 (needs odfpy module)
-class MonoCategoryDataSet(DataSet):
-
-    # Ctor
-    # * dfTransects: Transects infos with columns : transectPlaceCols (n), passIdCol (1), effortCol (1)
-    #                If None, auto generated from input sightings
-    # * effortConstVal: if dfTransects is None and effortCol not in source table, use this constant value
-    def __init__(self, source, dSurveyArea, importDecFields=[], dfTransects=None,
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleDecFields=['Effort', 'Distance'], effortConstVal=1,
-                 sheet=None, separator='\t'):
-        
-        super().__init__(sources=source, importDecFields=importDecFields, sheet=sheet, separator=separator)
-        
-        self.dSurveyArea = dSurveyArea
-        self.transectPlaceCols = transectPlaceCols
-        self.passIdCol = passIdCol
-        self.effortCol = effortCol
-        self.sampleDecFields = sampleDecFields
-    
-        self.dfTransects = dfTransects
-        if self.dfTransects is None or self.dfTransects.empty:
-            self.dfTransects = self._extractTransects(self._dfData, transectPlaceCols=self.transectPlaceCols,
-                                                      passIdCol=self.passIdCol, effortConstVal=effortConstVal)
-        elif effortCol not in self.dfTransects.columns:
-            self.dfTransects[effortCol] = effortConstVal
-
-        # A cache used by sampleDataSet to optimise consecutive calls with same sample specs (happens often).
-        self._sdsSampleDataSetCache = None
-        self._sSampleSpecsCache = None
-        
-        logger.info(f'Individuals data : {len(self)} sightings, {len(self.dfTransects)} transects')
-
-    @property
-    def dfData(self):
-        
-        return self._dfData
-
-    @dfData.setter
-    def dfData(self, dfData_):
-        
-        assert sorted(self._dfData.columns) == sorted(dfData_.columns), "Can't set data with different columns"
-        
-        self._dfData = dfData_
-    
-    # Extract transect infos from individuals sightings
-    # * effortConstVal: if effortCol not in dfIndivSightings, create one with this constant value
-    @staticmethod
-    def _extractTransects(dfIndivSightings, transectPlaceCols=['Transect'], passIdCol='Pass', 
-                          effortCol='Effort', effortConstVal=1):
-    
-        transCols = transectPlaceCols + [passIdCol]
-        if effortCol in dfIndivSightings.columns:
-            transCols.append(effortCol)
-        
-        dfTrans = dfIndivSightings[transCols]
-        
-        dfTrans = dfTrans.drop_duplicates()
-        
-        dfTrans.reset_index(drop=True, inplace=True)
-        
-        if effortCol not in dfTrans.columns:
-            dfTrans[effortCol] = effortConstVal
-
-        return dfTrans
-    
-    # Select sample sightings from an all-samples sightings table,
-    # and compute the associated total sample effort.
-    # * dSample : { key, value } selection criteria (with '+' support for 'or' operator in value),
-    #             keys being columns of dfAllSights (dict protocol : dict, pd.Series, ...)
-    # * dfAllSights : the all-samples (individual) sightings table to search into
-    # * dfAllEffort : effort values for each transect x pass really done, for the all-sample survey
-    # * transectPlaceCols : name of the input dfAllEffort and dSample columns to identify the transects (not passes)
-    # * passIdCol : name of the input dfAllEffort and dSample column to identify the passes (not transects)
-    # * effortCol : name of the input dfAllEffort and output effort column to add / replace
-    @staticmethod
-    def _selectSampleSightings(dSample, dfAllSights, dfAllEffort,
-                               transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort'):
-        
-        # Select sightings
-        dfSampSights = dfAllSights
-        for key, values in dSample.items():
-            values = str(values).strip()  # For ints as strings that get forced to int in io sometimes (ex. from_excel)
-            if values and values not in ['nan', 'None']:  # Empty value means "no selection criteria for this columns"
-                values = values.split('+') if '+' in values else [values]
-                dfSampSights = dfSampSights[dfSampSights[key].astype(str).isin(values)]
-
-        # Compute sample effort
-        passes = str(dSample[passIdCol])  # Same as above ...
-        if passes and passes not in ['nan', 'None']:  # Same as above ...
-            passes = passes.split('+') if '+' in passes else [passes]
-            dfSampEffort = dfAllEffort[dfAllEffort[passIdCol].astype(str).isin(passes)]
-        else:
-            dfSampEffort = dfAllEffort
-        dfSampEffort = dfSampEffort[transectPlaceCols + [effortCol]].groupby(transectPlaceCols).sum()
-        
-        # Add effort column
-        dfSampSights = dfSampSights.drop(columns=effortCol, errors='ignore').join(dfSampEffort, on=transectPlaceCols)
-
-        return dfSampSights, dfSampEffort
-        
-    # Add "absence" sightings to field data collected on transects for a given sample
-    # * dfInSights : input data table
-    # * sampleCols : the names of the sample identification columns
-    # * dfExpdTransects : the expected transects, as a data frame indexed by the transectPlace,
-    #     an index with same name as the corresponding column in dfInSights,
-    #     and with other info columns to duplicate in absence sightings (at least the effort value)
-    @staticmethod
-    def _addAbsenceSightings(dfInSights, sampleCols, dfExpdTransects):
-        
-        assert not dfInSights.empty, 'Error : Empty sightings data to add absence ones to !'
-
-        # Use the 1st sightings of the sample to build the absence sightings prototype
-        # (all null columns except for the sample identification ones, lefts as is)
-        dAbscSightTmpl = dfInSights.iloc[0].to_dict()
-        dAbscSightTmpl.update({k: None for k in dAbscSightTmpl.keys() if k not in sampleCols})
-
-        # Determine missing transects for the sample
-        transectPlaceCols = dfExpdTransects.index.name
-        dfSampTransects = dfInSights.drop_duplicates(subset=transectPlaceCols)
-        dfSampTransects.set_index(transectPlaceCols, inplace=True)
-        dfSampTransects = dfSampTransects[dfExpdTransects.columns]
-
-        dfMissgTransects = dfExpdTransects.loc[dfExpdTransects.index.difference(dfSampTransects.index)]
-
-        # Generate the absence sightings : 1 row per missing transect
-        ldAbscSights = list()
-        for _, sMissgTrans in dfMissgTransects.reset_index().iterrows():
-
-            dAbscSight = dAbscSightTmpl.copy()  # Copy template sightings
-            dAbscSight.update(sMissgTrans.to_dict())  # Update transect columns only
-
-            ldAbscSights.append(dAbscSight)
-
-        # Add them to the input sightings
-        return dfInSights.append(pd.DataFrame(pd.DataFrame(ldAbscSights)))
-
-    # Add survey area information to sightings data (in-place modification)
-    # * dfInSights : input data table
-    # * dSurveyArea : a column name to scalar dictionary-like data to add to the table
-    @staticmethod
-    def _addSurveyAreaInfo(dfInSights, dSurveyArea):
-        
-        for col, values in dSurveyArea.items():
-            dfInSights[col] = values
-            
-        return dfInSights
-    
-    # Sample individuals data for given sampling criteria, as a SampleDataSet.
-    # * sSample : { key, value } selection criteria (with '+' support for 'or' operator in value, no separating space),
-    #             keys being columns of dfAllSights (dict protocol : dict, pd.Series, ...)
-    def sampleDataSet(self, sSampleSpecs):
-        
-        # Don't redo what have just been done.
-        if self._sSampleSpecsCache is not None and self._sSampleSpecsCache.equals(sSampleSpecs):
-            return self._sdsSampleDataSetCache
-        
-        # Select sample data.
-        dfSampIndivObs, dfSampTransInfo = \
-            self._selectSampleSightings(dSample=sSampleSpecs, dfAllSights=self._dfData,
-                                        dfAllEffort=self.dfTransects, transectPlaceCols=self.transectPlaceCols,
-                                        passIdCol=self.passIdCol, effortCol=self.effortCol)
-        
-        # Don't go on if no selected data.
-        if dfSampIndivObs.empty:
-            logger.warning(f'Not even a single individual sighting selected for these specs: {sSampleSpecs.to_dict()}')
-            return None
-
-        # Add absence sightings
-        dfSampIndivObs = self._addAbsenceSightings(dfSampIndivObs, sampleCols=sSampleSpecs.index,
-                                                   dfExpdTransects=dfSampTransInfo)
-
-        # Add information about the studied geographical area
-        dfSampIndivObs = self._addSurveyAreaInfo(dfSampIndivObs, dSurveyArea=self.dSurveyArea)
-
-        # Create SampleDataSet instance (sort by transectPlaceCols : mandatory for MCDS analysis)
-        # and save it into cache.
-        self._sdsSampleDataSetCache = \
-            SampleDataSet(dfSampIndivObs, decimalFields=self.sampleDecFields, sortFields=self.transectPlaceCols)
-        self._sSampleSpecsCache = sSampleSpecs
-        
-        # Done.
-        return self._sdsSampleDataSetCache
-
-
-class SampleDataSet(DataSet):
-
-    """
-    A tabular input data set for multiple analyses on the same sample, with 1 or 0 individual per row
-    Warning:
-    * Only Point transect supported as for now
-    * No change made afterwards on decimal precision : provide what you need !
-    * Rows can be sorted if and as specified
-    * Input support provided for pandas.DataFrame, Excel .xlsx file, tab-separated .csv/.txt files,
-      and even OpenDoc .ods file with pandas >= 0.25 (needs odfpy module)
-    """
-    def __init__(self, source, decimalFields=[], sortFields=[], sheet=None, separator='\t'):
-        
-        self.decimalFields = decimalFields
-
-        super().__init__(sources=source, importDecFields=decimalFields, sheet=sheet, separator=separator)
-                
-        assert not self._dfData.empty, 'No data in set'
-        assert len(self._dfData.columns) >= 5, 'Not enough columns (should be at leat 5)'
-        
-        missCols = [decFld for decFld in self.decimalFields if decFld not in self._dfData.columns]
-        assert not missCols, '{} declared decimal field(s) are not in source columns {} : {}' \
-                             .format(len(missCols), ','.join(self._dfData.columns), ','.join(missCols))
-                             
-        # Sort / group sightings as specified
-        if sortFields:
-            self._dfData.sort_values(by=sortFields, inplace=True)
-
-        # Report some basic stats.
-        nAbscRows = self._dfData.isna().any(axis='columns').sum()
-        logger.info('Sample data : {} sightings = {} individuals + {} absence rows'
-                    .format(len(self), len(self) - nAbscRows, nAbscRows))
-
-
-class ResultsSet(object):
-    
-    """
-    A tabular result set for some computation process repeated multiple times with different input / results,
-    each process result(s) being given as a pd.Series (1 row for the target table) or a pd.DataFrame (multiple rows).
-    
-    With ability to prepend custom heading columns to each process results ones
-    (same value for each 1-process results rows).
-    
-    The columns of the internal pd.DataFrame, like those of each process result(s), can be a multi-index.
-    
-    Support for column translation is included (a mono-indexed).
-    """
-
-    def __init__(self, miCols, dfColTrans=None, miCustomCols=None, dfCustomColTrans=None,
-                 dComputedCols=None, dfComputedColTrans=None, sortCols=[], sortAscend=[], dropNACols=True):
-    
-        """Ctor
-        
-        Parameters:
-        :param miCols: process results columns (MultiIndex or not)
-        :param dfColTrans: translation DataFrame for results column labels,
-                           * index=column labels,
-                           * data=dict(<lang>=[<translation for each label> for all labels] for all <lang>)
-        :param miCustomCols: custom columns to prepend (on the left) to process results columns (MultiIndex or not)
-        :param dfCustomColTrans: translation DataFrame for custom column labels,
-        :param dComputedCols: dict(label: position) for inserting computed columns in results table (None = after end)
-        :param dfComputedColTrans: translation DataFrame for computed column labels,
-        :param sortCols: if not empty, iterable of columns to sort values by in dfData()
-        :param sortAscend: sorting order for all (bool), or each column (iterable of bool) in dfData()
-        :param dropNACols: If True, dfData() won't return columns with no relevant results data
-                           (except for custom cols).
-        """
-        
-        assert len(sortCols) == len(sortAscend), 'sortCols and sortAscend must have same length'
-
-        # Columns stuff.
-        if dComputedCols is None:
-            dComputedCols = dict()
-            dfComputedColTrans = pd.DataFrame()
-
-        # Columns : Process results + computed results (at the specified position if any)
-        self.miCols = miCols.copy()
-        for col, ind in dComputedCols.items():
-            if ind is not None:
-                self.miCols = self.miCols.insert(ind, col)
-        lastCompCols = [col for col, ind in dComputedCols.items() if ind is None]
-        if lastCompCols:
-            self.miCols = self.miCols.append(pd.MultiIndex.from_tuples(lastCompCols))
-
-        # 
-        self.computedCols = list(dComputedCols.keys())
-        self.miCustomCols = miCustomCols.copy() if miCustomCols is not None else list()
-        
-        self.isMultiIndexedCols = isinstance(miCols, pd.MultiIndex)
-
-        # DataFrames for translating 3-level multi-index columns to 1 level lang-translated columns
-        self.dfColTrans = pd.concat([dfColTrans if dfColTrans is not None else pd.DataFrame(),
-                                     dfComputedColTrans if dfColTrans is not None else pd.DataFrame()])
-        self.dfCustomColTrans = dfCustomColTrans.copy() if dfCustomColTrans is not None else pd.DataFrame()
-        
-        # Sorting and cleaning parameters (after postComputing)
-        self.sortCols = sortCols
-        self.sortAscend = sortAscend
-        self.dropNACols = dropNACols
-        
-        # Non-constant data members
-        self._dfData = pd.DataFrame()  # The real data (frame).
-        self.rightColOrder = False  # self._dfData columns are assumed to be in a wrong order.
-        self.postComputed = False  # Post-computation not yet done.
-    
-        # Specifications of computations that led to the results
-        self.specs = dict()
-
-    def __len__(self):
-        
-        return len(self._dfData)
-    
-    @property
-    def empty(self):
-        
-        return self._dfData.empty
-
-    @property
-    def columns(self):
-    
-        return self.dfData.columns
-        
-    @property
-    def index(self):
-        
-        return self._dfData.index
-
-    def dropRows(self, sbSelRows):
-    
-        """Drop specific rows in-place, selected through boolean indexing on self.dfData
-        
-        Parameters:
-        :param sbSelRows: boolean series with same index as self.dfData"""
-    
-        self._dfData.drop(self._dfData[sbSelRows].index, inplace=True)
-        
-    def copy(self, withData=True):
-    
-        """Clone function (shallow), with optional (deep) data copy"""
-
-        # 1. Call ctor without computed columns stuff for now and here (we no more have initial data)
-        clone = ResultsSet(miCols=self.miCols,
-                           miCustomCols=self.miCustomCols.copy(),
-                           dfCustomColTrans=self.dfCustomColTrans.copy(),
-                           sortCols=self.sortCols.copy(), sortAscend=self.sortAscend.copy())
-    
-        # 2. Complete clone initialisation.
-        clone.miCols = self.miCols.copy()
-        clone.computedCols = self.computedCols.copy()
-        
-        # DataFrames for translating 3-level multi-index columns to 1 level lang-translated columns
-        clone.dfColTrans = self.dfColTrans.copy()
-        
-        # Copy data if needed.
-        if withData:
-            clone._dfData = self._dfData.copy()
-            clone.rightColOrder = self.rightColOrder
-            clone.postComputed = self.postComputed
-
-        return clone
-
-    def _acceptNewColumns(self, newCols):
-
-        """Update results columns list (self.miCols) with new columns if not already present"""
-
-        logger.debug1(f'_acceptNewColumns: {newCols=}')
-        logger.debug2(f'{self.miCols=}')
-        logger.debug2(f'{self.miCustomCols=}')
-        
-        # Select columns to really append.
-        newCols = [col for col in newCols if col not in self.miCols and col not in self.miCustomCols]
-        logger.debug2(f'{newCols=}')
-
-        if self.isMultiIndexedCols:
-            self.miCols = self.miCols.append(newCols)
-            newColTrans = [' '.join(col) for col in newCols]  # Quite rough, but what else ?
-        else:
-            self.miCols += newCols
-            newColTrans = newCols
-        logger.debug2(f'{self.miCols=}')
-
-        # Update columns translation table also.
-        dfNewColTrans = pd.DataFrame(index=newCols,
-                                     data={lang: newColTrans for lang in self.dfColTrans.columns})
-        self.dfColTrans = self.dfColTrans.append(dfNewColTrans)
-
-    def append(self, sdfResult, sCustomHead=None, acceptNewCols=False):
-        
-        """Append row(s) of results to the all-results table
-        :param sdfResult: the Series (1 row) or DataFrame (N rows) to append
-        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
-            as unexpected columns appear in results to append
-        :param sCustomHead: Series holding custom cols values, to prepend (left) to each row of sdfResult,
-            before appending to the internal table.
-        
-          |---- custom heading columns ----|---- real results ---------------------|
-          |          |          |          |         |         |         |         |
-          |               ...              |                  ...                  |
-          |                ( data already there before append)                     |
-          |               ...              |                  ...                  |
-          |          |          |          |         |         |         |         |
-          --------------------------------------------------------------------------
-          |     x     |    y    |    z     |    a1   |    b1   |    c1   |    d1   |<- append(5 rows)
-          |     x     |    y    |    z     |    a2   |    b2   |    c2   |    d2   |
-          | (from sCustomHead, replicated) | (from sdfResult: here a 5-row table)  | 
-          |     x     |    y    |    z     |    a4   |    b4   |    c4   |    d4   |
-          |     x     |    y    |    z     |    a5   |    b5   |    c5   |    d5   |
-          --------------------------------------------------------------------------
-        """
-
-        assert not self.postComputed, "Can't append after columns post-computation"
-        
-        assert isinstance(sdfResult, (pd.Series, pd.DataFrame)), \
-               'sdfResult : Can only append a pd.Series or pd.DataFrame'
-        assert sCustomHead is None or isinstance(sCustomHead, pd.Series), \
-               'sCustomHead : Can only append a pd.Series'
-
-        # If specified, update results columns list (self.miCols) dynamically 
-        # as unexpected columns appear in results to append.
-        if acceptNewCols:
-            resCols = list(sdfResult.index) if isinstance(sdfResult, pd.Series) else list(sdfResult.columns)
-            self._acceptNewColumns(resCols)
-
-        # Prepend header columns to results (on the left) if any.
-        if sCustomHead is not None:
-            if isinstance(sdfResult, pd.Series):
-                sdfResult = sCustomHead.append(sdfResult)
-            else:  # DataFrame
-                dfCustomHead = pd.DataFrame([sCustomHead]*len(sdfResult)).reset_index(drop=True)
-                sdfResult = pd.concat([dfCustomHead, sdfResult], axis='columns')
-        
-        # Append results rows to the already present ones (at the end)
-        if self._dfData.columns.empty:
-            # In order to preserve types, we can't use self._dfData.append(sdfResult),
-            # because it doesn't preserve original types (int => float)
-            if isinstance(sdfResult, pd.Series):
-                # self._dfData = sdfResult.to_frame().T  # This doesn't preserve types (=> all object)
-                self._dfData = pd.DataFrame([sdfResult])
-            else:  # DataFrame
-                self._dfData = sdfResult
-        else:
-            self._dfData = self._dfData.append(sdfResult, ignore_index=True)
-        
-        # Appending (or concat'ing) often changes columns order
-        self.rightColOrder = False
-        
-    # Post-computations.
-    def postComputeColumns(self):
-        
-        # Derive class to really compute things (=> work directly on self._dfData),
-        pass
-        
-    @property
-    def dfRawData(self):
-
-        """Direct access to unpostprocessed data
-
-        May have to remove computed columns and reset postCompute state.
-        """
-
-        # Post-computation is now "not yet done" (and remove any computed column: will be recomputed later on).
-        self.setPostComputed(False)
-
-        return self._dfData
-        
-    def getData(self, copy=True):
-        
-        # Do post-computation and sorting if not already done.
-        if not(self._dfData.empty or self.postComputed):
-        
-            # Make sure we keep a MultiIndex for columns if it was the case (append breaks this, not fromExcel)
-            if self.isMultiIndexedCols and not isinstance(self._dfData.columns, pd.MultiIndex):
-                self._dfData.columns = pd.MultiIndex.from_tuples(self._dfData.columns)
-            
-            # Post-compute as specified (or not).
-            self.postComputeColumns()
-            self.postComputed = True  # No need to do it again from now on !
-            
-            # Sort as/if specified.
-            if self.sortCols:
-                self._dfData.sort_values(by=self.sortCols, ascending=self.sortAscend, inplace=True)
-        
-        # Enforce right columns order.
-        if not(self._dfData.empty or self.rightColOrder):
-            
-            miTgtColumns = self.miCols
-            if self.miCustomCols is not None:
-                if self.isMultiIndexedCols:
-                    miTgtColumns = self.miCustomCols.append(miTgtColumns)
-                else:
-                    miTgtColumns = self.miCustomCols + miTgtColumns
-            self._dfData = self._dfData.reindex(columns=miTgtColumns)
-            self.rightColOrder = True  # No need to do it again, until next append() !
-        
-        # This is also documentation line !
-        if self.isMultiIndexedCols and not self._dfData.empty:
-            assert isinstance(self._dfData.columns, pd.MultiIndex)
-        
-        # If specified, don't return columns with no relevant results data (unless among custom cols).
-        if self.dropNACols and not self._dfData.empty:
-            miCols2Cleanup = self._dfData.columns
-            if self.miCustomCols is not None:
-                if self.isMultiIndexedCols:
-                    miCols2Cleanup = miCols2Cleanup.drop(self.miCustomCols.to_list())
-                else:
-                    miCols2Cleanup = [col for col in miCols2Cleanup if col not in self.miCustomCols]
-            cols2Drop = [col for col in miCols2Cleanup if self._dfData[col].isna().all()]
-            logger.debug(f'Dropping all-NaN columns {cols2Drop}')
-            self._dfData.drop(columns=cols2Drop, inplace=True)
-
-        # Done.
-        return self._dfData.copy() if copy else self._dfData
-
-    @property
-    def dfData(self):
-
-        return self.getData(copy=True)
-    
-    def setPostComputed(self, on=True):
-
-        # If not specified as already postComputed, prepare for re-computation later by removing any "computed" column.
-        if not on:
-            # ... but ignore those which are not there : backward compat. / tolerance ...
-            self._dfData.drop(columns=self.computedCols, errors='ignore', inplace=True)
-        
-        # Post-computation not yet done (unless told it _is_: accept blindly).
-        self.postComputed = on
-
-    def setData(self, dfData, postComputed=False, acceptNewCols=False):
-
-        # Prerequisites
-        assert isinstance(dfData, pd.DataFrame), 'dfData must be a pd.DataFrame'
-        assert dfData.index.nunique() == len(dfData), 'dfData index must be unique'
-
-        # Take source data as ours now (after copying).
-        self._dfData = dfData.copy()
-        
-        # If specified, update results columns list (self.miCols) dynamically 
-        # as unexpected columns appear in results to append.
-        if acceptNewCols:
-            self._acceptNewColumns(dfData.columns)
-
-        # Let's assume that columns order is dirty.
-        self.rightColOrder = False
-        
-        # Post-computation as not yet done, unless told it _is_: accept blindly.
-        self.setPostComputed(postComputed)
-    
-    @dfData.setter
-    def dfData(self, dfData):
-        
-        self.setData(dfData)
-    
-    # Sort rows in place and overwrite initial sortCols / sortAscend values.
-    def sortRows(self, by, ascending=True):
-    
-        self._dfData.sort_values(by=by, ascending=ascending, inplace=True)
-        
-        self.sortCols = by
-        self.sortAscend = ascending
-    
-    # Add columns translations (update if already there)
-    def addColumnsTrans(self, dColsTrans=dict()):
-
-        for col, dTrans in dColsTrans.items():
-            self.dfColTrans.loc[col] = dTrans
-
-    # Build translation table for lang (from custom and other columns)
-    def transTable(self):
-        
-        dfColTrans = self.dfColTrans
-        if self.dfCustomColTrans is not None:
-            dfColTrans = self.dfCustomColTrans.append(dfColTrans, sort=False)
-            
-        return dfColTrans
-        
-    # Get translated names of some columns (custom or not)
-    def transColumns(self, columns, lang):
-        
-        dTransCols = self.transTable()[lang]
-        return [dTransCols.get(col, str(col)) for col in columns]
-    
-    # Get translated names of custom columns
-    def transCustomColumns(self, lang):
-        
-        return self.dfCustomColTrans[lang].to_list()
-    
-    # Get translated names of some specific column (custom or not)
-    def transColumn(self, column, lang):
-        
-        return self.dfColTrans.loc[column, lang] if column in self.dfColTrans.index \
-               else self.dfCustomColTrans.loc[column, lang]
-    
-    def dfSubData(self, index=None, columns=None, copy=False):
-    
-        """Get a subset of the all-results table rows and columns
-
-        Parameters:
-        :param index: rows to select, as an iterable of a subset of self.dfData.index values
-                      (anything suitable for pd.DataFrame.loc[...]) ; None = all rows.
-        :param columns: columns to select, as a list(string) or pd.Index when mono-indexed columns
-        :param copy: if True, return a full copy of the selected data, not a "reference" to the internal table
-        """
-        
-        assert columns is None or isinstance(columns, list) or isinstance(columns, (pd.Index, pd.MultiIndex)), \
-               'columns must be specified as None/[] (all), or as a list of tuples, or as a pandas.[Multi]Index'
-
-        # Make a copy of / extract selected columns of dfData.
-        dfSbData = self.getData(copy=False)
-        if columns is not None and len(columns) > 0:
-            if self.isMultiIndexedCols and isinstance(columns, list):
-                iColumns = pd.MultiIndex.from_tuples(columns)
-            else:
-                iColumns = columns
-            dfSbData = dfSbData.reindex(columns=iColumns)
-        
-        if index is not None:
-            dfSbData = dfSbData.loc[index]
-
-        if copy:
-            dfSbData = dfSbData.copy()
-            
-        return dfSbData
-
-    # Access a mono-indexed translated columns version of the data table
-    def dfTransData(self, lang='en', index=None, columns=None):
-        
-        """Get a subset of the all-results table rows and columns with translated column names (mono-index)
-
-        Note: The resulting table holds a copy (of part) of the internal table ; this is needed because
-              of the translation of the column names ; hence the absence of the expected "copy" parameter !
-
-        Parameters:
-        :param lang: target language for translation ('en' or 'fr')
-        :param index: rows to select, as a list(int) or pd.Index (subset of self.dfData.index) ; None = all rows.
-        :param columns: columns to select, as a list(string) or pd.Index when mono-indexed columns
-                       or as a list(tuple(string*)) or pd.MultiIndex when multi-indexed ; None = all columns.
-        """
-
-        assert lang in ['en', 'fr'], 'No support for "{}" language'.format(lang)
-        
-        # Extract (and may be copy) selected rows and columns of dfData.
-        dfTrData = self.dfSubData(index=index, columns=columns, copy=True)
-        
-        # Translate column names.
-        dfTrData.columns = self.transColumns(dfTrData.columns, lang)
-        
-        return dfTrData
-
-    def updateSpecs(self, reset=False, overwrite=False, **specs):
-
-        """Update specs as given
-
-        Parameters:
-        :param reset: if True, cleanup before updating => like a set !
-        :param overwrite: if False, and at least 1 of the spec already exists,
-                      raise an exception (refuse to update) ; otherwise, overwrite silently
-        :param specs: named specs to add/update
-        """
-        if reset:
-            self.specs.clear()
-
-        if not overwrite:
-            assert all(name not in self.specs for name in specs), \
-                   "Unless explicitly specified, won't overwrite already present specs {}" \
-                   .format(', '.join(name for name in specs if name in self.specs))
-
-        self.specs.update(specs)
-
-    def toPickle(self, fileName, specs=True, raw=False):
-
-        """Save raw data and specs to a pickle file (XZ compressed format if requested).
-
-        Parameters:
-        :param fileName: target file pathname ; file is auto-compressed to XZ format
-                         through the lzma module if its extension is .xz or .lzma,
-                         or else not compressed.
-        :param specs: if False, don't save specs (actually save empty specs)
-        :param raw: if False, save post-processed data (through dfData) ;
-                    if True, save raw data (through dfRawData)
-        """
-        
-        start = pd.Timestamp.now()
-
-        dfOutData = self.dfRawData if raw else self.dfData
-
-        logger.debug(f'toPickle: {dfOutData.columns=}')
-
-        compress = pl.Path(fileName).suffix in ['.xz', '.lzma']
-        with lzma.open(fileName, 'wb') if compress else open(fileName, 'wb') as file:
-            pickle.dump((dfOutData, self.specs if specs else dict()), file)
-
-        logger.info('{}x{} results rows x columns and {} specs saved to {} in {:.3f}s'
-                    .format(len(dfOutData), len(dfOutData.columns),
-                            len(self.specs) if specs else 'no', fileName,
-                            (pd.Timestamp.now() - start).total_seconds()))
-
-    def specs2Tables(self):
-
-        """Transform specs to tables
-
-        :return: dict(name=DataFrame)
-        """
-        ddfSpecs = dict()
-
-        for spName, spData in self.specs.items():
-            if isinstance(spData, (dict, list, pd.Series)):
-                if not isinstance(spData, pd.Series):
-                    spData = pd.Series(spData)
-                spData = spData.to_frame()
-            elif not isinstance(spData, pd.DataFrame):
-                raise NotImplementedError
-            ddfSpecs[spName] = spData
-
-        return ddfSpecs
-
-    @staticmethod
-    def specsFromTables(ddfTables):
-
-        specs = dict()
-        for spName, dfSpData in ddfTables.items():
-            if len(dfSpData.columns) == 1:  # Output a Series, or dict or list
-                if dfSpData.columns[0] == 0:
-                    if dfSpData.index.equals(pd.RangeIndex(stop=len(dfSpData))):
-                        specs[spName] = dfSpData.loc[:, 0].to_list()
-                    else:
-                        specs[spName] = dfSpData.loc[:, 0].to_dict()
-                else:  # Output a Series
-                    specs[spName] = dfSpData[dfSpData.columns[0]]
-            else:  # Output a DataFrame
-                specs[spName] = dfSpData
-
-        return specs
-
-    DefAllResultsSheetName = 'all-results'
-
-    def toExcel(self, fileName, sheetName=None, lang=None, subset=None, index=True,
-                specs=True, specSheetsPrfx='sp-', engine=None):
-
-        """Save data and specs to a worksheet format (Excel, ODF, ...) :
-        * newer XLSX format for .xlsx extensions (through 'openpyxl' or 'xlrd' module)
-        * .xls (through 'xlwt').
-        * .ods (through 'odfpy'), if pandas >= 0.25.
-
-        Details:
-        * data to the named sheet (sheetName)
-        * specs to the sheets given spec name, prefixed as specified (specSheetsPrfx)
-
-        Parameters:
-        :param fileName: target file name
-        :param sheetName: for results data only
-        :param lang: if not None, save translated data columns names
-        :param subset: subset of data columns to save
-        :param index: if True, save data index column
-        :param specs: if False, don't save specs
-        :param specSheetsPrfx: prefix to spec names to use to build spec sheet names
-        :param engine: None => auto-selection from file extension ; otherwise, use xlrd, openpyxl or odf.
-        """
-        
-        assert sheetName is None or not specs or not sheetName.lower().startswith(specSheetsPrfx), \
-               f"Results data sheet name can't start with reserved prefix {specSheetsPrfx} (whatever case)"
-        assert not (sheetName is None
-                    and specs and self.DefAllResultsSheetName.lower().startswith(specSheetsPrfx.lower())), \
-               "Sheet prefix '{}' can't be a heading part of {} (whatever case)" \
-               .format(specSheetsPrfx, self.DefAllResultsSheetName)
-
-        start = pd.Timestamp.now()
-        
-        dfOutData = self.dfSubData(columns=subset) if lang is None else self.dfTransData(columns=subset, lang=lang)
-        
-        with pd.ExcelWriter(fileName, engine=engine) as xlWrtr:
-            dfOutData.to_excel(xlWrtr, sheet_name=sheetName or self.DefAllResultsSheetName, index=index)
-            for spName, dfSpData in self.specs2Tables().items():
-                dfSpData.to_excel(xlWrtr, sheet_name=specSheetsPrfx + spName, index=True)
-
-        logger.info('{}x{} results rows x columns and {} specs saved to {} in {:.3f}s'
-                    .format(len(dfOutData), len(dfOutData.columns),
-                            len(self.specs) if specs else 'no', fileName,
-                            (pd.Timestamp.now() - start).total_seconds()))
-
-    def toOpenDoc(self, fileName, sheetName=None, lang=None, subset=None, index=True,
-                  specs=True, specSheetsPrfx='sp-'):
-        
-        """Save data and specs to ODF worksheet format
-        
-        Note: Needs pandas >= 0.25.
-
-        Details:
-        * data to the named sheet (sheetName)
-        * specs to the sheets given spec name, prefixed as specified (specSheetsPrfx)
-
-        Parameters:
-        :param fileName: target file name
-        :param sheetName: for results data only
-        :param lang: if not None, save translated data columns names
-        :param subset: subset of data columns to save
-        :param index: if True, save data index column
-        :param specs: if False, don't save specs
-        :param specSheetsPrfx: prefix to spec names to use to build spec sheet names
-        """
-
-        assert pkgver.parse(pd.__version__).release >= (1, 1), \
-               "Don't know how to write to OpenDoc format before Pandas 1.1"
-        
-        self.toExcel(fileName, sheetName=sheetName, lang=lang, subset=subset, index=index,
-                     specs=specs, specSheetsPrfx=specSheetsPrfx, engine='odf')
-
-    def fromPickle(self, fileName, specs=True, postComputed=False, acceptNewCols=False, dDefMissingCols=dict()):
-
-        """Load (overwrite) data and optionnaly specs from a pickle file, possibly lzma-compressed,
-        assuming ctor params match the results object used for prior toPickle(),
-        which can well be ensured by using the same ctor params as used for saving !
-
-        :param fileName: source file pathname ; file is auto-decompressed through the lzma module
-                         if its extension is .xz or .lzma.
-        :param specs: if False, don't load specs
-        :param postComputed: if True, prevents next post-computation 
-        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
-            if unexpected columns appear in loaded data to append
-        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
-            (Warning: only from self.miCols)
-        """
-        
-        start = pd.Timestamp.now()
-
-        # Load results data and spec from file
-        compressed = pl.Path(fileName).suffix in ['.xz', '.lzma']
-        with lzma.open(fileName, 'rb') if compressed else open(fileName, 'rb') as file:
-            dfData, dSpecs = pickle.load(file)
-
-        # Complete missing columns if any.
-        for colName, colDefVal in dDefMissingCols.items():
-            if colName not in dfData.columns:
-                dfData[colName] = colDefVal
-
-        # Set data.
-        self.setData(dfData, postComputed=postComputed)
-
-        # If specified, update results columns list (self.miCols) dynamically 
-        # if unexpected columns appear in loaded data.
-        if acceptNewCols:
-            self._acceptNewColumns(dfData.columns)
-
-        # Set specs if specified.
-        if specs:
-            self.specs = dSpecs
-
-        logger.info('{}x{} results rows x columns and {} specs loaded from {} in {:.3f}s'
-                    .format(len(dfData), len(dfData.columns),
-                            len(self.specs) if specs else 'no', fileName,
-                            (pd.Timestamp.now() - start).total_seconds()))
-
-    def fromExcel(self, fileName, sheetName=None, header=[0, 1, 2], skipRows=[3], indexCol=0,
-                  specs=True, specSheetsPrfx='sp-', postComputed=False,
-                  acceptNewCols=False, dDefMissingCols=dict(), engine=None):
-        
-        """Load (overwrite) data from the first or named sheet of an Excel worksheet (XLSX or XLS format),
-        assuming ctor params match with Excel sheet column names and list,
-        which can well be ensured by using the same ctor params as used for saving !
-
-        Also optionnaly load specs from other sheets named with given prefix, as dataframes
-        (ignore others ; empty prefix => all others)
-
-        Parameters:
-        :param fileName: source file name
-        :param sheetName: name of the sheet to load data from (default None => 1st sheetw)
-        :param header: list of source data row indexes to use for column index (1st sheet only)
-        :param skipRows: list of source data row indexes to ignore (1st sheet only)
-        :param indexCol: index of the source data column to use as index (1st sheet only)
-                         (None => auto-generated, not read)
-        :param specs: if False, don't load specs
-        :param specSheetsPrfx: name prefix to use to detect spec sheets
-        :param postComputed: if True, prevents next post-computation 
-        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
-            if unexpected columns appear in loaded data to append
-        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
-            (Warning: only from self.miCols)
-        :param engine: None => auto-selection from file extension ; otherwise, use xlrd, openpyxl or odf.
-        """
-
-        start = pd.Timestamp.now()
-        
-        # Load results data.
-        logger.info1(f'Loading {fileName}:')
-        xlReader = pd.ExcelFile(fileName, engine=engine)
-        logger.info2(f'* {sheetName or xlReader.sheet_names[0]} ...')
-        dfData = pd.read_excel(xlReader, sheet_name=sheetName or 0, header=header,
-                               skiprows=skipRows, index_col=indexCol)
-
-        # Complete missing columns if any.
-        for colName, colDefVal in dDefMissingCols.items():
-            if colName not in dfData.columns:
-                dfData[colName] = colDefVal
-
-        # Set data.
-        self.setData(dfData, postComputed=postComputed)
-
-        # If specified, update results columns list (self.miCols) dynamically 
-        # if unexpected columns appear in loaded data.
-        if acceptNewCols:
-            self._acceptNewColumns(dfData.columns)
-
-        # Load specs
-        self.specs = dict()
-        if specs:
-            ddfSpecs = dict()
-            for shName in xlReader.sheet_names:
-                if shName.startswith(specSheetsPrfx):
-                    logger.info2(f'* {shName} ...')
-                    ddfSpecs[shName[len(specSheetsPrfx):]] = \
-                        pd.read_excel(xlReader, sheet_name=shName, index_col=0)
-            self.specs = self.specsFromTables(ddfSpecs)
-
-        logger.info('{}x{} results rows x columns and {} specs loaded from {} in {:.3f}s'
-                    .format(len(dfData), len(dfData.columns),
-                            len(self.specs) if specs else 'no', fileName,
-                            (pd.Timestamp.now() - start).total_seconds()))
-
-    def fromOpenDoc(self, fileName, sheetName=None, header=[0, 1, 2], skipRows=[3], indexCol=0,
-                    specs=True, specSheetsPrfx='sp-', postComputed=False,
-                    acceptNewCols=False, dDefMissingCols=dict()):
-        
-        """Load (overwrite) data from the first or named sheet of an Open Document worksheet (ODS format),
-        assuming ctor params match with ODF sheet column names and list,
-        which can well be ensured by using the same ctor params as used for saving !
-
-        Also optionnaly load specs from other sheets with given prefix as dataframes
-        (ignore others ; empty prefix => all others)
-
-        Parameters:
-        :param fileName: source file name
-        :param sheetName: name of the sheet to load data from (default None => 1st)
-        :param header: list of source data row indexes to use for column index
-        :param skipRows: list of source data row indexes to ignore
-        :param indexCol: index of the source data column to use as index (None => auto-generated, not read)
-        :param specs: if False, don't load specs
-        :param specSheetsPrfx: name prefix to use to detect spec sheets
-        :param postComputed: if True, prevents next post-computation 
-        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
-            if unexpected columns appear in loaded data to append
-        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
-            (Warning: only from self.miCols)
-        """
-
-        assert pkgver.parse(pd.__version__).release >= (0, 25, 1), \
-               'Don\'t know how to read from OpenDoc format before Pandas 0.25.1 (using odfpy module)'
-        
-        self.fromExcel(fileName, sheetName=sheetName, header=header, skipRows=skipRows, indexCol=indexCol,
-                       specs=specs, specSheetsPrfx=specSheetsPrfx, postComputed=postComputed,
-                       acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
-
-    def fromFile(self, fileName, sheetName=None, header=[0, 1, 2], skipRows=[3], indexCol=0,
-                 specs=True, specSheetsPrfx='sp-', postComputed=False,
-                 acceptNewCols=False, dDefMissingCols=dict()):
-        
-        """Load (overwrite) data data and eventually specs from a given file,
-        (supported formats are .pickle, .pickle.xz, .ods, .xlsx, .xls, auto-detected from file name)
-        assuming ctor params match with the results object used to generate source file,
-        which can well be ensured by using the same ctor params as used for saving !
-        Notes: Needs odfpy module and pandas.version >= 0.25.1
-
-        Parameters:
-        :param fileName: source file name
-        :param sheetName: name of the sheet to load data from (default None => 1st)
-        :param header: list of source data row indexes to use for column index
-        :param skipRows: list of source data row indexes to ignore
-        :param indexCol: index of the source data column to use as index (None => auto-generated, not read)
-        :param specs: if False, don't load specs
-        :param specSheetsPrfx: name prefix to use to detect spec sheets
-        :param postComputed: if True, prevents next post-computation 
-        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
-            if unexpected columns appear in loaded data to append
-        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
-            (Warning: only from self.miCols)
-        """
-
-        fpn = pl.Path(fileName)
-        if fpn.suffix in ['.xz', '.pickle']:
-            self.fromPickle(fileName, specs=specs, postComputed=postComputed,
-                            acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
-        elif fpn.suffix in ['.ods']:
-            self.fromOpenDoc(fileName, sheetName=sheetName, header=header, skipRows=skipRows,
-                             indexCol=indexCol, specs=specs, specSheetsPrfx=specSheetsPrfx, postComputed=postComputed,
-                             acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
-        elif fpn.suffix in ['.xls', '.xlsx']:
-            self.fromExcel(fileName, sheetName=sheetName, header=header, skipRows=skipRows,
-                           indexCol=indexCol, specs=specs, specSheetsPrfx=specSheetsPrfx, postComputed=postComputed,
-                           acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
-        else:
-            raise NotImplementedError(f'Unsupported ResultsSet input file format: {fileName}')
-
-    def compare(self, other, subsetCols=[], indexCols=[], dropCloser=np.inf, dropNans=True, dropCloserCols=False):
-    
-        """
-        Compare 2 results sets.
-        
-        The resulting diagnosis DataFrame will have the same columns and merged index,
-        with a "closeness" value for each cell (see _closeness method) ;
-        rows where all cells have closeness > dropCloser (or eventually NaN) are yet dropped.
-        and columns where all cells have closeness > dropCloser (or eventually NaN) can also be dropped.
-        
-        Parameters:
-        :param other: Right results or DataFrame object to compare
-        :param list subsetCols: on a subset of columns,
-        :param list indexCols: ignoring these columns, but keeping them as the index and sorting order,
-        :param float dropCloser: result will only include rows with all cell closeness > dropCloser
-                                 (default: np.inf => all cols and rows kept).
-        :param bool dropNans: smoother condition for dropCloser : if True, NaN values are also considered > dropCloser
-                              ('cause NaN != NaN :-( ).
-        :param bool dropCloserCols: if True, also drop "> dropCloser (or eventually NaN)"-all-cell columns,
-                                    just as rows
-
-        :return: the diagnostic DataFrame.
-        """
-        
-        return DataSet.compareDataFrames(dfLeft=self.dfData,
-                                         dfRight=other if isinstance(other, pd.DataFrame) else other.dfData,
-                                         subsetCols=subsetCols, indexCols=indexCols,
-                                         dropCloser=dropCloser, dropNans=dropNans, dropCloserCols=dropCloserCols)
-        
-
-if __name__ == '__main__':
-
-    sys.exit(0)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "data": Input and output DS data manipulation tools
+
+import sys
+import pathlib as pl
+from packaging import version as pkgver
+
+import lzma
+import pickle
+
+import numpy as np
+import pandas as pd
+
+from . import log, runtime
+
+runtime.update(numpy=np.__version__, pandas=pd.__version__)
+
+logger = log.logger('ads.dat')
+
+
+class DataSet(object):
+
+    """"A tabular data set built by concatenating various-formatted source tables into one.
+    
+    Note: visionat module also defines this class: no reason it differs in any way => synchronise please !
+    Why ? Just to keep the 2 modules independent ; might change in the future"""
+    
+    def __init__(self, sources, dRenameCols={}, dComputeCols={}, importDecFields=[],
+                 sheet=None, skipRows=None, headerRows=0, indexCols=None, separator='\t', encoding='utf-8'):
+    
+        """Ctor
+        :param sources: data sources to read from ; input support provided for:
+             * pandas.DataFrame,
+             * Excel .xlsx (through 'openpyxl' or 'xlrd' module)
+               and .xls files (through 'openpyxl' module, no need for 'xlwt'),
+             * tab-separated .csv/.txt files,
+             * and even OpenDoc .ods file with pandas >= 0.25 (through 'odfpy' module) ;
+             when multiple source provided, sources are supposed to have compatible columns names,
+             and data rows from each source are appended 1 source after the previous.
+        :param dRenameCols: dict for renaming input columns right after loading data
+        :param dComputeCols: name and compute method for computed columns to be auto-added ;
+                             as a dict { new col. name => constant, or function to apply
+                             to each row to auto-compute the new sightings column } ;
+                             note: these columns can also be renamed, through dRenameCols
+        :param importDecFields: for smart ./, decimal character management in CSV sources (pandas is not smart on this)
+        :param sheet: name of the sheet to read from, for multi-sheet data files (like Excel or Open Doc. workbooks)
+        :param skipRows: list of indexes of initial rows to skip for file sources (before the column names row)
+        :param headerRows: index (or list of) of rows holding columns names (default 0 => 1st row)
+        :param indexCols: index (or list of) of first columns to use as (multi)index (default None => auto-gen index)
+        :param separator: columns separator for CSV sources
+        :param encoding: encoding for CSV sources
+        """
+    
+        if isinstance(sources, (str, pl.Path)):
+            self._dfData = self._fromDataFile(sources, sheet=sheet, decimalFields=importDecFields,
+                                              skipRows=skipRows, headerRows=headerRows, indexCols=indexCols,
+                                              separator=separator, encoding=encoding)
+        elif isinstance(sources, pd.DataFrame):
+            self._dfData = self._fromDataFrame(sources)
+        elif isinstance(sources, list):
+            ldfData = list()
+            for source in sources:
+                if isinstance(source, (str, pl.Path)):
+                    dfData = self._fromDataFile(source, sheet=sheet, decimalFields=importDecFields,
+                                                skipRows=skipRows, headerRows=headerRows, indexCols=indexCols,
+                                                separator=separator, encoding=encoding)
+                elif isinstance(source, pd.DataFrame):
+                    dfData = self._fromDataFrame(source)
+                    logger.info1('Loaded {} rows x {} columns from data frame'.format(len(dfData), len(dfData.columns)))
+                else:
+                    raise Exception('source for DataSet must be a pandas.DataFrame or an existing file')
+                ldfData.append(dfData)
+            self._dfData = pd.concat(ldfData, ignore_index=True)
+        else:
+            raise Exception('Source for DataSet must be a pandas.DataFrame or an existing file')
+
+        if self._dfData.empty:
+            logger.warning('No data in source data set')
+            return
+            
+        logger.info(f'Loaded {len(self)} x {len(self.columns)} total rows x columns in data set ...')
+        logger.info('... found columns: [{}]'.format('|'.join(str(c) for c in self.columns)))
+        
+        # Rename columns if requested.
+        if dRenameCols:
+            for key in dRenameCols.keys():
+                if key in dComputeCols:
+                    dComputeCols[dRenameCols[key]] = dComputeCols[key]
+                    dComputeCols.pop(key)            
+            self.renameColumns(dRenameCols)
+
+        # Add auto-computed columns if any.
+        if dComputeCols:
+
+            self.addColumns(dComputeCols)
+
+    # Wrapper around pd.read_csv for smart ./, decimal character management (pandas is not smart on this)
+    # TODO: Make this more efficient
+    @staticmethod
+    def _csv2df(fileName, decCols, skipRows=None, headerRows=0, indexCols=None, sep='\t', encoding='utf-8'):
+        df = pd.read_csv(fileName, sep=sep, skiprows=skipRows,
+                         header=headerRows, index_col=indexCols)
+        allRight = True
+        for col in decCols:
+            if df[col].dropna().apply(lambda v: isinstance(v, str)).any():
+                allRight = False
+                break
+        if not allRight:
+            df = pd.read_csv(fileName, sep=sep, skiprows=skipRows, encoding=encoding,
+                             header=headerRows, index_col=indexCols, decimal=',')
+
+        return df
+    
+    SupportedFileExts = \
+        ['.xlsx', '.xls', '.csv', '.txt'] + (['.ods'] if pkgver.parse(pd.__version__).release >= (0, 25) else [])
+    
+    @classmethod
+    def _fromDataFile(cls, sourceFpn, sheet=None, skipRows=None, headerRows=0, indexCols=None,
+                      decimalFields=[], separator='\t', encoding='utf-8'):
+        
+        if isinstance(sourceFpn, str):
+            sourceFpn = pl.Path(sourceFpn)
+    
+        assert sourceFpn.exists(), 'Source file for DataSet not found : {}'.format(sourceFpn)
+
+        ext = sourceFpn.suffix.lower()
+        assert ext in cls.SupportedFileExts, \
+               'Unsupported source file type {}: not from {{{}}}'.format(ext, ','.join(cls.SupportedFileExts))
+
+        logger.info1('Loading set from file {} ...'.format(sourceFpn.as_posix()))
+
+        if ext in ['.xlsx', '.xls', '.ods']:
+            dfData = pd.read_excel(sourceFpn, sheet_name=sheet or 0,
+                                   skiprows=skipRows, header=headerRows, index_col=indexCols)
+        elif ext in ['.csv', '.txt']:
+            dfData = cls._csv2df(sourceFpn, decCols=decimalFields, sep=separator, encoding=encoding,
+                                 skipRows=skipRows, headerRows=headerRows, indexCols=indexCols)
+        else:
+            raise NotImplementedError(f'Unsupported DataSet file input format {ext}')
+
+        logger.info1('... loaded {} rows x {} columns'.format(len(dfData), len(dfData.columns)))
+
+        return dfData
+    
+    @classmethod
+    def _fromDataFrame(cls, sourceDf):
+        
+        return sourceDf.copy()
+    
+    def __len__(self):
+        
+        return len(self._dfData)
+    
+    @property
+    def empty(self):
+        
+        return self._dfData.empty
+
+    @property
+    def columns(self):
+        
+        return self._dfData.columns
+
+    @property
+    def index(self):
+        
+        return self._dfData.index
+
+    @property
+    def dfData(self):
+        
+        return self._dfData
+
+    @dfData.setter
+    def dfData(self, dfData_):
+        
+        raise NotImplementedError('No change allowed to data ; create a new dataset !')
+
+    def dfSubData(self, index=None, columns=None, copy=False):
+    
+        """Get a subset of the all-results table rows and columns
+
+        Parameters:
+        :param index: rows to select, as an iterable of a subset of self.dfData.index values
+                      (anything suitable for pd.DataFrame.loc[...]) ; None = all rows.
+        :param columns: columns to select, as a list(string) or pd.Index when mono-indexed columns
+                       or as a list(tuple(string*)) or pd.MultiIndex when multi-indexed ; None = all columns.
+        :param copy: if True, return a full copy of the selected data, not a "reference" to the internal table
+        """
+        
+        assert columns is None or isinstance(columns, list) or isinstance(columns, (pd.Index, pd.MultiIndex)), \
+               'columns must be specified as None (all), or as a list of tuples, or as a pandas.MultiIndex'
+
+        # Make a copy of / extract selected columns of dfData.
+        if columns is None or len(columns) == 0:
+            dfSbData = self.dfData
+        else:
+            if isinstance(self._dfData.columns, pd.MultiIndex) and isinstance(columns, list):
+                iColumns = pd.MultiIndex.from_tuples(columns)
+            else:
+                iColumns = columns
+            dfSbData = self.dfData.reindex(columns=iColumns)
+        
+        if index is not None:
+            dfSbData = dfSbData.loc[index]
+
+        if copy:
+            dfSbData = dfSbData.copy()
+            
+        return dfSbData
+
+    def dropColumns(self, cols):
+    
+        self._dfData.drop(columns=cols, inplace=True)
+        
+    def dropRows(self, sbSelRows):
+        
+        self._dfData.drop(self._dfData[sbSelRows].index, inplace=True)
+        
+    @staticmethod
+    def _addComputedColumns(dfData, dComputeCols):
+    
+        """Add computed columns to a DataFrame
+        
+        :param dfData: the DataFrame to update
+        :param dComputeCols: dict new col. name => constant, or function to apply
+                          to each row to compute its value
+        """
+        
+        for colName, computeCol in dComputeCols.items():
+            if callable(computeCol):
+                dfData[colName] = dfData.apply(computeCol, axis='columns')
+            else:
+                dfData[colName] = computeCol
+               
+        return dfData  # Can be useful when chaining ...
+
+    def addColumns(self, dComputeCols):
+    
+        """Add computed columns to the sightings data set
+        
+        :param dComputeCols: dict new col. name => constant, or function to apply
+                          to each row to compute its value
+        """
+            
+        self._addComputedColumns(self._dfData, dComputeCols)
+
+    def renameColumns(self, dRenameCols):
+
+        self._dfData.rename(columns=dRenameCols, inplace=True)
+
+    def toPickle(self, fileName, subset=None, index=True):
+
+        """Save data table to a pickle file (XZ compressed format if requested).
+
+        Parameters:
+        :param fileName: target file pathname ; file is auto-compressed to XZ format
+                         through the lzma module if its extension is .xz or .lzma,
+                         or else not compressed.
+        :param subset: subset of columns to save
+        :param index: if True, save index column as is (otherwise reset&drop it to a range(size)-like)
+        """
+        
+        start = pd.Timestamp.now()
+
+        dfOutData = self.dfSubData(columns=subset)
+        if not index:
+            dfOutData = dfOutData.reset_index(drop=True)
+
+        compress = pl.Path(fileName).suffix in ['.xz', '.lzma']
+        with lzma.open(fileName, 'wb') if compress else open(fileName, 'wb') as file:
+            pickle.dump(dfOutData, file)
+
+        logger.info('{} results rows saved to {} in {:.3f}s'
+                    .format(len(self), fileName, (pd.Timestamp.now() - start).total_seconds()))
+
+    def toExcel(self, fileName, sheetName=None, subset=None, index=True, engine=None):
+        
+        """Save data table to a worksheet format (Excel, ODF, ...) :
+        * newer XLSX format for .xlsx extensions (through 'openpyxl' or 'xlrd' module)
+        * .xls (through 'xlwt').
+        * .ods (through 'odfpy'), if pandas >= 0.25.
+
+        Parameters:
+        :param fileName: target file pathname
+        :param sheetName: for results data only
+        :param subset: subset of columns to save
+        :param index: if True, save index column
+        :param engine: None => auto-selection from file extension ; otherwise, use xlrd, openpyxl or odf.
+        """
+
+        dfOutData = self.dfSubData(columns=subset)
+        
+        dfOutData.to_excel(fileName, sheet_name=sheetName or 'AllResults',
+                           index=index, engine=engine)
+
+    # Save data to an Open Document worksheet (through 'odfpy' module)
+    def toOpenDoc(self, fileName, sheetName=None, subset=None, index=True):
+
+        """Save data table to an Open Document worksheet, ODF format (through 'odfpy' module)
+
+        Warning: Needs pandas >= 0.25
+
+        Parameters:
+        :param fileName: target file pathname
+        :param sheetName: for results data only
+        :param subset: subset of columns to save
+        :param index: if True, save index column
+        """
+        
+        assert pkgver.parse(pd.__version__).release >= (1, 1), \
+               'Don\'t know how to write to OpenDoc format before Pandas 1.1'
+        
+        return self.toExcel(fileName, sheetName=sheetName, subset=subset,
+                            index=index, engine='odf')  # Force engine in case not a .ods.
+
+    @staticmethod
+    def _closeness(sLeftRight):
+
+        """
+        Relative closeness of 2 numbers : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)
+        = Compute the order of magnitude that separates the difference to the absolute max. of the two values.
+        
+        The greater it is, the lower the relative difference
+           Ex: 3 = 10**3 ratio between max absolute difference of the two,
+               +inf = NO difference at all,
+               0 = bad, one of the two is 0, and the other not.
+        """
+
+        x, y = sLeftRight.to_list()
+        
+        # Special cases with 1 NaN, or 1 or more inf => all different
+        if np.isnan(x):
+            if not np.isnan(y):
+                return 0  # All different
+        elif np.isnan(y):
+            return 0  # All different
+        
+        if np.isinf(x) or np.isinf(y):
+            return 0  # All different
+        
+        # Normal case
+        c = abs(x - y)
+        if not np.isnan(c) and c != 0:
+            c = c / max(abs(x), abs(y))
+        
+        return np.inf if c == 0 else round(-np.log10(c), 1)
+
+    # Make results cell values hashable (and especially analysis model params)
+    # * needed for use in indexes (hashability)
+    # * needed to cope with to_excel/read_excel Inconsistent None management
+    @staticmethod
+    def _toHashable(value):
+    
+        if isinstance(value, list):
+            hValue = str([float(v) for v in value])
+        elif pd.isnull(value):
+            hValue = 'None'
+        elif isinstance(value, (int, float)):
+            hValue = value
+        elif isinstance(value, str):
+            if ',' in value:  # Assumed already somewhat stringified list
+                hValue = str([float(v) for v in value.strip('[]').split(',')])
+            else:
+                hValue = str(value)
+        else:
+            hValue = str(value)
+        
+        return hValue
+    
+    @classmethod
+    def compareDataFrames(cls, dfLeft, dfRight, subsetCols=[], indexCols=[],
+                          dropCloser=np.inf, dropNans=True, dropCloserCols=False):
+    
+        """
+        Compare 2 DataFrames.
+
+        The resulting diagnosis DataFrame will have the same columns and merged index,
+        with a "closeness" value for each cell (see _closeness method) ;
+        rows where all cells have closeness > dropCloser (or eventually NaN) are yet dropped.
+        and columns where all cells have closeness > dropCloser (or eventually NaN) can also be dropped.
+        
+        Parameters:
+        :param dfLeft: Left DataFrame
+        :param dfRight: Right DataFrame
+        :param list subsetCols: on a subset of columns,
+        :param list indexCols: ignoring these columns, but keeping them as the index and sorting order,
+        :param float dropCloser: result will only include rows with all cell closeness > dropCloser
+                                 (default: np.inf => all cols and rows kept).
+        :param bool dropNans: smoother condition for dropCloser : if True, NaN values are also considered > dropCloser
+                              ('cause NaN != NaN :-( ).
+        :param bool dropCloserCols: if True, also drop all "> dropCloser (or eventually NaN)"-all-cell columns,
+                                    just as rows
+
+        :return: the diagnostic DataFrame.
+        """
+
+        if dfLeft.empty and dfRight.empty:
+            return pd.DataFrame()
+
+        # Make copies : we need to change the frames.
+        dfLeft = dfLeft.copy()
+        dfRight = dfRight.copy()
+        
+        # Check input columns
+        dColsSets = {'Subset column': subsetCols, 'Index column': indexCols}
+        for colsSetName, colsSet in dColsSets.items():
+            for col in colsSet:
+                if col not in dfLeft.columns:
+                    raise KeyError('{} {} not in left result set'.format(colsSetName, col))
+                if col not in dfRight.columns:
+                    raise KeyError('{} {} not in right result set'.format(colsSetName, col))
+        
+        # Set specified cols as the index (after making them hashable) and sort it.
+        dfLeft[indexCols] = dfLeft[indexCols].applymap(cls._toHashable)
+        dfLeft.set_index(indexCols, inplace=True)
+        dfLeft = dfLeft.sort_index()  # Not inplace: don't modify a copy/slice
+
+        dfRight[indexCols] = dfRight[indexCols].applymap(cls._toHashable)
+        dfRight.set_index(indexCols, inplace=True)
+        dfRight = dfRight.sort_index()  # Idem
+
+        # Filter data to compare (subset of columns).
+        if subsetCols:
+            dfLeft = dfLeft[subsetCols]
+            dfRight = dfRight[subsetCols]
+
+        # Append mutually missing rows to the 2 tables => a complete and identical index.
+        anyCol = dfLeft.columns[0]  # Need one, whichever.
+        dfLeft = dfLeft.join(dfRight[[anyCol]], rsuffix='_r', how='outer')
+        dfLeft.drop(columns=dfLeft.columns[-1], inplace=True)
+        dfRight = dfRight.join(dfLeft[[anyCol]], rsuffix='_l', how='outer')
+        dfRight.drop(columns=dfRight.columns[-1], inplace=True)
+
+        # Compare : Compute closeness 
+        nColLevels = dfLeft.columns.nlevels
+        KRightCol = 'tmp' if nColLevels == 1 else tuple('tmp{}'.format(i) for i in range(nColLevels))
+        dfRelDiff = dfLeft.copy()
+        exception = False
+        for leftCol in dfLeft.columns:
+            dfRelDiff[KRightCol] = dfRight[leftCol]
+            try:
+                dfRelDiff[leftCol] = dfRelDiff[[leftCol, KRightCol]].apply(cls._closeness, axis='columns')
+            except TypeError as exc:
+                logger.error(f'Column {leftCol} : {exc}')
+                exception = True
+            dfRelDiff.drop(columns=[KRightCol], inplace=True)
+            
+        if exception:
+            raise TypeError('Stopping: Some columns could not be compared (see errors logged above)')
+            
+        # Complete comparison : rows with index not in both frames forced to all-0 closeness
+        # (of course they should result so ... unless some NaNs here and there : fix this)
+        dfRelDiff.loc[dfLeft[~dfLeft.index.isin(dfRight.index)].index, :] = 0
+        dfRelDiff.loc[dfRight[~dfRight.index.isin(dfLeft.index)].index, :] = 0
+        
+        # Drop rows (and may be columns) with closeness over the threshold (or of NaN value if authorized)
+        dfCells2Drop = dfRelDiff.applymap(lambda v: v > dropCloser or (dropNans and pd.isnull(v)))
+        dfRelDiff.drop(dfRelDiff[dfCells2Drop.all(axis='columns')].index, inplace=True)
+        if dropCloserCols:
+            # dfRelDiff.drop(columns=dfRelDiff.T[dfCells2Drop.all(axis='index')].index, axis='index', inplace=True)
+            dfRelDiff.drop(columns=[col for col, drop in dfCells2Drop.all(axis='index').items() if drop],
+                           inplace=True)  # 40% faster.
+            
+        return dfRelDiff
+
+    def compare(self, other, subsetCols=[], indexCols=[], dropCloser=np.inf, dropNans=True, dropCloserCols=False):
+    
+        """
+        Compare 2 data sets.
+        
+        The resulting diagnosis DataFrame will have the same columns and merged index,
+        with a "closeness" value for each cell (see _closeness method) ;
+        rows where all cells have closeness > dropCloser (or eventually NaN) are yet dropped.
+        and columns where all cells have closeness > dropCloser (or eventually NaN) can also be dropped.
+        
+        Parameters:
+        :param other: Right data set or DataFrame object to compare
+        :param list subsetCols: on a subset of columns,
+        :param list indexCols: ignoring these columns, but keeping them as the index and sorting order,
+        :param float dropCloser: result will only include rows with all cell closeness > dropCloser
+                                 (default: np.inf => all cols and rows kept).
+        :param bool dropNans: smoother condition for dropCloser : if True, NaN values are also considered > dropCloser
+                              ('cause NaN != NaN :-( ).
+        :param bool dropCloserCols: if True, also drop all "> dropCloser (or eventually NaN)" columns-all-cell,
+                                    just as rows
+
+        :return: the diagnostic DataFrame.
+        """
+        
+        return self.compareDataFrames(dfLeft=self.dfData,
+                                      dfRight=other if isinstance(other, pd.DataFrame) else other.dfData,
+                                      subsetCols=subsetCols, indexCols=indexCols,
+                                      dropCloser=dropCloser, dropNans=dropNans, dropCloserCols=dropCloserCols)
+
+
+class FieldDataSet(DataSet):
+
+    """A tabular data set for producing mono-category or even individuals data sets from "raw sightings data",
+    aka "field data" (with possibly multiple category counts on each row)
+
+    Input support provided for pandas.DataFrame, Excel .xlsx file, tab-separated .csv/.txt files,
+      and even OpenDoc .ods file with pandas >= 0.25 (needs odfpy module)
+    """
+    def __init__(self, source, countCols, addMonoCatCols=dict(), importDecFields=[], sheet=None, separator='\t'):
+
+        """Ctor
+
+        Parameters:
+        :param source: the input field-data table
+        :param countCols: the category columns (each of them holds counts of individuals for the category)
+        :param addMonoCatCols: name and method of computing for columns to add after separating multi-category counts
+             (each column to add is computed through :
+                dfMonoCatSights[colName] = dfMonoCatSights[].apply(computeCol, axis='columns')
+                for colName, computeCol in addMonoCatCols.items())
+        :param importDecFields: for smart ./, decimal character management in CSV sources (pandas is not smart on this)
+        :param sheet: name of the sheet to read from, for multi-sheet data input file
+             (like Excel or Open Doc. workbooks)
+        :param separator: columns separator for CSV input file
+        """
+
+        super().__init__(sources=source, importDecFields=importDecFields, sheet=sheet, separator=separator)
+        
+        self.countCols = countCols
+        self.dCompdMonoCatColSpecs = addMonoCatCols
+        
+        self.dfIndivSights = None  # Not yet computed.
+        self.dfMonoCatSights = None  # Idem.
+        
+        logger.info(f'Field data : {len(self)} sightings')
+
+    @property
+    def dfData(self):
+        
+        return self._dfData
+
+    @dfData.setter
+    def dfData(self, dfData_):
+        
+        assert sorted(self._dfData.columns) == sorted(dfData_.columns), "Can't set data with different columns"
+        
+        self._dfData = dfData_
+        
+        self.dfIndivSights = None  # Not yet computed.
+        self.dfMonoCatSights = None  # Idem.
+    
+    # Transform a multi-category sightings set into an equivalent mono-category sightings set,
+    # that is where no sightings has more that one category with positive count (keeping the same total counts).
+    # Highly optimized version.
+    # Ex: A sightings set with 2 category count columns nMales and nFemales
+    #     * in the input set, you may have 1 sightings with nMales = 5 and nFemales = 2
+    #     * in the output set, this sightings have been separated in 2 distinct ones
+    #       (all other properties left untouched) :
+    #       the 1st with nMales = 5 and nFemales = 0, the 2nd with nMales = 0 and nFemales = 2.
+    @staticmethod
+    def _separateMultiCategoryCounts(dfInSights, countColumns):
+        
+        # For each count column
+        ldfMonoCat = list()
+        for col in countColumns:
+
+            # Select rows with some individuals in this column
+            dfOneCat = dfInSights[dfInSights[col] > 0].copy()
+
+            # Set all other count cols to 0.
+            otherCols = countColumns.copy()
+            otherCols.remove(col)
+            dfOneCat[otherCols] = 0
+
+            # Store into a list for later
+            ldfMonoCat.append(dfOneCat)
+
+        # Concat all data frames into one.
+        dfOutSights = pd.concat(ldfMonoCat)
+
+        # Sort to "initial" order (easier to read), and reset index (for unique labels).
+        dfOutSights.sort_index(inplace=True)
+        dfOutSights.reset_index(inplace=True, drop=True)
+
+        # Done.
+        return dfOutSights
+
+    # Transform a multi-individual mono-category sightings set into an equivalent mono-individual
+    # mono-category sightings set, that is where no sightings has more that one individual
+    # per category (keeping the same total counts).
+    # Highly optimized version.
+    # Ex: A sightings set with 2 mono-category count columns nMales and nFemales
+    #     * in tyhe input set, you may have 1 sightings with nMales = 3 and nFemales = 0
+    #       (but none with nMales and nFemales > 0)
+    #     * in the output set, this sightings have been separated in 3 distinct ones
+    #       (all other properties left untouched) : all with nMales = 1 and nFemales = 0.
+    @staticmethod
+    def _individualiseMonoCategoryCounts(dfInSights, countColumns):
+        
+        # For each category column
+        ldfIndiv = list()
+        for col in countColumns:
+            
+            # Select rows with some individuals in this column
+            dfOneCat = dfInSights[dfInSights[col] > 0]
+            
+            # Repeat each one by its count of individuals
+            dfIndiv = dfInSights.loc[np.repeat(dfOneCat.index.values, dfOneCat[col].astype(int).values)]
+
+            # Replace non-zero counts by 1.
+            dfIndiv.loc[dfIndiv[col] > 0, col] = 1
+            
+            # Store into a list for later
+            ldfIndiv.append(dfIndiv)
+            
+        # Concat all data frames into one.
+        dfOutSights = pd.concat(ldfIndiv)
+
+        # Sort to "initial" order (easier to read), and reset index (for unique labels).
+        dfOutSights.sort_index(inplace=True)
+        dfOutSights.reset_index(inplace=True, drop=True)
+        
+        # Done.
+        return dfOutSights
+        
+    # Access to the resulting mono-category data set
+    def monoCategorise(self, copy=True):
+    
+        # Compute only if not already done.
+        if self.dfMonoCatSights is None:
+        
+            # Separate multi-category counts
+            self.dfMonoCatSights = self._separateMultiCategoryCounts(self._dfData, self.countCols)
+            
+            # Compute and add supplementary mono-category columns
+            for colName, computeCol in self.dCompdMonoCatColSpecs.items():
+                self.dfMonoCatSights[colName] = self.dfMonoCatSights[self.countCols].apply(computeCol, axis='columns')
+        
+        return self.dfMonoCatSights if copy else self.dfMonoCatSights
+    
+    # Access to the resulting individuals data set 
+    # = a mono-category data set with individualised data : 1 individuals per row
+    def individualise(self, copy=True):
+    
+        # Compute only if not already done.
+        if self.dfIndivSights is None:
+        
+            # Individualise mono-category counts
+            self.dfIndivSights = \
+                self._individualiseMonoCategoryCounts(self.monoCategorise(copy=False), self.countCols)
+
+        return self.dfIndivSights.copy() if copy else self.dfIndivSights
+        
+
+# A tabular data set for producing on-demand sample data sets from "mono-category sightings data"
+# * Input support provided for pandas.DataFrame (from FieldDataSet.individualise/monoCategorise),
+#   Excel .xlsx file, tab-separated .csv/.txt files, and even OpenDoc .ods file with pandas >= 0.25 (needs odfpy module)
+class MonoCategoryDataSet(DataSet):
+
+    # Ctor
+    # * dfTransects: Transects infos with columns : transectPlaceCols (n), passIdCol (1), effortCol (1)
+    #                If None, auto generated from input sightings
+    # * effortConstVal: if dfTransects is None and effortCol not in source table, use this constant value
+    def __init__(self, source, dSurveyArea, importDecFields=[], dfTransects=None,
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleDecFields=['Effort', 'Distance'], effortConstVal=1,
+                 sheet=None, separator='\t'):
+        
+        super().__init__(sources=source, importDecFields=importDecFields, sheet=sheet, separator=separator)
+        
+        self.dSurveyArea = dSurveyArea
+        self.transectPlaceCols = transectPlaceCols
+        self.passIdCol = passIdCol
+        self.effortCol = effortCol
+        self.sampleDecFields = sampleDecFields
+    
+        self.dfTransects = dfTransects
+        if self.dfTransects is None or self.dfTransects.empty:
+            self.dfTransects = self._extractTransects(self._dfData, transectPlaceCols=self.transectPlaceCols,
+                                                      passIdCol=self.passIdCol, effortConstVal=effortConstVal)
+        elif effortCol not in self.dfTransects.columns:
+            self.dfTransects[effortCol] = effortConstVal
+
+        # A cache used by sampleDataSet to optimise consecutive calls with same sample specs (happens often).
+        self._sdsSampleDataSetCache = None
+        self._sSampleSpecsCache = None
+        
+        logger.info(f'Individuals data : {len(self)} sightings, {len(self.dfTransects)} transects')
+
+    @property
+    def dfData(self):
+        
+        return self._dfData
+
+    @dfData.setter
+    def dfData(self, dfData_):
+        
+        assert sorted(self._dfData.columns) == sorted(dfData_.columns), "Can't set data with different columns"
+        
+        self._dfData = dfData_
+    
+    # Extract transect infos from individuals sightings
+    # * effortConstVal: if effortCol not in dfIndivSightings, create one with this constant value
+    @staticmethod
+    def _extractTransects(dfIndivSightings, transectPlaceCols=['Transect'], passIdCol='Pass', 
+                          effortCol='Effort', effortConstVal=1):
+    
+        transCols = transectPlaceCols + [passIdCol]
+        if effortCol in dfIndivSightings.columns:
+            transCols.append(effortCol)
+        
+        dfTrans = dfIndivSightings[transCols]
+        
+        dfTrans = dfTrans.drop_duplicates()
+        
+        dfTrans.reset_index(drop=True, inplace=True)
+        
+        if effortCol not in dfTrans.columns:
+            dfTrans[effortCol] = effortConstVal
+
+        return dfTrans
+    
+    # Select sample sightings from an all-samples sightings table,
+    # and compute the associated total sample effort.
+    # * dSample : { key, value } selection criteria (with '+' support for 'or' operator in value),
+    #             keys being columns of dfAllSights (dict protocol : dict, pd.Series, ...)
+    # * dfAllSights : the all-samples (individual) sightings table to search into
+    # * dfAllEffort : effort values for each transect x pass really done, for the all-sample survey
+    # * transectPlaceCols : name of the input dfAllEffort and dSample columns to identify the transects (not passes)
+    # * passIdCol : name of the input dfAllEffort and dSample column to identify the passes (not transects)
+    # * effortCol : name of the input dfAllEffort and output effort column to add / replace
+    @staticmethod
+    def _selectSampleSightings(dSample, dfAllSights, dfAllEffort,
+                               transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort'):
+        
+        # Select sightings
+        dfSampSights = dfAllSights
+        for key, values in dSample.items():
+            values = str(values).strip()  # For ints as strings that get forced to int in io sometimes (ex. from_excel)
+            if values and values not in ['nan', 'None']:  # Empty value means "no selection criteria for this columns"
+                values = values.split('+') if '+' in values else [values]
+                dfSampSights = dfSampSights[dfSampSights[key].astype(str).isin(values)]
+
+        # Compute sample effort
+        passes = str(dSample[passIdCol])  # Same as above ...
+        if passes and passes not in ['nan', 'None']:  # Same as above ...
+            passes = passes.split('+') if '+' in passes else [passes]
+            dfSampEffort = dfAllEffort[dfAllEffort[passIdCol].astype(str).isin(passes)]
+        else:
+            dfSampEffort = dfAllEffort
+        dfSampEffort = dfSampEffort[transectPlaceCols + [effortCol]].groupby(transectPlaceCols).sum()
+        
+        # Add effort column
+        dfSampSights = dfSampSights.drop(columns=effortCol, errors='ignore').join(dfSampEffort, on=transectPlaceCols)
+
+        return dfSampSights, dfSampEffort
+        
+    # Add "absence" sightings to field data collected on transects for a given sample
+    # * dfInSights : input data table
+    # * sampleCols : the names of the sample identification columns
+    # * dfExpdTransects : the expected transects, as a data frame indexed by the transectPlace,
+    #     an index with same name as the corresponding column in dfInSights,
+    #     and with other info columns to duplicate in absence sightings (at least the effort value)
+    @staticmethod
+    def _addAbsenceSightings(dfInSights, sampleCols, dfExpdTransects):
+        
+        assert not dfInSights.empty, 'Error : Empty sightings data to add absence ones to !'
+
+        # Use the 1st sightings of the sample to build the absence sightings prototype
+        # (all null columns except for the sample identification ones, lefts as is)
+        dAbscSightTmpl = dfInSights.iloc[0].to_dict()
+        dAbscSightTmpl.update({k: None for k in dAbscSightTmpl.keys() if k not in sampleCols})
+
+        # Determine missing transects for the sample
+        transectPlaceCols = dfExpdTransects.index.name
+        dfSampTransects = dfInSights.drop_duplicates(subset=transectPlaceCols)
+        dfSampTransects.set_index(transectPlaceCols, inplace=True)
+        dfSampTransects = dfSampTransects[dfExpdTransects.columns]
+
+        dfMissgTransects = dfExpdTransects.loc[dfExpdTransects.index.difference(dfSampTransects.index)]
+
+        # Generate the absence sightings : 1 row per missing transect
+        ldAbscSights = list()
+        for _, sMissgTrans in dfMissgTransects.reset_index().iterrows():
+
+            dAbscSight = dAbscSightTmpl.copy()  # Copy template sightings
+            dAbscSight.update(sMissgTrans.to_dict())  # Update transect columns only
+
+            ldAbscSights.append(dAbscSight)
+
+        # Add them to the input sightings
+        return dfInSights.append(pd.DataFrame(pd.DataFrame(ldAbscSights)))
+
+    # Add survey area information to sightings data (in-place modification)
+    # * dfInSights : input data table
+    # * dSurveyArea : a column name to scalar dictionary-like data to add to the table
+    @staticmethod
+    def _addSurveyAreaInfo(dfInSights, dSurveyArea):
+        
+        for col, values in dSurveyArea.items():
+            dfInSights[col] = values
+            
+        return dfInSights
+    
+    # Sample individuals data for given sampling criteria, as a SampleDataSet.
+    # * sSample : { key, value } selection criteria (with '+' support for 'or' operator in value, no separating space),
+    #             keys being columns of dfAllSights (dict protocol : dict, pd.Series, ...)
+    def sampleDataSet(self, sSampleSpecs):
+        
+        # Don't redo what have just been done.
+        if self._sSampleSpecsCache is not None and self._sSampleSpecsCache.equals(sSampleSpecs):
+            return self._sdsSampleDataSetCache
+        
+        # Select sample data.
+        dfSampIndivObs, dfSampTransInfo = \
+            self._selectSampleSightings(dSample=sSampleSpecs, dfAllSights=self._dfData,
+                                        dfAllEffort=self.dfTransects, transectPlaceCols=self.transectPlaceCols,
+                                        passIdCol=self.passIdCol, effortCol=self.effortCol)
+        
+        # Don't go on if no selected data.
+        if dfSampIndivObs.empty:
+            logger.warning(f'Not even a single individual sighting selected for these specs: {sSampleSpecs.to_dict()}')
+            return None
+
+        # Add absence sightings
+        dfSampIndivObs = self._addAbsenceSightings(dfSampIndivObs, sampleCols=sSampleSpecs.index,
+                                                   dfExpdTransects=dfSampTransInfo)
+
+        # Add information about the studied geographical area
+        dfSampIndivObs = self._addSurveyAreaInfo(dfSampIndivObs, dSurveyArea=self.dSurveyArea)
+
+        # Create SampleDataSet instance (sort by transectPlaceCols : mandatory for MCDS analysis)
+        # and save it into cache.
+        self._sdsSampleDataSetCache = \
+            SampleDataSet(dfSampIndivObs, decimalFields=self.sampleDecFields, sortFields=self.transectPlaceCols)
+        self._sSampleSpecsCache = sSampleSpecs
+        
+        # Done.
+        return self._sdsSampleDataSetCache
+
+
+class SampleDataSet(DataSet):
+
+    """
+    A tabular input data set for multiple analyses on the same sample, with 1 or 0 individual per row
+    Warning:
+    * Only Point transect supported as for now
+    * No change made afterwards on decimal precision : provide what you need !
+    * Rows can be sorted if and as specified
+    * Input support provided for pandas.DataFrame, Excel .xlsx file, tab-separated .csv/.txt files,
+      and even OpenDoc .ods file with pandas >= 0.25 (needs odfpy module)
+    """
+    def __init__(self, source, decimalFields=[], sortFields=[], sheet=None, separator='\t'):
+        
+        self.decimalFields = decimalFields
+
+        super().__init__(sources=source, importDecFields=decimalFields, sheet=sheet, separator=separator)
+                
+        assert not self._dfData.empty, 'No data in set'
+        assert len(self._dfData.columns) >= 5, 'Not enough columns (should be at leat 5)'
+        
+        missCols = [decFld for decFld in self.decimalFields if decFld not in self._dfData.columns]
+        assert not missCols, '{} declared decimal field(s) are not in source columns {} : {}' \
+                             .format(len(missCols), ','.join(self._dfData.columns), ','.join(missCols))
+                             
+        # Sort / group sightings as specified
+        if sortFields:
+            self._dfData.sort_values(by=sortFields, inplace=True)
+
+        # Report some basic stats.
+        nAbscRows = self._dfData.isna().any(axis='columns').sum()
+        logger.info('Sample data : {} sightings = {} individuals + {} absence rows'
+                    .format(len(self), len(self) - nAbscRows, nAbscRows))
+
+
+class ResultsSet(object):
+    
+    """
+    A tabular result set for some computation process repeated multiple times with different input / results,
+    each process result(s) being given as a pd.Series (1 row for the target table) or a pd.DataFrame (multiple rows).
+    
+    With ability to prepend custom heading columns to each process results ones
+    (same value for each 1-process results rows).
+    
+    The columns of the internal pd.DataFrame, like those of each process result(s), can be a multi-index.
+    
+    Support for column translation is included (a mono-indexed).
+    """
+
+    def __init__(self, miCols, dfColTrans=None, miCustomCols=None, dfCustomColTrans=None,
+                 dComputedCols=None, dfComputedColTrans=None, sortCols=[], sortAscend=[], dropNACols=True):
+    
+        """Ctor
+        
+        Parameters:
+        :param miCols: process results columns (MultiIndex or not)
+        :param dfColTrans: translation DataFrame for results column labels,
+                           * index=column labels,
+                           * data=dict(<lang>=[<translation for each label> for all labels] for all <lang>)
+        :param miCustomCols: custom columns to prepend (on the left) to process results columns (MultiIndex or not)
+        :param dfCustomColTrans: translation DataFrame for custom column labels,
+        :param dComputedCols: dict(label: position) for inserting computed columns in results table (None = after end)
+        :param dfComputedColTrans: translation DataFrame for computed column labels,
+        :param sortCols: if not empty, iterable of columns to sort values by in dfData()
+        :param sortAscend: sorting order for all (bool), or each column (iterable of bool) in dfData()
+        :param dropNACols: If True, dfData() won't return columns with no relevant results data
+                           (except for custom cols).
+        """
+        
+        assert len(sortCols) == len(sortAscend), 'sortCols and sortAscend must have same length'
+
+        # Columns stuff.
+        if dComputedCols is None:
+            dComputedCols = dict()
+            dfComputedColTrans = pd.DataFrame()
+
+        # Columns : Process results + computed results (at the specified position if any)
+        self.miCols = miCols.copy()
+        for col, ind in dComputedCols.items():
+            if ind is not None:
+                self.miCols = self.miCols.insert(ind, col)
+        lastCompCols = [col for col, ind in dComputedCols.items() if ind is None]
+        if lastCompCols:
+            self.miCols = self.miCols.append(pd.MultiIndex.from_tuples(lastCompCols))
+
+        # 
+        self.computedCols = list(dComputedCols.keys())
+        self.miCustomCols = miCustomCols.copy() if miCustomCols is not None else list()
+        
+        self.isMultiIndexedCols = isinstance(miCols, pd.MultiIndex)
+
+        # DataFrames for translating 3-level multi-index columns to 1 level lang-translated columns
+        self.dfColTrans = pd.concat([dfColTrans if dfColTrans is not None else pd.DataFrame(),
+                                     dfComputedColTrans if dfColTrans is not None else pd.DataFrame()])
+        self.dfCustomColTrans = dfCustomColTrans.copy() if dfCustomColTrans is not None else pd.DataFrame()
+        
+        # Sorting and cleaning parameters (after postComputing)
+        self.sortCols = sortCols
+        self.sortAscend = sortAscend
+        self.dropNACols = dropNACols
+        
+        # Non-constant data members
+        self._dfData = pd.DataFrame()  # The real data (frame).
+        self.rightColOrder = False  # self._dfData columns are assumed to be in a wrong order.
+        self.postComputed = False  # Post-computation not yet done.
+    
+        # Specifications of computations that led to the results
+        self.specs = dict()
+
+    def __len__(self):
+        
+        return len(self._dfData)
+    
+    @property
+    def empty(self):
+        
+        return self._dfData.empty
+
+    @property
+    def columns(self):
+    
+        return self.dfData.columns
+        
+    @property
+    def index(self):
+        
+        return self._dfData.index
+
+    def dropRows(self, sbSelRows):
+    
+        """Drop specific rows in-place, selected through boolean indexing on self.dfData
+        
+        Parameters:
+        :param sbSelRows: boolean series with same index as self.dfData"""
+    
+        self._dfData.drop(self._dfData[sbSelRows].index, inplace=True)
+        
+    def copy(self, withData=True):
+    
+        """Clone function (shallow), with optional (deep) data copy"""
+
+        # 1. Call ctor without computed columns stuff for now and here (we no more have initial data)
+        clone = ResultsSet(miCols=self.miCols,
+                           miCustomCols=self.miCustomCols.copy(),
+                           dfCustomColTrans=self.dfCustomColTrans.copy(),
+                           sortCols=self.sortCols.copy(), sortAscend=self.sortAscend.copy())
+    
+        # 2. Complete clone initialisation.
+        clone.miCols = self.miCols.copy()
+        clone.computedCols = self.computedCols.copy()
+        
+        # DataFrames for translating 3-level multi-index columns to 1 level lang-translated columns
+        clone.dfColTrans = self.dfColTrans.copy()
+        
+        # Copy data if needed.
+        if withData:
+            clone._dfData = self._dfData.copy()
+            clone.rightColOrder = self.rightColOrder
+            clone.postComputed = self.postComputed
+
+        return clone
+
+    def _acceptNewColumns(self, newCols):
+
+        """Update results columns list (self.miCols) with new columns if not already present"""
+
+        logger.debug1(f'_acceptNewColumns: {newCols=}')
+        logger.debug2(f'{self.miCols=}')
+        logger.debug2(f'{self.miCustomCols=}')
+        
+        # Select columns to really append.
+        newCols = [col for col in newCols if col not in self.miCols and col not in self.miCustomCols]
+        logger.debug2(f'{newCols=}')
+
+        if self.isMultiIndexedCols:
+            self.miCols = self.miCols.append(newCols)
+            newColTrans = [' '.join(col) for col in newCols]  # Quite rough, but what else ?
+        else:
+            self.miCols += newCols
+            newColTrans = newCols
+        logger.debug2(f'{self.miCols=}')
+
+        # Update columns translation table also.
+        dfNewColTrans = pd.DataFrame(index=newCols,
+                                     data={lang: newColTrans for lang in self.dfColTrans.columns})
+        self.dfColTrans = self.dfColTrans.append(dfNewColTrans)
+
+    def append(self, sdfResult, sCustomHead=None, acceptNewCols=False):
+        
+        """Append row(s) of results to the all-results table
+        :param sdfResult: the Series (1 row) or DataFrame (N rows) to append
+        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
+            as unexpected columns appear in results to append
+        :param sCustomHead: Series holding custom cols values, to prepend (left) to each row of sdfResult,
+            before appending to the internal table.
+        
+          |---- custom heading columns ----|---- real results ---------------------|
+          |          |          |          |         |         |         |         |
+          |               ...              |                  ...                  |
+          |                ( data already there before append)                     |
+          |               ...              |                  ...                  |
+          |          |          |          |         |         |         |         |
+          --------------------------------------------------------------------------
+          |     x     |    y    |    z     |    a1   |    b1   |    c1   |    d1   |<- append(5 rows)
+          |     x     |    y    |    z     |    a2   |    b2   |    c2   |    d2   |
+          | (from sCustomHead, replicated) | (from sdfResult: here a 5-row table)  | 
+          |     x     |    y    |    z     |    a4   |    b4   |    c4   |    d4   |
+          |     x     |    y    |    z     |    a5   |    b5   |    c5   |    d5   |
+          --------------------------------------------------------------------------
+        """
+
+        assert not self.postComputed, "Can't append after columns post-computation"
+        
+        assert isinstance(sdfResult, (pd.Series, pd.DataFrame)), \
+               'sdfResult : Can only append a pd.Series or pd.DataFrame'
+        assert sCustomHead is None or isinstance(sCustomHead, pd.Series), \
+               'sCustomHead : Can only append a pd.Series'
+
+        # If specified, update results columns list (self.miCols) dynamically 
+        # as unexpected columns appear in results to append.
+        if acceptNewCols:
+            resCols = list(sdfResult.index) if isinstance(sdfResult, pd.Series) else list(sdfResult.columns)
+            self._acceptNewColumns(resCols)
+
+        # Prepend header columns to results (on the left) if any.
+        if sCustomHead is not None:
+            if isinstance(sdfResult, pd.Series):
+                sdfResult = sCustomHead.append(sdfResult)
+            else:  # DataFrame
+                dfCustomHead = pd.DataFrame([sCustomHead]*len(sdfResult)).reset_index(drop=True)
+                sdfResult = pd.concat([dfCustomHead, sdfResult], axis='columns')
+        
+        # Append results rows to the already present ones (at the end)
+        if self._dfData.columns.empty:
+            # In order to preserve types, we can't use self._dfData.append(sdfResult),
+            # because it doesn't preserve original types (int => float)
+            if isinstance(sdfResult, pd.Series):
+                # self._dfData = sdfResult.to_frame().T  # This doesn't preserve types (=> all object)
+                self._dfData = pd.DataFrame([sdfResult])
+            else:  # DataFrame
+                self._dfData = sdfResult
+        else:
+            self._dfData = self._dfData.append(sdfResult, ignore_index=True)
+        
+        # Appending (or concat'ing) often changes columns order
+        self.rightColOrder = False
+        
+    # Post-computations.
+    def postComputeColumns(self):
+        
+        # Derive class to really compute things (=> work directly on self._dfData),
+        pass
+        
+    @property
+    def dfRawData(self):
+
+        """Direct access to unpostprocessed data
+
+        May have to remove computed columns and reset postCompute state.
+        """
+
+        # Post-computation is now "not yet done" (and remove any computed column: will be recomputed later on).
+        self.setPostComputed(False)
+
+        return self._dfData
+        
+    def getData(self, copy=True):
+        
+        # Do post-computation and sorting if not already done.
+        if not(self._dfData.empty or self.postComputed):
+        
+            # Make sure we keep a MultiIndex for columns if it was the case (append breaks this, not fromExcel)
+            if self.isMultiIndexedCols and not isinstance(self._dfData.columns, pd.MultiIndex):
+                self._dfData.columns = pd.MultiIndex.from_tuples(self._dfData.columns)
+            
+            # Post-compute as specified (or not).
+            self.postComputeColumns()
+            self.postComputed = True  # No need to do it again from now on !
+            
+            # Sort as/if specified.
+            if self.sortCols:
+                self._dfData.sort_values(by=self.sortCols, ascending=self.sortAscend, inplace=True)
+        
+        # Enforce right columns order.
+        if not(self._dfData.empty or self.rightColOrder):
+            
+            miTgtColumns = self.miCols
+            if self.miCustomCols is not None:
+                if self.isMultiIndexedCols:
+                    miTgtColumns = self.miCustomCols.append(miTgtColumns)
+                else:
+                    miTgtColumns = self.miCustomCols + miTgtColumns
+            self._dfData = self._dfData.reindex(columns=miTgtColumns)
+            self.rightColOrder = True  # No need to do it again, until next append() !
+        
+        # This is also documentation line !
+        if self.isMultiIndexedCols and not self._dfData.empty:
+            assert isinstance(self._dfData.columns, pd.MultiIndex)
+        
+        # If specified, don't return columns with no relevant results data (unless among custom cols).
+        if self.dropNACols and not self._dfData.empty:
+            miCols2Cleanup = self._dfData.columns
+            if self.miCustomCols is not None:
+                if self.isMultiIndexedCols:
+                    miCols2Cleanup = miCols2Cleanup.drop(self.miCustomCols.to_list())
+                else:
+                    miCols2Cleanup = [col for col in miCols2Cleanup if col not in self.miCustomCols]
+            cols2Drop = [col for col in miCols2Cleanup if self._dfData[col].isna().all()]
+            logger.debug(f'Dropping all-NaN columns {cols2Drop}')
+            self._dfData.drop(columns=cols2Drop, inplace=True)
+
+        # Done.
+        return self._dfData.copy() if copy else self._dfData
+
+    @property
+    def dfData(self):
+
+        return self.getData(copy=True)
+    
+    def setPostComputed(self, on=True):
+
+        # If not specified as already postComputed, prepare for re-computation later by removing any "computed" column.
+        if not on:
+            # ... but ignore those which are not there : backward compat. / tolerance ...
+            self._dfData.drop(columns=self.computedCols, errors='ignore', inplace=True)
+        
+        # Post-computation not yet done (unless told it _is_: accept blindly).
+        self.postComputed = on
+
+    def setData(self, dfData, postComputed=False, acceptNewCols=False):
+
+        # Prerequisites
+        assert isinstance(dfData, pd.DataFrame), 'dfData must be a pd.DataFrame'
+        assert dfData.index.nunique() == len(dfData), 'dfData index must be unique'
+
+        # Take source data as ours now (after copying).
+        self._dfData = dfData.copy()
+        
+        # If specified, update results columns list (self.miCols) dynamically 
+        # as unexpected columns appear in results to append.
+        if acceptNewCols:
+            self._acceptNewColumns(dfData.columns)
+
+        # Let's assume that columns order is dirty.
+        self.rightColOrder = False
+        
+        # Post-computation as not yet done, unless told it _is_: accept blindly.
+        self.setPostComputed(postComputed)
+    
+    @dfData.setter
+    def dfData(self, dfData):
+        
+        self.setData(dfData)
+    
+    # Sort rows in place and overwrite initial sortCols / sortAscend values.
+    def sortRows(self, by, ascending=True):
+    
+        self._dfData.sort_values(by=by, ascending=ascending, inplace=True)
+        
+        self.sortCols = by
+        self.sortAscend = ascending
+    
+    # Add columns translations (update if already there)
+    def addColumnsTrans(self, dColsTrans=dict()):
+
+        for col, dTrans in dColsTrans.items():
+            self.dfColTrans.loc[col] = dTrans
+
+    # Build translation table for lang (from custom and other columns)
+    def transTable(self):
+        
+        dfColTrans = self.dfColTrans
+        if self.dfCustomColTrans is not None:
+            dfColTrans = self.dfCustomColTrans.append(dfColTrans, sort=False)
+            
+        return dfColTrans
+        
+    # Get translated names of some columns (custom or not)
+    def transColumns(self, columns, lang):
+        
+        dTransCols = self.transTable()[lang]
+        return [dTransCols.get(col, str(col)) for col in columns]
+    
+    # Get translated names of custom columns
+    def transCustomColumns(self, lang):
+        
+        return self.dfCustomColTrans[lang].to_list()
+    
+    # Get translated names of some specific column (custom or not)
+    def transColumn(self, column, lang):
+        
+        return self.dfColTrans.loc[column, lang] if column in self.dfColTrans.index \
+               else self.dfCustomColTrans.loc[column, lang]
+    
+    def dfSubData(self, index=None, columns=None, copy=False):
+    
+        """Get a subset of the all-results table rows and columns
+
+        Parameters:
+        :param index: rows to select, as an iterable of a subset of self.dfData.index values
+                      (anything suitable for pd.DataFrame.loc[...]) ; None = all rows.
+        :param columns: columns to select, as a list(string) or pd.Index when mono-indexed columns
+        :param copy: if True, return a full copy of the selected data, not a "reference" to the internal table
+        """
+        
+        assert columns is None or isinstance(columns, list) or isinstance(columns, (pd.Index, pd.MultiIndex)), \
+               'columns must be specified as None/[] (all), or as a list of tuples, or as a pandas.[Multi]Index'
+
+        # Make a copy of / extract selected columns of dfData.
+        dfSbData = self.getData(copy=False)
+        if columns is not None and len(columns) > 0:
+            if self.isMultiIndexedCols and isinstance(columns, list):
+                iColumns = pd.MultiIndex.from_tuples(columns)
+            else:
+                iColumns = columns
+            dfSbData = dfSbData.reindex(columns=iColumns)
+        
+        if index is not None:
+            dfSbData = dfSbData.loc[index]
+
+        if copy:
+            dfSbData = dfSbData.copy()
+            
+        return dfSbData
+
+    # Access a mono-indexed translated columns version of the data table
+    def dfTransData(self, lang='en', index=None, columns=None):
+        
+        """Get a subset of the all-results table rows and columns with translated column names (mono-index)
+
+        Note: The resulting table holds a copy (of part) of the internal table ; this is needed because
+              of the translation of the column names ; hence the absence of the expected "copy" parameter !
+
+        Parameters:
+        :param lang: target language for translation ('en' or 'fr')
+        :param index: rows to select, as a list(int) or pd.Index (subset of self.dfData.index) ; None = all rows.
+        :param columns: columns to select, as a list(string) or pd.Index when mono-indexed columns
+                       or as a list(tuple(string*)) or pd.MultiIndex when multi-indexed ; None = all columns.
+        """
+
+        assert lang in ['en', 'fr'], 'No support for "{}" language'.format(lang)
+        
+        # Extract (and may be copy) selected rows and columns of dfData.
+        dfTrData = self.dfSubData(index=index, columns=columns, copy=True)
+        
+        # Translate column names.
+        dfTrData.columns = self.transColumns(dfTrData.columns, lang)
+        
+        return dfTrData
+
+    def updateSpecs(self, reset=False, overwrite=False, **specs):
+
+        """Update specs as given
+
+        Parameters:
+        :param reset: if True, cleanup before updating => like a set !
+        :param overwrite: if False, and at least 1 of the spec already exists,
+                      raise an exception (refuse to update) ; otherwise, overwrite silently
+        :param specs: named specs to add/update
+        """
+        if reset:
+            self.specs.clear()
+
+        if not overwrite:
+            assert all(name not in self.specs for name in specs), \
+                   "Unless explicitly specified, won't overwrite already present specs {}" \
+                   .format(', '.join(name for name in specs if name in self.specs))
+
+        self.specs.update(specs)
+
+    def toPickle(self, fileName, specs=True, raw=False):
+
+        """Save raw data and specs to a pickle file (XZ compressed format if requested).
+
+        Parameters:
+        :param fileName: target file pathname ; file is auto-compressed to XZ format
+                         through the lzma module if its extension is .xz or .lzma,
+                         or else not compressed.
+        :param specs: if False, don't save specs (actually save empty specs)
+        :param raw: if False, save post-processed data (through dfData) ;
+                    if True, save raw data (through dfRawData)
+        """
+        
+        start = pd.Timestamp.now()
+
+        dfOutData = self.dfRawData if raw else self.dfData
+
+        logger.debug(f'toPickle: {dfOutData.columns=}')
+
+        compress = pl.Path(fileName).suffix in ['.xz', '.lzma']
+        with lzma.open(fileName, 'wb') if compress else open(fileName, 'wb') as file:
+            pickle.dump((dfOutData, self.specs if specs else dict()), file)
+
+        logger.info('{}x{} results rows x columns and {} specs saved to {} in {:.3f}s'
+                    .format(len(dfOutData), len(dfOutData.columns),
+                            len(self.specs) if specs else 'no', fileName,
+                            (pd.Timestamp.now() - start).total_seconds()))
+
+    def specs2Tables(self):
+
+        """Transform specs to tables
+
+        :return: dict(name=DataFrame)
+        """
+        ddfSpecs = dict()
+
+        for spName, spData in self.specs.items():
+            if isinstance(spData, (dict, list, pd.Series)):
+                if not isinstance(spData, pd.Series):
+                    spData = pd.Series(spData)
+                spData = spData.to_frame()
+            elif not isinstance(spData, pd.DataFrame):
+                raise NotImplementedError
+            ddfSpecs[spName] = spData
+
+        return ddfSpecs
+
+    @staticmethod
+    def specsFromTables(ddfTables):
+
+        specs = dict()
+        for spName, dfSpData in ddfTables.items():
+            if len(dfSpData.columns) == 1:  # Output a Series, or dict or list
+                if dfSpData.columns[0] == 0:
+                    if dfSpData.index.equals(pd.RangeIndex(stop=len(dfSpData))):
+                        specs[spName] = dfSpData.loc[:, 0].to_list()
+                    else:
+                        specs[spName] = dfSpData.loc[:, 0].to_dict()
+                else:  # Output a Series
+                    specs[spName] = dfSpData[dfSpData.columns[0]]
+            else:  # Output a DataFrame
+                specs[spName] = dfSpData
+
+        return specs
+
+    DefAllResultsSheetName = 'all-results'
+
+    def toExcel(self, fileName, sheetName=None, lang=None, subset=None, index=True,
+                specs=True, specSheetsPrfx='sp-', engine=None):
+
+        """Save data and specs to a worksheet format (Excel, ODF, ...) :
+        * newer XLSX format for .xlsx extensions (through 'openpyxl' or 'xlrd' module)
+        * .xls (through 'xlwt').
+        * .ods (through 'odfpy'), if pandas >= 0.25.
+
+        Details:
+        * data to the named sheet (sheetName)
+        * specs to the sheets given spec name, prefixed as specified (specSheetsPrfx)
+
+        Parameters:
+        :param fileName: target file name
+        :param sheetName: for results data only
+        :param lang: if not None, save translated data columns names
+        :param subset: subset of data columns to save
+        :param index: if True, save data index column
+        :param specs: if False, don't save specs
+        :param specSheetsPrfx: prefix to spec names to use to build spec sheet names
+        :param engine: None => auto-selection from file extension ; otherwise, use xlrd, openpyxl or odf.
+        """
+        
+        assert sheetName is None or not specs or not sheetName.lower().startswith(specSheetsPrfx), \
+               f"Results data sheet name can't start with reserved prefix {specSheetsPrfx} (whatever case)"
+        assert not (sheetName is None
+                    and specs and self.DefAllResultsSheetName.lower().startswith(specSheetsPrfx.lower())), \
+               "Sheet prefix '{}' can't be a heading part of {} (whatever case)" \
+               .format(specSheetsPrfx, self.DefAllResultsSheetName)
+
+        start = pd.Timestamp.now()
+        
+        dfOutData = self.dfSubData(columns=subset) if lang is None else self.dfTransData(columns=subset, lang=lang)
+        
+        with pd.ExcelWriter(fileName, engine=engine) as xlWrtr:
+            dfOutData.to_excel(xlWrtr, sheet_name=sheetName or self.DefAllResultsSheetName, index=index)
+            for spName, dfSpData in self.specs2Tables().items():
+                dfSpData.to_excel(xlWrtr, sheet_name=specSheetsPrfx + spName, index=True)
+
+        logger.info('{}x{} results rows x columns and {} specs saved to {} in {:.3f}s'
+                    .format(len(dfOutData), len(dfOutData.columns),
+                            len(self.specs) if specs else 'no', fileName,
+                            (pd.Timestamp.now() - start).total_seconds()))
+
+    def toOpenDoc(self, fileName, sheetName=None, lang=None, subset=None, index=True,
+                  specs=True, specSheetsPrfx='sp-'):
+        
+        """Save data and specs to ODF worksheet format
+        
+        Note: Needs pandas >= 0.25.
+
+        Details:
+        * data to the named sheet (sheetName)
+        * specs to the sheets given spec name, prefixed as specified (specSheetsPrfx)
+
+        Parameters:
+        :param fileName: target file name
+        :param sheetName: for results data only
+        :param lang: if not None, save translated data columns names
+        :param subset: subset of data columns to save
+        :param index: if True, save data index column
+        :param specs: if False, don't save specs
+        :param specSheetsPrfx: prefix to spec names to use to build spec sheet names
+        """
+
+        assert pkgver.parse(pd.__version__).release >= (1, 1), \
+               "Don't know how to write to OpenDoc format before Pandas 1.1"
+        
+        self.toExcel(fileName, sheetName=sheetName, lang=lang, subset=subset, index=index,
+                     specs=specs, specSheetsPrfx=specSheetsPrfx, engine='odf')
+
+    def fromPickle(self, fileName, specs=True, postComputed=False, acceptNewCols=False, dDefMissingCols=dict()):
+
+        """Load (overwrite) data and optionnaly specs from a pickle file, possibly lzma-compressed,
+        assuming ctor params match the results object used for prior toPickle(),
+        which can well be ensured by using the same ctor params as used for saving !
+
+        :param fileName: source file pathname ; file is auto-decompressed through the lzma module
+                         if its extension is .xz or .lzma.
+        :param specs: if False, don't load specs
+        :param postComputed: if True, prevents next post-computation 
+        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
+            if unexpected columns appear in loaded data to append
+        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
+            (Warning: only from self.miCols)
+        """
+        
+        start = pd.Timestamp.now()
+
+        # Load results data and spec from file
+        compressed = pl.Path(fileName).suffix in ['.xz', '.lzma']
+        with lzma.open(fileName, 'rb') if compressed else open(fileName, 'rb') as file:
+            dfData, dSpecs = pickle.load(file)
+
+        # Complete missing columns if any.
+        for colName, colDefVal in dDefMissingCols.items():
+            if colName not in dfData.columns:
+                dfData[colName] = colDefVal
+
+        # Set data.
+        self.setData(dfData, postComputed=postComputed)
+
+        # If specified, update results columns list (self.miCols) dynamically 
+        # if unexpected columns appear in loaded data.
+        if acceptNewCols:
+            self._acceptNewColumns(dfData.columns)
+
+        # Set specs if specified.
+        if specs:
+            self.specs = dSpecs
+
+        logger.info('{}x{} results rows x columns and {} specs loaded from {} in {:.3f}s'
+                    .format(len(dfData), len(dfData.columns),
+                            len(self.specs) if specs else 'no', fileName,
+                            (pd.Timestamp.now() - start).total_seconds()))
+
+    def fromExcel(self, fileName, sheetName=None, header=[0, 1, 2], skipRows=[3], indexCol=0,
+                  specs=True, specSheetsPrfx='sp-', postComputed=False,
+                  acceptNewCols=False, dDefMissingCols=dict(), engine=None):
+        
+        """Load (overwrite) data from the first or named sheet of an Excel worksheet (XLSX or XLS format),
+        assuming ctor params match with Excel sheet column names and list,
+        which can well be ensured by using the same ctor params as used for saving !
+
+        Also optionnaly load specs from other sheets named with given prefix, as dataframes
+        (ignore others ; empty prefix => all others)
+
+        Parameters:
+        :param fileName: source file name
+        :param sheetName: name of the sheet to load data from (default None => 1st sheetw)
+        :param header: list of source data row indexes to use for column index (1st sheet only)
+        :param skipRows: list of source data row indexes to ignore (1st sheet only)
+        :param indexCol: index of the source data column to use as index (1st sheet only)
+                         (None => auto-generated, not read)
+        :param specs: if False, don't load specs
+        :param specSheetsPrfx: name prefix to use to detect spec sheets
+        :param postComputed: if True, prevents next post-computation 
+        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
+            if unexpected columns appear in loaded data to append
+        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
+            (Warning: only from self.miCols)
+        :param engine: None => auto-selection from file extension ; otherwise, use xlrd, openpyxl or odf.
+        """
+
+        start = pd.Timestamp.now()
+        
+        # Load results data.
+        logger.info1(f'Loading {fileName}:')
+        xlReader = pd.ExcelFile(fileName, engine=engine)
+        logger.info2(f'* {sheetName or xlReader.sheet_names[0]} ...')
+        dfData = pd.read_excel(xlReader, sheet_name=sheetName or 0, header=header,
+                               skiprows=skipRows, index_col=indexCol)
+
+        # Complete missing columns if any.
+        for colName, colDefVal in dDefMissingCols.items():
+            if colName not in dfData.columns:
+                dfData[colName] = colDefVal
+
+        # Set data.
+        self.setData(dfData, postComputed=postComputed)
+
+        # If specified, update results columns list (self.miCols) dynamically 
+        # if unexpected columns appear in loaded data.
+        if acceptNewCols:
+            self._acceptNewColumns(dfData.columns)
+
+        # Load specs
+        self.specs = dict()
+        if specs:
+            ddfSpecs = dict()
+            for shName in xlReader.sheet_names:
+                if shName.startswith(specSheetsPrfx):
+                    logger.info2(f'* {shName} ...')
+                    ddfSpecs[shName[len(specSheetsPrfx):]] = \
+                        pd.read_excel(xlReader, sheet_name=shName, index_col=0)
+            self.specs = self.specsFromTables(ddfSpecs)
+
+        logger.info('{}x{} results rows x columns and {} specs loaded from {} in {:.3f}s'
+                    .format(len(dfData), len(dfData.columns),
+                            len(self.specs) if specs else 'no', fileName,
+                            (pd.Timestamp.now() - start).total_seconds()))
+
+    def fromOpenDoc(self, fileName, sheetName=None, header=[0, 1, 2], skipRows=[3], indexCol=0,
+                    specs=True, specSheetsPrfx='sp-', postComputed=False,
+                    acceptNewCols=False, dDefMissingCols=dict()):
+        
+        """Load (overwrite) data from the first or named sheet of an Open Document worksheet (ODS format),
+        assuming ctor params match with ODF sheet column names and list,
+        which can well be ensured by using the same ctor params as used for saving !
+
+        Also optionnaly load specs from other sheets with given prefix as dataframes
+        (ignore others ; empty prefix => all others)
+
+        Parameters:
+        :param fileName: source file name
+        :param sheetName: name of the sheet to load data from (default None => 1st)
+        :param header: list of source data row indexes to use for column index
+        :param skipRows: list of source data row indexes to ignore
+        :param indexCol: index of the source data column to use as index (None => auto-generated, not read)
+        :param specs: if False, don't load specs
+        :param specSheetsPrfx: name prefix to use to detect spec sheets
+        :param postComputed: if True, prevents next post-computation 
+        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
+            if unexpected columns appear in loaded data to append
+        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
+            (Warning: only from self.miCols)
+        """
+
+        assert pkgver.parse(pd.__version__).release >= (0, 25, 1), \
+               'Don\'t know how to read from OpenDoc format before Pandas 0.25.1 (using odfpy module)'
+        
+        self.fromExcel(fileName, sheetName=sheetName, header=header, skipRows=skipRows, indexCol=indexCol,
+                       specs=specs, specSheetsPrfx=specSheetsPrfx, postComputed=postComputed,
+                       acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
+
+    def fromFile(self, fileName, sheetName=None, header=[0, 1, 2], skipRows=[3], indexCol=0,
+                 specs=True, specSheetsPrfx='sp-', postComputed=False,
+                 acceptNewCols=False, dDefMissingCols=dict()):
+        
+        """Load (overwrite) data data and eventually specs from a given file,
+        (supported formats are .pickle, .pickle.xz, .ods, .xlsx, .xls, auto-detected from file name)
+        assuming ctor params match with the results object used to generate source file,
+        which can well be ensured by using the same ctor params as used for saving !
+        Notes: Needs odfpy module and pandas.version >= 0.25.1
+
+        Parameters:
+        :param fileName: source file name
+        :param sheetName: name of the sheet to load data from (default None => 1st)
+        :param header: list of source data row indexes to use for column index
+        :param skipRows: list of source data row indexes to ignore
+        :param indexCol: index of the source data column to use as index (None => auto-generated, not read)
+        :param specs: if False, don't load specs
+        :param specSheetsPrfx: name prefix to use to detect spec sheets
+        :param postComputed: if True, prevents next post-computation 
+        :param acceptNewCols: if True, append results columns list (self.miCols) dynamically 
+            if unexpected columns appear in loaded data to append
+        :param dDefMissingCols: default row value to use for missing columns (as a dict/pd.Series)
+            (Warning: only from self.miCols)
+        """
+
+        fpn = pl.Path(fileName)
+        if fpn.suffix in ['.xz', '.pickle']:
+            self.fromPickle(fileName, specs=specs, postComputed=postComputed,
+                            acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
+        elif fpn.suffix in ['.ods']:
+            self.fromOpenDoc(fileName, sheetName=sheetName, header=header, skipRows=skipRows,
+                             indexCol=indexCol, specs=specs, specSheetsPrfx=specSheetsPrfx, postComputed=postComputed,
+                             acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
+        elif fpn.suffix in ['.xls', '.xlsx']:
+            self.fromExcel(fileName, sheetName=sheetName, header=header, skipRows=skipRows,
+                           indexCol=indexCol, specs=specs, specSheetsPrfx=specSheetsPrfx, postComputed=postComputed,
+                           acceptNewCols=acceptNewCols, dDefMissingCols=dDefMissingCols)
+        else:
+            raise NotImplementedError(f'Unsupported ResultsSet input file format: {fileName}')
+
+    def compare(self, other, subsetCols=[], indexCols=[], dropCloser=np.inf, dropNans=True, dropCloserCols=False):
+    
+        """
+        Compare 2 results sets.
+        
+        The resulting diagnosis DataFrame will have the same columns and merged index,
+        with a "closeness" value for each cell (see _closeness method) ;
+        rows where all cells have closeness > dropCloser (or eventually NaN) are yet dropped.
+        and columns where all cells have closeness > dropCloser (or eventually NaN) can also be dropped.
+        
+        Parameters:
+        :param other: Right results or DataFrame object to compare
+        :param list subsetCols: on a subset of columns,
+        :param list indexCols: ignoring these columns, but keeping them as the index and sorting order,
+        :param float dropCloser: result will only include rows with all cell closeness > dropCloser
+                                 (default: np.inf => all cols and rows kept).
+        :param bool dropNans: smoother condition for dropCloser : if True, NaN values are also considered > dropCloser
+                              ('cause NaN != NaN :-( ).
+        :param bool dropCloserCols: if True, also drop "> dropCloser (or eventually NaN)"-all-cell columns,
+                                    just as rows
+
+        :return: the diagnostic DataFrame.
+        """
+        
+        return DataSet.compareDataFrames(dfLeft=self.dfData,
+                                         dfRight=other if isinstance(other, pd.DataFrame) else other.dfData,
+                                         subsetCols=subsetCols, indexCols=indexCols,
+                                         dropCloser=dropCloser, dropNans=dropNans, dropCloserCols=dropCloserCols)
+        
+
+if __name__ == '__main__':
+
+    sys.exit(0)
```

### Comparing `pyaudisam-0.9.3/pyaudisam/engine.py` & `pyaudisam-1.0.1/pyaudisam/engine.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,936 +1,936 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "engine": Interface to (external) DS engines
-
-import sys
-import os
-import re
-import pathlib as pl
-import copy
-import tempfile
-
-from collections import namedtuple as ntuple
-
-import subprocess as sproc
-
-import numpy as np
-import pandas as pd
-
-from . import log, runtime
-
-logger = log.logger('ads.eng', level=log.INFO)  # Initial config (can be changed later)
-
-from .executor import Executor
-
-# Keep ruin'dows from opening a GPF dialog box every time a launched executable (like MCDS.exe) crashes !
-# BUT: This does NOT work :-(
-if sys.platform.startswith('win'):
-    import ctypes
-    import msvcrt
-    ctypes.windll.kernel32.SetErrorMode(msvcrt.SEM_NOGPFAULTERRORBOX)
-
-# Actual package install dir.
-KInstDirPath = pl.Path(__file__).parent.resolve()
-
-
-# DSEngine (abstract) classes.
-# An engine for running multiple DS analyses with same options (engine ctor params),
-# but various parameters (submitAnalysis() parameters), possibly as parallel threads / processes.
-# Warning: No option change allowed while started analyses are running / all their getResults() have returned.
-class DSEngine(object):
-    
-    # Options possible values.
-    DistUnits = ['Meter', 'Kilometer', 'Mile', 'Inch', 'Feet', 'Yard', 'Nautical mile']
-    AreaUnits = ['Hectare', 'Acre'] + ['Sq. ' + distUnit for distUnit in DistUnits]
-    
-    # Forbidden chars in workDir path name (Distance DS engines are real red necks)
-    # TODO: stronger protection (more special chars ? more generic method, through re ?)
-    ForbidPathChars = [' ', '(', ')', ',']
-    
-    # Distance software detection params.
-    DistanceMajorVersions = [7, 6]  # Lastest first.
-    DistanceInstDirNameFmt = 'Distance {majorVersion}'
-    DistancePossInstPaths = map(pl.Path, ['C:/Program files (x86)', 'C:/Program files', 'C:/PortableApps', '.'])
-
-    # Find given executable installation dir.
-    # Note: MCDS.exe is an autonomous executable : simply put it in a "Distance 7" sub-folder
-    #       of this package's one, and it'll work ! Or else install Distance 7 (or later) the normal way :-)
-    @staticmethod
-    def findExecutable(exeFileName):
-
-        exeFilePathName = None
-        logger.debug('Looking for {} ...'.format(exeFileName))
-        for ver in DSEngine.DistanceMajorVersions:
-            for path in DSEngine.DistancePossInstPaths:
-                exeFN = path / DSEngine.DistanceInstDirNameFmt.format(majorVersion=ver) / exeFileName
-                if not exeFN.exists():
-                    logger.info2('  Checking {} : No,'.format(exeFN))
-                else:
-                    logger.info('Found {} here: {}.'.format(exeFileName, exeFN))
-                    exeFilePathName = exeFN
-                    break
-            if exeFilePathName:
-                break
-
-        if not exeFilePathName:
-            raise Exception('Could not find {} ; please install Distance software (V6 or later)'.format(exeFileName))
-
-        runtime.update({'DS engine': exeFilePathName.as_posix()})
-            
-        return exeFilePathName
-    
-    # Specifications of output stats.
-    DfStatRowSpecs, DfStatModSpecs, DfStatModNotes, MIStatModCols, DfStatModColTrans = None, None, None, None, None
-    
-    def __init__(self, workDir='.', executor=None, runMethod='subprocess.run', timeOut=None,
-                 distanceUnit='Meter', areaUnit='Hectare', **options):
-
-        """Ctor
-        :param workDir: As a simple str, or a pl.Path
-        :param executor: Executor object to use (None => a sequential one will be auto-generated)
-        :param runMethod: for calling engine executable : 'os.system' or 'subprocess.run'
-        :param timeOut: engine call time limit (s) ; None => no limit ;
-                WARNING: Not implemented (no way) for 'os.system' run method (see MCDSAnalysis for this)
-        """
-
-        # Check base options
-        assert distanceUnit in self.DistUnits, \
-               'Invalid distance unit {}: should be in {}'.format(distanceUnit, self.DistUnits)
-        assert areaUnit in self.AreaUnits, \
-               'Invalid area unit {}: should be in {}'.format(areaUnit, self.AreaUnits)
-        
-        # Save specific options (as a named tuple for easier use through dot operator).
-        options = copy.deepcopy(options)
-        options.update(distanceUnit=distanceUnit, areaUnit=areaUnit)
-        self.OptionsClass = ntuple('Options', options.keys())
-        self.options = self.OptionsClass(**options) 
-        
-        # Set executor for runAnalysis().
-        self.executor = executor if executor is not None else Executor()
-        assert timeOut is None or runMethod != 'os.system' or self.executor.isAsync(), \
-               f"Can't care about {timeOut}s execution time limit" \
-               " with a non-asynchronous executor and os.system run method"
-
-        # Parameters for engine subprocess creation.
-        self.runMethod = runMethod
-        self.timeOut = timeOut
-        
-        # Check and prepare workdir if needed, and save.
-        assert all(c not in str(workDir) for c in self.ForbidPathChars), \
-               'Invalid character from "{}" in workDir folder "{}"' \
-               .format(''.join(self.ForbidPathChars), workDir)
-        self.workDir = pl.Path(workDir)
-        self.workDir.mkdir(exist_ok=True)
-        logger.info('DSEngine work folder: {}'.format(self.workDir.absolute().as_posix()))
-    
-    # Possible regexps for auto-detection of columns to import (into Distance / MCDS) from data sets / exports
-    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK).
-    ImportFieldAliasREs = \
-        {'STR_LABEL':  ['region', 'zone', 'secteur', 'strate', 'stratum'],
-         'STR_AREA':   ['surface', 'area', 'ha', 'km2'],
-         'SMP_LABEL':  ['point', 'lieu', 'location', 'transect'],
-         'SMP_EFFORT': ['effort', 'passes', 'surveys', 'samplings', 'longueur', 'length'],
-         'DISTANCE':   ['dist'],
-         'ANGLE':      ['angl', 'azim', 'direct'],
-         'SIZE':       ['nombre', 'nb', 'indiv', 'obj', 'tail', 'num', 'siz']}  # Cluster size
-    
-    # Data fields of decimal type.
-    # TODO: Complete for non 'Point transect' modes
-    DecimalFields = ['SMP_EFFORT', 'DISTANCE', 'ANGLE']
-    
-    # Match srcFields with tgtAliasREs ones ; keep remaining ones ; sort decimal fields.
-    @classmethod
-    def matchDataFields(cls, srcFields, tgtAliasREs={}):
-        
-        logger.debug('Matching required data columns:')
-        
-        # Try and match required data columns.
-        matFields = list()
-        matDecFields = list()
-        for tgtField in tgtAliasREs:
-            logger.debug1(' * ' + tgtField + ':')
-            foundTgtField = False
-            for srcField in srcFields:
-                for pat in tgtAliasREs[tgtField]:
-                    if re.search(pat, srcField, flags=re.IGNORECASE):
-                        logger.debug2('  . ' + srcField)
-                        matFields.append(srcField)
-                        if tgtField in cls.DecimalFields:
-                            matDecFields.append(srcField)
-                        foundTgtField = True
-                        break
-                if foundTgtField:
-                    break
-            if not foundTgtField:
-                raise KeyError('Could not find a match for expected field {} in sample data set columns [{}]'
-                               .format(tgtField, ', '.join(srcFields)))
-        
-        # Extra fields.
-        extFields = [field for field in srcFields if field not in matFields]
-
-        logger.debug('... success.')
-        
-        return matFields, matDecFields, extFields
-
-    # Setup a thread & process safe run folder for an analysis
-    # * runPrefix : user-friendly prefix for the generated folder-name (may be None)
-    # Note: Not a class method because it uses self.workDir
-    def setupRunFolder(self, runPrefix=None):
-
-        # MCDS does not support folder and file names with spaces inside ...
-        # And one never knows ... remove any other special chars also.
-        if runPrefix is None:
-            runPrefix = ''
-        else:
-            runPrefix = runPrefix.translate(str.maketrans({c: '' for c in ' ,.:;()/'})) + '-'
-
-        return pl.Path(tempfile.mkdtemp(dir=self.workDir, prefix=runPrefix))
-
-    # Engine in/out file names.
-    CmdFileName = 'cmd.txt'
-    DataFileName = 'data.txt'
-    OutputFileName = 'output.txt'
-    LogFileName = 'log.txt'
-    StatsFileName = 'stats.txt'
-    PlotsFileName = 'plots.txt'
-        
-    # Shutdown : release any used resource.
-    # Post-condition: Instance can no more run analyses.
-    def shutdown(self, executor=True):
-    
-        if executor and self.executor is not None:
-            self.executor.shutdown()
-        self.executor = None
-   
-    def __del__(self):
-    
-        self.shutdown()
-
-
-# MCDS engine (Conventional Distance Sampling)
-class MCDSEngine(DSEngine):
-    
-    # Possible suervy types and distance types.
-    SurveyTypes = ['Point', 'Line']
-    DistTypes = ['Radial', 'Perpendicular', 'Radial & Angle']
-    
-    # First data fields in exports for distance / MCDS importing
-    # (N/A ones may get removed before use, according to distance type and clustering options).
-    FirstDataFields = {'Point': ['STR_LABEL', 'STR_AREA', 'SMP_LABEL', 'SMP_EFFORT', 'DISTANCE', 'SIZE'],
-                       'Line': ['STR_LABEL', 'STR_AREA', 'SMP_LABEL', 'SMP_EFFORT', 'DISTANCE', 'ANGLE', 'SIZE']}
-
-    # Estimator key functions (Order: Distance .chm doc, "MCDS Engine Stats File", note 2 below second table).
-    EstKeyFns = ['UNIFORM', 'HNORMAL', 'NEXPON', 'HAZARD']
-    EstKeyFnDef = EstKeyFns[0]
-
-    # Estimator adjustment series (Order: Distance .chm doc, "MCDS Engine Stats File", note 3 below second table).
-    EstAdjustFns = ['POLY', 'HERMITE', 'COSINE']
-    EstAdjustFnDef = EstAdjustFns[0]
-
-    # Estimator adjustment terms selection criterion.
-    EstCriteria = ['AIC', 'AICC', 'BIC', 'LR']
-    EstCriterionDef = EstCriteria[0]
-    
-    # Estimator confidence value for output interval.
-    EstCVIntervalDef = 95  # %
-    
-    # Distance truncation / cut points parameters.
-    DistMinDef = None  # No left truncation (min = 0)
-    DistMaxDef = None  # No right truncation (max = max. distance observed)
-    DistFitCutsDef = None  # Model fitting : Automatic engine distance cuts determination.
-    DistDiscrCutsDef = None  # Distance values discretisation : None (keep exact values).
-
-    # Executable 
-    ExeFilePathName = DSEngine.findExecutable('MCDS.exe')
-
-    # Output stats specs : load from external files (extracts from Distance doc).
-    @classmethod
-    def loadStatSpecs(cls, nMaxAdjParams=10):
-        
-        if MCDSEngine.DfStatRowSpecs is not None:
-            return
-        
-        logger.debug('MCDS : Loading output stats specs ...')
-        
-        # Output stats row specifications
-        fileName = KInstDirPath / 'mcds/stat-row-specs.txt'
-        logger.debug1('* {}'.format(fileName))
-        with open(fileName, mode='r', encoding='utf8') as fStatRowSpecs:
-            statRowSpecLines = [line.rstrip('\n') for line in fStatRowSpecs.readlines() if not line.startswith('#')]
-            statRowSpecs = [(statRowSpecLines[i].strip(), statRowSpecLines[i+1].strip())
-                            for i in range(0, len(statRowSpecLines)-2, 3)]
-            cls.DfStatRowSpecs = pd.DataFrame(columns=['Name', 'Description'],
-                                              data=statRowSpecs).set_index('Name')
-            assert not cls.DfStatRowSpecs.empty, 'Empty MCDS stats row specs'
-        
-        # Module and stats number to description table
-        fileName = KInstDirPath / 'mcds/stat-mod-specs.txt'
-        logger.debug1('* {}'.format(fileName))
-        with open(fileName, mode='r', encoding='utf8') as fStatModSpecs:
-            statModSpecLines = [line.rstrip('\n') for line in fStatModSpecs.readlines() if not line.startswith('#')]
-            reModSpecNumName = re.compile('(.+) – (.+)')
-            statModSpecs = list()
-            moModule = None
-            for line in statModSpecLines:
-                if not line:
-                    continue
-                if moModule is None:
-                    moModule = reModSpecNumName.match(line.strip())
-                    continue
-                if line == ' ':
-                    moModule = None
-                    continue
-                moStatistic = reModSpecNumName.match(line.strip())
-                modNum, modDesc, statNum, statDescNotes = \
-                    moModule.group(1), moModule.group(2), moStatistic.group(1), moStatistic.group(2)
-                for i in range(len(statDescNotes)-1, -1, -1):
-                    if not re.match('[\d ,]', statDescNotes[i]):
-                        statDesc = statDescNotes[:i+1]
-                        statNotes = statDescNotes[i+1:].replace(' ', '')
-                        break
-                modNum = int(modNum)
-                if statNum.startswith('101 '):
-                    for num in range(nMaxAdjParams):  # Assume no more than that ... a bit hacky !
-                        statModSpecs.append((modNum, modDesc, 101+num,  # Make statDesc unique for later indexing
-                                             statDesc.replace('each', 'A({})'.format(num+1)), statNotes))
-                else:
-                    statNum = int(statNum)
-                    if modNum == 2 and statNum == 3:  # Actually, there are 0 or 3 of these ...
-                        for num in range(3):
-                            statModSpecs.append((modNum, modDesc, num+201,
-                                                 # Change statNum & Make statDesc unique for later indexing
-                                                 statDesc+' (distance set {})'.format(num+1), statNotes))
-                    else:
-                        statModSpecs.append((modNum, modDesc, statNum, statDesc, statNotes))
-            cls.DfStatModSpecs = pd.DataFrame(columns=['modNum', 'modDesc', 'statNum', 'statDesc', 'statNotes'],
-                                              data=statModSpecs).set_index(['modNum', 'statNum'])
-            assert not cls.DfStatModSpecs.empty, 'Empty MCDS stats module specs'
-        
-        # Produce a MultiIndex for output stats as columns in the desired order.
-        indexItems = list()
-        for lbl, modDesc, statDesc, statNotes in cls.DfStatModSpecs[['modDesc', 'statDesc', 'statNotes']].itertuples():
-            indexItems.append((modDesc, statDesc, 'Value'))
-            if '1' in statNotes:
-                indexItems += [(modDesc, statDesc, valName) for valName in ['Cv', 'Lcl', 'Ucl', 'Df']]
-        cls.MIStatModCols = pd.MultiIndex.from_tuples(indexItems)
-    
-        # Notes about stats.
-        fileName = KInstDirPath / 'mcds/stat-mod-notes.txt'
-        logger.debug1('* {}'.format(fileName))
-        with open(fileName, mode='r', encoding='utf8') as fStatModNotes:
-            statModNoteLines = [line.rstrip('\n') for line in fStatModNotes.readlines() if not line.startswith('#')]
-            statModNotes = [(int(line[:2]), line[2:].strip()) for line in statModNoteLines if line]
-            cls.DfStatModNotes = pd.DataFrame(data=statModNotes, columns=['Note', 'Text']).set_index('Note')
-            assert not cls.DfStatModNotes.empty, 'Empty MCDS stats module notes'
-            
-        # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
-        fileName = KInstDirPath / 'mcds/stat-mod-trans.txt'
-        logger.debug1('* {}'.format(fileName))
-        cls.DfStatModColsTrans = pd.read_csv(fileName, sep='\t')
-        cls.DfStatModColsTrans.set_index(['Module', 'Statistic', 'Figure'], inplace=True)
-        
-        logger.debug('MCDS : Loaded output stats specs.')
-        
-    # Accessors to class variables.
-    @classmethod
-    def statRowSpecs(cls):
-        
-        if cls.DfStatRowSpecs is None:
-            cls.loadStatSpecs()    
-
-        return cls.DfStatRowSpecs
-        
-    @classmethod
-    def statModSpecs(cls):
-        
-        if cls.DfStatModSpecs is None:
-            cls.loadStatSpecs()    
-
-        return cls.DfStatModSpecs
-        
-    @classmethod
-    def statModCols(cls):
-        
-        if cls.MIStatModCols is None:
-            cls.loadStatSpecs()    
-
-        return cls.MIStatModCols
-        
-    @classmethod
-    def statModNotes(cls):
-        
-        if cls.DfStatModNotes is None:
-            cls.loadStatSpecs()    
-
-        return cls.DfStatModNotes
-        
-    @classmethod
-    def statModColTrans(cls):
-        
-        if cls.DfStatModColsTrans is None:
-            cls.loadStatSpecs()    
-
-        return cls.DfStatModColsTrans
-
-    # Sample stats columns
-    MIStatSampCols = pd.MultiIndex.from_tuples([('sample stats', 'total number of observations', 'Value'),
-                                                ('sample stats', 'minimal observation distance', 'Value'),
-                                                ('sample stats', 'maximal observation distance', 'Value')])
-
-    @classmethod
-    def statSampCols(cls):
-        
-        return cls.MIStatSampCols
-    
-    # Sample stats columns translation
-    _DfStatSampColsTrans = \
-        pd.DataFrame(index=MIStatSampCols,
-                     data=dict(en=['NTot Obs', 'Min Dist', 'Max Dist'], fr=['NTot Obs', 'Min Dist', 'Max Dist']))
-
-    @classmethod
-    def statSampColTrans(cls):
-
-        return cls._DfStatSampColsTrans
-
-    def __init__(self, workDir='.', executor=None, runMethod='subprocess.run', timeOut=None,
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False):
-        
-        """Ctor.
-        :param workDir: As a simple str, or a pl.Path
-        :param executor: Executor object to use (None => a sequential one will be auto-generated)
-        :param runMethod: Method used to run the MCDS executable : os.system or subprocess.run
-        :param timeOut: Time-out (s) for analysis execution (None => no limit);
-                        WARNING: NOT implemented here when 'os.system' runMethod ... see MCDSAnalysis
-        """
-
-        # Initialize dynamic class variables.
-        MCDSEngine.loadStatSpecs()    
-
-        # Check options
-        assert surveyType in self.SurveyTypes, \
-               'Invalid survey type {} : should be in {}'.format(surveyType, self.SurveyTypes)
-        assert distanceType in self.DistTypes, \
-               'Invalid distance type {} : should be in {}'.format(distanceType, self.DistTypes)
-        
-        # Specialise class level regexps for matching import fields,
-        # according to distance type and clustering options
-        self.importFieldAliasREs = self.ImportFieldAliasREs.copy()
-        if not clustering:
-            del self.importFieldAliasREs['SIZE']
-        if distanceType != 'Radial & Angle':
-            del self.importFieldAliasREs['ANGLE']            
-    
-        # Initialise base.
-        firstDataFields = [fld for fld in self.FirstDataFields[surveyType] if fld in self.importFieldAliasREs]
-        super().__init__(workDir=workDir, executor=executor, runMethod=runMethod, timeOut=timeOut,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
-                         firstDataFields=firstDataFields)
-                         
-    # Command file template (for str.format()ing).
-    CmdTxt = \
-        '\n'.join(map(str.strip,
-                  """{output}
-                     {log}
-                     {stats}
-                     {plots}
-                     {bootstrap}
-                     {bootpgrss}
-                     Options;
-                     Type={survType};
-                     Distance={distType} /Measure='{distUnit}';
-                     Area /Units='{areaUnit}';
-                     Object=Single;
-                     SF=1;
-                     Selection=Sequential;
-                     Lookahead=1;
-                     Maxterms=5;
-                     Confidence={cvInterv};
-                     print=Selection;
-                     End;
-                     Data /Structure=Flat;
-                     Fields={dataFields};
-                     Infile={dataFileName} /{echoData};
-                     End;
-                     Estimate;
-                     Distance{distDiscrSpecs};
-                     Density=All;
-                     Encounter=All;
-                     Detection=All;
-                     Size=All;
-                     Estimator /Key={estKeyFn} /Adjust={estAdjustFn} /Criterion={estCriterion};
-                     Monotone=Strict;
-                     Pick=AIC;
-                     GOF{gOFitSpecs};
-                     Cluster /Bias=GXLOG;
-                     VarN=Empirical;
-                     End;
-                  """.split('\n'))) + '\n'
-    
-    # Build command file from options and params
-    # * runDir : pl.Path where to create cmd file.
-    # Note: Not a classmethod because it uses self.options
-    def buildCmdFile(self, runDir, **params):
-
-        # Default params values
-        if 'logData' not in params:
-            params['logData'] = False
-        if 'estimKeyFn' not in params:
-            params['estimKeyFn'] = self.EstKeyFnDef
-        if 'estimAdjustFn' not in params:
-            params['estimAdjustFn'] = self.EstAdjustFnDef
-        if 'estimCriterion' not in params:
-            params['estimCriterion'] = self.EstCriterionDef
-        if 'cvInterval' not in params:
-            params['cvInterval'] = self.EstCVIntervalDef
-        if 'maxDist' not in params:
-            params['maxDist'] = self.DistMaxDef
-        if 'minDist' not in params:
-            params['minDist'] = self.DistMinDef
-        if 'fitDistCuts' not in params:
-            params['fitDistCuts'] = self.DistFitCutsDef
-        if 'discrDistCuts' not in params:
-            params['discrDistCuts'] = self.DistDiscrCutsDef
-
-        # Generate file contents
-        # a. Compute non trivial data fields
-        distDiscrSpecs = ''
-        gOFitSpecs = ''
-        
-        minDist = params['minDist']
-        maxDist = params['maxDist']
-        fitDistCuts = params['fitDistCuts']
-        discrDistCuts = params['discrDistCuts']
-        if discrDistCuts is not None:
-            
-            if isinstance(discrDistCuts, list):
-                assert not (minDist is None or maxDist is None)
-                distDiscrSpecs += ' /Int=' + ','.join(format(d, 'g') for d in [minDist] + discrDistCuts + [maxDist])
-            elif isinstance(discrDistCuts, (int, float)):
-                distDiscrSpecs += ' /NClass=' + format(discrDistCuts, 'g')
-            # Other cases not supported, should be asserted by the caller.
-        
-        elif fitDistCuts is not None:  # Can't fit model on other distance intervals than used for discretisation.
-            
-            if isinstance(fitDistCuts, list):
-                assert not (minDist is None or maxDist is None)
-                gOFitSpecs += ' /Int=' + ','.join(format(d, 'g') for d in [minDist] + fitDistCuts + [maxDist])
-            elif isinstance(fitDistCuts, (int, float)):
-                gOFitSpecs += ' /NClass=' + format(fitDistCuts, 'g')
-            # Other cases not supported, should be asserted by the caller.
-                
-        if minDist is not None:
-            distDiscrSpecs += ' /Left=' + format(minDist, 'g')
-
-        if maxDist is not None:
-            distDiscrSpecs += ' /Width=' + format(maxDist, 'g')
-            
-        # b. Format contents string
-        cmdTxt = self.CmdTxt.format(output=runDir/self.OutputFileName, log=runDir/self.LogFileName,
-                                    stats=runDir/self.StatsFileName, plots=runDir/self.PlotsFileName,
-                                    bootstrap='None', bootpgrss='None',  # No support for the moment.
-                                    survType=self.options.surveyType, distType=self.options.distanceType,
-                                    distUnit=self.options.distanceUnit, areaUnit=self.options.areaUnit,
-                                    dataFields=','.join(self.options.firstDataFields),
-                                    dataFileName=runDir/self.DataFileName,
-                                    echoData=('' if params['logData'] else 'No') + 'Echo',
-                                    estKeyFn=params['estimKeyFn'], estAdjustFn=params['estimAdjustFn'],
-                                    estCriterion=params['estimCriterion'], cvInterv=params['cvInterval'],
-                                    distDiscrSpecs=distDiscrSpecs, gOFitSpecs=gOFitSpecs)
-
-        # Write file.
-        cmdFileName = runDir / self.CmdFileName
-        with open(cmdFileName, mode='w', encoding='utf-8') as cmdFile:
-            cmdFile.write(cmdTxt)
-
-        logger.debug('Commands written to {}'.format(cmdFileName))
-
-        # Done.
-        return cmdFileName
-    
-    # Workaround pd.DataFrame.to_csv(float_format='%.xf') not working when NaNs in serie
-    @staticmethod
-    def safeFloat2Str(val, prec=None, decPt='.'):
-        strVal = '' if pd.isnull(val) else str(val) if prec is None \
-                    else '{:.{prec}f}'.format(val, prec=prec)
-        if decPt != '.':
-            strVal = strVal.replace('.', decPt)
-        return strVal
-
-    # Build input data table from a sample data set (check and match mandatory columns, enforce order).
-    # TODO: Add support for covariate columns (through extraFields)
-    def buildExportTable(self, sampleDataSet, withExtraFields=True, decPoint='.'):
-        
-        # Match sampleDataSet table columns to MCDS expected fields from possible aliases
-        matchFields, matchDecFields, extraFields = \
-            self.matchDataFields(sampleDataSet.dfData.columns, self.importFieldAliasREs)
-        exportFields = matchFields
-        if withExtraFields:
-            exportFields += extraFields
-        else:
-            extraFields.clear()
-        
-        logger.debug2('Final data columns export order: ' + str(exportFields))
-        
-        # Put columns in the right order (first data fields ... first, in the same order)
-        dfExport = sampleDataSet.dfData[exportFields].copy()
-
-        # Prepare safe export of decimal data with may be some NaNs
-        allDecFields = set(matchDecFields + sampleDataSet.decimalFields).intersection(exportFields)
-        logger.debug2('Decimal columns: ' + str(allDecFields))
-        for field in allDecFields:
-            dfExport[field] = dfExport[field].apply(self.safeFloat2Str, decPt=decPoint)
-                
-        return dfExport, extraFields
-
-    # Build MCDS input data file from a sample data set.
-    # Note: Data = sighting order not changed, same as in input sampleDataSet !
-    # * runDir : pl.Path where to create data file.
-    # TODO: Add support for covariate columns (through extraFields)
-    def buildDataFile(self, runDir, sampleDataSet):
-        
-        # Build data to export (check and match mandatory columns, enforce order, ignore extra cols).
-        dfExport, extraFields = \
-            self.buildExportTable(sampleDataSet, withExtraFields=False, decPoint='.')
-        
-        # Export.
-        dataFileName = runDir / self.DataFileName
-        dfExport.to_csv(dataFileName, index=False, sep='\t', encoding='utf-8', header=None)
-        
-        logger.debug('Data MCDS-exported to {}'.format(dataFileName))
-        
-        return dataFileName
-    
-    # Run status codes (from MCDS documentation)
-    RCNotRun = 0
-    RCOK = 1
-    RCWarnings = 2
-    RCErrors = 3
-    RCFileErrors = 4
-    RCOtherErrors = 5  # and above, straight from MCDS.exe.
-    RCTimedOut = 555  # as named (through subprocess or concurrent.futures modules)
-    
-    @classmethod
-    def wasRun(cls, runCode):
-        return runCode != cls.RCNotRun
-    
-    @classmethod
-    def success(cls, runCode):
-        return runCode == cls.RCOK
-    
-    @classmethod
-    def warnings(cls, runCode):
-        return runCode == cls.RCWarnings
-    
-    @classmethod
-    def errors(cls, runCode):
-        return runCode >= cls.RCErrors
-    
-    @classmethod
-    def _runThroughOSSystem(cls, execFileName, cmdFileName, forReal=True):
-
-        """Run MCDS command through os.system
-
-        Under Ruin'dows, this means an intermediate "cmd.exe" subprocess.
-        """
-
-        # Call executable (no " around cmdFile, don't forget the space after ',', ...)
-        cmd = '"{}" 0, {}'.format(execFileName, cmdFileName)
-        if forReal:
-            logger.info1(f'Running MCDS through os.system({cmd}) ...')
-            startTime = pd.Timestamp.now()
-            status = os.system(cmd)
-            elapsedTime = (pd.Timestamp.now() - startTime).total_seconds()
-            logger.info2(f'... MCDS done : status={status}, elapsed={elapsedTime:.2f}s')
-            
-        # ... unless specified not to (input files generated, but no execution).
-        else:
-            logger.info1(f'NOT running MCDS through os.system({cmd}).')
-            startTime = pd.NaT
-            status = cls.RCNotRun
-            elapsedTime = 0
-
-        return status, startTime, elapsedTime
-
-    # MCDS.exe subprocess creation flags under ruin'dows: no window please !
-    # BUT: Does not help with fucking Ruin'dows crash window ... alas !
-    ExeCrFlags = sproc.CREATE_NO_WINDOW if sys.platform.startswith('win') else 0
-
-    @classmethod
-    def _runThroughSubProcessRun(cls, execFileName, cmdFileName, forReal=True, timeOut=None):
-
-        """Run MCDS command through subprocess.run
-
-        Under Ruin'dows, no "cmd.exe" intermediate subprocess, but only a conhost.exe.
-        """
-
-        # Call executable (no " around cmdFile, don't forget the space after ',', ...)
-        cmd = [str(execFileName), '0,', str(cmdFileName)]
-        if forReal:
-            logger.info1(f'Running MCDS through subprocess.run({cmd}, ) ...')
-            startTime = pd.Timestamp.now()
-            try:
-                proc = sproc.run(cmd, text=True, stdout=sproc.PIPE, stderr=sproc.STDOUT,
-                                 timeout=timeOut, creationflags=cls.ExeCrFlags)
-                status = proc.returncode
-                stdouterr = proc.stdout
-            except sproc.TimeoutExpired as toExc:
-                logger.error(f'MCDS timed out after {toExc.timeout:.2f}s')
-                status = cls.RCTimedOut
-                stdouterr = toExc.stdout
-            elapsedTime = (pd.Timestamp.now() - startTime).total_seconds()
-            logger.info3('MCDS stdout&err:')
-            logger.info3(stdouterr)
-            logger.info2(f'... MCDS done : status={status}, elapsed={elapsedTime:.2f}s')
-
-        # ... unless specified not to (input files generated, but no execution).
-        else:
-            logger.info1(f'NOT running MCDS through subprocess.run({cmd}).')
-            startTime = pd.NaT
-            status = cls.RCNotRun
-            elapsedTime = 0
-
-        return status, startTime, elapsedTime
-
-    @classmethod
-    def _run(cls, execFileName, cmdFileName, forReal=True, method='subprocess.run', timeOut=None):
-
-        """Run MCDS command through the given method
-        """
-
-        if method == 'os.system':
-            return cls._runThroughOSSystem(execFileName, cmdFileName, forReal=forReal)
-        elif method == 'subprocess.run':
-            return cls._runThroughSubProcessRun(execFileName, cmdFileName, forReal=forReal, timeOut=timeOut)
-
-        raise NotImplementedError(f'Unknown MCDSEngine run method "{method}"')
-
-    # Run 1 MCDS analysis from the beginning to the end (blocking for the calling thread)
-    # * runPrefix : user-friendly prefix for the generated folder-name (may be None)
-    def _runAnalysis(self, sampleDataSet, runPrefix='mcds', realRun=True, **analysisParms):
-        
-        # Create a new exclusive thread and process-safe run folder
-        anlysStartTime = pd.Timestamp.now()
-        runDir = self.setupRunFolder(runPrefix)
-        logger.debug('Will run in {}'.format(runDir))
-        
-        # Generate data and command files into this folder
-        logger.info2('MCDS analysis params: ' + str(analysisParms))
-        self.buildDataFile(runDir, sampleDataSet)
-        cmdFileName = self.buildCmdFile(runDir, **analysisParms)
-        
-        anlysElapsedTime = (pd.Timestamp.now() - anlysStartTime).total_seconds()
-
-        # Run executable as an OS sub-process.
-        runStatus, engStartTime, engElapsedTime = \
-            self._run(self.ExeFilePathName, cmdFileName, forReal=realRun,
-                      method=self.runMethod, timeOut=self.timeOut)
-        anlysElapsedTime += engElapsedTime
-
-        # Extract and decode results.
-        startTime = pd.Timestamp.now()
-
-        if self.success(runStatus) or self.warnings(runStatus):
-            sResults = self.decodeStats(runDir)
-        else:
-            sResults = None
-
-        anlysElapsedTime += (pd.Timestamp.now() - startTime).total_seconds()
-
-        return runStatus, anlysStartTime, anlysElapsedTime + engElapsedTime, runDir, sResults
-    
-    # Start running an MCDS analysis, using the executor (possibly asynchronously if it is not a sequential one)
-    def submitAnalysis(self, sampleDataSet, runPrefix='mcds', realRun=True, **analysisParms):
-        
-        # Check really implemented options
-        assert self.options.surveyType == 'Point', \
-               'Not yet implemented survey type {}'.format(self.options.surveyType)
-        assert self.options.distanceType == 'Radial', \
-               'Not yet implemented distance type {}'.format(self.options.distanceType)
-        
-        # Submit analysis work and return a Future object to ask from and wait for its results.
-        return self.executor.submit(self._runAnalysis, sampleDataSet, runPrefix, realRun, **analysisParms)
-    
-    # Decode output stats file to a value series
-    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
-    # * runDir : string or pl.Path where to find stats file.
-    # Warning: No support for more than 1 stratum, 1 sample, 1 estimator.
-    @classmethod
-    def decodeStats(cls, runDir):
-
-        statsFileName = pl.Path(runDir) / cls.StatsFileName
-        logger.debug('Decoding stats from {} ...'.format(statsFileName))
-        
-        # 1. Load table (text format, with space separated and fixed width columns,
-        #    columns headers from cls.DfStatRowSpecs)
-        dfStats = pd.read_csv(statsFileName, sep=' +', engine='python', names=cls.DfStatRowSpecs.index)
-        
-        # 2. Remove Stratum, Sample and Estimator columns (no support for multiple ones for the moment)
-        dfStats.drop(columns=['Stratum', 'Sample', 'Estimator'], inplace=True)
-        
-        # 3. Stack figure columns to rows, to get more comfortable
-        dfStats.set_index(['Module', 'Statistic'], append=True, inplace=True)
-        dfStats = dfStats.stack().reset_index()
-        dfStats.rename(columns={'level_0': 'id', 'level_3': 'Figure', 0: 'Value'}, inplace=True)
-
-        # 4. Fix multiple Module=2 & Statistic=3 rows (before joining with cls.DfStatModSpecs)
-        newStatNum = 200
-        for lbl, sRow in dfStats[(dfStats.Module == 2) & (dfStats.Statistic == 3)].iterrows():
-            if dfStats.loc[lbl, 'Figure'] == 'Value':
-                newStatNum += 1
-            dfStats.loc[lbl, 'Statistic'] = newStatNum
-        
-        # 5. Add descriptive / naming columns for modules and statistics,
-        #    from cls.DfStatModSpecs (more user friendly than numeric ids + help for detecting N/A figures)
-        dfStats = dfStats.join(cls.DfStatModSpecs, on=['Module', 'Statistic'])
-        
-        # 6. Check that supposed N/A figures (as told by cls.DfStatModSpecs.statNotes) are really such
-        #    Warning: There seems to be a bug in MCDS with Module=2 & Statistic=10x : some Cv values not always 0 ...
-        sKeepOnlyValueFig = ~dfStats.statNotes.apply(lambda s: pd.notnull(s) and '1' in s)
-        sFigs2Drop = (dfStats.Figure != 'Value') & sKeepOnlyValueFig
-        assert ~dfStats[sFigs2Drop & ((dfStats.Module != 2) | (dfStats.Statistic < 100))].Value.any(), \
-               'Warning: Some so-called "N/A" figures are not zeroes !'
-        
-        # 7. Remove so-called N/A figures
-        dfStats.drop(dfStats[sFigs2Drop].index, inplace=True)
-        
-        # 8. Make some values more readable.
-        lblKeyFn = (dfStats.Module == 2) & (dfStats.Statistic == 13)
-        dfStats.loc[lblKeyFn, 'Value'] = \
-            dfStats.loc[lblKeyFn, 'Value'].astype(int).apply(lambda n: cls.EstKeyFns[n-1])
-        lblAdjFn = (dfStats.Module == 2) & (dfStats.Statistic == 14)
-        dfStats.loc[lblAdjFn, 'Value'] = \
-            dfStats.loc[lblAdjFn, 'Value'].astype(int).apply(lambda n: cls.EstAdjustFns[n-1])
-        
-        # 9. Final indexing
-        dfStats = dfStats.reindex(columns=['modDesc', 'statDesc', 'Figure', 'Value'])
-        dfStats.set_index(['modDesc', 'statDesc', 'Figure'], inplace=True)
-
-        # That's all folks !
-        logger.debug('Done decoding from {}.'.format(statsFileName))
-        
-        return dfStats.T.iloc[0]
-
-    # Decode output log file to a string
-    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
-    # * runDir : string or pl.Path folder path-name where the analysis was run.
-    @classmethod
-    def decodeLog(cls, runDir):
-        
-        return dict(text=open(pl.Path(runDir) / cls.LogFileName).read().strip())
-    
-    # Decode output ... output file to a dict of chapters
-    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
-    # * runDir : string or pl.Path folder path-name where the analysis was run.
-    @classmethod
-    def decodeOutput(cls, runDir):
-        
-        outLst = open(pl.Path(runDir) / cls.OutputFileName).read().strip().split('\t')
-        
-        return [dict(id=title.translate(str.maketrans({c: '' for c in ' ,.-:()/'})),
-                     title=title.strip(), text=text.strip('\n'))
-                for title, text in [outLst[i:i+2] for i in range(0, len(outLst), 2)]]
-            
-    # Decode output plots file as a dict of plot dicts (key = output chapter title)
-    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
-    # * runDir : string or pl.Path folder path-name where the analysis was run.
-    @classmethod
-    def decodePlots(cls, runDir):
-        
-        dPlots = dict()
-        lines = (line.strip() for line in open(pl.Path(runDir) / cls.PlotsFileName, 'r').readlines())
-        for title in lines:
-            
-            title = title.strip()
-            subTitle = next(lines).strip()
-            xLabel = next(lines).strip()
-            yLabel = next(lines).strip()
-            xMin, xMax, yMin, yMax = [float(s) for s in next(lines).split()]
-            nDataRows = int(next(lines))
-            dataRows = list()
-            for _ in range(nDataRows):
-                dataRows.append([np.nan if '*' in s else float(s) for s in next(lines).split()])
-                
-            dPlots[title] = dict(title=title, subTitle=subTitle, dataRows=dataRows,  # nDataRows=nDataRows,
-                                 xLabel=xLabel, yLabel=yLabel, xMin=xMin, xMax=xMax, yMin=yMin, yMax=yMax)
-
-        return dPlots
-
-    @classmethod
-    def loadDataFile(cls, runDir):
-
-        runDir = pl.Path(runDir)
-        with open(runDir / cls.CmdFileName, 'r') as cmdFile:
-            fieldsLine = next(line for line in cmdFile.readlines() if line.startswith('Fields='))
-
-        dataCols = fieldsLine.strip('\n;')[len('Fields='):].split(',')
-
-        dataFilePathName = runDir / cls.DataFileName
-        dfData = pd.read_csv(dataFilePathName, sep='\t', names=dataCols)
-
-        logger.debug('Loaded {} rows with columns {} from MCDS data file {}.'
-                     .format(len(dfData), ','.join(dataCols), dataFilePathName.as_posix()))
-
-        return dfData
-
-    # Columns names for exporting to Distance import format with explicit columns headers
-    # (or for explicitating their contents in a standard way).
-    FirstDistanceExportFields = \
-        {'Point': dict(STR_LABEL='Region*Label', STR_AREA='Region*Area',
-                       SMP_LABEL='Point transect*Label', SMP_EFFORT='Point transect*Survey effort',
-                       DISTANCE='Observation*Radial distance',
-                       SIZE='Observation*Cluster size'),
-         'Line':  dict(STR_LABEL='Region*Label', STR_AREA='Region*Area',
-                       SMP_LABEL='Line transect*Label', SMP_EFFORT='Line transect*Line length',
-                       DISTANCE='Observation*Perp distance', ANGLE='Observation*Angle',
-                       SIZE='Observation*Cluster size')}
- 
-    def distanceFields(self, dsFields):
-        return [self.FirstDistanceExportFields[self.options.surveyType][name] for name in dsFields]
-    
-    # Compute sample stats
-    def computeSampleStats(self, sampleDataSet):
-
-        # Match sampleDataSet table columns to MCDS expected fields from possible aliases
-        matchFields, _, _ = \
-            self.matchDataFields(sampleDataSet.dfData.columns, self.importFieldAliasREs)
-        
-        # Extract usefull columns and rename them tfor standard semantics.
-        dfSample = sampleDataSet.dfData[matchFields]
-        dfSample.columns = self.distanceFields(self.options.firstDataFields)
-
-        # Compute sample stats.
-        distCol = self.FirstDistanceExportFields[self.options.surveyType]['DISTANCE']
-        stats = [sum(dfSample[distCol].notnull()), dfSample[distCol].min(), dfSample[distCol].max()]
-
-        # Done
-        return pd.Series(data=stats, index=self.MIStatSampCols)
-
-    # Build Distance/MCDS input data file from a sample data set to given target folder and file name.
-    def buildDistanceDataFile(self, sampleDataSet, tgtFilePathName, decimalPoint=',', withExtraFields=False):
-                
-        # Build data to export (check and match mandatory columns, enforce order, keep other cols).
-        dfExport, extraFields = \
-            self.buildExportTable(sampleDataSet, withExtraFields=withExtraFields, decPoint=decimalPoint)
-                
-        # Export.
-        dfExport.to_csv(tgtFilePathName, index=False, sep='\t', encoding='utf-8',
-                        header=self.distanceFields(self.options.firstDataFields) + extraFields)
-
-        logger.debug('Data Distance-exported to {}.'.format(tgtFilePathName))
-        
-        return tgtFilePathName
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "engine": Interface to (external) DS engines
+
+import sys
+import os
+import re
+import pathlib as pl
+import copy
+import tempfile
+
+from collections import namedtuple as ntuple
+
+import subprocess as sproc
+
+import numpy as np
+import pandas as pd
+
+from . import log, runtime
+
+logger = log.logger('ads.eng', level=log.INFO)  # Initial config (can be changed later)
+
+from .executor import Executor
+
+# Keep ruin'dows from opening a GPF dialog box every time a launched executable (like MCDS.exe) crashes !
+# BUT: This does NOT work :-(
+if sys.platform.startswith('win'):
+    import ctypes
+    import msvcrt
+    ctypes.windll.kernel32.SetErrorMode(msvcrt.SEM_NOGPFAULTERRORBOX)
+
+# Actual package install dir.
+KInstDirPath = pl.Path(__file__).parent.resolve()
+
+
+# DSEngine (abstract) classes.
+# An engine for running multiple DS analyses with same options (engine ctor params),
+# but various parameters (submitAnalysis() parameters), possibly as parallel threads / processes.
+# Warning: No option change allowed while started analyses are running / all their getResults() have returned.
+class DSEngine(object):
+    
+    # Options possible values.
+    DistUnits = ['Meter', 'Kilometer', 'Mile', 'Inch', 'Feet', 'Yard', 'Nautical mile']
+    AreaUnits = ['Hectare', 'Acre'] + ['Sq. ' + distUnit for distUnit in DistUnits]
+    
+    # Forbidden chars in workDir path name (Distance DS engines are real red necks)
+    # TODO: stronger protection (more special chars ? more generic method, through re ?)
+    ForbidPathChars = [' ', '(', ')', ',']
+    
+    # Distance software detection params.
+    DistanceMajorVersions = [7, 6]  # Lastest first.
+    DistanceInstDirNameFmt = 'Distance {majorVersion}'
+    DistancePossInstPaths = map(pl.Path, ['C:/Program files (x86)', 'C:/Program files', 'C:/PortableApps', '.'])
+
+    # Find given executable installation dir.
+    # Note: MCDS.exe is an autonomous executable : simply put it in a "Distance 7" sub-folder
+    #       of this package's one, and it'll work ! Or else install Distance 7 (or later) the normal way :-)
+    @staticmethod
+    def findExecutable(exeFileName):
+
+        exeFilePathName = None
+        logger.debug('Looking for {} ...'.format(exeFileName))
+        for ver in DSEngine.DistanceMajorVersions:
+            for path in DSEngine.DistancePossInstPaths:
+                exeFN = path / DSEngine.DistanceInstDirNameFmt.format(majorVersion=ver) / exeFileName
+                if not exeFN.exists():
+                    logger.info2('  Checking {} : No,'.format(exeFN))
+                else:
+                    logger.info('Found {} here: {}.'.format(exeFileName, exeFN))
+                    exeFilePathName = exeFN
+                    break
+            if exeFilePathName:
+                break
+
+        if not exeFilePathName:
+            raise Exception('Could not find {} ; please install Distance software (V6 or later)'.format(exeFileName))
+
+        runtime.update({'DS engine': exeFilePathName.as_posix()})
+            
+        return exeFilePathName
+    
+    # Specifications of output stats.
+    DfStatRowSpecs, DfStatModSpecs, DfStatModNotes, MIStatModCols, DfStatModColTrans = None, None, None, None, None
+    
+    def __init__(self, workDir='.', executor=None, runMethod='subprocess.run', timeOut=None,
+                 distanceUnit='Meter', areaUnit='Hectare', **options):
+
+        """Ctor
+        :param workDir: As a simple str, or a pl.Path
+        :param executor: Executor object to use (None => a sequential one will be auto-generated)
+        :param runMethod: for calling engine executable : 'os.system' or 'subprocess.run'
+        :param timeOut: engine call time limit (s) ; None => no limit ;
+                WARNING: Not implemented (no way) for 'os.system' run method (see MCDSAnalysis for this)
+        """
+
+        # Check base options
+        assert distanceUnit in self.DistUnits, \
+               'Invalid distance unit {}: should be in {}'.format(distanceUnit, self.DistUnits)
+        assert areaUnit in self.AreaUnits, \
+               'Invalid area unit {}: should be in {}'.format(areaUnit, self.AreaUnits)
+        
+        # Save specific options (as a named tuple for easier use through dot operator).
+        options = copy.deepcopy(options)
+        options.update(distanceUnit=distanceUnit, areaUnit=areaUnit)
+        self.OptionsClass = ntuple('Options', options.keys())
+        self.options = self.OptionsClass(**options) 
+        
+        # Set executor for runAnalysis().
+        self.executor = executor if executor is not None else Executor()
+        assert timeOut is None or runMethod != 'os.system' or self.executor.isAsync(), \
+               f"Can't care about {timeOut}s execution time limit" \
+               " with a non-asynchronous executor and os.system run method"
+
+        # Parameters for engine subprocess creation.
+        self.runMethod = runMethod
+        self.timeOut = timeOut
+        
+        # Check and prepare workdir if needed, and save.
+        assert all(c not in str(workDir) for c in self.ForbidPathChars), \
+               'Invalid character from "{}" in workDir folder "{}"' \
+               .format(''.join(self.ForbidPathChars), workDir)
+        self.workDir = pl.Path(workDir)
+        self.workDir.mkdir(exist_ok=True)
+        logger.info('DSEngine work folder: {}'.format(self.workDir.absolute().as_posix()))
+    
+    # Possible regexps for auto-detection of columns to import (into Distance / MCDS) from data sets / exports
+    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK).
+    ImportFieldAliasREs = \
+        {'STR_LABEL':  ['region', 'zone', 'secteur', 'strate', 'stratum'],
+         'STR_AREA':   ['surface', 'area', 'ha', 'km2'],
+         'SMP_LABEL':  ['point', 'lieu', 'location', 'transect'],
+         'SMP_EFFORT': ['effort', 'passes', 'surveys', 'samplings', 'longueur', 'length'],
+         'DISTANCE':   ['dist'],
+         'ANGLE':      ['angl', 'azim', 'direct'],
+         'SIZE':       ['nombre', 'nb', 'indiv', 'obj', 'tail', 'num', 'siz']}  # Cluster size
+    
+    # Data fields of decimal type.
+    # TODO: Complete for non-'Point transect' modes
+    DecimalFields = ['SMP_EFFORT', 'DISTANCE', 'ANGLE']
+    
+    # Match srcFields with tgtAliasREs ones ; keep remaining ones ; sort decimal fields.
+    @classmethod
+    def matchDataFields(cls, srcFields, tgtAliasREs={}):
+        
+        logger.debug('Matching required data columns:')
+        
+        # Try and match required data columns.
+        matFields = list()
+        matDecFields = list()
+        for tgtField in tgtAliasREs:
+            logger.debug1(' * ' + tgtField + ':')
+            foundTgtField = False
+            for srcField in srcFields:
+                for pat in tgtAliasREs[tgtField]:
+                    if re.search(pat, srcField, flags=re.IGNORECASE):
+                        logger.debug2('  . ' + srcField)
+                        matFields.append(srcField)
+                        if tgtField in cls.DecimalFields:
+                            matDecFields.append(srcField)
+                        foundTgtField = True
+                        break
+                if foundTgtField:
+                    break
+            if not foundTgtField:
+                raise KeyError('Could not find a match for expected field {} in sample data set columns [{}]'
+                               .format(tgtField, ', '.join(srcFields)))
+        
+        # Extra fields.
+        extFields = [field for field in srcFields if field not in matFields]
+
+        logger.debug('... success.')
+        
+        return matFields, matDecFields, extFields
+
+    # Setup a thread & process safe run folder for an analysis
+    # * runPrefix : user-friendly prefix for the generated folder-name (may be None)
+    # Note: Not a class method because it uses self.workDir
+    def setupRunFolder(self, runPrefix=None):
+
+        # MCDS does not support folder and file names with spaces inside ...
+        # And one never knows ... remove any other special chars also.
+        if runPrefix is None:
+            runPrefix = ''
+        else:
+            runPrefix = runPrefix.translate(str.maketrans({c: '' for c in ' ,.:;()/'})) + '-'
+
+        return pl.Path(tempfile.mkdtemp(dir=self.workDir, prefix=runPrefix))
+
+    # Engine in/out file names.
+    CmdFileName = 'cmd.txt'
+    DataFileName = 'data.txt'
+    OutputFileName = 'output.txt'
+    LogFileName = 'log.txt'
+    StatsFileName = 'stats.txt'
+    PlotsFileName = 'plots.txt'
+        
+    # Shutdown : release any used resource.
+    # Post-condition: Instance can no more run analyses.
+    def shutdown(self, executor=True):
+    
+        if executor and self.executor is not None:
+            self.executor.shutdown()
+        self.executor = None
+   
+    def __del__(self):
+    
+        self.shutdown()
+
+
+# MCDS engine (Conventional Distance Sampling)
+class MCDSEngine(DSEngine):
+    
+    # Possible suervy types and distance types.
+    SurveyTypes = ['Point', 'Line']
+    DistTypes = ['Radial', 'Perpendicular', 'Radial & Angle']
+    
+    # First data fields in exports for distance / MCDS importing
+    # (N/A ones may get removed before use, according to distance type and clustering options).
+    FirstDataFields = {'Point': ['STR_LABEL', 'STR_AREA', 'SMP_LABEL', 'SMP_EFFORT', 'DISTANCE', 'SIZE'],
+                       'Line': ['STR_LABEL', 'STR_AREA', 'SMP_LABEL', 'SMP_EFFORT', 'DISTANCE', 'ANGLE', 'SIZE']}
+
+    # Estimator key functions (Order: Distance .chm doc, "MCDS Engine Stats File", note 2 below second table).
+    EstKeyFns = ['UNIFORM', 'HNORMAL', 'NEXPON', 'HAZARD']
+    EstKeyFnDef = EstKeyFns[0]
+
+    # Estimator adjustment series (Order: Distance .chm doc, "MCDS Engine Stats File", note 3 below second table).
+    EstAdjustFns = ['POLY', 'HERMITE', 'COSINE']
+    EstAdjustFnDef = EstAdjustFns[0]
+
+    # Estimator adjustment terms selection criterion.
+    EstCriteria = ['AIC', 'AICC', 'BIC', 'LR']
+    EstCriterionDef = EstCriteria[0]
+    
+    # Estimator confidence value for output interval.
+    EstCVIntervalDef = 95  # %
+    
+    # Distance truncation / cut points parameters.
+    DistMinDef = None  # No left truncation (min = 0)
+    DistMaxDef = None  # No right truncation (max = max. distance observed)
+    DistFitCutsDef = None  # Model fitting : Automatic engine distance cuts determination.
+    DistDiscrCutsDef = None  # Distance values discretisation : None (keep exact values).
+
+    # Executable 
+    ExeFilePathName = DSEngine.findExecutable('MCDS.exe')
+
+    # Output stats specs : load from external files (extracts from Distance doc).
+    @classmethod
+    def loadStatSpecs(cls, nMaxAdjParams=10):
+        
+        if MCDSEngine.DfStatRowSpecs is not None:
+            return
+        
+        logger.debug('MCDS : Loading output stats specs ...')
+        
+        # Output stats row specifications
+        fileName = KInstDirPath / 'mcds/stat-row-specs.txt'
+        logger.debug1('* {}'.format(fileName))
+        with open(fileName, mode='r', encoding='utf8') as fStatRowSpecs:
+            statRowSpecLines = [line.rstrip('\n') for line in fStatRowSpecs.readlines() if not line.startswith('#')]
+            statRowSpecs = [(statRowSpecLines[i].strip(), statRowSpecLines[i+1].strip())
+                            for i in range(0, len(statRowSpecLines)-2, 3)]
+            cls.DfStatRowSpecs = pd.DataFrame(columns=['Name', 'Description'],
+                                              data=statRowSpecs).set_index('Name')
+            assert not cls.DfStatRowSpecs.empty, 'Empty MCDS stats row specs'
+        
+        # Module and stats number to description table
+        fileName = KInstDirPath / 'mcds/stat-mod-specs.txt'
+        logger.debug1('* {}'.format(fileName))
+        with open(fileName, mode='r', encoding='utf8') as fStatModSpecs:
+            statModSpecLines = [line.rstrip('\n') for line in fStatModSpecs.readlines() if not line.startswith('#')]
+            reModSpecNumName = re.compile('(.+) – (.+)')
+            statModSpecs = list()
+            moModule = None
+            for line in statModSpecLines:
+                if not line:
+                    continue
+                if moModule is None:
+                    moModule = reModSpecNumName.match(line.strip())
+                    continue
+                if line == ' ':
+                    moModule = None
+                    continue
+                moStatistic = reModSpecNumName.match(line.strip())
+                modNum, modDesc, statNum, statDescNotes = \
+                    moModule.group(1), moModule.group(2), moStatistic.group(1), moStatistic.group(2)
+                for i in range(len(statDescNotes)-1, -1, -1):
+                    if not re.match('[\d ,]', statDescNotes[i]):
+                        statDesc = statDescNotes[:i+1]
+                        statNotes = statDescNotes[i+1:].replace(' ', '')
+                        break
+                modNum = int(modNum)
+                if statNum.startswith('101 '):
+                    for num in range(nMaxAdjParams):  # Assume no more than that ... a bit hacky !
+                        statModSpecs.append((modNum, modDesc, 101+num,  # Make statDesc unique for later indexing
+                                             statDesc.replace('each', 'A({})'.format(num+1)), statNotes))
+                else:
+                    statNum = int(statNum)
+                    if modNum == 2 and statNum == 3:  # Actually, there are 0 or 3 of these ...
+                        for num in range(3):
+                            statModSpecs.append((modNum, modDesc, num+201,
+                                                 # Change statNum & Make statDesc unique for later indexing
+                                                 statDesc+' (distance set {})'.format(num+1), statNotes))
+                    else:
+                        statModSpecs.append((modNum, modDesc, statNum, statDesc, statNotes))
+            cls.DfStatModSpecs = pd.DataFrame(columns=['modNum', 'modDesc', 'statNum', 'statDesc', 'statNotes'],
+                                              data=statModSpecs).set_index(['modNum', 'statNum'])
+            assert not cls.DfStatModSpecs.empty, 'Empty MCDS stats module specs'
+        
+        # Produce a MultiIndex for output stats as columns in the desired order.
+        indexItems = list()
+        for lbl, modDesc, statDesc, statNotes in cls.DfStatModSpecs[['modDesc', 'statDesc', 'statNotes']].itertuples():
+            indexItems.append((modDesc, statDesc, 'Value'))
+            if '1' in statNotes:
+                indexItems += [(modDesc, statDesc, valName) for valName in ['Cv', 'Lcl', 'Ucl', 'Df']]
+        cls.MIStatModCols = pd.MultiIndex.from_tuples(indexItems)
+    
+        # Notes about stats.
+        fileName = KInstDirPath / 'mcds/stat-mod-notes.txt'
+        logger.debug1('* {}'.format(fileName))
+        with open(fileName, mode='r', encoding='utf8') as fStatModNotes:
+            statModNoteLines = [line.rstrip('\n') for line in fStatModNotes.readlines() if not line.startswith('#')]
+            statModNotes = [(int(line[:2]), line[2:].strip()) for line in statModNoteLines if line]
+            cls.DfStatModNotes = pd.DataFrame(data=statModNotes, columns=['Note', 'Text']).set_index('Note')
+            assert not cls.DfStatModNotes.empty, 'Empty MCDS stats module notes'
+            
+        # DataFrame for translating 3-level multi-index columns to 1 level lang-translated columns
+        fileName = KInstDirPath / 'mcds/stat-mod-trans.txt'
+        logger.debug1('* {}'.format(fileName))
+        cls.DfStatModColsTrans = pd.read_csv(fileName, sep='\t')
+        cls.DfStatModColsTrans.set_index(['Module', 'Statistic', 'Figure'], inplace=True)
+        
+        logger.debug('MCDS : Loaded output stats specs.')
+        
+    # Accessors to class variables.
+    @classmethod
+    def statRowSpecs(cls):
+        
+        if cls.DfStatRowSpecs is None:
+            cls.loadStatSpecs()    
+
+        return cls.DfStatRowSpecs
+        
+    @classmethod
+    def statModSpecs(cls):
+        
+        if cls.DfStatModSpecs is None:
+            cls.loadStatSpecs()    
+
+        return cls.DfStatModSpecs
+        
+    @classmethod
+    def statModCols(cls):
+        
+        if cls.MIStatModCols is None:
+            cls.loadStatSpecs()    
+
+        return cls.MIStatModCols
+        
+    @classmethod
+    def statModNotes(cls):
+        
+        if cls.DfStatModNotes is None:
+            cls.loadStatSpecs()    
+
+        return cls.DfStatModNotes
+        
+    @classmethod
+    def statModColTrans(cls):
+        
+        if cls.DfStatModColsTrans is None:
+            cls.loadStatSpecs()    
+
+        return cls.DfStatModColsTrans
+
+    # Sample stats columns
+    MIStatSampCols = pd.MultiIndex.from_tuples([('sample stats', 'total number of observations', 'Value'),
+                                                ('sample stats', 'minimal observation distance', 'Value'),
+                                                ('sample stats', 'maximal observation distance', 'Value')])
+
+    @classmethod
+    def statSampCols(cls):
+        
+        return cls.MIStatSampCols
+    
+    # Sample stats columns translation
+    _DfStatSampColsTrans = \
+        pd.DataFrame(index=MIStatSampCols,
+                     data=dict(en=['NTot Obs', 'Min Dist', 'Max Dist'], fr=['NTot Obs', 'Min Dist', 'Max Dist']))
+
+    @classmethod
+    def statSampColTrans(cls):
+
+        return cls._DfStatSampColsTrans
+
+    def __init__(self, workDir='.', executor=None, runMethod='subprocess.run', timeOut=None,
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False):
+        
+        """Ctor.
+        :param workDir: As a simple str, or a pl.Path
+        :param executor: Executor object to use (None => a sequential one will be auto-generated)
+        :param runMethod: Method used to run the MCDS executable : os.system or subprocess.run
+        :param timeOut: Time-out (s) for analysis execution (None => no limit);
+                        WARNING: NOT implemented here when 'os.system' runMethod ... see MCDSAnalysis
+        """
+
+        # Initialize dynamic class variables.
+        MCDSEngine.loadStatSpecs()    
+
+        # Check options
+        assert surveyType in self.SurveyTypes, \
+               'Invalid survey type {} : should be in {}'.format(surveyType, self.SurveyTypes)
+        assert distanceType in self.DistTypes, \
+               'Invalid distance type {} : should be in {}'.format(distanceType, self.DistTypes)
+        
+        # Specialise class level regexps for matching import fields,
+        # according to distance type and clustering options
+        self.importFieldAliasREs = self.ImportFieldAliasREs.copy()
+        if not clustering:
+            del self.importFieldAliasREs['SIZE']
+        if distanceType != 'Radial & Angle':
+            del self.importFieldAliasREs['ANGLE']            
+    
+        # Initialise base.
+        firstDataFields = [fld for fld in self.FirstDataFields[surveyType] if fld in self.importFieldAliasREs]
+        super().__init__(workDir=workDir, executor=executor, runMethod=runMethod, timeOut=timeOut,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
+                         firstDataFields=firstDataFields)
+                         
+    # Command file template (for str.format()ing).
+    CmdTxt = \
+        '\n'.join(map(str.strip,
+                  """{output}
+                     {log}
+                     {stats}
+                     {plots}
+                     {bootstrap}
+                     {bootpgrss}
+                     Options;
+                     Type={survType};
+                     Distance={distType} /Measure='{distUnit}';
+                     Area /Units='{areaUnit}';
+                     Object=Single;
+                     SF=1;
+                     Selection=Sequential;
+                     Lookahead=1;
+                     Maxterms=5;
+                     Confidence={cvInterv};
+                     print=Selection;
+                     End;
+                     Data /Structure=Flat;
+                     Fields={dataFields};
+                     Infile={dataFileName} /{echoData};
+                     End;
+                     Estimate;
+                     Distance{distDiscrSpecs};
+                     Density=All;
+                     Encounter=All;
+                     Detection=All;
+                     Size=All;
+                     Estimator /Key={estKeyFn} /Adjust={estAdjustFn} /Criterion={estCriterion};
+                     Monotone=Strict;
+                     Pick=AIC;
+                     GOF{gOFitSpecs};
+                     Cluster /Bias=GXLOG;
+                     VarN=Empirical;
+                     End;
+                  """.split('\n'))) + '\n'
+    
+    # Build command file from options and params
+    # * runDir : pl.Path where to create cmd file.
+    # Note: Not a classmethod because it uses self.options
+    def buildCmdFile(self, runDir, **params):
+
+        # Default params values
+        if 'logData' not in params:
+            params['logData'] = False
+        if 'estimKeyFn' not in params:
+            params['estimKeyFn'] = self.EstKeyFnDef
+        if 'estimAdjustFn' not in params:
+            params['estimAdjustFn'] = self.EstAdjustFnDef
+        if 'estimCriterion' not in params:
+            params['estimCriterion'] = self.EstCriterionDef
+        if 'cvInterval' not in params:
+            params['cvInterval'] = self.EstCVIntervalDef
+        if 'maxDist' not in params:
+            params['maxDist'] = self.DistMaxDef
+        if 'minDist' not in params:
+            params['minDist'] = self.DistMinDef
+        if 'fitDistCuts' not in params:
+            params['fitDistCuts'] = self.DistFitCutsDef
+        if 'discrDistCuts' not in params:
+            params['discrDistCuts'] = self.DistDiscrCutsDef
+
+        # Generate file contents
+        # a. Compute non-trivial data fields
+        distDiscrSpecs = ''
+        gOFitSpecs = ''
+        
+        minDist = params['minDist']
+        maxDist = params['maxDist']
+        fitDistCuts = params['fitDistCuts']
+        discrDistCuts = params['discrDistCuts']
+        if discrDistCuts is not None:
+            
+            if isinstance(discrDistCuts, list):
+                assert not (minDist is None or maxDist is None)
+                distDiscrSpecs += ' /Int=' + ','.join(format(d, 'g') for d in [minDist] + discrDistCuts + [maxDist])
+            elif isinstance(discrDistCuts, (int, float)):
+                distDiscrSpecs += ' /NClass=' + format(discrDistCuts, 'g')
+            # Other cases not supported, should be asserted by the caller.
+        
+        elif fitDistCuts is not None:  # Can't fit model on other distance intervals than used for discretisation.
+            
+            if isinstance(fitDistCuts, list):
+                assert not (minDist is None or maxDist is None)
+                gOFitSpecs += ' /Int=' + ','.join(format(d, 'g') for d in [minDist] + fitDistCuts + [maxDist])
+            elif isinstance(fitDistCuts, (int, float)):
+                gOFitSpecs += ' /NClass=' + format(fitDistCuts, 'g')
+            # Other cases not supported, should be asserted by the caller.
+                
+        if minDist is not None:
+            distDiscrSpecs += ' /Left=' + format(minDist, 'g')
+
+        if maxDist is not None:
+            distDiscrSpecs += ' /Width=' + format(maxDist, 'g')
+            
+        # b. Format contents string
+        cmdTxt = self.CmdTxt.format(output=runDir/self.OutputFileName, log=runDir/self.LogFileName,
+                                    stats=runDir/self.StatsFileName, plots=runDir/self.PlotsFileName,
+                                    bootstrap='None', bootpgrss='None',  # No support for the moment.
+                                    survType=self.options.surveyType, distType=self.options.distanceType,
+                                    distUnit=self.options.distanceUnit, areaUnit=self.options.areaUnit,
+                                    dataFields=','.join(self.options.firstDataFields),
+                                    dataFileName=runDir/self.DataFileName,
+                                    echoData=('' if params['logData'] else 'No') + 'Echo',
+                                    estKeyFn=params['estimKeyFn'], estAdjustFn=params['estimAdjustFn'],
+                                    estCriterion=params['estimCriterion'], cvInterv=params['cvInterval'],
+                                    distDiscrSpecs=distDiscrSpecs, gOFitSpecs=gOFitSpecs)
+
+        # Write file.
+        cmdFileName = runDir / self.CmdFileName
+        with open(cmdFileName, mode='w', encoding='utf-8') as cmdFile:
+            cmdFile.write(cmdTxt)
+
+        logger.debug('Commands written to {}'.format(cmdFileName))
+
+        # Done.
+        return cmdFileName
+    
+    # Workaround pd.DataFrame.to_csv(float_format='%.xf') not working when NaNs in serie
+    @staticmethod
+    def safeFloat2Str(val, prec=None, decPt='.'):
+        strVal = '' if pd.isnull(val) else str(val) if prec is None \
+                    else '{:.{prec}f}'.format(val, prec=prec)
+        if decPt != '.':
+            strVal = strVal.replace('.', decPt)
+        return strVal
+
+    # Build input data table from a sample data set (check and match mandatory columns, enforce order).
+    # TODO: Add support for covariate columns (through extraFields)
+    def buildExportTable(self, sampleDataSet, withExtraFields=True, decPoint='.'):
+        
+        # Match sampleDataSet table columns to MCDS expected fields from possible aliases
+        matchFields, matchDecFields, extraFields = \
+            self.matchDataFields(sampleDataSet.dfData.columns, self.importFieldAliasREs)
+        exportFields = matchFields
+        if withExtraFields:
+            exportFields += extraFields
+        else:
+            extraFields.clear()
+        
+        logger.debug2('Final data columns export order: ' + str(exportFields))
+        
+        # Put columns in the right order (first data fields ... first, in the same order)
+        dfExport = sampleDataSet.dfData[exportFields].copy()
+
+        # Prepare safe export of decimal data with may be some NaNs
+        allDecFields = set(matchDecFields + sampleDataSet.decimalFields).intersection(exportFields)
+        logger.debug2('Decimal columns: ' + str(allDecFields))
+        for field in allDecFields:
+            dfExport[field] = dfExport[field].apply(self.safeFloat2Str, decPt=decPoint)
+                
+        return dfExport, extraFields
+
+    # Build MCDS input data file from a sample data set.
+    # Note: Data = sighting order not changed, same as in input sampleDataSet !
+    # * runDir : pl.Path where to create data file.
+    # TODO: Add support for covariate columns (through extraFields)
+    def buildDataFile(self, runDir, sampleDataSet):
+        
+        # Build data to export (check and match mandatory columns, enforce order, ignore extra cols).
+        dfExport, extraFields = \
+            self.buildExportTable(sampleDataSet, withExtraFields=False, decPoint='.')
+        
+        # Export.
+        dataFileName = runDir / self.DataFileName
+        dfExport.to_csv(dataFileName, index=False, sep='\t', encoding='utf-8', header=None)
+        
+        logger.debug('Data MCDS-exported to {}'.format(dataFileName))
+        
+        return dataFileName
+    
+    # Run status codes (from MCDS documentation)
+    RCNotRun = 0
+    RCOK = 1
+    RCWarnings = 2
+    RCErrors = 3
+    RCFileErrors = 4
+    RCOtherErrors = 5  # and above, straight from MCDS.exe.
+    RCTimedOut = 555  # as named (through subprocess or concurrent.futures modules)
+    
+    @classmethod
+    def wasRun(cls, runCode):
+        return runCode != cls.RCNotRun
+    
+    @classmethod
+    def success(cls, runCode):
+        return runCode == cls.RCOK
+    
+    @classmethod
+    def warnings(cls, runCode):
+        return runCode == cls.RCWarnings
+    
+    @classmethod
+    def errors(cls, runCode):
+        return runCode >= cls.RCErrors
+    
+    @classmethod
+    def _runThroughOSSystem(cls, execFileName, cmdFileName, forReal=True):
+
+        """Run MCDS command through os.system
+
+        Under Ruin'dows, this means an intermediate "cmd.exe" subprocess.
+        """
+
+        # Call executable (no " around cmdFile, don't forget the space after ',', ...)
+        cmd = '"{}" 0, {}'.format(execFileName, cmdFileName)
+        if forReal:
+            logger.info1(f'Running MCDS through os.system({cmd}) ...')
+            startTime = pd.Timestamp.now()
+            status = os.system(cmd)
+            elapsedTime = (pd.Timestamp.now() - startTime).total_seconds()
+            logger.info2(f'... MCDS done : status={status}, elapsed={elapsedTime:.2f}s')
+            
+        # ... unless specified not to (input files generated, but no execution).
+        else:
+            logger.info1(f'NOT running MCDS through os.system({cmd}).')
+            startTime = pd.NaT
+            status = cls.RCNotRun
+            elapsedTime = 0
+
+        return status, startTime, elapsedTime
+
+    # MCDS.exe subprocess creation flags under ruin'dows: no window please !
+    # BUT: Does not help with fucking Ruin'dows crash window ... alas !
+    ExeCrFlags = sproc.CREATE_NO_WINDOW if sys.platform.startswith('win') else 0
+
+    @classmethod
+    def _runThroughSubProcessRun(cls, execFileName, cmdFileName, forReal=True, timeOut=None):
+
+        """Run MCDS command through subprocess.run
+
+        Under Ruin'dows, no "cmd.exe" intermediate subprocess, but only a conhost.exe.
+        """
+
+        # Call executable (no " around cmdFile, don't forget the space after ',', ...)
+        cmd = [str(execFileName), '0,', str(cmdFileName)]
+        if forReal:
+            logger.info1(f'Running MCDS through subprocess.run({cmd}, ) ...')
+            startTime = pd.Timestamp.now()
+            try:
+                proc = sproc.run(cmd, text=True, stdout=sproc.PIPE, stderr=sproc.STDOUT,
+                                 timeout=timeOut, creationflags=cls.ExeCrFlags)
+                status = proc.returncode
+                stdouterr = proc.stdout
+            except sproc.TimeoutExpired as toExc:
+                logger.error(f'MCDS timed out after {toExc.timeout:.2f}s')
+                status = cls.RCTimedOut
+                stdouterr = toExc.stdout
+            elapsedTime = (pd.Timestamp.now() - startTime).total_seconds()
+            logger.info3('MCDS stdout&err:')
+            logger.info3(stdouterr)
+            logger.info2(f'... MCDS done : status={status}, elapsed={elapsedTime:.2f}s')
+
+        # ... unless specified not to (input files generated, but no execution).
+        else:
+            logger.info1(f'NOT running MCDS through subprocess.run({cmd}).')
+            startTime = pd.NaT
+            status = cls.RCNotRun
+            elapsedTime = 0
+
+        return status, startTime, elapsedTime
+
+    @classmethod
+    def _run(cls, execFileName, cmdFileName, forReal=True, method='subprocess.run', timeOut=None):
+
+        """Run MCDS command through the given method
+        """
+
+        if method == 'os.system':
+            return cls._runThroughOSSystem(execFileName, cmdFileName, forReal=forReal)
+        elif method == 'subprocess.run':
+            return cls._runThroughSubProcessRun(execFileName, cmdFileName, forReal=forReal, timeOut=timeOut)
+
+        raise NotImplementedError(f'Unknown MCDSEngine run method "{method}"')
+
+    # Run 1 MCDS analysis from the beginning to the end (blocking for the calling thread)
+    # * runPrefix : user-friendly prefix for the generated folder-name (may be None)
+    def _runAnalysis(self, sampleDataSet, runPrefix='mcds', realRun=True, **analysisParms):
+        
+        # Create a new exclusive thread and process-safe run folder
+        anlysStartTime = pd.Timestamp.now()
+        runDir = self.setupRunFolder(runPrefix)
+        logger.debug('Will run in {}'.format(runDir))
+        
+        # Generate data and command files into this folder
+        logger.info2('MCDS analysis params: ' + str(analysisParms))
+        self.buildDataFile(runDir, sampleDataSet)
+        cmdFileName = self.buildCmdFile(runDir, **analysisParms)
+        
+        anlysElapsedTime = (pd.Timestamp.now() - anlysStartTime).total_seconds()
+
+        # Run executable as an OS sub-process.
+        runStatus, engStartTime, engElapsedTime = \
+            self._run(self.ExeFilePathName, cmdFileName, forReal=realRun,
+                      method=self.runMethod, timeOut=self.timeOut)
+        anlysElapsedTime += engElapsedTime
+
+        # Extract and decode results.
+        startTime = pd.Timestamp.now()
+
+        if self.success(runStatus) or self.warnings(runStatus):
+            sResults = self.decodeStats(runDir)
+        else:
+            sResults = None
+
+        anlysElapsedTime += (pd.Timestamp.now() - startTime).total_seconds()
+
+        return runStatus, anlysStartTime, anlysElapsedTime + engElapsedTime, runDir, sResults
+    
+    # Start running an MCDS analysis, using the executor (possibly asynchronously if it is not a sequential one)
+    def submitAnalysis(self, sampleDataSet, runPrefix='mcds', realRun=True, **analysisParms):
+        
+        # Check really implemented options
+        assert self.options.surveyType == 'Point', \
+               'Not yet implemented survey type {}'.format(self.options.surveyType)
+        assert self.options.distanceType == 'Radial', \
+               'Not yet implemented distance type {}'.format(self.options.distanceType)
+        
+        # Submit analysis work and return a Future object to ask from and wait for its results.
+        return self.executor.submit(self._runAnalysis, sampleDataSet, runPrefix, realRun, **analysisParms)
+    
+    # Decode output stats file to a value series
+    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
+    # * runDir : string or pl.Path where to find stats file.
+    # Warning: No support for more than 1 stratum, 1 sample, 1 estimator.
+    @classmethod
+    def decodeStats(cls, runDir):
+
+        statsFileName = pl.Path(runDir) / cls.StatsFileName
+        logger.debug('Decoding stats from {} ...'.format(statsFileName))
+        
+        # 1. Load table (text format, with space separated and fixed width columns,
+        #    columns headers from cls.DfStatRowSpecs)
+        dfStats = pd.read_csv(statsFileName, sep=' +', engine='python', names=cls.DfStatRowSpecs.index)
+        
+        # 2. Remove Stratum, Sample and Estimator columns (no support for multiple ones for the moment)
+        dfStats.drop(columns=['Stratum', 'Sample', 'Estimator'], inplace=True)
+        
+        # 3. Stack figure columns to rows, to get more comfortable
+        dfStats.set_index(['Module', 'Statistic'], append=True, inplace=True)
+        dfStats = dfStats.stack().reset_index()
+        dfStats.rename(columns={'level_0': 'id', 'level_3': 'Figure', 0: 'Value'}, inplace=True)
+
+        # 4. Fix multiple Module=2 & Statistic=3 rows (before joining with cls.DfStatModSpecs)
+        newStatNum = 200
+        for lbl, sRow in dfStats[(dfStats.Module == 2) & (dfStats.Statistic == 3)].iterrows():
+            if dfStats.loc[lbl, 'Figure'] == 'Value':
+                newStatNum += 1
+            dfStats.loc[lbl, 'Statistic'] = newStatNum
+        
+        # 5. Add descriptive / naming columns for modules and statistics,
+        #    from cls.DfStatModSpecs (more user friendly than numeric ids + help for detecting N/A figures)
+        dfStats = dfStats.join(cls.DfStatModSpecs, on=['Module', 'Statistic'])
+        
+        # 6. Check that supposed N/A figures (as told by cls.DfStatModSpecs.statNotes) are really such
+        #    Warning: There seems to be a bug in MCDS with Module=2 & Statistic=10x : some Cv values not always 0 ...
+        sKeepOnlyValueFig = ~dfStats.statNotes.apply(lambda s: pd.notnull(s) and '1' in s)
+        sFigs2Drop = (dfStats.Figure != 'Value') & sKeepOnlyValueFig
+        assert ~dfStats[sFigs2Drop & ((dfStats.Module != 2) | (dfStats.Statistic < 100))].Value.any(), \
+               'Warning: Some so-called "N/A" figures are not zeroes !'
+        
+        # 7. Remove so-called N/A figures
+        dfStats.drop(dfStats[sFigs2Drop].index, inplace=True)
+        
+        # 8. Make some values more readable.
+        lblKeyFn = (dfStats.Module == 2) & (dfStats.Statistic == 13)
+        dfStats.loc[lblKeyFn, 'Value'] = \
+            dfStats.loc[lblKeyFn, 'Value'].astype(int).apply(lambda n: cls.EstKeyFns[n-1])
+        lblAdjFn = (dfStats.Module == 2) & (dfStats.Statistic == 14)
+        dfStats.loc[lblAdjFn, 'Value'] = \
+            dfStats.loc[lblAdjFn, 'Value'].astype(int).apply(lambda n: cls.EstAdjustFns[n-1])
+        
+        # 9. Final indexing
+        dfStats = dfStats.reindex(columns=['modDesc', 'statDesc', 'Figure', 'Value'])
+        dfStats.set_index(['modDesc', 'statDesc', 'Figure'], inplace=True)
+
+        # That's all folks !
+        logger.debug('Done decoding from {}.'.format(statsFileName))
+        
+        return dfStats.T.iloc[0]
+
+    # Decode output log file to a string
+    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
+    # * runDir : string or pl.Path folder path-name where the analysis was run.
+    @classmethod
+    def decodeLog(cls, runDir):
+        
+        return dict(text=open(pl.Path(runDir) / cls.LogFileName).read().strip())
+    
+    # Decode output ... output file to a dict of chapters
+    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
+    # * runDir : string or pl.Path folder path-name where the analysis was run.
+    @classmethod
+    def decodeOutput(cls, runDir):
+        
+        outLst = open(pl.Path(runDir) / cls.OutputFileName).read().strip().split('\t')
+        
+        return [dict(id=title.translate(str.maketrans({c: '' for c in ' ,.-:()/'})),
+                     title=title.strip(), text=text.strip('\n'))
+                for title, text in [outLst[i:i+2] for i in range(0, len(outLst), 2)]]
+            
+    # Decode output plots file as a dict of plot dicts (key = output chapter title)
+    # Precondition: self.runAnalysis(...) was called and took place in :param:runDir
+    # * runDir : string or pl.Path folder path-name where the analysis was run.
+    @classmethod
+    def decodePlots(cls, runDir):
+        
+        dPlots = dict()
+        lines = (line.strip() for line in open(pl.Path(runDir) / cls.PlotsFileName, 'r').readlines())
+        for title in lines:
+            
+            title = title.strip()
+            subTitle = next(lines).strip()
+            xLabel = next(lines).strip()
+            yLabel = next(lines).strip()
+            xMin, xMax, yMin, yMax = [float(s) for s in next(lines).split()]
+            nDataRows = int(next(lines))
+            dataRows = list()
+            for _ in range(nDataRows):
+                dataRows.append([np.nan if '*' in s else float(s) for s in next(lines).split()])
+                
+            dPlots[title] = dict(title=title, subTitle=subTitle, dataRows=dataRows,  # nDataRows=nDataRows,
+                                 xLabel=xLabel, yLabel=yLabel, xMin=xMin, xMax=xMax, yMin=yMin, yMax=yMax)
+
+        return dPlots
+
+    @classmethod
+    def loadDataFile(cls, runDir):
+
+        runDir = pl.Path(runDir)
+        with open(runDir / cls.CmdFileName, 'r') as cmdFile:
+            fieldsLine = next(line for line in cmdFile.readlines() if line.startswith('Fields='))
+
+        dataCols = fieldsLine.strip('\n;')[len('Fields='):].split(',')
+
+        dataFilePathName = runDir / cls.DataFileName
+        dfData = pd.read_csv(dataFilePathName, sep='\t', names=dataCols)
+
+        logger.debug('Loaded {} rows with columns {} from MCDS data file {}.'
+                     .format(len(dfData), ','.join(dataCols), dataFilePathName.as_posix()))
+
+        return dfData
+
+    # Columns names for exporting to Distance import format with explicit columns headers
+    # (or for explicitating their contents in a standard way).
+    FirstDistanceExportFields = \
+        {'Point': dict(STR_LABEL='Region*Label', STR_AREA='Region*Area',
+                       SMP_LABEL='Point transect*Label', SMP_EFFORT='Point transect*Survey effort',
+                       DISTANCE='Observation*Radial distance',
+                       SIZE='Observation*Cluster size'),
+         'Line':  dict(STR_LABEL='Region*Label', STR_AREA='Region*Area',
+                       SMP_LABEL='Line transect*Label', SMP_EFFORT='Line transect*Line length',
+                       DISTANCE='Observation*Perp distance', ANGLE='Observation*Angle',
+                       SIZE='Observation*Cluster size')}
+ 
+    def distanceFields(self, dsFields):
+        return [self.FirstDistanceExportFields[self.options.surveyType][name] for name in dsFields]
+    
+    # Compute sample stats
+    def computeSampleStats(self, sampleDataSet):
+
+        # Match sampleDataSet table columns to MCDS expected fields from possible aliases
+        matchFields, _, _ = \
+            self.matchDataFields(sampleDataSet.dfData.columns, self.importFieldAliasREs)
+        
+        # Extract usefull columns and rename them tfor standard semantics.
+        dfSample = sampleDataSet.dfData[matchFields]
+        dfSample.columns = self.distanceFields(self.options.firstDataFields)
+
+        # Compute sample stats.
+        distCol = self.FirstDistanceExportFields[self.options.surveyType]['DISTANCE']
+        stats = [sum(dfSample[distCol].notnull()), dfSample[distCol].min(), dfSample[distCol].max()]
+
+        # Done
+        return pd.Series(data=stats, index=self.MIStatSampCols)
+
+    # Build Distance/MCDS input data file from a sample data set to given target folder and file name.
+    def buildDistanceDataFile(self, sampleDataSet, tgtFilePathName, decimalPoint=',', withExtraFields=False):
+                
+        # Build data to export (check and match mandatory columns, enforce order, keep other cols).
+        dfExport, extraFields = \
+            self.buildExportTable(sampleDataSet, withExtraFields=withExtraFields, decPoint=decimalPoint)
+                
+        # Export.
+        dfExport.to_csv(tgtFilePathName, index=False, sep='\t', encoding='utf-8',
+                        header=self.distanceFields(self.options.firstDataFields) + extraFields)
+
+        logger.debug('Data Distance-exported to {}.'.format(tgtFilePathName))
+        
+        return tgtFilePathName
```

### Comparing `pyaudisam-0.9.3/pyaudisam/executor.py` & `pyaudisam-1.0.1/pyaudisam/executor.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,209 +1,209 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "executor": Tools for easily running analyses sequentially or parallely
-
-import sys
-import os
-import concurrent.futures as cofu
-import warnings
-
-from . import log
-
-logger = log.logger('ads.exr')
-
-
-class ImmediateFuture(object):
-    
-    """Synchronous concurrent.futures.Future minimal and trivial implementation,
-       for use with SequentialExecutor
-    """
-
-    def __init__(self, result):
-        
-        self._result = result
-            
-    def result(self, timeout=None):
-        
-        return self._result
-
-    def exception(self, timeout=None):
-        
-        return None
-
-    def cancel(self):
-        
-        return False
-
-    def cancelled(self):
-        
-        return False
-
-    def running(self):
-        
-        return False
-
-    def done(self):
-        
-        return True
-
-
-class SequentialExecutor(cofu.Executor):
-
-    """Non-parallel concurrent.futures.Executor minimal implementation
-    """
-
-    def __init__(self):
-        
-        logger.info2('Started the SequentialExecutor.')
-            
-    def submit(self, func, *args, **kwargs):
-        
-        return ImmediateFuture(func(*args, **kwargs))  # Do it now !
-    
-    def map(self, func, *iterables, timeout=None, chunksize=1):
-        
-        return map(func, *iterables)  # Do it now !
-    
-    def shutdown(self, wait=True):
-        
-        pass
-        
-        
-class Executor(object):
-
-    """Wrapper class for simpler concurrent.futures.Executor interface,
-       and access to added non-parallel SequentialExecutor
-    """
-
-    # The only SequentialExecutor (only one needed)
-    TheSeqExor = None
-
-    def __init__(self, threads=None, processes=None,
-                 name_prefix='', mp_context=None, initializer=None, initargs=()):
-
-        """Ctor
-        
-        Parameters:
-        :param threads: Must be None or >= 0 ; 0 for auto-number (see expectedWorkers function) ;
-                        None or 1 for no actual parallelism = sequential execution,
-                        but 1 is slightly different, in that it means asynchronous call,
-                        whereas None means pure sequential calling ;
-                        if processes is not None, must be None (= unspecified)
-        :param processes: Must be None or >= 0 ; 0 for auto-number (see expectedWorkers function), ;
-                          None or 1 for no actual parallelism = sequential execution,
-                          but 1 is slightly different, in that it means asynchronous call,
-                          whereas None means pure sequential calling ;,
-                          if threads is not None, must be None (= unspecified)
-        :param name_prefix: See concurrent module (only for multi-threading)
-        :param mp_context: See concurrent module (only for multi-processing)
-        :param initializer: See concurrent module
-        :param initargs: See concurrent module
-        """
-        
-        assert (threads is None and (processes is None or processes >= 0)) \
-               or (processes is None and (threads is None or threads >= 0)), \
-               'An Executor can\'t implement multi-threading _and_ multi-processing at the same time'
-               
-        # Keep original parallelism (or not) specs for expectedWorkers().
-        self.threads = threads
-        self.processes = processes
-
-        # Create / Get the actual executor object.
-        self.realExor = None
-
-        if threads is not None:
-            self.realExor = \
-                cofu.ThreadPoolExecutor(max_workers=threads or None,
-                                        thread_name_prefix=name_prefix,
-                                        initializer=None, initargs=initargs)
-            logger.info1('Started a ThreadPoolExecutor(max_workers={})'.format(threads or 'None'))
-        
-        elif processes is not None:
-            self.realExor = \
-                cofu.ProcessPoolExecutor(max_workers=processes or None,
-                                         mp_context=mp_context,
-                                         initializer=initializer, initargs=initargs)
-            logger.info1('Started a ProcessPoolExecutor(max_workers={})'.format(processes or 'None'))
-                    
-        else:
-            if self.TheSeqExor is None:
-                self.TheSeqExor = SequentialExecutor()
-            self.realExor = self.TheSeqExor
-    
-    def expectedWorkers(self):
-
-        """Compute the theoretically expected to be used number of thread/process workers
-        of an Executor instance, from the specified number of threads / processes
-
-        Warning: Fully reports what's in the actual implementation of concurrent.futures.ThreadPoolExecutor
-                 and concurrent.futures.ProcessPoolExecutor : figures are verified only for python versions
-                 from 3.0 to 3.10pre
-        """
-
-        if sys.version_info.major < 3 or sys.version_info.minor < 5 or sys.version_info.minor > 10:
-            warnings.warn('Executor.expectedWorkers() may not report accurate figures as Python < 3.5 or > 3.10',
-                          RuntimeWarning)
-
-        if self.threads is None:
-            if self.processes is None:
-                return 1
-            elif self.processes == 0:
-                return os.cpu_count()
-            else:
-                return self.processes
-        elif self.threads == 0:
-            return 5 * os.cpu_count() if sys.version_info.minor < 8 else min(32, os.cpu_count() + 4)
-        else:
-            return self.threads
-
-    def isParallel(self):
-    
-        return self.realExor is not self.TheSeqExor and self.realExor._max_workers > 1
-    
-    def isAsync(self):
-    
-        return self.realExor is not self.TheSeqExor
-    
-    def submit(self, func, *args, **kwargs):
-    
-        assert self.realExor is not None, 'Can\'t submit after shutdown'
-        
-        return self.realExor.submit(func, *args, **kwargs)
-    
-    def map(self, func, *iterables, timeout=None, chunksize=1):
-        
-        return self.realExor.map(func, *iterables, timeout=timeout, chunksize=chunksize)
-        
-    def asCompleted(self, futures):
-    
-        return iter(futures) if isinstance(self.realExor, SequentialExecutor) \
-               else cofu.as_completed(futures)
-    
-    def shutdown(self, wait=True):
-              
-        if self.realExor is not None and self.realExor is not self.TheSeqExor:
-            logger.info2(self.realExor.__class__.__name__ + ' shut down.')
-            self.realExor.shutdown(wait=wait)
-        self.realExor = None
-        
-#    def __del__(self):
-#    
-#        self.shutdown()
-            
-
-if __name__ == '__main__':
-
-    sys.exit(0)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "executor": Tools for easily running analyses sequentially or parallely
+
+import sys
+import os
+import concurrent.futures as cofu
+import warnings
+
+from . import log
+
+logger = log.logger('ads.exr')
+
+
+class ImmediateFuture(object):
+    
+    """Synchronous concurrent.futures.Future minimal and trivial implementation,
+       for use with SequentialExecutor
+    """
+
+    def __init__(self, result):
+        
+        self._result = result
+            
+    def result(self, timeout=None):
+        
+        return self._result
+
+    def exception(self, timeout=None):
+        
+        return None
+
+    def cancel(self):
+        
+        return False
+
+    def cancelled(self):
+        
+        return False
+
+    def running(self):
+        
+        return False
+
+    def done(self):
+        
+        return True
+
+
+class SequentialExecutor(cofu.Executor):
+
+    """Non-parallel concurrent.futures.Executor minimal implementation
+    """
+
+    def __init__(self):
+        
+        logger.info2('Started the SequentialExecutor.')
+            
+    def submit(self, func, *args, **kwargs):
+        
+        return ImmediateFuture(func(*args, **kwargs))  # Do it now !
+    
+    def map(self, func, *iterables, timeout=None, chunksize=1):
+        
+        return map(func, *iterables)  # Do it now !
+    
+    def shutdown(self, wait=True):
+        
+        pass
+        
+        
+class Executor(object):
+
+    """Wrapper class for simpler concurrent.futures.Executor interface,
+       and access to added non-parallel SequentialExecutor
+    """
+
+    # The only SequentialExecutor (only one needed)
+    TheSeqExor = None
+
+    def __init__(self, threads=None, processes=None,
+                 name_prefix='', mp_context=None, initializer=None, initargs=()):
+
+        """Ctor
+        
+        Parameters:
+        :param threads: Must be None or >= 0 ; 0 for auto-number (see expectedWorkers function) ;
+                        None or 1 for no actual parallelism = sequential execution,
+                        but 1 is slightly different, in that it means asynchronous call,
+                        whereas None means pure sequential calling ;
+                        if processes is not None, must be None (= unspecified)
+        :param processes: Must be None or >= 0 ; 0 for auto-number (see expectedWorkers function), ;
+                          None or 1 for no actual parallelism = sequential execution,
+                          but 1 is slightly different, in that it means asynchronous call,
+                          whereas None means pure sequential calling ;,
+                          if threads is not None, must be None (= unspecified)
+        :param name_prefix: See concurrent module (only for multi-threading)
+        :param mp_context: See concurrent module (only for multi-processing)
+        :param initializer: See concurrent module
+        :param initargs: See concurrent module
+        """
+        
+        assert (threads is None and (processes is None or processes >= 0)) \
+               or (processes is None and (threads is None or threads >= 0)), \
+               'An Executor can\'t implement multi-threading _and_ multi-processing at the same time'
+               
+        # Keep original parallelism (or not) specs for expectedWorkers().
+        self.threads = threads
+        self.processes = processes
+
+        # Create / Get the actual executor object.
+        self.realExor = None
+
+        if threads is not None:
+            self.realExor = \
+                cofu.ThreadPoolExecutor(max_workers=threads or None,
+                                        thread_name_prefix=name_prefix,
+                                        initializer=None, initargs=initargs)
+            logger.info1('Started a ThreadPoolExecutor(max_workers={})'.format(threads or 'None'))
+        
+        elif processes is not None:
+            self.realExor = \
+                cofu.ProcessPoolExecutor(max_workers=processes or None,
+                                         mp_context=mp_context,
+                                         initializer=initializer, initargs=initargs)
+            logger.info1('Started a ProcessPoolExecutor(max_workers={})'.format(processes or 'None'))
+                    
+        else:
+            if self.TheSeqExor is None:
+                self.TheSeqExor = SequentialExecutor()
+            self.realExor = self.TheSeqExor
+    
+    def expectedWorkers(self):
+
+        """Compute the theoretically expected to be used number of thread/process workers
+        of an Executor instance, from the specified number of threads / processes
+
+        Warning: Fully reports what's in the actual implementation of concurrent.futures.ThreadPoolExecutor
+                 and concurrent.futures.ProcessPoolExecutor : figures are verified only for python versions
+                 from 3.0 to 3.10pre
+        """
+
+        if sys.version_info.major < 3 or sys.version_info.minor < 5 or sys.version_info.minor > 10:
+            warnings.warn('Executor.expectedWorkers() may not report accurate figures as Python < 3.5 or > 3.10',
+                          RuntimeWarning)
+
+        if self.threads is None:
+            if self.processes is None:
+                return 1
+            elif self.processes == 0:
+                return os.cpu_count()
+            else:
+                return self.processes
+        elif self.threads == 0:
+            return 5 * os.cpu_count() if sys.version_info.minor < 8 else min(32, os.cpu_count() + 4)
+        else:
+            return self.threads
+
+    def isParallel(self):
+    
+        return self.realExor is not self.TheSeqExor and self.realExor._max_workers > 1
+    
+    def isAsync(self):
+    
+        return self.realExor is not self.TheSeqExor
+    
+    def submit(self, func, *args, **kwargs):
+    
+        assert self.realExor is not None, 'Can\'t submit after shutdown'
+        
+        return self.realExor.submit(func, *args, **kwargs)
+    
+    def map(self, func, *iterables, timeout=None, chunksize=1):
+        
+        return self.realExor.map(func, *iterables, timeout=timeout, chunksize=chunksize)
+        
+    def asCompleted(self, futures):
+    
+        return iter(futures) if isinstance(self.realExor, SequentialExecutor) \
+               else cofu.as_completed(futures)
+    
+    def shutdown(self, wait=True):
+              
+        if self.realExor is not None and self.realExor is not self.TheSeqExor:
+            logger.info2(self.realExor.__class__.__name__ + ' shut down.')
+            self.realExor.shutdown(wait=wait)
+        self.realExor = None
+        
+#    def __del__(self):
+#    
+#        self.shutdown()
+            
+
+if __name__ == '__main__':
+
+    sys.exit(0)
```

### Comparing `pyaudisam-0.9.3/pyaudisam/log.py` & `pyaudisam-1.0.1/pyaudisam/log.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,225 +1,225 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "log": Thin wrapper above logging to get more debug and info levels, and easier configuration.
-
-import sys
-import pathlib as pl
-import logging
-from logging import NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL
-
-
-# Define new logging levels.
-DEBUG0 = DEBUG
-logging.addLevelName(DEBUG0, "DEBUG0")
-DEBUG1 = DEBUG - 1
-logging.addLevelName(DEBUG1, "DEBUG1")
-DEBUG2 = DEBUG - 2
-logging.addLevelName(DEBUG2, "DEBUG2")
-DEBUG3 = DEBUG - 3
-logging.addLevelName(DEBUG3, "DEBUG3")
-DEBUG4 = DEBUG - 4
-logging.addLevelName(DEBUG4, "DEBUG4")
-DEBUG5 = DEBUG - 5
-logging.addLevelName(DEBUG5, "DEBUG5")
-DEBUG6 = DEBUG - 6
-logging.addLevelName(DEBUG6, "DEBUG6")
-DEBUG7 = DEBUG - 7
-logging.addLevelName(DEBUG7, "DEBUG7")
-DEBUG8 = DEBUG - 8
-logging.addLevelName(DEBUG8, "DEBUG8")
-
-INFO0 = INFO
-logging.addLevelName(INFO0, "INFO0")
-INFO1 = INFO - 1
-logging.addLevelName(INFO1, "INFO1")
-INFO2 = INFO - 2
-logging.addLevelName(INFO2, "INFO2")
-INFO3 = INFO - 3
-logging.addLevelName(INFO3, "INFO3")
-INFO4 = INFO - 4
-logging.addLevelName(INFO4, "INFO4")
-INFO5 = INFO - 5
-logging.addLevelName(INFO5, "INFO5")
-INFO6 = INFO - 6
-logging.addLevelName(INFO6, "INFO6")
-INFO7 = INFO - 7
-logging.addLevelName(INFO7, "INFO7")
-INFO8 = INFO - 8
-logging.addLevelName(INFO8, "INFO8")
-
-
-class Logger(logging.Logger):
-
-    """A Logger class with methods associated to the new levels
-    """
-
-    Configured = False
-
-    def __init__(self, name):
-        super().__init__(name)
-
-    info0 = logging.Logger.info
-    
-    def info1(self, msg, *args, **kwargs):
-        self.log(INFO1, msg, *args, **kwargs)
-
-    def info2(self, msg, *args, **kwargs):
-        self.log(INFO2, msg, *args, **kwargs)
-
-    def info3(self, msg, *args, **kwargs):
-        self.log(INFO3, msg, *args, **kwargs)
-
-    def info4(self, msg, *args, **kwargs):
-        self.log(INFO4, msg, *args, **kwargs)
-
-    def info5(self, msg, *args, **kwargs):
-        self.log(INFO5, msg, *args, **kwargs)
-
-    def info6(self, msg, *args, **kwargs):
-        self.log(INFO6, msg, *args, **kwargs)
-
-    def info7(self, msg, *args, **kwargs):
-        self.log(INFO7, msg, *args, **kwargs)
-
-    def info8(self, msg, *args, **kwargs):
-        self.log(INFO8, msg, *args, **kwargs)
-
-    debug0 = logging.Logger.debug
-    
-    def debug1(self, msg, *args, **kwargs):
-        self.log(DEBUG1, msg, *args, **kwargs)
-
-    def debug2(self, msg, *args, **kwargs):
-        self.log(DEBUG2, msg, *args, **kwargs)
-
-    def debug3(self, msg, *args, **kwargs):
-        self.log(DEBUG3, msg, *args, **kwargs)
-
-    def debug4(self, msg, *args, **kwargs):
-        self.log(DEBUG4, msg, *args, **kwargs)
-
-    def debug5(self, msg, *args, **kwargs):
-        self.log(DEBUG5, msg, *args, **kwargs)
-
-    def debug6(self, msg, *args, **kwargs):
-        self.log(DEBUG6, msg, *args, **kwargs)
-
-    def debug7(self, msg, *args, **kwargs):
-        self.log(DEBUG7, msg, *args, **kwargs)
-
-    def debug8(self, msg, *args, **kwargs):
-        self.log(DEBUG8, msg, *args, **kwargs)
-
-    @staticmethod
-    def _handlerId(hdlr):
-        if isinstance(hdlr, pl.Path):
-            hdlr = hdlr.as_posix()
-        return 'File({})'.format(hdlr) if isinstance(hdlr, str) else 'Stream({})'.format(hdlr.name)
-
-    @staticmethod
-    def configure(loggers=[dict(name='child', level=logging.ERROR)],
-                  level=NOTSET, handlers=[sys.stdout], fileMode='w', verbose=False,
-                  format='%(asctime)s %(process)d %(name)s %(levelname)s\t%(message)s', reset=False):
-        
-        """Configure logging system, mainly the root logger (levels, handlers, formatter, ...)
-
-        Parameters:
-        :param loggers: if not None, list of dict(name, [level]) to apply
-        :param level: for root only, see logging.Logger.setLevel
-        :param handlers: a list of "handler specs" ; according to type,
-            * str / pathlib.Path: logging.FileHandler for given file path-name
-            * otherwise: StreamHandler (for sys.stdout and so on)
-            * None or empty list => use currently configured ones for root logger
-        :param fileMode: see logging.FileHandler ctor
-        :param format: see logging.Handler.setFormatter
-        :param verbose: if True, write a first INFO msg to the handlers' targets
-        :param reset: if True, hard cleanup logger config. (useful in jupyter notebooks)
-        """
-
-        # Configure root logger (assuming children have propagate=on).
-        # Note: Setting handlers for multiple children rather than once and for all for root ...
-        #        gives bad things on FileHandlers, with many missing / intermixed / unsorted lines ...
-        #        => unusable. Whereas it seems to work well with StreamHandlers
-        root = logging.getLogger()
-
-        if reset:
-            while root.handlers:
-                root.handlers.pop()
-     
-        formatter = logging.Formatter(format)
-        for hdlr in handlers:
-            if isinstance(hdlr, str):
-                handler = logging.FileHandler(hdlr, mode=fileMode)
-            elif isinstance(hdlr, pl.Path):
-                hdlr = hdlr.as_posix()
-                handler = logging.FileHandler(hdlr, mode=fileMode)
-            else:
-                handler = logging.StreamHandler(stream=hdlr)
-            handler.setFormatter(formatter)
-            root.addHandler(handler)
-        
-        if verbose:
-            msg = 'Logging to {}'.format(', '.join(Logger._handlerId(hdlr) for hdlr in handlers))
-            root.setLevel(INFO)
-            root.info(msg)
-
-        if not verbose or level != INFO:
-            root.setLevel(level)
-
-        # Configure children loggers.
-        for logrCfg in loggers:
-            logr = logging.getLogger(logrCfg['name'])
-            if verbose:
-                logr.info(msg)
-            if 'level' in logrCfg:
-                logr.setLevel(logrCfg['level'])
-
-        Logger.Configured = True
-
-    @staticmethod
-    def logger(name, level=None, reset=False):
-
-        """ Create, or retrieve, and eventually update the logger with given name.
-        
-        Parameters:
-        :param name: name of the target logger (see logging.getLogger)
-        :param level: if not None, level to set (see logging.Logger.setLevel)
-        :param reset: if True, hard cleanup of the logger config (useful in jupyter notebooks)
-        """
-        
-        if not Logger.Configured:
-            Logger.configure(level=INFO, reset=reset)
-
-        # Create / get logger.
-        logr = logging.getLogger(name)
-        
-        # Cleanup any default handler if any (ex: jupyter does some logging initialisation itself ...)
-        if reset:
-            while logr.handlers:
-                logr.handlers.pop()
-        
-        # Set level
-        if level is not None:
-            logr.setLevel(level)
-        
-        return logr
-
-
-logging.setLoggerClass(Logger)
-
-configure = Logger.configure
-
-logger = Logger.logger
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "log": Thin wrapper above logging to get more debug and info levels, and easier configuration.
+
+import sys
+import pathlib as pl
+import logging
+from logging import NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL
+
+
+# Define new logging levels.
+DEBUG0 = DEBUG
+logging.addLevelName(DEBUG0, "DEBUG0")
+DEBUG1 = DEBUG - 1
+logging.addLevelName(DEBUG1, "DEBUG1")
+DEBUG2 = DEBUG - 2
+logging.addLevelName(DEBUG2, "DEBUG2")
+DEBUG3 = DEBUG - 3
+logging.addLevelName(DEBUG3, "DEBUG3")
+DEBUG4 = DEBUG - 4
+logging.addLevelName(DEBUG4, "DEBUG4")
+DEBUG5 = DEBUG - 5
+logging.addLevelName(DEBUG5, "DEBUG5")
+DEBUG6 = DEBUG - 6
+logging.addLevelName(DEBUG6, "DEBUG6")
+DEBUG7 = DEBUG - 7
+logging.addLevelName(DEBUG7, "DEBUG7")
+DEBUG8 = DEBUG - 8
+logging.addLevelName(DEBUG8, "DEBUG8")
+
+INFO0 = INFO
+logging.addLevelName(INFO0, "INFO0")
+INFO1 = INFO - 1
+logging.addLevelName(INFO1, "INFO1")
+INFO2 = INFO - 2
+logging.addLevelName(INFO2, "INFO2")
+INFO3 = INFO - 3
+logging.addLevelName(INFO3, "INFO3")
+INFO4 = INFO - 4
+logging.addLevelName(INFO4, "INFO4")
+INFO5 = INFO - 5
+logging.addLevelName(INFO5, "INFO5")
+INFO6 = INFO - 6
+logging.addLevelName(INFO6, "INFO6")
+INFO7 = INFO - 7
+logging.addLevelName(INFO7, "INFO7")
+INFO8 = INFO - 8
+logging.addLevelName(INFO8, "INFO8")
+
+
+class Logger(logging.Logger):
+
+    """A Logger class with methods associated to the new levels
+    """
+
+    Configured = False
+
+    def __init__(self, name):
+        super().__init__(name)
+
+    info0 = logging.Logger.info
+    
+    def info1(self, msg, *args, **kwargs):
+        self.log(INFO1, msg, *args, **kwargs)
+
+    def info2(self, msg, *args, **kwargs):
+        self.log(INFO2, msg, *args, **kwargs)
+
+    def info3(self, msg, *args, **kwargs):
+        self.log(INFO3, msg, *args, **kwargs)
+
+    def info4(self, msg, *args, **kwargs):
+        self.log(INFO4, msg, *args, **kwargs)
+
+    def info5(self, msg, *args, **kwargs):
+        self.log(INFO5, msg, *args, **kwargs)
+
+    def info6(self, msg, *args, **kwargs):
+        self.log(INFO6, msg, *args, **kwargs)
+
+    def info7(self, msg, *args, **kwargs):
+        self.log(INFO7, msg, *args, **kwargs)
+
+    def info8(self, msg, *args, **kwargs):
+        self.log(INFO8, msg, *args, **kwargs)
+
+    debug0 = logging.Logger.debug
+    
+    def debug1(self, msg, *args, **kwargs):
+        self.log(DEBUG1, msg, *args, **kwargs)
+
+    def debug2(self, msg, *args, **kwargs):
+        self.log(DEBUG2, msg, *args, **kwargs)
+
+    def debug3(self, msg, *args, **kwargs):
+        self.log(DEBUG3, msg, *args, **kwargs)
+
+    def debug4(self, msg, *args, **kwargs):
+        self.log(DEBUG4, msg, *args, **kwargs)
+
+    def debug5(self, msg, *args, **kwargs):
+        self.log(DEBUG5, msg, *args, **kwargs)
+
+    def debug6(self, msg, *args, **kwargs):
+        self.log(DEBUG6, msg, *args, **kwargs)
+
+    def debug7(self, msg, *args, **kwargs):
+        self.log(DEBUG7, msg, *args, **kwargs)
+
+    def debug8(self, msg, *args, **kwargs):
+        self.log(DEBUG8, msg, *args, **kwargs)
+
+    @staticmethod
+    def _handlerId(hdlr):
+        if isinstance(hdlr, pl.Path):
+            hdlr = hdlr.as_posix()
+        return 'File({})'.format(hdlr) if isinstance(hdlr, str) else 'Stream({})'.format(hdlr.name)
+
+    @staticmethod
+    def configure(loggers=[dict(name='child', level=logging.ERROR)],
+                  level=NOTSET, handlers=[sys.stdout], fileMode='w', verbose=False,
+                  format='%(asctime)s %(process)d %(name)s %(levelname)s\t%(message)s', reset=False):
+        
+        """Configure logging system, mainly the root logger (levels, handlers, formatter, ...)
+
+        Parameters:
+        :param loggers: if not None, list of dict(name, [level]) to apply
+        :param level: for root only, see logging.Logger.setLevel
+        :param handlers: a list of "handler specs" ; according to type,
+            * str / pathlib.Path: logging.FileHandler for given file path-name
+            * otherwise: StreamHandler (for sys.stdout and so on)
+            * None or empty list => use currently configured ones for root logger
+        :param fileMode: see logging.FileHandler ctor
+        :param format: see logging.Handler.setFormatter
+        :param verbose: if True, write a first INFO msg to the handlers' targets
+        :param reset: if True, hard cleanup logger config. (useful in jupyter notebooks)
+        """
+
+        # Configure root logger (assuming children have propagate=on).
+        # Note: Setting handlers for multiple children rather than once and for all for root ...
+        #        gives bad things on FileHandlers, with many missing / intermixed / unsorted lines ...
+        #        => unusable. Whereas it seems to work well with StreamHandlers
+        root = logging.getLogger()
+
+        if reset:
+            while root.handlers:
+                root.handlers.pop()
+     
+        formatter = logging.Formatter(format)
+        for hdlr in handlers:
+            if isinstance(hdlr, str):
+                handler = logging.FileHandler(hdlr, mode=fileMode)
+            elif isinstance(hdlr, pl.Path):
+                hdlr = hdlr.as_posix()
+                handler = logging.FileHandler(hdlr, mode=fileMode)
+            else:
+                handler = logging.StreamHandler(stream=hdlr)
+            handler.setFormatter(formatter)
+            root.addHandler(handler)
+        
+        if verbose:
+            msg = 'Logging to {}'.format(', '.join(Logger._handlerId(hdlr) for hdlr in handlers))
+            root.setLevel(INFO)
+            root.info(msg)
+
+        if not verbose or level != INFO:
+            root.setLevel(level)
+
+        # Configure children loggers.
+        for logrCfg in loggers:
+            logr = logging.getLogger(logrCfg['name'])
+            if verbose:
+                logr.info(msg)
+            if 'level' in logrCfg:
+                logr.setLevel(logrCfg['level'])
+
+        Logger.Configured = True
+
+    @staticmethod
+    def logger(name, level=None, reset=False):
+
+        """ Create, or retrieve, and eventually update the logger with given name.
+        
+        Parameters:
+        :param name: name of the target logger (see logging.getLogger)
+        :param level: if not None, level to set (see logging.Logger.setLevel)
+        :param reset: if True, hard cleanup of the logger config (useful in jupyter notebooks)
+        """
+        
+        if not Logger.Configured:
+            Logger.configure(level=INFO, reset=reset)
+
+        # Create / get logger.
+        logr = logging.getLogger(name)
+        
+        # Cleanup any default handler if any (ex: jupyter does some logging initialisation itself ...)
+        if reset:
+            while logr.handlers:
+                logr.handlers.pop()
+        
+        # Set level
+        if level is not None:
+            logr.setLevel(level)
+        
+        return logr
+
+
+logging.setLoggerClass(Logger)
+
+configure = Logger.configure
+
+logger = Logger.logger
```

### Comparing `pyaudisam-0.9.3/pyaudisam/mcds/anlys.htpl` & `pyaudisam-1.0.1/pyaudisam/mcds/anlys.htpl`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,231 +1,231 @@
-<!DOCTYPE HTML>
-
-<!-- PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-     Copyright (C) 2021 Jean-Philippe Meuret
-     
-     This program is free software: you can redistribute it and/or modify it under the terms
-     of the GNU General Public License as published by the Free Software Foundation,
-     either version 3 of the License, or (at your option) any later version.
-     This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-     without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-     See the GNU General Public License for more details.
-     You should have received a copy of the GNU General Public License along with this program.
-     If not, see https://www.gnu.org/licenses/.
--->
-
-<head>
-
-  <meta charset="utf-8">
-  <meta name="author" content="Jean-Philippe Meuret"/>
-  <meta name="copyright" content="Jean-Philippe Meuret 2021"/>
-  <meta name="license" content="CC BY NC SA"/>
-  <meta name="description" content="{{title}}"/>
-  <meta name="keywords" content="distance, sampling, mcds, {{keywords}}"/>
-  <meta name="datetime" content="{{genDateTime}}"/>
-  
-  <title>{{title}}</title>
-  
-  <link rel="stylesheet" media="screen" type="text/css" href="../report.css">
-  
-  <script>
-    // Back to top floating button managment
-    // When the user scrolls down100 px from the top of the document, show the button
-    window.onscroll = function()
-    {
-    	if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100)
-    		document.getElementById("toTopBtn").style.display = "block";
-    	else
-    		document.getElementById("toTopBtn").style.display = "none";
-    }
-    // When the user clicks on the button, scroll to the top of the document
-    function scrollToTop()
-    {
-    	document.body.scrollTop = 0;
-    	document.documentElement.scrollTop = 0;
-    }
-  </script>
-  
-</head>
-
-<body>
-
-  <table id="title">
-    <tr>
-      <td style="font-size: 480%; text-align: center">{{title}}</td>
-    </tr>
-    <tr>
-      <td style="font-size: 240%; text-align: center">{{subtitle}}</td>
-    </tr>
-    <tr>
-      <td style="font-size: 120%; text-align: center">{{description}}</td>
-    </tr>
-  </table>
-  
-  <div style="margin-left: 10px">
-
-    <img class="center" height="32" style="margin-top: 30px"
-         src="../fa-feather-alt.svg" alt="---" />
-
-    <h2 id="synthesis">
-      {{tr['Main results']}}
-      <a href="./{{navUrls.prevAnlys}}">
-        <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
-             onmouseover="this.src='../fa-arrow-left-hover.svg';"
-             onmouseout="this.src='../fa-arrow-left.svg';"
-             title="{{tr['Previous analysis']}}"
-             alt="{{tr['Previous analysis']}}" /></a>
-      <a href="./{{navUrls.back2Top}}">
-        <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
-             onmouseover="this.src='../fa-arrow-up-hover.svg';"
-             onmouseout="this.src='../fa-arrow-up.svg';"
-             title="{{tr['Back to top']}}"
-             alt="{{tr['Back to top']}}" /></a>
-      <a href="./{{navUrls.nextAnlys}}">
-        <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
-             onmouseover="this.src='../fa-arrow-right-hover.svg';"
-             onmouseout="this.src='../fa-arrow-right.svg';"
-             title="{{tr['Next analysis']}}"
-             alt="{{tr['Next analysis']}}" /></a>
-    </h2>
-    <div class="chapter" style="margin-left: 10px">
-
-      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
-      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
-      <p>{{tr['Note: Some figures rounded, but not converted']}}</p>
-	  {{synthesis}}
-
-    </div>
-    
-    <img class="center" height="32" style="margin-top: 30px"
-         src="../fa-feather-alt.svg" alt="---" />
-
-    <h2 id="details">
-      {{tr['Detailed results']}}
-      <a href="./{{navUrls.prevAnlys}}">
-        <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
-             onmouseover="this.src='../fa-arrow-left-hover.svg';"
-             onmouseout="this.src='../fa-arrow-left.svg';"
-             title="{{tr['Previous analysis']}}"
-             alt="{{tr['Previous analysis']}}" /></a>
-      <a href="./{{navUrls.back2Top}}">
-        <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
-             onmouseover="this.src='../fa-arrow-up-hover.svg';"
-             onmouseout="this.src='../fa-arrow-up.svg';"
-             title="{{tr['Back to top']}}"
-             alt="{{tr['Back to top']}}" /></a>
-      <a href="./{{navUrls.nextAnlys}}">
-        <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
-             onmouseover="this.src='../fa-arrow-right-hover.svg';"
-             onmouseout="this.src='../fa-arrow-right.svg';"
-             title="{{tr['Next analysis']}}"
-             alt="{{tr['Next analysis']}}" /></a>
-    </h2>
-    <div class="chapter" style="margin-left: 10px">
-
-      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
-      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
-      <p>{{tr['Note: All figures untouched, as output by MCDS']}}</p>
-	  {{details}}
-
-    </div>
-    
-    <img class="center" height="32" style="margin-top: 30px"
-         src="../fa-feather-alt.svg" alt="---" />
-
-    <h2 id="log">
-      {{tr['Summary computation log']}}
-      <a href="./{{navUrls.prevAnlys}}">
-        <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
-             onmouseover="this.src='../fa-arrow-left-hover.svg';"
-             onmouseout="this.src='../fa-arrow-left.svg';"
-             title="{{tr['Previous analysis']}}"
-             alt="{{tr['Previous analysis']}}" /></a>
-      <a href="./{{navUrls.back2Top}}">
-        <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
-             onmouseover="this.src='../fa-arrow-up-hover.svg';"
-             onmouseout="this.src='../fa-arrow-up.svg';"
-             title="{{tr['Back to top']}}"
-             alt="{{tr['Back to top']}}" /></a>
-      <a href="./{{navUrls.nextAnlys}}">
-        <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
-             onmouseover="this.src='../fa-arrow-right-hover.svg';"
-             onmouseout="this.src='../fa-arrow-right.svg';"
-             title="{{tr['Next analysis']}}"
-             alt="{{tr['Next analysis']}}" /></a>
-    </h2>
-    <div class="chapter" style="margin-left: 10px">
-
-	  <pre>{{log.text}}</pre>
-
-    </div>
-    
-    <img class="center" height="32" style="margin-top: 30px"
-         src="../fa-feather-alt.svg" alt="---" />
-
-    <h2 id="log">{{tr['Detailed computation log']}}</h2>
-    <div style="margin-left: 10px">
-
-      {% for ochap in output %}
-  
-        <h3 id="{{ochap.id}}">
-          {{ochap.title}}
-          <a href="./{{navUrls.prevAnlys}}">
-            <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
-                 onmouseover="this.src='../fa-arrow-left-hover.svg';"
-                 onmouseout="this.src='../fa-arrow-left.svg';"
-                 title="{{tr['Previous analysis']}}"
-                 alt="{{tr['Previous analysis']}}" /></a>
-          <a href="./{{navUrls.back2Top}}">
-            <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
-                 onmouseover="this.src='../fa-arrow-up-hover.svg';"
-                 onmouseout="this.src='../fa-arrow-up.svg';"
-                 title="{{tr['Back to top']}}"
-                 alt="{{tr['Back to top']}}" /></a>
-          <a href="./{{navUrls.nextAnlys}}">
-            <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
-                 onmouseover="this.src='../fa-arrow-right-hover.svg';"
-                 onmouseout="this.src='../fa-arrow-right.svg';"
-                 title="{{tr['Next analysis']}}"
-                 alt="{{tr['Next analysis']}}" /></a>
-        </h3>
-        <div class="chapter" style="margin-left: 10px">
-  	  
-          {% if ochap.title in plots %}
-            <img src="./{{plots[ochap.title]}}"/>
-          {% else %}
-            <pre>{{ochap.text}}<pre>
-          {% endif %}
-  	  
-        </div>
-  
-      {% endfor %} <!-- ochap in output -->
-
-    </div>
-    
-    <h6 style="margin-bottom: 10px">
-      {{tr['Page generated']}} {{tr['on']}} {{genDateTime}}
-      {{tr['with']}} <a href="https://www.python.org/" target="_blank">Python 3</a>,
-      <a href="https://numpy.org/" target="_blank">NumPy {{libVersions['NumPy']}}</a>,
-      <a href="https://pandas.pydata.org/" target="_blank">Pandas</a>,
-      <a href="https://github.com/polixir/ZOOpt/" target="_blank">ZOOpt {{libVersions['ZOOpt']}}</a>,
-      <a href="https://matplotlib.org/" target="_blank">Matplotlib</a>,
-      <a href="https://palletsprojects.com/p/jinja/" target="_blank">Jinja 2</a>
-      ... {{tr['and']}}
-      <a href="http://jpmeuret.free.fr/informatique.html#AutoDS" target="_blank">AutoDS {{autodsVersion}}</a>,
-      {{tr['with icons from']}} <a href="https://fontawesome.com/" target="_blank">Font Awesome</a>
-      {%if pySources %}
-        ({{tr['sources']}} :
-        {% for pySrc in pySources %}
-          <a href="./{{pySrc}}" target="_blank">{{pySrc}}</a>,
-        {% endfor %})
-      {% endif %}.
-    </h6>
-
-  </div>
-
-  <button onclick="scrollToTop()" id="toTopBtn" title="Haut de page">
-    <img width="64" height="64" src="../fa-angle-up.svg"/>
-  </button>
-
-</body>
+<!DOCTYPE HTML>
+
+<!-- PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+     Copyright (C) 2021 Jean-Philippe Meuret
+     
+     This program is free software: you can redistribute it and/or modify it under the terms
+     of the GNU General Public License as published by the Free Software Foundation,
+     either version 3 of the License, or (at your option) any later version.
+     This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+     without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+     See the GNU General Public License for more details.
+     You should have received a copy of the GNU General Public License along with this program.
+     If not, see https://www.gnu.org/licenses/.
+-->
+
+<head>
+
+  <meta charset="utf-8">
+  <meta name="author" content="Jean-Philippe Meuret"/>
+  <meta name="copyright" content="Jean-Philippe Meuret 2021"/>
+  <meta name="license" content="CC BY NC SA"/>
+  <meta name="description" content="{{title}}"/>
+  <meta name="keywords" content="distance, sampling, mcds, {{keywords}}"/>
+  <meta name="datetime" content="{{genDateTime}}"/>
+  
+  <title>{{title}}</title>
+  
+  <link rel="stylesheet" media="screen" type="text/css" href="../report.css">
+  
+  <script>
+    // Back to top floating button managment
+    // When the user scrolls down100 px from the top of the document, show the button
+    window.onscroll = function()
+    {
+    	if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100)
+    		document.getElementById("toTopBtn").style.display = "block";
+    	else
+    		document.getElementById("toTopBtn").style.display = "none";
+    }
+    // When the user clicks on the button, scroll to the top of the document
+    function scrollToTop()
+    {
+    	document.body.scrollTop = 0;
+    	document.documentElement.scrollTop = 0;
+    }
+  </script>
+  
+</head>
+
+<body>
+
+  <table id="title">
+    <tr>
+      <td style="font-size: 480%; text-align: center">{{title}}</td>
+    </tr>
+    <tr>
+      <td style="font-size: 240%; text-align: center">{{subtitle}}</td>
+    </tr>
+    <tr>
+      <td style="font-size: 120%; text-align: center">{{description}}</td>
+    </tr>
+  </table>
+  
+  <div style="margin-left: 10px">
+
+    <img class="center" height="32" style="margin-top: 30px"
+         src="../fa-feather-alt.svg" alt="---" />
+
+    <h2 id="synthesis">
+      {{tr['Main results']}}
+      <a href="./{{navUrls.prevAnlys}}">
+        <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
+             onmouseover="this.src='../fa-arrow-left-hover.svg';"
+             onmouseout="this.src='../fa-arrow-left.svg';"
+             title="{{tr['Previous analysis']}}"
+             alt="{{tr['Previous analysis']}}" /></a>
+      <a href="./{{navUrls.back2Top}}">
+        <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
+             onmouseover="this.src='../fa-arrow-up-hover.svg';"
+             onmouseout="this.src='../fa-arrow-up.svg';"
+             title="{{tr['Back to top']}}"
+             alt="{{tr['Back to top']}}" /></a>
+      <a href="./{{navUrls.nextAnlys}}">
+        <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
+             onmouseover="this.src='../fa-arrow-right-hover.svg';"
+             onmouseout="this.src='../fa-arrow-right.svg';"
+             title="{{tr['Next analysis']}}"
+             alt="{{tr['Next analysis']}}" /></a>
+    </h2>
+    <div class="chapter" style="margin-left: 10px">
+
+      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
+      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
+      <p>{{tr['Note: Some figures rounded, but not converted']}}</p>
+	  {{synthesis}}
+
+    </div>
+    
+    <img class="center" height="32" style="margin-top: 30px"
+         src="../fa-feather-alt.svg" alt="---" />
+
+    <h2 id="details">
+      {{tr['Detailed results']}}
+      <a href="./{{navUrls.prevAnlys}}">
+        <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
+             onmouseover="this.src='../fa-arrow-left-hover.svg';"
+             onmouseout="this.src='../fa-arrow-left.svg';"
+             title="{{tr['Previous analysis']}}"
+             alt="{{tr['Previous analysis']}}" /></a>
+      <a href="./{{navUrls.back2Top}}">
+        <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
+             onmouseover="this.src='../fa-arrow-up-hover.svg';"
+             onmouseout="this.src='../fa-arrow-up.svg';"
+             title="{{tr['Back to top']}}"
+             alt="{{tr['Back to top']}}" /></a>
+      <a href="./{{navUrls.nextAnlys}}">
+        <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
+             onmouseover="this.src='../fa-arrow-right-hover.svg';"
+             onmouseout="this.src='../fa-arrow-right.svg';"
+             title="{{tr['Next analysis']}}"
+             alt="{{tr['Next analysis']}}" /></a>
+    </h2>
+    <div class="chapter" style="margin-left: 10px">
+
+      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
+      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
+      <p>{{tr['Note: All figures untouched, as output by MCDS']}}</p>
+	  {{details}}
+
+    </div>
+    
+    <img class="center" height="32" style="margin-top: 30px"
+         src="../fa-feather-alt.svg" alt="---" />
+
+    <h2 id="log">
+      {{tr['Summary computation log']}}
+      <a href="./{{navUrls.prevAnlys}}">
+        <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
+             onmouseover="this.src='../fa-arrow-left-hover.svg';"
+             onmouseout="this.src='../fa-arrow-left.svg';"
+             title="{{tr['Previous analysis']}}"
+             alt="{{tr['Previous analysis']}}" /></a>
+      <a href="./{{navUrls.back2Top}}">
+        <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
+             onmouseover="this.src='../fa-arrow-up-hover.svg';"
+             onmouseout="this.src='../fa-arrow-up.svg';"
+             title="{{tr['Back to top']}}"
+             alt="{{tr['Back to top']}}" /></a>
+      <a href="./{{navUrls.nextAnlys}}">
+        <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
+             onmouseover="this.src='../fa-arrow-right-hover.svg';"
+             onmouseout="this.src='../fa-arrow-right.svg';"
+             title="{{tr['Next analysis']}}"
+             alt="{{tr['Next analysis']}}" /></a>
+    </h2>
+    <div class="chapter" style="margin-left: 10px">
+
+	  <pre>{{log.text}}</pre>
+
+    </div>
+    
+    <img class="center" height="32" style="margin-top: 30px"
+         src="../fa-feather-alt.svg" alt="---" />
+
+    <h2 id="log">{{tr['Detailed computation log']}}</h2>
+    <div style="margin-left: 10px">
+
+      {% for ochap in output %}
+  
+        <h3 id="{{ochap.id}}">
+          {{ochap.title}}
+          <a href="./{{navUrls.prevAnlys}}">
+            <img height="32" style="margin-left: 32px" src="../fa-arrow-left.svg"
+                 onmouseover="this.src='../fa-arrow-left-hover.svg';"
+                 onmouseout="this.src='../fa-arrow-left.svg';"
+                 title="{{tr['Previous analysis']}}"
+                 alt="{{tr['Previous analysis']}}" /></a>
+          <a href="./{{navUrls.back2Top}}">
+            <img height="40" style="margin-left: 16px" src="../fa-arrow-up.svg"
+                 onmouseover="this.src='../fa-arrow-up-hover.svg';"
+                 onmouseout="this.src='../fa-arrow-up.svg';"
+                 title="{{tr['Back to top']}}"
+                 alt="{{tr['Back to top']}}" /></a>
+          <a href="./{{navUrls.nextAnlys}}">
+            <img height="32" style="margin-left: 16px" src="../fa-arrow-right.svg"
+                 onmouseover="this.src='../fa-arrow-right-hover.svg';"
+                 onmouseout="this.src='../fa-arrow-right.svg';"
+                 title="{{tr['Next analysis']}}"
+                 alt="{{tr['Next analysis']}}" /></a>
+        </h3>
+        <div class="chapter" style="margin-left: 10px">
+  	  
+          {% if ochap.title in plots %}
+            <img src="./{{plots[ochap.title]}}"/>
+          {% else %}
+            <pre>{{ochap.text}}<pre>
+          {% endif %}
+  	  
+        </div>
+  
+      {% endfor %} <!-- ochap in output -->
+
+    </div>
+    
+    <h6 style="margin-bottom: 10px">
+      {{tr['Page generated']}} {{tr['on']}} {{genDateTime}}
+      {{tr['with']}} <a href="https://www.python.org/" target="_blank">Python 3</a>,
+      <a href="https://numpy.org/" target="_blank">NumPy {{libVersions['NumPy']}}</a>,
+      <a href="https://pandas.pydata.org/" target="_blank">Pandas</a>,
+      <a href="https://github.com/polixir/ZOOpt/" target="_blank">ZOOpt {{libVersions['ZOOpt']}}</a>,
+      <a href="https://matplotlib.org/" target="_blank">Matplotlib</a>,
+      <a href="https://palletsprojects.com/p/jinja/" target="_blank">Jinja 2</a>
+      ... {{tr['and']}}
+      <a href="http://jpmeuret.free.fr/informatique.html#AutoDS" target="_blank">AutoDS {{autodsVersion}}</a>,
+      {{tr['with icons from']}} <a href="https://fontawesome.com/" target="_blank">Font Awesome</a>
+      {%if pySources %}
+        ({{tr['sources']}} :
+        {% for pySrc in pySources %}
+          <a href="./{{pySrc}}" target="_blank">{{pySrc}}</a>,
+        {% endfor %})
+      {% endif %}.
+    </h6>
+
+  </div>
+
+  <button onclick="scrollToTop()" id="toTopBtn" title="Haut de page">
+    <img width="64" height="64" src="../fa-angle-up.svg"/>
+  </button>
+
+</body>
```

### Comparing `pyaudisam-0.9.3/pyaudisam/mcds/fulltop.htpl` & `pyaudisam-1.0.1/pyaudisam/mcds/fulltop.htpl`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,211 +1,211 @@
-<!DOCTYPE HTML>
-
-<!-- PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-     Copyright (C) 2021 Jean-Philippe Meuret
-     
-     This program is free software: you can redistribute it and/or modify it under the terms
-     of the GNU General Public License as published by the Free Software Foundation,
-     either version 3 of the License, or (at your option) any later version.
-     This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-     without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-     See the GNU General Public License for more details.
-     You should have received a copy of the GNU General Public License along with this program.
-     If not, see https://www.gnu.org/licenses/.
--->
-
-<head>
-
-  <meta charset="utf-8">
-  <meta name="author" content="Jean-Philippe Meuret"/>
-  <meta name="copyright" content="Jean-Philippe Meuret 2021"/>
-  <meta name="license" content="CC BY NC SA"/>
-  <meta name="description" content="{{title}}"/>
-  <meta name="keywords" content="distance, sampling, mcds, full, {{keywords}}"/>
-  <meta name="datetime" content="{{genDateTime}}"/>
-  
-  <title>{{title}}</title>
-  
-  <link rel="stylesheet" media="screen" type="text/css" href="./report.css">
-  
-  <script>
-    // Back to top floating button managment
-    // When the user scrolls down 100 px from the top of the document, show the button
-    window.onscroll = function()
-    {
-    	if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100)
-    		document.getElementById("toTopBtn").style.display = "block";
-    	else
-    		document.getElementById("toTopBtn").style.display = "none";
-    }
-    // When the user clicks on the button, scroll to the top of the document
-    function scrollToTop()
-    {
-    	document.body.scrollTop = 0;
-    	document.documentElement.scrollTop = 0;
-    }
-  </script>
-  
-</head>
-
-<body>
-
-  <table id="title">
-    <tr>
-      <td style="font-size: 480%; text-align: center">{{title}}</td>
-    </tr>
-    <tr>
-      <td style="font-size: 240%; text-align: center">{{subtitle}}</td>
-    </tr>
-    <tr>
-      <td style="font-size: 120%; text-align: center">{{description}}</td>
-    </tr>
-  </table>
-  
-  <div style="margin-left: 10px">
-
-    <h4>
-      {{tr['Table of contents']}}
-    </h4>
-    
-    <div class="chapter" style="margin-left: 10px">
-
-      <ul>
-        <li><a href="#super-synthesis">{{tr['Quick-view results']}}</a></li>
-        <li><a href="#synthesis">{{tr['Main results']}}</a></li>
-        <li><a href="#details">{{tr['Detailed results']}}</a></li>
-        <li><a href="#traceability">{{tr['Traceability']}}</a></li>
-      </ul>
-      
-    </div>
-        
-    <h2 id="super-synthesis">
-      {{tr['Quick-view results']}}
-    </h2>
-    
-    <div class="chapter" style="margin-left: 10px">
-
-      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
-      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
-      <p>{{tr['Confidence value interval:']}} {{tr['If not listed in table below, please']}}
-         {% if confIntervals|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{confIntervals|join(', '|safe)}}
-         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{confIntervals[0]}}
-         {% endif %}
-         {% if confIntervals|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
-      </p>
-      <p>{{tr['Estimator selection criterion:']}} {{tr['If not listed in table below, please']}}
-         {% if estimSelCrits|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{estimSelCrits|join(', '|safe)}}
-         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{estimSelCrits[0]}}
-         {% endif %}
-         {% if estimSelCrits|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
-      </p>
-      <p>{{tr['Note: Some figures rounded, but not converted']}}</p>
-      <p style='font-size: 80%'>{{tr['Click on analysis # for details']}}</p>
-      
-	  {{supersynthesis}}
-
-    </div>
-        
-    <img class="center" height="32" style="margin-top: 30px"
-         src="./fa-feather-alt.svg" alt="---" />
-
-    <h2 id="synthesis">
-      <a href="./{{xlUrl}}" target="_blank">
-        <img height="48" style="margin-right: 16px" src="./fa-file-excel.svg"
-             onmouseover="this.src='./fa-file-excel-hover.svg';"
-             onmouseout="this.src='./fa-file-excel.svg';"
-             title="{{tr['Download Excel']}}"
-             alt="{{tr['Download Excel']}}" /></a>
-      {{tr['Main results']}}
-    </h2>
-    <div class="chapter" style="margin-left: 10px">
-
-      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
-      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
-      <p>{{tr['Confidence value interval:']}} {{tr['If not listed in table below, please']}}
-         {% if confIntervals|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{confIntervals|join(', '|safe)}}
-         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{confIntervals[0]}}
-         {% endif %}
-         {% if confIntervals|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
-      </p>
-      <p>{{tr['Estimator selection criterion:']}} {{tr['If not listed in table below, please']}}
-         {% if estimSelCrits|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{estimSelCrits|join(', '|safe)}}
-         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{estimSelCrits[0]}}
-         {% endif %}
-         {% if estimSelCrits|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
-      </p>
-      <p>{{tr['Note: Some figures rounded, but not converted']}}</p>
-      <p style='font-size: 80%'>{{tr['Click on analysis # for details']}}</p>
-      
-      {{synthesis}}
-
-    </div>
-    
-    <img class="center" height="32" style="margin-top: 30px"
-         src="./fa-feather-alt.svg" alt="---" />
-
-    <h2 id="details">
-      <a href="./{{xlUrl}}" target="_blank">
-        <img height="48" style="margin-right: 8px" src="./fa-file-excel.svg"
-             onmouseover="this.src='./fa-file-excel-hover.svg';"
-             onmouseout="this.src='./fa-file-excel.svg';"
-             title="{{tr['Download Excel']}}"
-             alt="{{tr['Download Excel']}}" /></a>
-      {{tr['Detailed results']}}
-    </h2>
-    <div class="chapter" style="margin-left: 10px">
-
-      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
-      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
-      <p>{{tr['Note: All figures untouched, as output by MCDS']}}</p>
-      <p style='font-size: 80%'>{{tr['Click on analysis # for details']}}</p>
-      
-      {{details}}
-      
-    </div>
-    
-    <img class="center" height="32" style="margin-top: 30px"
-         src="./fa-feather-alt.svg" alt="---" />
-
-    <h2 id="traceability">
-      {{tr['Traceability']}}
-    </h2>
-    <div class="chapter" style="margin-left: 10px">
-
-      <p>{{tr['Traceability tech details']}}</p>
-
-      {% for name, table in traceability.items() %}
-        <div style="margin-left: 10px">
-          <h3>{{name}}</h3>
-          {{table}}
-        </div>
-      {% endfor %}
-      
-    </div>
-    
-    <h6 style="margin-bottom: 10px">
-      {{tr['Page generated']}} {{tr['on']}} {{genDateTime}}
-      {{tr['with']}} <a href="https://www.python.org/" target="_blank">Python {{libVersions['Python']}}</a>,
-      <a href="https://numpy.org/" target="_blank">NumPy {{libVersions['NumPy']}}</a>,
-      <a href="https://pandas.pydata.org/" target="_blank">Pandas {{libVersions['Pandas']}}</a>,
-      <a href="https://github.com/polixir/ZOOpt/" target="_blank">ZOOpt {{libVersions['ZOOpt']}}</a>,
-      <a href="https://matplotlib.org/" target="_blank">Matplotlib {{libVersions['Matplotlib']}}</a>,
-      <a href="https://palletsprojects.com/p/jinja/" target="_blank">Jinja {{libVersions['Jinja']}}</a>
-      ... {{tr['and']}}
-      <a href="https://pypi.org/project/pyaudisam/" target="_blank">PyAuDiSam {{version}}</a>,
-      {{tr['with icons from']}} <a href="https://fontawesome.com/" target="_blank">Font Awesome</a>
-      {%if pySources %}
-        ({{tr['sources']}} :
-        {% for pySrc in pySources %}
-          <a href="./{{pySrc}}" target="_blank">{{pySrc}}</a>,
-        {% endfor %})
-      {% endif %}.
-    </h6>
-
-  </div>
-
-  <button onclick="scrollToTop()" id="toTopBtn" title="Haut de page">
-    <img width="64" height="64" src="./fa-angle-up.svg"/>
-  </button>
-
-</body>
+<!DOCTYPE HTML>
+
+<!-- PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+     Copyright (C) 2021 Jean-Philippe Meuret
+     
+     This program is free software: you can redistribute it and/or modify it under the terms
+     of the GNU General Public License as published by the Free Software Foundation,
+     either version 3 of the License, or (at your option) any later version.
+     This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+     without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+     See the GNU General Public License for more details.
+     You should have received a copy of the GNU General Public License along with this program.
+     If not, see https://www.gnu.org/licenses/.
+-->
+
+<head>
+
+  <meta charset="utf-8">
+  <meta name="author" content="Jean-Philippe Meuret"/>
+  <meta name="copyright" content="Jean-Philippe Meuret 2021"/>
+  <meta name="license" content="CC BY NC SA"/>
+  <meta name="description" content="{{title}}"/>
+  <meta name="keywords" content="distance, sampling, mcds, full, {{keywords}}"/>
+  <meta name="datetime" content="{{genDateTime}}"/>
+  
+  <title>{{title}}</title>
+  
+  <link rel="stylesheet" media="screen" type="text/css" href="./report.css">
+  
+  <script>
+    // Back to top floating button managment
+    // When the user scrolls down 100 px from the top of the document, show the button
+    window.onscroll = function()
+    {
+    	if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100)
+    		document.getElementById("toTopBtn").style.display = "block";
+    	else
+    		document.getElementById("toTopBtn").style.display = "none";
+    }
+    // When the user clicks on the button, scroll to the top of the document
+    function scrollToTop()
+    {
+    	document.body.scrollTop = 0;
+    	document.documentElement.scrollTop = 0;
+    }
+  </script>
+  
+</head>
+
+<body>
+
+  <table id="title">
+    <tr>
+      <td style="font-size: 480%; text-align: center">{{title}}</td>
+    </tr>
+    <tr>
+      <td style="font-size: 240%; text-align: center">{{subtitle}}</td>
+    </tr>
+    <tr>
+      <td style="font-size: 120%; text-align: center">{{description}}</td>
+    </tr>
+  </table>
+  
+  <div style="margin-left: 10px">
+
+    <h4>
+      {{tr['Table of contents']}}
+    </h4>
+    
+    <div class="chapter" style="margin-left: 10px">
+
+      <ul>
+        <li><a href="#super-synthesis">{{tr['Quick-view results']}}</a></li>
+        <li><a href="#synthesis">{{tr['Main results']}}</a></li>
+        <li><a href="#details">{{tr['Detailed results']}}</a></li>
+        <li><a href="#traceability">{{tr['Traceability']}}</a></li>
+      </ul>
+      
+    </div>
+        
+    <h2 id="super-synthesis">
+      {{tr['Quick-view results']}}
+    </h2>
+    
+    <div class="chapter" style="margin-left: 10px">
+
+      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
+      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
+      <p>{{tr['Confidence value interval:']}} {{tr['If not listed in table below, please']}}
+         {% if confIntervals|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{confIntervals|join(', '|safe)}}
+         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{confIntervals[0]}}
+         {% endif %}
+         {% if confIntervals|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
+      </p>
+      <p>{{tr['Estimator selection criterion:']}} {{tr['If not listed in table below, please']}}
+         {% if estimSelCrits|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{estimSelCrits|join(', '|safe)}}
+         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{estimSelCrits[0]}}
+         {% endif %}
+         {% if estimSelCrits|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
+      </p>
+      <p>{{tr['Note: Some figures rounded, but not converted']}}</p>
+      <p style='font-size: 80%'>{{tr['Click on analysis # for details']}}</p>
+      
+	  {{supersynthesis}}
+
+    </div>
+        
+    <img class="center" height="32" style="margin-top: 30px"
+         src="./fa-feather-alt.svg" alt="---" />
+
+    <h2 id="synthesis">
+      <a href="./{{xlUrl}}" target="_blank">
+        <img height="48" style="margin-right: 16px" src="./fa-file-excel.svg"
+             onmouseover="this.src='./fa-file-excel-hover.svg';"
+             onmouseout="this.src='./fa-file-excel.svg';"
+             title="{{tr['Download Excel']}}"
+             alt="{{tr['Download Excel']}}" /></a>
+      {{tr['Main results']}}
+    </h2>
+    <div class="chapter" style="margin-left: 10px">
+
+      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
+      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
+      <p>{{tr['Confidence value interval:']}} {{tr['If not listed in table below, please']}}
+         {% if confIntervals|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{confIntervals|join(', '|safe)}}
+         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{confIntervals[0]}}
+         {% endif %}
+         {% if confIntervals|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
+      </p>
+      <p>{{tr['Estimator selection criterion:']}} {{tr['If not listed in table below, please']}}
+         {% if estimSelCrits|length > 1 %} {{tr['BE AWARE that different values have been used among analyses']}}: {{estimSelCrits|join(', '|safe)}}
+         {% else %} {{tr['note that all analyses have been run with the same value']}}: {{estimSelCrits[0]}}
+         {% endif %}
+         {% if estimSelCrits|length > 1 %} ({{tr['see detailed table below']}}){% endif %}.
+      </p>
+      <p>{{tr['Note: Some figures rounded, but not converted']}}</p>
+      <p style='font-size: 80%'>{{tr['Click on analysis # for details']}}</p>
+      
+      {{synthesis}}
+
+    </div>
+    
+    <img class="center" height="32" style="margin-top: 30px"
+         src="./fa-feather-alt.svg" alt="---" />
+
+    <h2 id="details">
+      <a href="./{{xlUrl}}" target="_blank">
+        <img height="48" style="margin-right: 8px" src="./fa-file-excel.svg"
+             onmouseover="this.src='./fa-file-excel-hover.svg';"
+             onmouseout="this.src='./fa-file-excel.svg';"
+             title="{{tr['Download Excel']}}"
+             alt="{{tr['Download Excel']}}" /></a>
+      {{tr['Detailed results']}}
+    </h2>
+    <div class="chapter" style="margin-left: 10px">
+
+      <p>{{tr['Study type:']}} {{surveyType}}, {{distanceType}}, {{clustering}}.</p>
+      <p>{{tr['Units used:']}} {{distanceUnit}} {{tr['for distances']}}, {{areaUnit}} {{tr['for areas']}}.</p>
+      <p>{{tr['Note: All figures untouched, as output by MCDS']}}</p>
+      <p style='font-size: 80%'>{{tr['Click on analysis # for details']}}</p>
+      
+      {{details}}
+      
+    </div>
+    
+    <img class="center" height="32" style="margin-top: 30px"
+         src="./fa-feather-alt.svg" alt="---" />
+
+    <h2 id="traceability">
+      {{tr['Traceability']}}
+    </h2>
+    <div class="chapter" style="margin-left: 10px">
+
+      <p>{{tr['Traceability tech details']}}</p>
+
+      {% for name, table in traceability.items() %}
+        <div style="margin-left: 10px">
+          <h3>{{name}}</h3>
+          {{table}}
+        </div>
+      {% endfor %}
+      
+    </div>
+    
+    <h6 style="margin-bottom: 10px">
+      {{tr['Page generated']}} {{tr['on']}} {{genDateTime}}
+      {{tr['with']}} <a href="https://www.python.org/" target="_blank">Python {{libVersions['Python']}}</a>,
+      <a href="https://numpy.org/" target="_blank">NumPy {{libVersions['NumPy']}}</a>,
+      <a href="https://pandas.pydata.org/" target="_blank">Pandas {{libVersions['Pandas']}}</a>,
+      <a href="https://github.com/polixir/ZOOpt/" target="_blank">ZOOpt {{libVersions['ZOOpt']}}</a>,
+      <a href="https://matplotlib.org/" target="_blank">Matplotlib {{libVersions['Matplotlib']}}</a>,
+      <a href="https://palletsprojects.com/p/jinja/" target="_blank">Jinja {{libVersions['Jinja']}}</a>
+      ... {{tr['and']}}
+      <a href="https://pypi.org/project/pyaudisam/" target="_blank">PyAuDiSam {{version}}</a>,
+      {{tr['with icons from']}} <a href="https://fontawesome.com/" target="_blank">Font Awesome</a>
+      {%if pySources %}
+        ({{tr['sources']}} :
+        {% for pySrc in pySources %}
+          <a href="./{{pySrc}}" target="_blank">{{pySrc}}</a>,
+        {% endfor %})
+      {% endif %}.
+    </h6>
+
+  </div>
+
+  <button onclick="scrollToTop()" id="toTopBtn" title="Haut de page">
+    <img width="64" height="64" src="./fa-angle-up.svg"/>
+  </button>
+
+</body>
```

### Comparing `pyaudisam-0.9.3/pyaudisam/mcds/stat-mod-notes.txt` & `pyaudisam-1.0.1/pyaudisam/mcds/stat-mod-notes.txt`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Extract from Distance 7.3 documentation (http://distancesampling.org/)
-# The following was copied and pasted as is from the notes bellow the 2nd table of .chm documentation,
-# "MCDS Engine Stats File" page.
-1 Values for CV, LCL, UCL and DF are included for these statistics.
-2 Key function types are: 1 = uniform, 2 = half-normal, 3 = negative exponential, 4 = hazard rate
-3 Adjustment series types are: 1 = simple polynomial, 2 = Hermite polynomial, 3 = cosine
-4 Bootstrap CV calculated as bootstrap SE / bootstrap point estimate; df field here is the number of bootstraps
-5 Statistic 101 corresponds with the parameter identified as A(1) in the results, 102 with A(2), etc.
+# Extract from Distance 7.3 documentation (http://distancesampling.org/)
+# The following was copied and pasted as is from the notes bellow the 2nd table of .chm documentation,
+# "MCDS Engine Stats File" page.
+1 Values for CV, LCL, UCL and DF are included for these statistics.
+2 Key function types are: 1 = uniform, 2 = half-normal, 3 = negative exponential, 4 = hazard rate
+3 Adjustment series types are: 1 = simple polynomial, 2 = Hermite polynomial, 3 = cosine
+4 Bootstrap CV calculated as bootstrap SE / bootstrap point estimate; df field here is the number of bootstraps
+5 Statistic 101 corresponds with the parameter identified as A(1) in the results, 102 with A(2), etc.
```

### Comparing `pyaudisam-0.9.3/pyaudisam/mcds/stat-mod-trans.txt` & `pyaudisam-1.0.1/pyaudisam/mcds/stat-mod-trans.txt`

 * *Ordering differences only*

 * *Files 6% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-Module	Statistic	Figure	en	fr
-encounter rate	number of observations (n)	Value	NObs	NObs
-encounter rate	number of samples (k)	Value	NSamp	NEchant
-encounter rate	effort (L or K or T)	Value	Effort	Effort
-encounter rate	encounter rate (n/L or n/K or n/T)	Value	EncRate	TxContact
-encounter rate	encounter rate (n/L or n/K or n/T)	Cv	CoefVar EncRate	CoefVar TxContact
-encounter rate	encounter rate (n/L or n/K or n/T)	Lcl	Min EncRate	Min TxContact
-encounter rate	encounter rate (n/L or n/K or n/T)	Ucl	Max EncRate	Max TxContact
-encounter rate	encounter rate (n/L or n/K or n/T)	Df	DoF EncRate	DegLib TxContact
-encounter rate	left truncation distance	Value	Left Trunc	Tronc Gche
-encounter rate	right truncation distance (w)	Value	Right Trunc	Tronc Dte
-detection probability	total number of parameters (m)	Value	TotNum Pars	NbTot Pars
-detection probability	AIC value	Value	AIC	AIC
-detection probability	chi-square test probability (distance set 1)	Value	Chi2 P 1	Chi2 P 1
-detection probability	chi-square test probability (distance set 2)	Value	Chi2 P 2	Chi2 P 2
-detection probability	chi-square test probability (distance set 3)	Value	Chi2 P 3	Chi2 P 3
-detection probability	f(0) or h(0)	Value	f/h(0)	f/h(0)
-detection probability	f(0) or h(0)	Cv	CoefVar f/h(0)	CoefVar f/h(0)
-detection probability	f(0) or h(0)	Lcl	Min f/h(0)	Min f/h(0)
-detection probability	f(0) or h(0)	Ucl	Max f/h(0)	Max f/h(0)
-detection probability	f(0) or h(0)	Df	DoF f/h(0)	DegLib f/h(0)
-detection probability	probability of detection (Pw)	Value	PDetec	PDetec
-detection probability	probability of detection (Pw)	Cv	CoefVar PDetec	CoefVar PDetec
-detection probability	probability of detection (Pw)	Lcl	Min PDetec	Min PDetec
-detection probability	probability of detection (Pw)	Ucl	Max PDetec	Max PDetec
-detection probability	probability of detection (Pw)	Df	DoF PDetec	DegLib PDetec
-detection probability	effective strip width (ESW) or effective detection radius (EDR)	Value	EDR/ESW	EDR/ESW
-detection probability	effective strip width (ESW) or effective detection radius (EDR)	Cv	CoefVar EDR/ESW	CoefVar EDR/ESW
-detection probability	effective strip width (ESW) or effective detection radius (EDR)	Lcl	Min EDR/ESW	Min EDR/ESW
-detection probability	effective strip width (ESW) or effective detection radius (EDR)	Ucl	Max EDR/ESW	Max EDR/ESW
-detection probability	effective strip width (ESW) or effective detection radius (EDR)	Df	DoF EDR/ESW	DegLib EDR/ESW
-detection probability	AICc	Value	AICc	AICc
-detection probability	BIC	Value	BIC	BIC
-detection probability	Log likelihood	Value	LogLhood	LogProba
-detection probability	Kolmogorov-Smirnov test probability	Value	KS P	KS P
-detection probability	Cramér-von Mises (uniform weighting) test probability	Value	CvM Uw P	CvM Uw P
-detection probability	Cramér-von Mises (cosine weighting) test probability	Value	CvM Cw P	CvM Cw P
-detection probability	key function type	Value	Key Fn	Fn Clé
-detection probability	adjustment series type	Value	Adj Ser	Sér Ajust
-detection probability	number of key function parameters (NKP)	Value	NumPars KeyFn	NbPars FnClé
-detection probability	number of adjustment term parameters (NAP)	Value	NumPars AdjSer	NbPars SérAjust
-detection probability	number of covariate parameters (NCP)	Value	Num Covars	Nb Covars
-detection probability	estimated value of A(1) adjustment term parameter	Value	EstA(1)	EstA(1)
-detection probability	estimated value of A(2) adjustment term parameter	Value	EstA(2)	EstA(2)
-detection probability	estimated value of A(3) adjustment term parameter	Value	EstA(3)	EstA(3)
-detection probability	estimated value of A(4) adjustment term parameter	Value	EstA(4)	EstA(4)
-detection probability	estimated value of A(5) adjustment term parameter	Value	EstA(5)	EstA(5)
-detection probability	estimated value of A(6) adjustment term parameter	Value	EstA(6)	EstA(6)
-detection probability	estimated value of A(7) adjustment term parameter	Value	EstA(7)	EstA(7)
-detection probability	estimated value of A(8) adjustment term parameter	Value	EstA(8)	EstA(8)
-detection probability	estimated value of A(9) adjustment term parameter	Value	EstA(9)	EstA(9)
-detection probability	estimated value of A(10) adjustment term parameter	Value	EstA(10)	EstA(10)
-cluster size	average cluster size	Value	AvgSz Clust	TailMoyClust
-cluster size	average cluster size	Cv	CoefVar AvgSz Clust	CoefVar TailMoy Clust
-cluster size	average cluster size	Lcl	Min AvgSz Clust	Min TailMoy Clust
-cluster size	average cluster size	Ucl	Max AvgSz Clust	Max TailMoy Clust
-cluster size	average cluster size	Df	DoF AvgSz Clust	DegLib TailMoy Clust
-cluster size	size-bias regression correlation (r)	Value	SzBias RegCorr	CorrReg BiaisTail
-cluster size	p-value for correlation significance (r-p)	Value	Cor Sign PVal	PVal SignifCorr
-cluster size	estimate of expected cluster size corrected for size bias	Value	EstExp FixedCluSz	TailCorr CluAttEst
-cluster size	estimate of expected cluster size corrected for size bias	Cv	CoefVar EstExp FixedCluSz	CoefVar TailCorr CluAttEst
-cluster size	estimate of expected cluster size corrected for size bias	Lcl	Min EstExp FixedCluSz	Min TailCorr CluAttEst
-cluster size	estimate of expected cluster size corrected for size bias	Ucl	Max EstExp FixedCluSz	Max TailCorr CluAttEst
-cluster size	estimate of expected cluster size corrected for size bias	Df	DoF EstExp FixedCluSz	DegLib TailCorr CluAttEst
-density/abundance	density of clusters (or animal density if non-clustered)	Value	DensClu	DensClu
-density/abundance	density of clusters (or animal density if non-clustered)	Cv	CoefVar DensClu	CoefVar DensClu
-density/abundance	density of clusters (or animal density if non-clustered)	Lcl	Min DensClu	Min DensClu
-density/abundance	density of clusters (or animal density if non-clustered)	Ucl	Max DensClu	Max DensClu
-density/abundance	density of clusters (or animal density if non-clustered)	Df	DoF DensClu	DegLib DensClu
-density/abundance	density of animals	Value	Density	Densité
-density/abundance	density of animals	Cv	CoefVar Density	CoefVar Densité
-density/abundance	density of animals	Lcl	Min Density	Min Densité
-density/abundance	density of animals	Ucl	Max Density	Max Densité
-density/abundance	density of animals	Df	DoF Density	DegLib Densité
-density/abundance	number of animals, if survey area is specified	Value	Number	Nombre
-density/abundance	number of animals, if survey area is specified	Cv	CoefVar Number	CoefVar Nombre
-density/abundance	number of animals, if survey area is specified	Lcl	Min Number	Min Nombre
-density/abundance	number of animals, if survey area is specified	Ucl	Max Number	Max Nombre
-density/abundance	number of animals, if survey area is specified	Df	DoF Number	DegLib Nombre
-density/abundance	bootstrap density of clusters	Value	BootsDens Clu	BootsDensClu
-density/abundance	bootstrap density of clusters	Cv	CoefVar BootsDens Clu	CoefVar DensBoots Clu
-density/abundance	bootstrap density of clusters	Lcl	Min BootsDens Clu	Min DensBoots Clu
-density/abundance	bootstrap density of clusters	Ucl	Max BootsDens Clu	Max DensBoots Clu
-density/abundance	bootstrap density of clusters	Df	DoF BootsDens Clu	DegLib DensBoots Clu
-density/abundance	bootstrap density of animals	Value	BootsDens	DensBoots
-density/abundance	bootstrap density of animals	Cv	CoefVar BootsDens	CoefVar DensBoots
-density/abundance	bootstrap density of animals	Lcl	Min BootsDens	Min DensBoots
-density/abundance	bootstrap density of animals	Ucl	Max BootsDens	Max DensBoots
-density/abundance	bootstrap density of animals	Df	DoF BootsDens	DegLib DensBoots
-density/abundance	bootstrap number of animals	Value	BootsNum	NbBoots
-density/abundance	bootstrap number of animals	Cv	CoefVar BootsNum	CoefVar NbBoots
-density/abundance	bootstrap number of animals	Lcl	Min BootsNum	Min NbBoots
-density/abundance	bootstrap number of animals	Ucl	Max BootsNum	Max NbBoots
-density/abundance	bootstrap number of animals	Df	DoF BootsNum	DegLib NbBoots
+Module	Statistic	Figure	en	fr
+encounter rate	number of observations (n)	Value	NObs	NObs
+encounter rate	number of samples (k)	Value	NSamp	NEchant
+encounter rate	effort (L or K or T)	Value	Effort	Effort
+encounter rate	encounter rate (n/L or n/K or n/T)	Value	EncRate	TxContact
+encounter rate	encounter rate (n/L or n/K or n/T)	Cv	CoefVar EncRate	CoefVar TxContact
+encounter rate	encounter rate (n/L or n/K or n/T)	Lcl	Min EncRate	Min TxContact
+encounter rate	encounter rate (n/L or n/K or n/T)	Ucl	Max EncRate	Max TxContact
+encounter rate	encounter rate (n/L or n/K or n/T)	Df	DoF EncRate	DegLib TxContact
+encounter rate	left truncation distance	Value	Left Trunc	Tronc Gche
+encounter rate	right truncation distance (w)	Value	Right Trunc	Tronc Dte
+detection probability	total number of parameters (m)	Value	TotNum Pars	NbTot Pars
+detection probability	AIC value	Value	AIC	AIC
+detection probability	chi-square test probability (distance set 1)	Value	Chi2 P 1	Chi2 P 1
+detection probability	chi-square test probability (distance set 2)	Value	Chi2 P 2	Chi2 P 2
+detection probability	chi-square test probability (distance set 3)	Value	Chi2 P 3	Chi2 P 3
+detection probability	f(0) or h(0)	Value	f/h(0)	f/h(0)
+detection probability	f(0) or h(0)	Cv	CoefVar f/h(0)	CoefVar f/h(0)
+detection probability	f(0) or h(0)	Lcl	Min f/h(0)	Min f/h(0)
+detection probability	f(0) or h(0)	Ucl	Max f/h(0)	Max f/h(0)
+detection probability	f(0) or h(0)	Df	DoF f/h(0)	DegLib f/h(0)
+detection probability	probability of detection (Pw)	Value	PDetec	PDetec
+detection probability	probability of detection (Pw)	Cv	CoefVar PDetec	CoefVar PDetec
+detection probability	probability of detection (Pw)	Lcl	Min PDetec	Min PDetec
+detection probability	probability of detection (Pw)	Ucl	Max PDetec	Max PDetec
+detection probability	probability of detection (Pw)	Df	DoF PDetec	DegLib PDetec
+detection probability	effective strip width (ESW) or effective detection radius (EDR)	Value	EDR/ESW	EDR/ESW
+detection probability	effective strip width (ESW) or effective detection radius (EDR)	Cv	CoefVar EDR/ESW	CoefVar EDR/ESW
+detection probability	effective strip width (ESW) or effective detection radius (EDR)	Lcl	Min EDR/ESW	Min EDR/ESW
+detection probability	effective strip width (ESW) or effective detection radius (EDR)	Ucl	Max EDR/ESW	Max EDR/ESW
+detection probability	effective strip width (ESW) or effective detection radius (EDR)	Df	DoF EDR/ESW	DegLib EDR/ESW
+detection probability	AICc	Value	AICc	AICc
+detection probability	BIC	Value	BIC	BIC
+detection probability	Log likelihood	Value	LogLhood	LogProba
+detection probability	Kolmogorov-Smirnov test probability	Value	KS P	KS P
+detection probability	Cramér-von Mises (uniform weighting) test probability	Value	CvM Uw P	CvM Uw P
+detection probability	Cramér-von Mises (cosine weighting) test probability	Value	CvM Cw P	CvM Cw P
+detection probability	key function type	Value	Key Fn	Fn Clé
+detection probability	adjustment series type	Value	Adj Ser	Sér Ajust
+detection probability	number of key function parameters (NKP)	Value	NumPars KeyFn	NbPars FnClé
+detection probability	number of adjustment term parameters (NAP)	Value	NumPars AdjSer	NbPars SérAjust
+detection probability	number of covariate parameters (NCP)	Value	Num Covars	Nb Covars
+detection probability	estimated value of A(1) adjustment term parameter	Value	EstA(1)	EstA(1)
+detection probability	estimated value of A(2) adjustment term parameter	Value	EstA(2)	EstA(2)
+detection probability	estimated value of A(3) adjustment term parameter	Value	EstA(3)	EstA(3)
+detection probability	estimated value of A(4) adjustment term parameter	Value	EstA(4)	EstA(4)
+detection probability	estimated value of A(5) adjustment term parameter	Value	EstA(5)	EstA(5)
+detection probability	estimated value of A(6) adjustment term parameter	Value	EstA(6)	EstA(6)
+detection probability	estimated value of A(7) adjustment term parameter	Value	EstA(7)	EstA(7)
+detection probability	estimated value of A(8) adjustment term parameter	Value	EstA(8)	EstA(8)
+detection probability	estimated value of A(9) adjustment term parameter	Value	EstA(9)	EstA(9)
+detection probability	estimated value of A(10) adjustment term parameter	Value	EstA(10)	EstA(10)
+cluster size	average cluster size	Value	AvgSz Clust	TailMoyClust
+cluster size	average cluster size	Cv	CoefVar AvgSz Clust	CoefVar TailMoy Clust
+cluster size	average cluster size	Lcl	Min AvgSz Clust	Min TailMoy Clust
+cluster size	average cluster size	Ucl	Max AvgSz Clust	Max TailMoy Clust
+cluster size	average cluster size	Df	DoF AvgSz Clust	DegLib TailMoy Clust
+cluster size	size-bias regression correlation (r)	Value	SzBias RegCorr	CorrReg BiaisTail
+cluster size	p-value for correlation significance (r-p)	Value	Cor Sign PVal	PVal SignifCorr
+cluster size	estimate of expected cluster size corrected for size bias	Value	EstExp FixedCluSz	TailCorr CluAttEst
+cluster size	estimate of expected cluster size corrected for size bias	Cv	CoefVar EstExp FixedCluSz	CoefVar TailCorr CluAttEst
+cluster size	estimate of expected cluster size corrected for size bias	Lcl	Min EstExp FixedCluSz	Min TailCorr CluAttEst
+cluster size	estimate of expected cluster size corrected for size bias	Ucl	Max EstExp FixedCluSz	Max TailCorr CluAttEst
+cluster size	estimate of expected cluster size corrected for size bias	Df	DoF EstExp FixedCluSz	DegLib TailCorr CluAttEst
+density/abundance	density of clusters (or animal density if non-clustered)	Value	DensClu	DensClu
+density/abundance	density of clusters (or animal density if non-clustered)	Cv	CoefVar DensClu	CoefVar DensClu
+density/abundance	density of clusters (or animal density if non-clustered)	Lcl	Min DensClu	Min DensClu
+density/abundance	density of clusters (or animal density if non-clustered)	Ucl	Max DensClu	Max DensClu
+density/abundance	density of clusters (or animal density if non-clustered)	Df	DoF DensClu	DegLib DensClu
+density/abundance	density of animals	Value	Density	Densité
+density/abundance	density of animals	Cv	CoefVar Density	CoefVar Densité
+density/abundance	density of animals	Lcl	Min Density	Min Densité
+density/abundance	density of animals	Ucl	Max Density	Max Densité
+density/abundance	density of animals	Df	DoF Density	DegLib Densité
+density/abundance	number of animals, if survey area is specified	Value	Number	Nombre
+density/abundance	number of animals, if survey area is specified	Cv	CoefVar Number	CoefVar Nombre
+density/abundance	number of animals, if survey area is specified	Lcl	Min Number	Min Nombre
+density/abundance	number of animals, if survey area is specified	Ucl	Max Number	Max Nombre
+density/abundance	number of animals, if survey area is specified	Df	DoF Number	DegLib Nombre
+density/abundance	bootstrap density of clusters	Value	BootsDens Clu	BootsDensClu
+density/abundance	bootstrap density of clusters	Cv	CoefVar BootsDens Clu	CoefVar DensBoots Clu
+density/abundance	bootstrap density of clusters	Lcl	Min BootsDens Clu	Min DensBoots Clu
+density/abundance	bootstrap density of clusters	Ucl	Max BootsDens Clu	Max DensBoots Clu
+density/abundance	bootstrap density of clusters	Df	DoF BootsDens Clu	DegLib DensBoots Clu
+density/abundance	bootstrap density of animals	Value	BootsDens	DensBoots
+density/abundance	bootstrap density of animals	Cv	CoefVar BootsDens	CoefVar DensBoots
+density/abundance	bootstrap density of animals	Lcl	Min BootsDens	Min DensBoots
+density/abundance	bootstrap density of animals	Ucl	Max BootsDens	Max DensBoots
+density/abundance	bootstrap density of animals	Df	DoF BootsDens	DegLib DensBoots
+density/abundance	bootstrap number of animals	Value	BootsNum	NbBoots
+density/abundance	bootstrap number of animals	Cv	CoefVar BootsNum	CoefVar NbBoots
+density/abundance	bootstrap number of animals	Lcl	Min BootsNum	Min NbBoots
+density/abundance	bootstrap number of animals	Ucl	Max BootsNum	Max NbBoots
+density/abundance	bootstrap number of animals	Df	DoF BootsNum	DegLib NbBoots
```

### Comparing `pyaudisam-0.9.3/pyaudisam/mcds/stat-row-specs.txt` & `pyaudisam-1.0.1/pyaudisam/mcds/stat-row-specs.txt`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-# Extract from Distance 7.3 documentation (http://distancesampling.org/)
-# The following was copied and pasted _as_is_ from the 1st table of .chm documentation,
-# "MCDS Engine Stats File" page.
-Stratum  
- stratum number (or 0 if the estimate is for a sample or all data).
- 
- Sample   
- sample number (or 0 if the estimate is for a stratum or all data).
- 
- Estimator
- number of the estimator (in the order given in the Estimate procedure)
- 
- Module   
- number of the parameter module (see below)
- 
- Statistic
- number of the statistic within the parameter module (see below)
- 
- Value    
- estimate value
- 
- Cv       
- coefficient of variation of estimate or 0.0
- 
- Lcl      
- lower confidence limit or 0.0
- 
- Ucl      
- upper confidence limit or 0.0
- 
- Df       
- degrees of freedom for interval or 0
- 
+# Extract from Distance 7.3 documentation (http://distancesampling.org/)
+# The following was copied and pasted _as_is_ from the 1st table of .chm documentation,
+# "MCDS Engine Stats File" page.
+Stratum  
+ stratum number (or 0 if the estimate is for a sample or all data).
+ 
+ Sample   
+ sample number (or 0 if the estimate is for a stratum or all data).
+ 
+ Estimator
+ number of the estimator (in the order given in the Estimate procedure)
+ 
+ Module   
+ number of the parameter module (see below)
+ 
+ Statistic
+ number of the statistic within the parameter module (see below)
+ 
+ Value    
+ estimate value
+ 
+ Cv       
+ coefficient of variation of estimate or 0.0
+ 
+ Lcl      
+ lower confidence limit or 0.0
+ 
+ Ucl      
+ upper confidence limit or 0.0
+ 
+ Df       
+ degrees of freedom for interval or 0
+
```

### Comparing `pyaudisam-0.9.3/pyaudisam/optanalyser.py` & `pyaudisam-1.0.1/pyaudisam/optanalyser.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,540 +1,540 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "optanalyser": Run a bunch of DS analyses according to a user-friendly set of analysis specs
-#  with possibly some undetermined analysis parameters in specs :
-#  for these analyses, an auto-computation of these parameters will then be run first
-#  through some optimisation engine specified in specs (only "zoopt" supported as for now),
-#  and for some kind of parameters (only distance truncations supported as for now).
-
-import numpy as np
-import pandas as pd
-
-from . import log
-from .engine import MCDSEngine
-from .analysis import MCDSAnalysis
-from .analyser import MCDSAnalyser, MCDSAnalysisResultsSet
-from .optimiser import MCDSZerothOrderTruncationOptimiser
-
-logger = log.logger('ads.onr')
-
-
-class MCDSTruncOptanalysisResultsSet(MCDSAnalysisResultsSet):
-
-    """A specialized results set for MCDS optanalysers"""
-
-    Super = MCDSAnalysisResultsSet
-
-    # Name of the spec. column to hold the "is truncation stuff optimised" 0/1 flag.
-    OptimTruncFlagCol = 'OptimTrunc'
-    CLOptimTruncFlag = ('header (tail)', OptimTruncFlagCol, 'Value')
-    
-    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,
-                 sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
-                                    dict(col='right', minDist=25.0, maxLen=25.0)],
-                 truncIntrvEpsilon=1e-6, ldFilSorKeySchemes=None):
-        
-        """Ctor
-
-        Parameters:
-        :param miSampleCols: columns to use for grouping by sample ; defaults to miCustomCols if None
-        :param sampleIndCol: multi-column index for the sample Id column ; no default, must be there !
-        :param ldFilSorKeySchemes: Replacement for predefined filter-sort key schemes
-                                   None => use predefined ones AutoFilSorKeySchemes.
-        """
-
-        super().__init__(miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
-                         miSampleCols=miSampleCols, sampleIndCol=sampleIndCol,
-                         sortCols=sortCols, sortAscend=sortAscend, distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
-                         ldTruncIntrvSpecs=ldTruncIntrvSpecs, truncIntrvEpsilon=truncIntrvEpsilon,
-                         ldFilSorKeySchemes=ldFilSorKeySchemes)
-
-        assert self.CLOptimTruncFlag in self.miCustomCols
-
-    def copy(self, withData=True):
-    
-        """Clone function, with optional data copy"""
-    
-        # Create new instance with same ctor params.
-        clone = MCDSTruncOptanalysisResultsSet(miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
-                                               miSampleCols=self.miSampleCols, sampleIndCol=self.sampleIndCol,
-                                               sortCols=self.sortCols, sortAscend=self.sortAscend,
-                                               distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                               surveyType=self.surveyType, distanceType=self.distanceType,
-                                               clustering=self.clustering,
-                                               ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
-                                               truncIntrvEpsilon=self.truncIntrvEpsilon)
-
-        # Copy data if needed.
-        if withData:
-            clone._dfData = self._dfData.copy()
-            clone.rightColOrder = self.rightColOrder
-            clone.postComputed = self.postComputed
-
-        return clone
-    
-    # Post computations : Truncations groups.
-    @classmethod
-    def _sampleDistTruncGroups(cls, dfSampRes, ldIntrvSpecs, intrvEpsilon=1e-6):
-
-        """Compute distance truncation groups for 1 sample, for each target distance truncation column"""
-
-        # For each optimised or not case,
-        dTruncGroups = dict()  # key=ldIntrvSpecs[*]['col'], value=list(Series of group nums)
-        for isOpt in sorted(dfSampRes[cls.CLOptimTruncFlag].unique()):
-
-            logger.info4('* {}optimised'.format('' if isOpt else 'un').title())
-
-            # Compute truncation groups for this case and sample
-            dOptTruncGroups = \
-                super()._sampleDistTruncGroups(dfSampRes=dfSampRes[dfSampRes[cls.CLOptimTruncFlag] == isOpt],
-                                               ldIntrvSpecs=ldIntrvSpecs, intrvEpsilon=intrvEpsilon)
-
-            # Store them for later concatenation
-            for colAlias, sGrpNums in dOptTruncGroups.items():
-                if colAlias not in dTruncGroups:
-                    dTruncGroups[colAlias] = list()
-                dTruncGroups[colAlias].append(sGrpNums)
-
-        # Concat series of computed group nums (opt or not) for each target distance column to group
-        return {colAlias: pd.concat(lsGrpNums) for colAlias, lsGrpNums in dTruncGroups.items()}
-
-    # Post computations : Schemes for computing filtering and sorting keys (see inherited _postComputeFilterSortKeys).
-    AutoFilSorKeySchemes = \
-        [  # Orders inside groups with identical truncation params.
-         dict(key=Super.CLGrpOrdSmTrAic,  # Best AIC, for same left & right truncations (but variable nb of cut points)
-              sort=[MCDSAnalysis.CLParTruncLeft, MCDSAnalysis.CLParTruncRight,
-                    Super.CLDeltaAic, Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
-              ascend=[True, True, True, False, False, True, False, True],
-              group=[MCDSAnalysis.CLParTruncLeft, MCDSAnalysis.CLParTruncRight, MCDSAnalysis.CLParModFitDistCuts]),
-      
-         # Orders inside groups of close truncation params.
-         # dict(key=Super.CLGrpOrdClTrChi2,  # Best Chi2 inside groups of close truncation params
-         #      sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-         #            Super.CLChi2],
-         #      ascend=[True, True, True, False],
-         #      group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         # dict(key=Super.CLGrpOrdClTrKS,  # Best KS inside groups of close truncation params
-         #      sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-         #            Super.CLKS],
-         #      ascend=[True, True, True, False],
-         #      group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrDCv,  # Best DCv inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLDCv],
-              ascend=[True, True, True, True],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrChi2KSDCv,  # Best Chi2 & KS & DCv inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
-              ascend=[True, True, True, False, False, True, False, True],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-
-         dict(key=Super.CLGrpOrdClTrQuaBal1,  # Best Combined Quality 1 inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLCmbQuaBal1],
-              ascend=[True, True, True, False],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrQuaBal2,  # Best Combined Quality 2 inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLCmbQuaBal2],
-              ascend=[True, True, True, False],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrQuaBal3,  # Best Combined Quality 3 inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLCmbQuaBal3],
-              ascend=[True, True, True, False],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrQuaChi2,  # Best Combined Quality Chi2+ inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLCmbQuaChi2],
-              ascend=[True, True, True, False],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrQuaKS,  # Best Combined Quality KS+ inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLCmbQuaKS],
-              ascend=[True, True, True, False],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-         dict(key=Super.CLGrpOrdClTrQuaDCv,  # Best Combined Quality DCv+ inside groups of close truncation params
-              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
-                    Super.CLCmbQuaDCv],
-              ascend=[True, True, True, False],
-              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
-
-         # Global orders (no grouping by close or identical truncations)
-         dict(key=Super.CLGblOrdChi2KSDCv,
-              sort=[Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
-              ascend=[False, False, True, False, True]),
-         dict(key=Super.CLGblOrdQuaBal1,
-              sort=[Super.CLCmbQuaBal1], ascend=False),
-         dict(key=Super.CLGblOrdQuaBal2,
-              sort=[Super.CLCmbQuaBal2], ascend=False),
-         dict(key=Super.CLGblOrdQuaBal3,
-              sort=[Super.CLCmbQuaBal3], ascend=False),
-         dict(key=Super.CLGblOrdQuaChi2,
-              sort=[Super.CLCmbQuaChi2], ascend=False),
-         dict(key=Super.CLGblOrdQuaKS,
-              sort=[Super.CLCmbQuaKS], ascend=False),
-         dict(key=Super.CLGblOrdQuaDCv,
-              sort=[Super.CLCmbQuaDCv], ascend=False),
-
-         dict(key=Super.CLGblOrdDAicChi2KSDCv,
-              sort=[MCDSAnalysis.CLParTruncLeft, MCDSAnalysis.CLParTruncRight, MCDSAnalysis.CLParModFitDistCuts,
-                    Super.CLDeltaAic, Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
-              ascend=[True, True, True,
-                      True, False, False, True, False, True], napos='first')]
-
-    # Enforce uniqueness of keys in filter and sort key specs.
-    assert len(AutoFilSorKeySchemes) == len(set(scheme['key'] for scheme in AutoFilSorKeySchemes)), \
-           'Duplicated scheme key in MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
-
-    # Enforce uniqueness of sort and group column in filter and sort key specs.
-    assert all(len(scheme['sort']) == len(set(scheme['sort'])) for scheme in AutoFilSorKeySchemes), \
-           'Duplicated sort column spec in some scheme of MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
-    assert all(len(scheme.get('group', [])) == len(set(scheme.get('group', []))) for scheme in AutoFilSorKeySchemes), \
-           'Duplicated group column spec in some scheme of MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
-
-    # Check sort vs ascend list lengths in filter and sort key specs.
-    assert all(isinstance(scheme['ascend'], bool) or len(scheme['ascend']) == len(scheme['sort'])
-               for scheme in AutoFilSorKeySchemes), \
-           'Inconsistent ascend vs sort specs in some scheme of MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
-
-
-class MCDSTruncationOptanalyser(MCDSAnalyser):
-
-    """Run a bunch of MCDS analyses, with possibly automatic truncation parameter computation before"""
-
-    # Name of the spec. column to hold the "is truncation stuff optimised" 0/1 flag.
-    OptimTruncFlagCol = MCDSTruncOptanalysisResultsSet.OptimTruncFlagCol
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], 
-                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
-                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], 
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
-                                    dict(col='right', minDist=25.0, maxLen=25.0)], truncIntrvEpsilon=1e-6,
-                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False,
-                 logAnlysProgressEvery=50, logOptimProgressEvery=5, backupOptimEvery=50, autoClean=True,
-                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
-                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
-                 defMinDist=MCDSEngine.DistMinDef, defMaxDist=MCDSEngine.DistMaxDef, 
-                 defFitDistCuts=MCDSEngine.DistFitCutsDef, defDiscrDistCuts=MCDSEngine.DistDiscrCutsDef,
-                 defExpr2Optimise='chi2', defMinimiseExpr=False,
-                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,
-                 defFitDistCutsFctr=dict(min=2/3, max=3/2),
-                 defDiscrDistCutsFctr=dict(min=1/3, max=1),
-                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),
-                 dDefOptimCoreParams=dict(core='zoopt', maxIters=100, termExprValue=None,
-                                          algorithm='racos', maxRetries=0)):
-
-        """Ctor
-        
-        Parameters (see base class for missing ones : only specific stuff here):
-        :param anlysIndCol: Must not be None, needed for joining optimisation results.
-        
-        Parameters for auto-computing truncation:
-        :param def*: default values for "auto" specs etc.
-
-        Other parameters: See base class.
-        """
-
-        assert anlysIndCol, 'anlysIndCol must not be None, needed for analysis identification'
-
-        # Add OptimTruncFlag column to anlysSpecCustCols if not there
-        if self.OptimTruncFlagCol not in anlysSpecCustCols:
-            anlysSpecCustCols = anlysSpecCustCols + [self.OptimTruncFlagCol]
-
-        # Add OptimTruncFlag column to resultsHeadCols['after']
-        resultsHeadCols['after'].append(self.OptimTruncFlagCol)
-        
-        # Initialise base.
-        super().__init__(dfMonoCatObs, dfTransects=dfTransects,
-                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
-                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
-                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
-                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
-                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,
-                         anlysSpecCustCols=anlysSpecCustCols,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
-                         resultsHeadCols=resultsHeadCols,
-                         ldTruncIntrvSpecs=ldTruncIntrvSpecs, truncIntrvEpsilon=truncIntrvEpsilon,
-                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut,
-                         logData=logData, logProgressEvery=logAnlysProgressEvery,
-                         defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,
-                         defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,
-                         defMinDist=defMinDist, defMaxDist=defMaxDist, 
-                         defFitDistCuts=defFitDistCuts, defDiscrDistCuts=defDiscrDistCuts)
-
-        # Save Optimisation-specific args.
-        self.sampleDistCol = sampleDistCol
-        
-        self.logOptimProgressEvery = logOptimProgressEvery
-        self.backupOptimEvery = backupOptimEvery
-        self.autoClean = autoClean
-
-        # Default values for optimisation parameters.
-        self.defExpr2Optimise = defExpr2Optimise
-        self.defMinimiseExpr = defMinimiseExpr
-        self.dDefOptimCoreParams = dDefOptimCoreParams
-        self.defSubmitTimes = defSubmitTimes
-        self.defSubmitOnlyBest = defSubmitOnlyBest
-        self.dDefSubmitOtherParams = dDefSubmitOtherParams
-        self.defOutliersMethod = defOutliersMethod
-        self.defOutliersQuantCutPct = defOutliersQuantCutPct
-        self.defFitDistCutsFctr = defFitDistCutsFctr
-        self.defDiscrDistCutsFctr = defDiscrDistCutsFctr
-
-        # Specs.
-        self.updateSpecs(**{name: getattr(self, name)
-                            for name in ['defExpr2Optimise', 'defMinimiseExpr', 'dDefOptimCoreParams',
-                                         'defSubmitTimes', 'defSubmitOnlyBest', 'dDefSubmitOtherParams',
-                                         'defOutliersMethod', 'defOutliersQuantCutPct',
-                                         'defFitDistCutsFctr', 'defDiscrDistCutsFctr']})
-
-        # An optimiser instance only there for explicitParamSpecs() delegation.
-        # Note: For the moment, only zoopt engine supported
-        # TODO: Add support for other engines, thanks to the OptimCore columns spec (default = zoopt)
-        #       provided the associated MCDSXXXTruncationOptimiser classes derive from MCDSTruncationOptimiser.
-        # TODO: Make this instantiation simpler (most params unused => default values would then fit well !)
-        self.zoptr4Specs = \
-            MCDSZerothOrderTruncationOptimiser(
-                self.dfMonoCatObs, dfTransects=self._mcDataSet.dfTransects,
-                dSurveyArea=self._mcDataSet.dSurveyArea, transectPlaceCols=self._mcDataSet.transectPlaceCols,
-                passIdCol=self._mcDataSet.passIdCol, effortCol=self._mcDataSet.effortCol,
-                sampleSelCols=self.sampleSelCols, sampleDecCols=self._mcDataSet.sampleDecFields,
-                sampleDistCol=self.sampleDistCol, abbrevCol=self.abbrevCol, abbrevBuilder=self.abbrevBuilder,
-                anlysIndCol=self.anlysIndCol, sampleIndCol=self.sampleIndCol, anlysSpecCustCols=anlysSpecCustCols,
-                distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                surveyType=self.surveyType, distanceType=self.distanceType, clustering=self.clustering,
-                resultsHeadCols=dict(),
-                workDir=self.workDir, runMethod=self.runMethod, runTimeOut=self.runTimeOut,
-                logData=self.logData, logProgressEvery=self.logOptimProgressEvery,
-                backupEvery=self.backupOptimEvery, autoClean=self.autoClean,
-                defEstimKeyFn=self.defEstimKeyFn, defEstimAdjustFn=self.defEstimAdjustFn,
-                defEstimCriterion=self.defEstimCriterion, defCVInterval=self.defCVInterval,
-                defExpr2Optimise=self.defExpr2Optimise, defMinimiseExpr=self.defMinimiseExpr,
-                defOutliersMethod=self.defOutliersMethod, defOutliersQuantCutPct=self.defOutliersQuantCutPct,
-                defFitDistCutsFctr=self.defFitDistCutsFctr, defDiscrDistCutsFctr=self.defDiscrDistCutsFctr,
-                defSubmitTimes=self.defSubmitTimes, defSubmitOnlyBest=self.defSubmitOnlyBest,
-                defCoreMaxIters=self.dDefOptimCoreParams['maxIters'],
-                defCoreTermExprValue=self.dDefOptimCoreParams['termExprValue'],
-                defCoreAlgorithm=self.dDefOptimCoreParams['algorithm'],
-                defCoreMaxRetries=self.dDefOptimCoreParams['maxRetries'])
-
-        # Optimiser really used for optimisations, create in run() (debug use only).
-        self.zoptr = None
-
-    def explicitParamSpecs(self, implParamSpecs=None, dfExplParamSpecs=None, dropDupes=True, check=False):
-    
-        return self.zoptr4Specs.explicitParamSpecs(implParamSpecs=implParamSpecs, dfExplParamSpecs=dfExplParamSpecs,
-                                                   dropDupes=dropDupes, check=check)
-
-    @staticmethod
-    def analysisNeedsOptimisationFirst(sAnlysSpec):
-        return any(isinstance(v, str) for v in sAnlysSpec.values)
-
-    def computeUndeterminedParamSpecs(self, dfExplParamSpecs=None, implParamSpecs=None,
-                                      threads=None, recover=False):
-    
-        """Run truncation optimisation for analyses with undetermined truncation param. specs
-        and merge the computed specs to the ones of analyses with already determined truncation param. specs.
-        
-        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
-
-        Parameters:
-        :param dfExplParamSpecs: Explicit MCDS analysis param specs, as a DataFrame
-          (generated through explicitVariantSpecs, as an example),
-        :param implParamSpecs: Implicit MCDS analysis param specs, suitable for explicitation
-          through explicitVariantSpecs
-        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
-        :param recover: Recover a previous run interrupted during optimisations ; using last available backup file
-           
-        :return: the merged explicit param. specs for all the analyses (with optimised or not truncation param. specs)
-        """
-        
-        # 1. Explicitate, complete and check analysis specs (for usability).
-        # (should be also done before calling run, to avoid failure).
-        dfExplParamSpecs, userParamSpecCols, intParamSpecCols, _, checkVerdict, checkErrors = \
-            self.explicitParamSpecs(implParamSpecs, dfExplParamSpecs, dropDupes=True, check=True)
-        assert checkVerdict, 'Error: Analysis & optimisation params check failed: {}'.format('; '.join(checkErrors))        
-    
-        # 2. Extract optimisation specs (params specified as optimisation specs).
-        # a. Get spec. columns to examine for deciding if analyses specs imply prior optimisation or not.
-        optimUserParamSpecCols = \
-            self.zoptr4Specs.optimisationParamSpecUserNames(userParamSpecCols, intParamSpecCols)
-        
-        # b. Search for (possibly) optimisation specs with string data (const params are numbers or lists)
-        sbNeed4Opt1st = dfExplParamSpecs[optimUserParamSpecCols].apply(self.analysisNeedsOptimisationFirst,
-                                                                       axis='columns')
-        dfExplOptimParamSpecs = dfExplParamSpecs[sbNeed4Opt1st]
-         
-        # 3. Run optimisations if needed and replace computed truncation params in analysis specs
-        logger.info('Found {}/{} analysis specs implying some prior optimisation'
-                    .format(len(dfExplOptimParamSpecs), len(dfExplParamSpecs)))
-        if not dfExplOptimParamSpecs.empty:
-        
-            # Note: For the moment, only zoopt engine supported
-            # TODO: Add support for other engines, thanks to the OptimCore columns spec (default = zoopt)
-            #       provided the associated MCDSXXXTruncationOptimiser classes derive from MCDSTruncationOptimiser.
-            # a. Create optimiser object
-            self.zoptr = MCDSZerothOrderTruncationOptimiser(
-                        self.dfMonoCatObs, dfTransects=self._mcDataSet.dfTransects,
-                        dSurveyArea=self._mcDataSet.dSurveyArea, transectPlaceCols=self._mcDataSet.transectPlaceCols,
-                        passIdCol=self._mcDataSet.passIdCol, effortCol=self._mcDataSet.effortCol,
-                        sampleSelCols=self.sampleSelCols, sampleDecCols=self._mcDataSet.sampleDecFields,
-                        sampleDistCol=self.sampleDistCol, abbrevCol=self.abbrevCol, abbrevBuilder=self.abbrevBuilder,
-                        anlysIndCol=self.anlysIndCol, sampleIndCol=self.sampleIndCol, 
-                        anlysSpecCustCols=[self.OptimTruncFlagCol],
-                        distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                        surveyType=self.surveyType, distanceType=self.distanceType,
-                        clustering=self.clustering,
-                        resultsHeadCols=dict(before=[self.anlysIndCol, self.sampleIndCol],
-                                             sample=self.sampleSelCols, after=userParamSpecCols),
-                        workDir=self.workDir, runMethod=self.runMethod, runTimeOut=self.runTimeOut,
-                        logData=self.logData, logProgressEvery=self.logOptimProgressEvery,
-                        backupEvery=self.backupOptimEvery, autoClean=self.autoClean,
-                        defEstimKeyFn=self.defEstimKeyFn, defEstimAdjustFn=self.defEstimAdjustFn,
-                        defEstimCriterion=self.defEstimCriterion, defCVInterval=self.defCVInterval,
-                        defExpr2Optimise=self.defExpr2Optimise, defMinimiseExpr=self.defMinimiseExpr,
-                        defOutliersMethod=self.defOutliersMethod, defOutliersQuantCutPct=self.defOutliersQuantCutPct,
-                        defFitDistCutsFctr=self.defFitDistCutsFctr, defDiscrDistCutsFctr=self.defDiscrDistCutsFctr,
-                        defSubmitTimes=self.defSubmitTimes, defSubmitOnlyBest=self.defSubmitOnlyBest,
-                        defCoreMaxIters=self.dDefOptimCoreParams['maxIters'],
-                        defCoreTermExprValue=self.dDefOptimCoreParams['termExprValue'],
-                        defCoreAlgorithm=self.dDefOptimCoreParams['algorithm'],
-                        defCoreMaxRetries=self.dDefOptimCoreParams['maxRetries'])
-
-            # b. Run optimisations
-            optimResults = self.zoptr.run(dfExplOptimParamSpecs, threads=threads, recover=recover)
-            self.zoptr.shutdown()
-
-            # c. Merge optimisation results into param. specs.
-            # * Extract computed (= optimised) analysis params.
-            dfOptimRes = \
-                optimResults.dfSubData(columns=[self.anlysIndCol] + optimResults.optimisationTargetColumns())
-            dfOptimRes.set_index(self.anlysIndCol, inplace=True)
-            dfOptimRes.sort_index(inplace=True)
-
-            # Debug code for 2021-01-16 PM exception on np.repeat (l451 below) :
-            # ValueError: operands could not be broadcast together with shape (2520,) (2516,)
-            # Not reproduced after restart from last backup
-            optimResults.toExcel(self.workDir / 'optim-res.debug.xlsx')
-            dfOptimRes.to_excel(self.workDir / 'optim-res-opt.debug.xlsx')
-            dfExplOptimParamSpecs.to_excel(self.workDir / 'optim-specs.debug.xlsx')
-            # End debug.
-
-            # * Replicate optimisation specs as much as there are associated results
-            #   (optimisation may keep more than 1 "best" result row)
-            dfExplCompdParamSpecs = dfExplOptimParamSpecs.copy()
-            dfExplCompdParamSpecs.set_index(self.anlysIndCol, inplace=True)
-            dfExplCompdParamSpecs.sort_index(inplace=True)
-            dfExplCompdParamSpecs = \
-                dfExplCompdParamSpecs.loc[np.repeat(dfExplCompdParamSpecs.index,
-                                                    np.unique(dfOptimRes.index, return_counts=True)[1])]
-
-            # * Replace optim. specs by optim. results (optim. target columns = truncation param. ones)
-            optimTgtUserSpecCols = self.zoptr.optimisationTargetColumnUserNames()
-            dfOptimRes.rename(columns=dict(zip(optimResults.optimisationTargetColumns(),
-                                               optimTgtUserSpecCols)), inplace=True)
-            bdfToBeKeptSpecCells = ~dfExplCompdParamSpecs[optimTgtUserSpecCols].applymap(lambda v: isinstance(v, str))
-            dfExplCompdParamSpecs[optimTgtUserSpecCols] = \
-                dfExplCompdParamSpecs[optimTgtUserSpecCols].where(bdfToBeKeptSpecCells,
-                                                                  other=dfOptimRes[optimTgtUserSpecCols])
-
-            # * Concat const specs to computed ones.
-            dfExplParamSpecs.set_index(self.anlysIndCol, inplace=True)
-
-            dfExplConstParamSpecs = \
-                dfExplParamSpecs.loc[~dfExplParamSpecs.index.isin(dfExplOptimParamSpecs[self.anlysIndCol])].copy()
-            dfExplConstParamSpecs.reset_index(inplace=True)
-            dfExplConstParamSpecs[self.OptimTruncFlagCol] = 0  # Const = "unoptimised" truncation params.
-
-            dfExplCompdParamSpecs.reset_index(inplace=True)
-            dfExplCompdParamSpecs[self.OptimTruncFlagCol] = 1  # Computed = "optimised" truncation params.
-
-            dfExplParamSpecs = dfExplConstParamSpecs.append(dfExplCompdParamSpecs, ignore_index=True)
-            dfExplParamSpecs.sort_values(by=self.anlysIndCol, inplace=True)
-
-        else:
-
-            # Add non optimise(d) truncations analysis flag.
-            dfExplParamSpecs[self.OptimTruncFlagCol] = 0  # Const = "unoptimised" truncation params.
-
-        # Done.
-        return dfExplParamSpecs
-
-    def setupResults(self, ldFilSorKeySchemes=None):
-    
-        """Build an empty results objects.
-
-        Parameters:
-        :param ldFilSorKeySchemes: Replacement for MCDSTruncOptanalysisResultsSet predefined filter-sort key schemes
-                                   None => use predefined ones MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes.
-        """
-
-        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \
-            self.prepareResultsColumns()
-
-        return MCDSTruncOptanalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,
-                                              miSampleCols=miSampCols, sampleIndCol=sampIndMCol,
-                                              sortCols=sortCols, sortAscend=sortAscend,
-                                              distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                              surveyType=self.surveyType, distanceType=self.distanceType,
-                                              clustering=self.clustering,
-                                              ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
-                                              truncIntrvEpsilon=self.truncIntrvEpsilon,
-                                              ldFilSorKeySchemes=ldFilSorKeySchemes)
-    
-    def run(self, dfExplParamSpecs=None, implParamSpecs=None, threads=None, recoverOptims=False):
-    
-        """Run specified analyses, after automatic computing of truncation parameter if needed
-        
-        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
-
-        Parameters:
-        :param dfExplParamSpecs: Explicit MCDS analysis param specs, as a DataFrame
-          (generated through explicitVariantSpecs, as an example),
-        :param implParamSpecs: Implicit MCDS analysis param specs, suitable for explicitation
-          through explicitVariantSpecs
-        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
-        :param recoverOptims: Recover a previous run interrupted during optimisations ; using last available backup file
-           
-        :return: the MCDSTruncOptanalysisResultsSet holding the analyses results
-        """
-        
-        # 1. Run optimisations when needed and replace computed truncation params in analysis specs
-        #    (warning: as some optimisation specs may specify to keep more than 1 "best" result,
-        #              dfExplParamSpecs may grow accordingly, as well as the final number of result rows).
-        dfExplParamSpecs = \
-            self.computeUndeterminedParamSpecs(dfExplParamSpecs, implParamSpecs,
-                                               threads=threads, recover=recoverOptims)
-
-        # 2. Run all analyses, now all parameters are there.
-        return super().run(dfExplParamSpecs, threads=threads)
-
-    def shutdown(self):
-    
-        self.zoptr4Specs.shutdown()
-
-        super().shutdown()
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "optanalyser": Run a bunch of DS analyses according to a user-friendly set of analysis specs
+#  with possibly some undetermined analysis parameters in specs :
+#  for these analyses, an auto-computation of these parameters will then be run first
+#  through some optimisation engine specified in specs (only "zoopt" supported as for now),
+#  and for some kind of parameters (only distance truncations supported as for now).
+
+import numpy as np
+import pandas as pd
+
+from . import log
+from .engine import MCDSEngine
+from .analysis import MCDSAnalysis
+from .analyser import MCDSAnalyser, MCDSAnalysisResultsSet
+from .optimiser import MCDSZerothOrderTruncationOptimiser
+
+logger = log.logger('ads.onr')
+
+
+class MCDSTruncOptanalysisResultsSet(MCDSAnalysisResultsSet):
+
+    """A specialized results set for MCDS optanalysers"""
+
+    Super = MCDSAnalysisResultsSet
+
+    # Name of the spec. column to hold the "is truncation stuff optimised" 0/1 flag.
+    OptimTruncFlagCol = 'OptimTrunc'
+    CLOptimTruncFlag = ('header (tail)', OptimTruncFlagCol, 'Value')
+    
+    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,
+                 sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
+                                    dict(col='right', minDist=25.0, maxLen=25.0)],
+                 truncIntrvEpsilon=1e-6, ldFilSorKeySchemes=None):
+        
+        """Ctor
+
+        Parameters:
+        :param miSampleCols: columns to use for grouping by sample ; defaults to miCustomCols if None
+        :param sampleIndCol: multi-column index for the sample Id column ; no default, must be there !
+        :param ldFilSorKeySchemes: Replacement for predefined filter-sort key schemes
+                                   None => use predefined ones AutoFilSorKeySchemes.
+        """
+
+        super().__init__(miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
+                         miSampleCols=miSampleCols, sampleIndCol=sampleIndCol,
+                         sortCols=sortCols, sortAscend=sortAscend, distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
+                         ldTruncIntrvSpecs=ldTruncIntrvSpecs, truncIntrvEpsilon=truncIntrvEpsilon,
+                         ldFilSorKeySchemes=ldFilSorKeySchemes)
+
+        assert self.CLOptimTruncFlag in self.miCustomCols
+
+    def copy(self, withData=True):
+    
+        """Clone function, with optional data copy"""
+    
+        # Create new instance with same ctor params.
+        clone = MCDSTruncOptanalysisResultsSet(miCustomCols=self.miCustomCols, dfCustomColTrans=self.dfCustomColTrans,
+                                               miSampleCols=self.miSampleCols, sampleIndCol=self.sampleIndCol,
+                                               sortCols=self.sortCols, sortAscend=self.sortAscend,
+                                               distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                               surveyType=self.surveyType, distanceType=self.distanceType,
+                                               clustering=self.clustering,
+                                               ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
+                                               truncIntrvEpsilon=self.truncIntrvEpsilon)
+
+        # Copy data if needed.
+        if withData:
+            clone._dfData = self._dfData.copy()
+            clone.rightColOrder = self.rightColOrder
+            clone.postComputed = self.postComputed
+
+        return clone
+    
+    # Post computations : Truncations groups.
+    @classmethod
+    def _sampleDistTruncGroups(cls, dfSampRes, ldIntrvSpecs, intrvEpsilon=1e-6):
+
+        """Compute distance truncation groups for 1 sample, for each target distance truncation column"""
+
+        # For each optimised or not case,
+        dTruncGroups = dict()  # key=ldIntrvSpecs[*]['col'], value=list(Series of group nums)
+        for isOpt in sorted(dfSampRes[cls.CLOptimTruncFlag].unique()):
+
+            logger.info4('* {}optimised'.format('' if isOpt else 'un').title())
+
+            # Compute truncation groups for this case and sample
+            dOptTruncGroups = \
+                super()._sampleDistTruncGroups(dfSampRes=dfSampRes[dfSampRes[cls.CLOptimTruncFlag] == isOpt],
+                                               ldIntrvSpecs=ldIntrvSpecs, intrvEpsilon=intrvEpsilon)
+
+            # Store them for later concatenation
+            for colAlias, sGrpNums in dOptTruncGroups.items():
+                if colAlias not in dTruncGroups:
+                    dTruncGroups[colAlias] = list()
+                dTruncGroups[colAlias].append(sGrpNums)
+
+        # Concat series of computed group nums (opt or not) for each target distance column to group
+        return {colAlias: pd.concat(lsGrpNums) for colAlias, lsGrpNums in dTruncGroups.items()}
+
+    # Post computations : Schemes for computing filtering and sorting keys (see inherited _postComputeFilterSortKeys).
+    AutoFilSorKeySchemes = \
+        [  # Orders inside groups with identical truncation params.
+         dict(key=Super.CLGrpOrdSmTrAic,  # Best AIC, for same left & right truncations (but variable nb of cut points)
+              sort=[MCDSAnalysis.CLParTruncLeft, MCDSAnalysis.CLParTruncRight,
+                    Super.CLDeltaAic, Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
+              ascend=[True, True, True, False, False, True, False, True],
+              group=[MCDSAnalysis.CLParTruncLeft, MCDSAnalysis.CLParTruncRight, MCDSAnalysis.CLParModFitDistCuts]),
+      
+         # Orders inside groups of close truncation params.
+         # dict(key=Super.CLGrpOrdClTrChi2,  # Best Chi2 inside groups of close truncation params
+         #      sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+         #            Super.CLChi2],
+         #      ascend=[True, True, True, False],
+         #      group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         # dict(key=Super.CLGrpOrdClTrKS,  # Best KS inside groups of close truncation params
+         #      sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+         #            Super.CLKS],
+         #      ascend=[True, True, True, False],
+         #      group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrDCv,  # Best DCv inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLDCv],
+              ascend=[True, True, True, True],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrChi2KSDCv,  # Best Chi2 & KS & DCv inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
+              ascend=[True, True, True, False, False, True, False, True],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+
+         dict(key=Super.CLGrpOrdClTrQuaBal1,  # Best Combined Quality 1 inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLCmbQuaBal1],
+              ascend=[True, True, True, False],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrQuaBal2,  # Best Combined Quality 2 inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLCmbQuaBal2],
+              ascend=[True, True, True, False],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrQuaBal3,  # Best Combined Quality 3 inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLCmbQuaBal3],
+              ascend=[True, True, True, False],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrQuaChi2,  # Best Combined Quality Chi2+ inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLCmbQuaChi2],
+              ascend=[True, True, True, False],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrQuaKS,  # Best Combined Quality KS+ inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLCmbQuaKS],
+              ascend=[True, True, True, False],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+         dict(key=Super.CLGrpOrdClTrQuaDCv,  # Best Combined Quality DCv+ inside groups of close truncation params
+              sort=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight,
+                    Super.CLCmbQuaDCv],
+              ascend=[True, True, True, False],
+              group=[CLOptimTruncFlag, Super.CLGroupTruncLeft, Super.CLGroupTruncRight]),
+
+         # Global orders (no grouping by close or identical truncations)
+         dict(key=Super.CLGblOrdChi2KSDCv,
+              sort=[Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
+              ascend=[False, False, True, False, True]),
+         dict(key=Super.CLGblOrdQuaBal1,
+              sort=[Super.CLCmbQuaBal1], ascend=False),
+         dict(key=Super.CLGblOrdQuaBal2,
+              sort=[Super.CLCmbQuaBal2], ascend=False),
+         dict(key=Super.CLGblOrdQuaBal3,
+              sort=[Super.CLCmbQuaBal3], ascend=False),
+         dict(key=Super.CLGblOrdQuaChi2,
+              sort=[Super.CLCmbQuaChi2], ascend=False),
+         dict(key=Super.CLGblOrdQuaKS,
+              sort=[Super.CLCmbQuaKS], ascend=False),
+         dict(key=Super.CLGblOrdQuaDCv,
+              sort=[Super.CLCmbQuaDCv], ascend=False),
+
+         dict(key=Super.CLGblOrdDAicChi2KSDCv,
+              sort=[MCDSAnalysis.CLParTruncLeft, MCDSAnalysis.CLParTruncRight, MCDSAnalysis.CLParModFitDistCuts,
+                    Super.CLDeltaAic, Super.CLChi2, Super.CLKS, Super.CLDCv, Super.CLNObs, MCDSAnalysis.CLRunStatus],
+              ascend=[True, True, True,
+                      True, False, False, True, False, True], napos='first')]
+
+    # Enforce uniqueness of keys in filter and sort key specs.
+    assert len(AutoFilSorKeySchemes) == len(set(scheme['key'] for scheme in AutoFilSorKeySchemes)), \
+           'Duplicated scheme key in MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
+
+    # Enforce uniqueness of sort and group column in filter and sort key specs.
+    assert all(len(scheme['sort']) == len(set(scheme['sort'])) for scheme in AutoFilSorKeySchemes), \
+           'Duplicated sort column spec in some scheme of MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
+    assert all(len(scheme.get('group', [])) == len(set(scheme.get('group', []))) for scheme in AutoFilSorKeySchemes), \
+           'Duplicated group column spec in some scheme of MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
+
+    # Check sort vs ascend list lengths in filter and sort key specs.
+    assert all(isinstance(scheme['ascend'], bool) or len(scheme['ascend']) == len(scheme['sort'])
+               for scheme in AutoFilSorKeySchemes), \
+           'Inconsistent ascend vs sort specs in some scheme of MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes'
+
+
+class MCDSTruncationOptanalyser(MCDSAnalyser):
+
+    """Run a bunch of MCDS analyses, with possibly automatic truncation parameter computation before"""
+
+    # Name of the spec. column to hold the "is truncation stuff optimised" 0/1 flag.
+    OptimTruncFlagCol = MCDSTruncOptanalysisResultsSet.OptimTruncFlagCol
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], 
+                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
+                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], 
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),
+                                    dict(col='right', minDist=25.0, maxLen=25.0)], truncIntrvEpsilon=1e-6,
+                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False,
+                 logAnlysProgressEvery=50, logOptimProgressEvery=5, backupOptimEvery=50, autoClean=True,
+                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
+                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
+                 defMinDist=MCDSEngine.DistMinDef, defMaxDist=MCDSEngine.DistMaxDef, 
+                 defFitDistCuts=MCDSEngine.DistFitCutsDef, defDiscrDistCuts=MCDSEngine.DistDiscrCutsDef,
+                 defExpr2Optimise='chi2', defMinimiseExpr=False,
+                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,
+                 defFitDistCutsFctr=dict(min=2/3, max=3/2),
+                 defDiscrDistCutsFctr=dict(min=1/3, max=1),
+                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),
+                 dDefOptimCoreParams=dict(core='zoopt', maxIters=100, termExprValue=None,
+                                          algorithm='racos', maxRetries=0)):
+
+        """Ctor
+        
+        Parameters (see base class for missing ones : only specific stuff here):
+        :param anlysIndCol: Must not be None, needed for joining optimisation results.
+        
+        Parameters for auto-computing truncation:
+        :param def*: default values for "auto" specs etc.
+
+        Other parameters: See base class.
+        """
+
+        assert anlysIndCol, 'anlysIndCol must not be None, needed for analysis identification'
+
+        # Add OptimTruncFlag column to anlysSpecCustCols if not there
+        if self.OptimTruncFlagCol not in anlysSpecCustCols:
+            anlysSpecCustCols = anlysSpecCustCols + [self.OptimTruncFlagCol]
+
+        # Add OptimTruncFlag column to resultsHeadCols['after']
+        resultsHeadCols['after'].append(self.OptimTruncFlagCol)
+        
+        # Initialise base.
+        super().__init__(dfMonoCatObs, dfTransects=dfTransects,
+                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
+                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
+                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
+                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
+                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,
+                         anlysSpecCustCols=anlysSpecCustCols,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
+                         resultsHeadCols=resultsHeadCols,
+                         ldTruncIntrvSpecs=ldTruncIntrvSpecs, truncIntrvEpsilon=truncIntrvEpsilon,
+                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut,
+                         logData=logData, logProgressEvery=logAnlysProgressEvery,
+                         defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,
+                         defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,
+                         defMinDist=defMinDist, defMaxDist=defMaxDist, 
+                         defFitDistCuts=defFitDistCuts, defDiscrDistCuts=defDiscrDistCuts)
+
+        # Save Optimisation-specific args.
+        self.sampleDistCol = sampleDistCol
+        
+        self.logOptimProgressEvery = logOptimProgressEvery
+        self.backupOptimEvery = backupOptimEvery
+        self.autoClean = autoClean
+
+        # Default values for optimisation parameters.
+        self.defExpr2Optimise = defExpr2Optimise
+        self.defMinimiseExpr = defMinimiseExpr
+        self.dDefOptimCoreParams = dDefOptimCoreParams
+        self.defSubmitTimes = defSubmitTimes
+        self.defSubmitOnlyBest = defSubmitOnlyBest
+        self.dDefSubmitOtherParams = dDefSubmitOtherParams
+        self.defOutliersMethod = defOutliersMethod
+        self.defOutliersQuantCutPct = defOutliersQuantCutPct
+        self.defFitDistCutsFctr = defFitDistCutsFctr
+        self.defDiscrDistCutsFctr = defDiscrDistCutsFctr
+
+        # Specs.
+        self.updateSpecs(**{name: getattr(self, name)
+                            for name in ['defExpr2Optimise', 'defMinimiseExpr', 'dDefOptimCoreParams',
+                                         'defSubmitTimes', 'defSubmitOnlyBest', 'dDefSubmitOtherParams',
+                                         'defOutliersMethod', 'defOutliersQuantCutPct',
+                                         'defFitDistCutsFctr', 'defDiscrDistCutsFctr']})
+
+        # An optimiser instance only there for explicitParamSpecs() delegation.
+        # Note: For the moment, only zoopt engine supported
+        # TODO: Add support for other engines, thanks to the OptimCore columns spec (default = zoopt)
+        #       provided the associated MCDSXXXTruncationOptimiser classes derive from MCDSTruncationOptimiser.
+        # TODO: Make this instantiation simpler (most params unused => default values would then fit well !)
+        self.zoptr4Specs = \
+            MCDSZerothOrderTruncationOptimiser(
+                self.dfMonoCatObs, dfTransects=self._mcDataSet.dfTransects,
+                dSurveyArea=self._mcDataSet.dSurveyArea, transectPlaceCols=self._mcDataSet.transectPlaceCols,
+                passIdCol=self._mcDataSet.passIdCol, effortCol=self._mcDataSet.effortCol,
+                sampleSelCols=self.sampleSelCols, sampleDecCols=self._mcDataSet.sampleDecFields,
+                sampleDistCol=self.sampleDistCol, abbrevCol=self.abbrevCol, abbrevBuilder=self.abbrevBuilder,
+                anlysIndCol=self.anlysIndCol, sampleIndCol=self.sampleIndCol, anlysSpecCustCols=anlysSpecCustCols,
+                distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                surveyType=self.surveyType, distanceType=self.distanceType, clustering=self.clustering,
+                resultsHeadCols=dict(),
+                workDir=self.workDir, runMethod=self.runMethod, runTimeOut=self.runTimeOut,
+                logData=self.logData, logProgressEvery=self.logOptimProgressEvery,
+                backupEvery=self.backupOptimEvery, autoClean=self.autoClean,
+                defEstimKeyFn=self.defEstimKeyFn, defEstimAdjustFn=self.defEstimAdjustFn,
+                defEstimCriterion=self.defEstimCriterion, defCVInterval=self.defCVInterval,
+                defExpr2Optimise=self.defExpr2Optimise, defMinimiseExpr=self.defMinimiseExpr,
+                defOutliersMethod=self.defOutliersMethod, defOutliersQuantCutPct=self.defOutliersQuantCutPct,
+                defFitDistCutsFctr=self.defFitDistCutsFctr, defDiscrDistCutsFctr=self.defDiscrDistCutsFctr,
+                defSubmitTimes=self.defSubmitTimes, defSubmitOnlyBest=self.defSubmitOnlyBest,
+                defCoreMaxIters=self.dDefOptimCoreParams['maxIters'],
+                defCoreTermExprValue=self.dDefOptimCoreParams['termExprValue'],
+                defCoreAlgorithm=self.dDefOptimCoreParams['algorithm'],
+                defCoreMaxRetries=self.dDefOptimCoreParams['maxRetries'])
+
+        # Optimiser really used for optimisations, create in run() (debug use only).
+        self.zoptr = None
+
+    def explicitParamSpecs(self, implParamSpecs=None, dfExplParamSpecs=None, dropDupes=True, check=False):
+    
+        return self.zoptr4Specs.explicitParamSpecs(implParamSpecs=implParamSpecs, dfExplParamSpecs=dfExplParamSpecs,
+                                                   dropDupes=dropDupes, check=check)
+
+    @staticmethod
+    def analysisNeedsOptimisationFirst(sAnlysSpec):
+        return any(isinstance(v, str) for v in sAnlysSpec.values)
+
+    def computeUndeterminedParamSpecs(self, dfExplParamSpecs=None, implParamSpecs=None,
+                                      threads=None, recover=False):
+    
+        """Run truncation optimisation for analyses with undetermined truncation param. specs
+        and merge the computed specs to the ones of analyses with already determined truncation param. specs.
+        
+        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
+
+        Parameters:
+        :param dfExplParamSpecs: Explicit MCDS analysis param specs, as a DataFrame
+          (generated through explicitVariantSpecs, as an example),
+        :param implParamSpecs: Implicit MCDS analysis param specs, suitable for explicitation
+          through explicitVariantSpecs
+        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
+        :param recover: Recover a previous run interrupted during optimisations ; using last available backup file
+           
+        :return: the merged explicit param. specs for all the analyses (with optimised or not truncation param. specs)
+        """
+        
+        # 1. Explicitate, complete and check analysis specs (for usability).
+        # (should be also done before calling run, to avoid failure).
+        dfExplParamSpecs, userParamSpecCols, intParamSpecCols, _, checkVerdict, checkErrors = \
+            self.explicitParamSpecs(implParamSpecs, dfExplParamSpecs, dropDupes=True, check=True)
+        assert checkVerdict, 'Error: Analysis & optimisation params check failed: {}'.format('; '.join(checkErrors))        
+    
+        # 2. Extract optimisation specs (params specified as optimisation specs).
+        # a. Get spec. columns to examine for deciding if analyses specs imply prior optimisation or not.
+        optimUserParamSpecCols = \
+            self.zoptr4Specs.optimisationParamSpecUserNames(userParamSpecCols, intParamSpecCols)
+        
+        # b. Search for (possibly) optimisation specs with string data (const params are numbers or lists)
+        sbNeed4Opt1st = dfExplParamSpecs[optimUserParamSpecCols].apply(self.analysisNeedsOptimisationFirst,
+                                                                       axis='columns')
+        dfExplOptimParamSpecs = dfExplParamSpecs[sbNeed4Opt1st]
+         
+        # 3. Run optimisations if needed and replace computed truncation params in analysis specs
+        logger.info('Found {}/{} analysis specs implying some prior optimisation'
+                    .format(len(dfExplOptimParamSpecs), len(dfExplParamSpecs)))
+        if not dfExplOptimParamSpecs.empty:
+        
+            # Note: For the moment, only zoopt engine supported
+            # TODO: Add support for other engines, thanks to the OptimCore columns spec (default = zoopt)
+            #       provided the associated MCDSXXXTruncationOptimiser classes derive from MCDSTruncationOptimiser.
+            # a. Create optimiser object
+            self.zoptr = MCDSZerothOrderTruncationOptimiser(
+                        self.dfMonoCatObs, dfTransects=self._mcDataSet.dfTransects,
+                        dSurveyArea=self._mcDataSet.dSurveyArea, transectPlaceCols=self._mcDataSet.transectPlaceCols,
+                        passIdCol=self._mcDataSet.passIdCol, effortCol=self._mcDataSet.effortCol,
+                        sampleSelCols=self.sampleSelCols, sampleDecCols=self._mcDataSet.sampleDecFields,
+                        sampleDistCol=self.sampleDistCol, abbrevCol=self.abbrevCol, abbrevBuilder=self.abbrevBuilder,
+                        anlysIndCol=self.anlysIndCol, sampleIndCol=self.sampleIndCol, 
+                        anlysSpecCustCols=[self.OptimTruncFlagCol],
+                        distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                        surveyType=self.surveyType, distanceType=self.distanceType,
+                        clustering=self.clustering,
+                        resultsHeadCols=dict(before=[self.anlysIndCol, self.sampleIndCol],
+                                             sample=self.sampleSelCols, after=userParamSpecCols),
+                        workDir=self.workDir, runMethod=self.runMethod, runTimeOut=self.runTimeOut,
+                        logData=self.logData, logProgressEvery=self.logOptimProgressEvery,
+                        backupEvery=self.backupOptimEvery, autoClean=self.autoClean,
+                        defEstimKeyFn=self.defEstimKeyFn, defEstimAdjustFn=self.defEstimAdjustFn,
+                        defEstimCriterion=self.defEstimCriterion, defCVInterval=self.defCVInterval,
+                        defExpr2Optimise=self.defExpr2Optimise, defMinimiseExpr=self.defMinimiseExpr,
+                        defOutliersMethod=self.defOutliersMethod, defOutliersQuantCutPct=self.defOutliersQuantCutPct,
+                        defFitDistCutsFctr=self.defFitDistCutsFctr, defDiscrDistCutsFctr=self.defDiscrDistCutsFctr,
+                        defSubmitTimes=self.defSubmitTimes, defSubmitOnlyBest=self.defSubmitOnlyBest,
+                        defCoreMaxIters=self.dDefOptimCoreParams['maxIters'],
+                        defCoreTermExprValue=self.dDefOptimCoreParams['termExprValue'],
+                        defCoreAlgorithm=self.dDefOptimCoreParams['algorithm'],
+                        defCoreMaxRetries=self.dDefOptimCoreParams['maxRetries'])
+
+            # b. Run optimisations
+            optimResults = self.zoptr.run(dfExplOptimParamSpecs, threads=threads, recover=recover)
+            self.zoptr.shutdown()
+
+            # c. Merge optimisation results into param. specs.
+            # * Extract computed (= optimised) analysis params.
+            dfOptimRes = \
+                optimResults.dfSubData(columns=[self.anlysIndCol] + optimResults.optimisationTargetColumns())
+            dfOptimRes.set_index(self.anlysIndCol, inplace=True)
+            dfOptimRes.sort_index(inplace=True)
+
+            # Debug code for 2021-01-16 PM exception on np.repeat (l451 below) :
+            # ValueError: operands could not be broadcast together with shape (2520,) (2516,)
+            # Not reproduced after restart from last backup
+            optimResults.toExcel(self.workDir / 'optim-res.debug.xlsx')
+            dfOptimRes.to_excel(self.workDir / 'optim-res-opt.debug.xlsx')
+            dfExplOptimParamSpecs.to_excel(self.workDir / 'optim-specs.debug.xlsx')
+            # End debug.
+
+            # * Replicate optimisation specs as much as there are associated results
+            #   (optimisation may keep more than 1 "best" result row)
+            dfExplCompdParamSpecs = dfExplOptimParamSpecs.copy()
+            dfExplCompdParamSpecs.set_index(self.anlysIndCol, inplace=True)
+            dfExplCompdParamSpecs.sort_index(inplace=True)
+            dfExplCompdParamSpecs = \
+                dfExplCompdParamSpecs.loc[np.repeat(dfExplCompdParamSpecs.index,
+                                                    np.unique(dfOptimRes.index, return_counts=True)[1])]
+
+            # * Replace optim. specs by optim. results (optim. target columns = truncation param. ones)
+            optimTgtUserSpecCols = self.zoptr.optimisationTargetColumnUserNames()
+            dfOptimRes.rename(columns=dict(zip(optimResults.optimisationTargetColumns(),
+                                               optimTgtUserSpecCols)), inplace=True)
+            bdfToBeKeptSpecCells = ~dfExplCompdParamSpecs[optimTgtUserSpecCols].applymap(lambda v: isinstance(v, str))
+            dfExplCompdParamSpecs[optimTgtUserSpecCols] = \
+                dfExplCompdParamSpecs[optimTgtUserSpecCols].where(bdfToBeKeptSpecCells,
+                                                                  other=dfOptimRes[optimTgtUserSpecCols])
+
+            # * Concat const specs to computed ones.
+            dfExplParamSpecs.set_index(self.anlysIndCol, inplace=True)
+
+            dfExplConstParamSpecs = \
+                dfExplParamSpecs.loc[~dfExplParamSpecs.index.isin(dfExplOptimParamSpecs[self.anlysIndCol])].copy()
+            dfExplConstParamSpecs.reset_index(inplace=True)
+            dfExplConstParamSpecs[self.OptimTruncFlagCol] = 0  # Const = "unoptimised" truncation params.
+
+            dfExplCompdParamSpecs.reset_index(inplace=True)
+            dfExplCompdParamSpecs[self.OptimTruncFlagCol] = 1  # Computed = "optimised" truncation params.
+
+            dfExplParamSpecs = dfExplConstParamSpecs.append(dfExplCompdParamSpecs, ignore_index=True)
+            dfExplParamSpecs.sort_values(by=self.anlysIndCol, inplace=True)
+
+        else:
+
+            # Add non-optimise(d) truncations analysis flag.
+            dfExplParamSpecs[self.OptimTruncFlagCol] = 0  # Const = "unoptimised" truncation params.
+
+        # Done.
+        return dfExplParamSpecs
+
+    def setupResults(self, ldFilSorKeySchemes=None):
+    
+        """Build an empty results objects.
+
+        Parameters:
+        :param ldFilSorKeySchemes: Replacement for MCDSTruncOptanalysisResultsSet predefined filter-sort key schemes
+                                   None => use predefined ones MCDSTruncOptanalysisResultsSet.AutoFilSorKeySchemes.
+        """
+
+        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \
+            self.prepareResultsColumns()
+
+        return MCDSTruncOptanalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,
+                                              miSampleCols=miSampCols, sampleIndCol=sampIndMCol,
+                                              sortCols=sortCols, sortAscend=sortAscend,
+                                              distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                              surveyType=self.surveyType, distanceType=self.distanceType,
+                                              clustering=self.clustering,
+                                              ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,
+                                              truncIntrvEpsilon=self.truncIntrvEpsilon,
+                                              ldFilSorKeySchemes=ldFilSorKeySchemes)
+    
+    def run(self, dfExplParamSpecs=None, implParamSpecs=None, threads=None, recoverOptims=False):
+    
+        """Run specified analyses, after automatic computing of truncation parameter if needed
+        
+        Call explicitParamSpecs(..., check=True) before this to make sure user specs are OK
+
+        Parameters:
+        :param dfExplParamSpecs: Explicit MCDS analysis param specs, as a DataFrame
+          (generated through explicitVariantSpecs, as an example),
+        :param implParamSpecs: Implicit MCDS analysis param specs, suitable for explicitation
+          through explicitVariantSpecs
+        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
+        :param recoverOptims: Recover a previous run interrupted during optimisations ; using last available backup file
+           
+        :return: the MCDSTruncOptanalysisResultsSet holding the analyses results
+        """
+        
+        # 1. Run optimisations when needed and replace computed truncation params in analysis specs
+        #    (warning: as some optimisation specs may specify to keep more than 1 "best" result,
+        #              dfExplParamSpecs may grow accordingly, as well as the final number of result rows).
+        dfExplParamSpecs = \
+            self.computeUndeterminedParamSpecs(dfExplParamSpecs, implParamSpecs,
+                                               threads=threads, recover=recoverOptims)
+
+        # 2. Run all analyses, now all parameters are there.
+        return super().run(dfExplParamSpecs, threads=threads)
+
+    def shutdown(self):
+    
+        self.zoptr4Specs.shutdown()
+
+        super().shutdown()
```

### Comparing `pyaudisam-0.9.3/pyaudisam/optimisation.py` & `pyaudisam-1.0.1/pyaudisam/optimisation.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,792 +1,792 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "optimisation": One layer above analysis, to optimise some of the parameters of one DS analysis.
-
-# Warning: If you add/remove/change optimisation parameters to XXOptimisation class ctors,
-#          remember to update ParmXX constants accordingly in XXOptimiser classes.
-
-from collections import namedtuple as ntuple
-
-import math
-import numpy as np
-import pandas as pd
-
-import zoopt
-import pkg_resources as pkgrsc  # zoopt has no standard __version__ !
-
-from . import log, runtime
-from .executor import Executor
-from .engine import MCDSEngine
-from .analysis import MCDSAnalysis
-from .analyser import MCDSAnalysisResultsSet
-
-runtime.update({'zoopt': pkgrsc.get_distribution('zoopt').version})
-
-logger = log.logger('ads.opn')
-
-
-class Interval(object):
-
-    """A basic closed interval class for numbers"""
-    
-    def __init__(self, min=0, max=-1):
-    
-        """Ctor
-        
-        Parameters:
-        :param min: min of interval if a number, 
-                    or interval itself if a (min, max) tuple/list or a dict(min=, max=) or a Interval
-        :param max: max of interval if min is a number, ignored otherwise
-        
-        Ex: Interval(min=3, max=6.9), Interval((2.3, 4.2)), Interval(dict(max=2.3, min=1.2))
-        """
-    
-        if isinstance(min, Interval):
-            self.min = min.min
-            self.max = min.max
-        elif isinstance(min, (tuple, list)):
-            self.min = min[0]
-            self.max = min[1]
-        elif isinstance(min, dict):
-            self.min = min['min']
-            self.max = min['max']
-        else:
-            self.min = min
-            self.max = max
-        
-    def check(self, order=False, minRange=(None, None), maxRange=(None, None)):
-    
-        errors = list()  # No error by default.
-        
-        if order and self.min > self.max:
-            errors.append(f'min:{self.min} > max:{self.max}')
-            
-        if minRange[0] is not None and self.min < minRange[0]:
-            errors.append(f'min:{self.min} < {minRange[0]}')
-        if minRange[1] is not None and self.min > minRange[1]:
-            errors.append(f'min:{self.min} > {minRange[1]}')
-        
-        if maxRange[0] is not None and self.max < maxRange[0]:
-            errors.append(f'max:{self.max} < {maxRange[0]}')
-        if maxRange[1] is not None and self.max > maxRange[1]:
-            errors.append(f'max:{self.max} > {maxRange[1]}')
-        
-        return ', '.join(errors)
-        
-    def __repr__(self):
-    
-        return '[{}, {}]'.format(self.min, self.max)
-        
-
-class Error(object):
-
-    """Error class for shipping error messages to the end user"""
-
-    def __init__(self, error=None, head=''):
-        
-        """Ctor
-        
-        Parameters:
-        :param error: string or Error
-        :error head: string ; ignored if error is an Error
-        """
-    
-        self.heads = list()
-        self.errors = list()
-        
-        if head or error:
-            self.append(error, head)
-        
-    def append(self, error, head=''):
-    
-        """Append an error to an other
-        
-        Parameters:
-        :param error: string or Error
-        :param head: string ; ignored if error is an Error
-        """
-        
-        if isinstance(error, self.__class__):
-            self.heads += error.heads
-            self.errors += error.errors
-        else:
-            self.heads.append(head)
-            self.errors.append(error)
-        
-    def __repr__(self):
-    
-        msgs = list()
-        prvHd = ''
-        for hd, err in zip(self.heads, self.errors):
-            msg = ''
-            if hd != prvHd and hd:
-                msg += hd + ' : '
-            msg += err
-            msgs.append(msg)
-            prvHd = hd
-        return ' & '.join(msgs)
-        
-    def __bool__(self):
-    
-        return any(err for err in self.errors)
-
-
-class DSOptimisation(object):
-    
-    """DSOptimisation (abstract) : A distance sampling analysis optimisation
-         possibly run in parallel with others, through an asynchronous "submit then getResults" scheme.
-    """
-    
-    def __init__(self, engine, sampleDataSet, name=None,
-                 executor=None, customData=None, error=None,
-                 expr2Optimise='chi2', minimiseExpr=False, **optimCoreOptions):
-        
-        """Ctor
-
-        Parameters:
-        :param engine: DS engine to use
-        :param sampleDataSet: data.SampleDataSet instance to use
-        :param name: used for prefixing run folders (sure to be automatically unique anyway),
-            analysis names, and so on, only for user-friendliness and deasier debugging ;
-            default: None => auto-generated from optimisation parameters
-        :param executor: Executor object to use for parallel execution of multiple optimisation instances
-             Note: Up to the caller to shut it down when no more needed (not owned).
-        :param customData: custom data for the run analyses to ship through
-        :param error: if an error occurred somewhere before, a string explaining it
-                      in order to prevent real submission, but still for keeping trace
-                      of unrun optimisations in optimiser results table :
-                      the optimisation then always returns at least a 1-row (empty/null) result + errors.
-        :param string expr2Optimise: Math. expression (python syntax) to optimise,
-                      using analyses results var. names inside (see derived classes for details)
-        :param minimiseExpr: True for minimisation of expr2Optimise, false for maximisation
-        :param optimCoreOptions: dict of specific options for the optimising core below
-        """
-        
-        self.engine = engine
-        self.executor = executor if executor is not None else Executor()
-        self.sampleDataSet = sampleDataSet
-        self.name = name
-        self.customData = customData
-        self.setupError = error
-        self.expr2Optimise = expr2Optimise
-        self.minimiseExpr = minimiseExpr
-        self.optimCoreOptions = optimCoreOptions
-        
-    def _run(self, times=1, onlyBest=None, *args, **kwargs):
-        
-        """Really do the optimisation work : run the optimiser for this
-        (this method is called by the executor thread/process that takes it from the submit queue)
-
-        Parameters:
-        :param times: Number of times to auto-run the optimisation (> 0 ; default = 1)
-        :param onlyBest: When multiple runs, number of best optimisations to retain (> 0 ; default = all runs)
-        :param *args, **kwargs: other _run params
-        :return: List of "solutions", each as a dict with target analysis params in the ctor order
-                 and then { expr2Optimise: analysis value }
-        """
-
-        raise NotImplementedError('DSOptimisation is an abstract class : implement _run in a derived class')
-    
-    def submit(self, times=1, onlyBest=None, error=None, *args, **kwargs):
-    
-        """Submit the optimisation, to run it possibly in parallel with others (through self.executor)
-        
-        :param times: Number of times to auto-run the optimisation (> 0 ; default = 1)
-        :param onlyBest: Number of best run results to keep (> 0 ; default None = all runs)
-        :param error: if an error occurred somewhere before since construction, a string explaining it
-                      in order to prevent real submission, but still for keeping trace
-                      of unrun optimisations in optimiser results table :
-                      the optimisation then always returns at least a 1-row (empty/null) result + errors.
-        :param args, kwargs: other _run arguments
-        """
-
-        # Submit optimisation work and return a Future object to ask from and wait for its results.
-        self.future = \
-            self.executor.submit(self._run, *args, 
-                                 **{'times': times, 'onlyBest': onlyBest, 'error': error, **kwargs})
-        
-        return self.future
-
-    def functionValue(self, anlysValue):
-    
-        """Analysis value to function (to optimise) value
-        
-        Inverse of functionValue (functionValue(analysisValue(x)) = x).
-        """
-        if pd.isnull(anlysValue):
-            funcValue = None
-        else:
-            funcValue = anlysValue if self.minimiseExpr else -anlysValue
-            
-        return funcValue
-    
-    def analysisValue(self, fnValue):
-    
-        """Function (to optimise) value to analysis value
-        
-        Inverse of functionValue (analysisValue(functionValue(x)) = x).
-        """
-    
-        return fnValue if self.minimiseExpr else -fnValue 
-    
-
-# class MCDSOptimisation(DSOptimisation):
-#
-#     """Optimisation for MCDS analyses ... stuff
-#     """
-#
-#     pass
-
-
-class MCDSTruncationOptimisation(DSOptimisation):
-
-    """Optimisation for MCDS analyses distance truncations
-    """
-    
-    EngineClass = MCDSEngine
-        
-    # Names of possible solution dimensions (the truncation parameter values we are searching for).
-    SolutionDimensionNames = ['minDist', 'maxDist', 'fitDistCuts', 'discrDistCuts']
-    
-    def __init__(self, engine, sampleDataSet, name=None, executor=None,
-                 distanceField='Distance', customData=None,
-                 logData=False, autoClean=True, error=None,
-                 estimKeyFn=EngineClass.EstKeyFnDef, estimAdjustFn=EngineClass.EstAdjustFnDef,
-                 estimCriterion=EngineClass.EstCriterionDef, cvInterval=EngineClass.EstCVIntervalDef,
-                 minDist=None, maxDist=None, fitDistCutsFctr=None, discrDistCutsFctr=None,
-                 fitDistCuts=None, discrDistCuts=None,
-                 expr2Optimise='chi2', minimiseExpr=False, **optimCoreOptions):
-
-        """Ctor
-        
-        Parameters (see base class, specific ones only here):
-        :param distanceField: Name of distance data column in sampleDataSet table
-        :param error: a string for explaining any pre-init error that'll prevent the optimisation from running,
-                      but will keep producing results (null of course) ;
-                      this is done for keeping trace of unrun optimisations in results table (1 empty/null row).
-
-        Fixed analysis parameters (see MCDSAnalysis):
-        :param estimKeyFn: 
-        :param estimAdjustFn: 
-        :param estimCriterion: 
-        :param cvInterval:  
-
-        Optimisation target parameters (at least one MUST be not None):
-        :param minDist: Min, max Interval() or 2-item list or tuple,
-                        or int / float constant for left truncation distance ;
-                        default: None => not optimised
-        :param maxDist: Idem, for right truncation distance ;
-                        default: None => not optimised
-        :param fitDistCutsFctr: Idem, for the mult. factor to apply to sqrt(nb of sightings)
-                        to get the number of distance cut intervals (for _model_fitting_) ;
-                        default: None => not optimised if not fitDistCuts
-        :param discrDistCutsFctr: Idem for the mult. factor to apply to sqrt(nb of sightings)
-                        for the number of distance cut intervals (for _distance_values_discretisation_) ;
-                        default: None => not optimised if not discrDistCuts
-        :param fitDistCuts: Idem for the absolute number of distance cut intervals
-                        (for _model_fitting_) if fitDistCutsFctr is None ;
-                        default: None => fitDistCuts not optimised if not fitDistCutsFctr
-        :param discrDistCuts: Idem for the number of distance cut intervals
-                        (for _distance_values_discretisation_) if discrDistCutsFctr is None ;
-                        default: None => not optimised if not discrDistCutsFctr
-        """
-
-        # Check engine
-        assert isinstance(engine, MCDSEngine), 'Engine must be an MCDSEngine'
-        
-        # Check and prepare analysis and optimisation params
-        moreError = Error()
-        if not (len(estimKeyFn) >= 2 and estimKeyFn in [kf[:len(estimKeyFn)] for kf in engine.EstKeyFns]):
-            moreError.append('Invalid estimate key function {}: should be in {} or at least 2-char abbreviations'
-                             .format(estimKeyFn, engine.EstKeyFns))
-        if not (len(estimAdjustFn) >= 2 and estimAdjustFn in [kf[:len(estimAdjustFn)] for kf in engine.EstAdjustFns]):
-            moreError.append('Invalid estimate adjust function {}: should be in {} or at least 2-char abbreviations'
-                             .format(estimAdjustFn, engine.EstAdjustFns))
-        if not (estimCriterion in engine.EstCriteria):
-            moreError.append('Invalid estimate criterion {}: should be in {}'
-                             .format(estimCriterion, engine.EstCriteria))
-        if not (0 < cvInterval < 100):
-            moreError.append('Invalid cvInterval {}% : should be in {}'.format(cvInterval, ']0%, 100%['))
-               
-        if not any(optPar is not None for optPar in [minDist, maxDist, fitDistCuts, discrDistCuts]):
-            moreError.append('At least 1 analysis parameter has to be optimised')
-            
-        self.dConstParams = dict()  # Params that won't be optimised : constants.
-        if minDist is not None and not isinstance(minDist, (int, float)):
-            minDist = Interval(minDist)
-        elif minDist is not None:
-            self.dConstParams['minDist'] = minDist
-            
-        if maxDist is not None and not isinstance(maxDist, (int, float)):
-            maxDist = Interval(maxDist)
-        elif maxDist is not None:
-            self.dConstParams['maxDist'] = maxDist
-            
-        if fitDistCutsFctr is not None:
-            fitDistCutsFctr = Interval(fitDistCutsFctr)
-        if fitDistCuts is not None and not isinstance(fitDistCuts, (int, float)):
-            fitDistCuts = Interval(fitDistCuts)
-        elif fitDistCuts is not None and fitDistCutsFctr is None:
-            self.dConstParams['fitDistCuts'] = fitDistCuts
-            
-        if discrDistCutsFctr is not None:
-            discrDistCutsFctr = Interval(discrDistCutsFctr)
-        if discrDistCuts is not None and not isinstance(discrDistCuts, (int, float)):
-            discrDistCuts = Interval(discrDistCuts)
-        elif discrDistCuts is not None and discrDistCutsFctr is None:
-            self.dConstParams['discrDistCuts'] = discrDistCuts
-
-        if self.dConstParams:
-            logger.info1(f'TrOptimisation({self.dConstParams})')
-        
-        if not (minDist is None or isinstance(minDist, (int, float)) or 0 <= minDist.min < minDist.max):
-            moreError.append('Invalid left truncation distance {}'.format(minDist))
-        if not (maxDist is None or isinstance(maxDist, (int, float)) or 0 <= maxDist.min < maxDist.max):
-            moreError.append('Invalid right truncation distance {}'.format(maxDist))
-        if not (minDist is None or isinstance(minDist, (int, float))
-                or maxDist is None or isinstance(maxDist, (int, float)) or minDist.max < maxDist.min):
-            moreError.append('Max left truncation distance {} must be lower than min right one {}'
-                             .format(minDist.max, maxDist.min))
-        
-        if not (fitDistCutsFctr is None or 0 <= fitDistCutsFctr.min < fitDistCutsFctr.max):
-            moreError.append('Invalid mult. factor for number of fitting distance cuts {}'
-                             .format(fitDistCutsFctr))
-        if not (discrDistCutsFctr is None or 0 <= discrDistCutsFctr.min < discrDistCutsFctr.max):
-            moreError.append('Invalid mult. factor number for distance discretisation cuts {}'
-                             .format(discrDistCutsFctr))
-        
-        if not (fitDistCutsFctr is None or fitDistCuts is None):
-            moreError.append('Can\'t specify both absolute value and mult. factor'
-                             ' for number of discretisation distance cuts')
-        if not (fitDistCuts is None or isinstance(fitDistCuts, (int, float))
-                or 2 <= fitDistCuts.min < fitDistCuts.max):
-            moreError.append('Invalid number of fitting distance cuts {}'.format(fitDistCuts))
-        if not (discrDistCutsFctr is None or discrDistCuts is None):
-            moreError.append('Can\'t specify both absolute value and mult. factor'
-                             ' for number of discretisation distance cuts')
-        if not (discrDistCuts is None or isinstance(discrDistCuts, (int, float))
-                or 2 <= discrDistCuts.min < discrDistCuts.max):
-            moreError.append('Invalid number of distance discretisation cuts {}'.format(discrDistCuts))
-        
-        # Build name from main params if not specified
-        if name is None:
-            fields = ['mcds'] + [p[:3].lower() for p in [estimKeyFn, estimAdjustFn]]
-            if estimCriterion != self.EngineClass.EstCriterionDef:
-                fields.append(estimCriterion.lower())
-            if cvInterval != self.EngineClass.EstCVIntervalDef:
-                fields.append(str(cvInterval))
-            name = '-'.join(fields)
-
-        # Show and merge errors if any.
-        if moreError:
-            logger.error('Check failed for optimisation params: ' + str(moreError))
-            if error:
-                error.append(moreError)
-            else:
-                error = moreError
-        
-        # Initialise base.
-        super().__init__(engine, sampleDataSet, name=name, 
-                         executor=executor, customData=customData, error=error,
-                         expr2Optimise=expr2Optimise, minimiseExpr=minimiseExpr, **optimCoreOptions)
-                
-        # Save / compute params.
-        # a. Analysis
-        self.logData = logData
-        self.autoClean = autoClean
-        self.estimKeyFn = estimKeyFn
-        self.estimAdjustFn = estimAdjustFn
-        self.estimCriterion = estimCriterion
-        self.cvInterval = cvInterval
-
-        # b. Analysis or optimisation (whether const or variant params)
-        self.minDist = minDist
-        self.maxDist = maxDist
-        if fitDistCutsFctr is not None or discrDistCutsFctr is not None:
-            sqrtNbSights = math.sqrt(len(sampleDataSet.dfData[distanceField].dropna()))
-        if fitDistCutsFctr is not None:
-            self.fitDistCuts = Interval(min=int(round(fitDistCutsFctr.min*sqrtNbSights)),
-                                        max=int(round(fitDistCutsFctr.max*sqrtNbSights)))
-        else:
-            self.fitDistCuts = fitDistCuts
-        if discrDistCutsFctr is not None:
-            self.discrDistCuts = Interval(min=int(round(discrDistCutsFctr.min*sqrtNbSights)),
-                                          max=int(round(discrDistCutsFctr.max*sqrtNbSights)))
-        else:
-            self.discrDistCuts = discrDistCuts
-            
-        # Other optimisation stuff.
-        fltSup = float('inf')  # sys.float_info.max
-        self.invalidFuncValue = fltSup if minimiseExpr else -fltSup
-
-        # Where to store analyses elapsed times before computing stats at the end.
-        self.elapsedTimes = list()
-
-    # Post-process analysis results (adapted from MCDSAnalysisResultsSet.postComputeColumns)
-    @staticmethod
-    def _postProcessAnalysisResults(sResults):
-
-        RSClass = MCDSAnalysisResultsSet  # ResultsSet class
-
-        # Chi2 test probability.
-        chi2AllColLbls = [col for col in RSClass.CLsChi2All if col in sResults.keys()]
-        sResults[RSClass.CLChi2] = RSClass._determineChi2Value(sResults[chi2AllColLbls])
-
-        # Combined quality indicators
-        # a. Make sure requested columns are there, and add them if not (NaN value)
-        miCompCols = RSClass.CLsQuaIndicSources
-        for miCol in miCompCols:
-            if miCol not in sResults.index:
-                sResults[miCol] = np.nan
-
-        # b. NaN value MUST kill down the indicators to compute => we have to enforce this
-        sResults.fillna({RSClass.CLNObs: RSClass.KilrNObs,
-                         RSClass.CLChi2: RSClass.KilrStaTest, RSClass.CLKS: RSClass.KilrStaTest,
-                         RSClass.CLCvMUw: RSClass.KilrStaTest, RSClass.CLCvMCw: RSClass.KilrStaTest,
-                         RSClass.CLDCv: RSClass.KilrDensCv,  # Usually considered good under 0.3
-                         RSClass.CLNTotObs: RSClass.KilrNTotObs,  # Should slap down _normObs whatever NObs
-                         RSClass.CLNAdjPars: RSClass.KilrNPars,  # Should slap down _normNAdjPars whatever NObs
-                         RSClass.CLNTotPars: RSClass.KilrNPars},
-                        inplace=True)
-
-        # c. Compute indicators at last !
-        aCombQuaData = np.expand_dims(sResults[miCompCols].values, axis=0)  # Series to 1 row results array.
-        sResults[RSClass.CLCmbQuaBal1] = RSClass._combinedQualityBalanced1(aCombQuaData)
-        for miCol, aIndic in zip(RSClass.CLsNewQuaIndics, RSClass._combinedQualityAll(aCombQuaData)):
-            sResults[miCol] = aIndic[0]
-
-        return sResults
-
-    # Alias and name / index (in analysis results) of results values available
-    # for use in analysis value computation expressions
-    # And the worst possible value for each, for (bad) default value when not present in results for some reason.
-    # Warning: Don't use np.nan for these worst values : at least zoopt doesn't like it !
-    RSClass = MCDSAnalysisResultsSet
-    AnlysResultsIndex = \
-        dict(chi2=(RSClass.CLChi2, RSClass.KilrStaTest), ks=(RSClass.CLKS, RSClass.KilrStaTest),
-             cvmuw=(RSClass.CLCvMUw, RSClass.KilrStaTest), cvmcw=(RSClass.CLCvMCw, RSClass.KilrStaTest),
-             dcv=(RSClass.CLDCv, RSClass.KilrDensCv),
-             balq1=(RSClass.CLCmbQuaBal1, RSClass.KilrBalQua), balq2=(RSClass.CLCmbQuaBal2, RSClass.KilrBalQua),
-             balq3=(RSClass.CLCmbQuaBal3, RSClass.KilrBalQua),
-             balqc2=(RSClass.CLCmbQuaChi2, RSClass.KilrBalQua), balqks=(RSClass.CLCmbQuaKS, RSClass.KilrBalQua),
-             balqcv=(RSClass.CLCmbQuaDCv, RSClass.KilrBalQua))
-
-    @classmethod
-    def _getAnalysisResultValue(cls, resultExpr, sResults, invalidValue):
-        
-        dLocals = {alias: sResults.get(name, worst) for alias, (name, worst) in cls.AnlysResultsIndex.items()}
-                                          
-        #logger.debug3('_getAnalysisResultValue: locals={}'.format(dLocals))
-        
-        try:
-            value = eval(resultExpr, None, dLocals)
-            if np.isnan(value):
-                value = invalidValue
-        except Exception as exc:
-            value = invalidValue
-            logger.warning('Failed to evaluate {} : {}'.format(resultExpr, exc))
-        
-        return value
-
-    def _runOneAnalysis(self, minDist=MCDSEngine.DistMinDef, maxDist=MCDSEngine.DistMaxDef, 
-                        fitDistCuts=MCDSEngine.DistFitCutsDef, discrDistCuts=MCDSEngine.DistDiscrCutsDef,
-                        valueExpr='chi2'):
-                              
-        """Run one analysis (among many others in the optimisation process) and compute its values to optimise
-           See MCDSAnalysis.__init__ for most parameters
-           :param string valueExpr: Math. expression (python syntax) for computing analysis value
-               (using result names from AnlysResultsIndex) (ex: chi2, chi2*ks, ...)
-        """
-
-        # Run analysis (Submit, and wait for end of execution) : parallelism taken care elsewhere.
-        startTime = pd.Timestamp.now()
-
-        dNameFlds = dict(l=minDist, r=maxDist, f=fitDistCuts, d=discrDistCuts)
-        nameSufx = ''.join(c+str(int(v)) for c, v in dNameFlds.items() if v is not None)
-
-        logger.debug2(f'Running analysis (minDist={minDist}, maxDist={maxDist},'
-                      f'fitDistCuts={fitDistCuts}, discrDistCuts={discrDistCuts}) ...')
-                      
-        anlys = MCDSAnalysis(engine=self.engine, sampleDataSet=self.sampleDataSet,
-                             name=self.name + '-' + nameSufx, logData=self.logData,
-                             estimKeyFn=self.estimKeyFn, estimAdjustFn=self.estimAdjustFn,
-                             estimCriterion=self.estimCriterion, cvInterval=self.cvInterval,
-                             minDist=minDist, maxDist=maxDist,
-                             fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts)
-
-        anlys.submit()
-        
-        sResults = anlys.getResults(postCleanup=self.autoClean)
-        #logger.debug3('Analysis results: {}'.format(sResults.to_dict()))
-
-        # Post-process results, and compute analysis values (_the_ values to optimise).
-        if anlys.success() or anlys.warnings():
-
-            sResults = self._postProcessAnalysisResults(sResults)
-
-            value = self._getAnalysisResultValue(valueExpr, sResults, self.invalidFuncValue)
-        
-        else:
-
-            value = self.invalidFuncValue
-
-        logger.debug1('Analysis result value: {} = {}'.format(valueExpr, value))
-
-        # Store elapsed time for this analysis, for later stats
-        self.elapsedTimes.append((pd.Timestamp.now() - startTime).total_seconds())
-
-        return value
-    
-    # Column names and translations
-    RunColumns = ['OptAbbrev', 'KeyFn', 'AdjSer', 'EstCrit', 'CVInt', 'OptCrit',
-                  'MinDist', 'MaxDist', 'FitDistCuts', 'DiscrDistCuts']
-    
-    DfRunColumnTrans = \
-        pd.DataFrame(index=RunColumns,
-                     data=dict(en=['Optim Abbrev', 'Mod Key Fn', 'Mod Adj Ser', 'Mod Chc Crit', 'Conf Interv',
-                                   'Optim Crit', 'Left Trunc Dist', 'Right Trunc Dist',
-                                   'Fit Dist Cuts', 'Discr Dist Cuts'],
-                               fr=['Abrév Optim', 'Fn Clé Mod', 'Sér Ajust Mod', 'Crit Chx Mod', 'Interv Conf',
-                                   'Crit Optim', 'Dist Tronc Gche', 'Dist Tronc Drte',
-                                   'Tranch Dist Mod', 'Tranch Dist Discr']))
-
-    def getResults(self):
-        
-        # Wait for availability (async) and get optimised results = target analysis parameters.
-        ldOptimResults = self.future.result()
-        
-        # Build header columns for all the optimisation results (same for all):
-        # actual (not default) analysis and optimisation params.
-        sHead = pd.Series(data=[self.name, self.estimKeyFn, self.estimAdjustFn,
-                                self.estimCriterion, self.cvInterval,
-                                '{}({})'.format('min' if self.minimiseExpr else 'max', self.expr2Optimise),
-                                str(self.minDist), str(self.maxDist),
-                                str(self.fitDistCuts), str(self.discrDistCuts)],
-                          index=self.RunColumns)
-        
-        # Build final table of optimisation results : header then results, for each optimisation.
-        dfOptimResults = pd.DataFrame(data=[sHead.append(pd.Series(optRes)) for optRes in ldOptimResults])
-        
-        # Done
-        return dfOptimResults
-
-
-class MCDSZerothOrderTruncationOptimisation(MCDSTruncationOptimisation):
-    
-    """Zero-order optimisation (no derivation used) for MCDS analyses distance truncations
-    """
-    
-    EngineClass = MCDSEngine
-        
-    # Column names and translations
-# TODO
-#    RunColumns = MCDSTruncationOptimisation.RunColumns + ['SetupStatus', 'SubmitStatus', 'NFunEvals', 'MeanFunElapd']
-#    
-#    DfRunColumnTrans = \
-#        pd.DataFrame(index=RunColumns,
-#                     data=dict(en=list(MCDSTruncationOptimisation.DfRunColumnTrans.en)
-#                                  + ['Setup Status', 'Submit Status', 'Num Fun Evals', 'Mean Fun Elapsed'],
-#                               fr=list(MCDSTruncationOptimisation.DfRunColumnTrans.fr)
-#                                  + ['Setup Status', 'Submit Status', 'Num Fun Evals', 'Mean Fun Elapd']))
-
-    @staticmethod
-    def zoopt(mxi=100, tv=None, a='racos', mxr=0):
-    
-        """Function for parsing optimisation specs:
-        * see optimiser.DSOptimiser._parseOptimCoreUserSpecs)
-        * see zoopt module for details
-        
-        Parameters:
-        :param mxi: max nb of iterations; default 100; 0 => no limit => use terminal value
-        :param tv: terminal value; default None = no terminal value check
-        :param a: algorithm to use: 'racos' (RACOS (AAAI'16) and Sequential RACOS (AAAI'17))
-                                    or 'poss' (Pareto Optimisation Subset Selection (NIPS'15))
-        :param mxr: max nb of retries on zoopt.Opt.min (exception); <= 0 => no retry
-        """
-    
-        dParms = dict(core='zoopt')
-        
-        mxi = max(0, mxi)
-        if mxi != 0:  # Default zoopt value
-            dParms.update(maxIters=mxi)
-
-        if tv is not None:  # Default zoopt value
-            dParms.update(termExprValue=tv)
-            
-        if a != 'racos':  # Default zoopt value
-            dParms.update(algorithm=a)
-            
-        mxr = max(0, mxr)
-        if mxr != 0:  # Default value
-            dParms.update(maxRetries=mxr)
-            
-        return dParms
-
-    CoreName = 'zoopt'
-    CoreParamNames = ['maxIters', 'termExprValue', 'algorithm', 'maxRetries']
-    CoreUserSpecParser = zoopt
-    
-    Parameter = ntuple('Parameter', ['name', 'interval', 'continuous', 'ordered'],
-                       defaults=['unknown', Interval(), True, True])
-    
-    def __init__(self, engine, sampleDataSet, name=None,
-                 distanceField='Distance', customData=None,
-                 executor=None, logData=False, autoClean=True, error=None,
-                 estimKeyFn=EngineClass.EstKeyFnDef, estimAdjustFn=EngineClass.EstAdjustFnDef, 
-                 estimCriterion=EngineClass.EstCriterionDef, cvInterval=EngineClass.EstCVIntervalDef,
-                 minDist=None, maxDist=None, fitDistCutsFctr=None, discrDistCutsFctr=None,
-                 fitDistCuts=None, discrDistCuts=None,
-                 expr2Optimise='chi2', minimiseExpr=False, 
-                 maxIters=100, termExprValue=None, algorithm='racos', maxRetries=0):  # CoreParamNames !
-
-        """Ctor
-        
-        Other parameters: See base class
-        
-        ZOOpt specific parameters:
-        :param algorithm: Zeroth Order optimisation algorithm to use
-                          (only 'racos' is suitable here, 'poss' is not, don't use))
-        :param maxRetries: Max number of retries on optim. core failure ; default: 0 => 0 retries = 1 try
-        :param maxIters: Number of iterations that stop optimisation algorithm when reached ; default: 0 => no limit
-        :param termExprValue: Value that stops optimisation algorithm when exceeded ;
-                          default: None => no such check done
-                          Note: when minimising, "exceeded" means that the function value becomes <= that termExprValue,
-                                and the other way round when maximising.
-        """
-        
-        # Initialise base.
-        super().__init__(engine, sampleDataSet, name=name,
-                         distanceField=distanceField, customData=customData,
-                         executor=executor, logData=logData, autoClean=autoClean, error=error,
-                         estimKeyFn=estimKeyFn, estimAdjustFn=estimAdjustFn,
-                         estimCriterion=estimCriterion, cvInterval=cvInterval,
-                         minDist=minDist, maxDist=maxDist,
-                         fitDistCutsFctr=fitDistCutsFctr, discrDistCutsFctr=discrDistCutsFctr,
-                         fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts,
-                         expr2Optimise=expr2Optimise, minimiseExpr=minimiseExpr,
-                         maxIters=maxIters, termExprValue=termExprValue, algorithm=algorithm, maxRetries=maxRetries)
-
-        # Prepare optimisation parameters.
-        self.dVariantParams = dict()  # Keys must be from SolutionDimensionNames
-        if isinstance(self.minDist, Interval):
-            self.dVariantParams.update(minDist=self.Parameter(name='MinDist', interval=self.minDist,
-                                                              continuous=True, ordered=True))
-        if isinstance(self.maxDist, Interval):
-            self.dVariantParams.update(maxDist=self.Parameter(name='MaxDist', interval=self.maxDist,
-                                                              continuous=True, ordered=True))
-        if isinstance(self.fitDistCuts, Interval):
-            self.dVariantParams.update(fitDistCuts=self.Parameter(name='FitDistCuts', interval=self.fitDistCuts,
-                                                                  continuous=False, ordered=True))
-        if isinstance(self.discrDistCuts, Interval):
-            self.dVariantParams.update(discrDistCuts=self.Parameter(name='DiscrDistCuts', interval=self.discrDistCuts,
-                                                                    continuous=False, ordered=True))
-        
-        assert all(name in self.SolutionDimensionNames for name in self.dVariantParams)
-        
-        logger.info1(f'ZOTrOptimisation({self.dVariantParams})')
-        
-        # Columns names for each optimisation result row (see _run).
-        self.resultsCols = ['SetupStatus', 'SubmitStatus', 'NFunEvals', 'MeanFunElapd'] \
-                           + list(self.dVariantParams.keys()) + [self.expr2Optimise]
-        
-        # zoopt optimiser initialisation.
-        self.zooptDims = \
-            zoopt.Dimension(size=len(self.dVariantParams),
-                            regs=[[param.interval.min, param.interval.max] for param in self.dVariantParams.values()],
-                            tys=[param.continuous for param in self.dVariantParams.values()],
-                            order=[param.ordered for param in self.dVariantParams.values()])
-
-        self.zooptObjtv = zoopt.Objective(func=self._function, dim=self.zooptDims)
-
-        self.zooptParams = zoopt.Parameter(budget=maxIters, algorithm=algorithm,
-                                           terminal_value=self.functionValue(termExprValue))
-
-        # Retry-on-failure stuff
-        self.maxRetries = max(maxRetries, 0)
-        self.retries = 0  # Just to keep track ...
-
-    def _function(self, solution):
-    
-        """The function to minimise : called as many times as needed by zoopt kernel.
-        :param solution: the zoopt "possible Solution" object to try and check if good enough
-        """
-
-        # Retrieve input parameters (to optimise)
-        dParams = dict(zip(self.dVariantParams.keys(), solution.get_x()))
-        dParams.update(self.dConstParams)
-        
-        # Run analysis and get value.
-        anlysValue = self._runOneAnalysis(valueExpr=self.expr2Optimise, **dParams)
-        
-        # One more function evaluated.
-        self.nFunEvals += 1
-
-        # Compute function value from analysis value.
-        return self.functionValue(anlysValue)
-    
-    def _optimize(self):
-
-        # Run the optimiser (for as many retries as requested in case of it fails)
-        nTriesLeft = maxTries = self.maxRetries + 1
-        while True:
-            try:
-                return zoopt.Opt.min(self.zooptObjtv, self.zooptParams)
-                # Done !
-            except Exception as exc:
-                nTriesLeft -= 1
-                if nTriesLeft > 0:
-                    self.retries += 1  # Just to keep track.
-                    logger.warning('zoopt.Opt.min retry #{} on {}'.format(maxTries - nTriesLeft, exc),
-                                   exc_info=True)
-                else:
-                    logger.warning('zoopt.Opt.min failed after #{} tries on {}'.format(maxTries, exc),
-                                   exc_info=True)
-                    return None
-
-    def _run(self, times=1, onlyBest=None, error=None, *args, **kwargs):
-        
-        """Really do the optimisation work (use the optimisation core for this).
-        (this method is called by the executor thread/process that takes it from the submit queue)
-        :return: List of "solutions", each as a dict with target analysis params in the ctor order
-                 preceded by { expr2Optimise: analysis value }
-        """
-        
-        # Number of evaluations of function to optimise.
-        self.nFunEvals = 0
-      
-        # When self.setupError or (submit) error, simply return a well-formed but empty results.
-        if self.setupError or error:
-            return [dict(zip(self.resultsCols, 
-                             [self.setupError, error, self.nFunEvals, 0.0] + [None]*(len(self.resultsCols) - 3)))]
-            
-        # Run the requested optimisations and get the solutions (ignore None ones).
-        solutions = [self._optimize() for _ in range(times)]
-        solutions = [sol for sol in solutions if sol is not None]
-        
-        # Keep only best solutions if requested.
-        if onlyBest is not None and len(solutions) >= onlyBest:
-            solutions = sorted(solutions, key=lambda sol: sol.get_value(), reverse=self.minimiseExpr)[:onlyBest]
-        
-        # Extract target results if any.
-        if not solutions:
-            return []
-        
-        nMeanFunEvals = int(round(self.nFunEvals / len(solutions)))
-        meanFunElapd = np.nan if not self.nFunEvals or not self.elapsedTimes \
-                              else sum(self.elapsedTimes) / self.nFunEvals
-        return [dict(zip(self.resultsCols, [None, None, nMeanFunEvals, meanFunElapd]
-                                           + sol.get_x() + [self.analysisValue(sol.get_value())]))
-                for sol in solutions]
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "optimisation": One layer above analysis, to optimise some of the parameters of one DS analysis.
+
+# Warning: If you add/remove/change optimisation parameters to XXOptimisation class ctors,
+#          remember to update ParmXX constants accordingly in XXOptimiser classes.
+
+from collections import namedtuple as ntuple
+
+import math
+import numpy as np
+import pandas as pd
+
+import zoopt
+import pkg_resources as pkgrsc  # zoopt has no standard __version__ !
+
+from . import log, runtime
+from .executor import Executor
+from .engine import MCDSEngine
+from .analysis import MCDSAnalysis
+from .analyser import MCDSAnalysisResultsSet
+
+runtime.update({'zoopt': pkgrsc.get_distribution('zoopt').version})
+
+logger = log.logger('ads.opn')
+
+
+class Interval(object):
+
+    """A basic closed interval class for numbers"""
+    
+    def __init__(self, min=0, max=-1):
+    
+        """Ctor
+        
+        Parameters:
+        :param min: min of interval if a number, 
+                    or interval itself if a (min, max) tuple/list or a dict(min=, max=) or a Interval
+        :param max: max of interval if min is a number, ignored otherwise
+        
+        Ex: Interval(min=3, max=6.9), Interval((2.3, 4.2)), Interval(dict(max=2.3, min=1.2))
+        """
+    
+        if isinstance(min, Interval):
+            self.min = min.min
+            self.max = min.max
+        elif isinstance(min, (tuple, list)):
+            self.min = min[0]
+            self.max = min[1]
+        elif isinstance(min, dict):
+            self.min = min['min']
+            self.max = min['max']
+        else:
+            self.min = min
+            self.max = max
+        
+    def check(self, order=False, minRange=(None, None), maxRange=(None, None)):
+    
+        errors = list()  # No error by default.
+        
+        if order and self.min > self.max:
+            errors.append(f'min:{self.min} > max:{self.max}')
+            
+        if minRange[0] is not None and self.min < minRange[0]:
+            errors.append(f'min:{self.min} < {minRange[0]}')
+        if minRange[1] is not None and self.min > minRange[1]:
+            errors.append(f'min:{self.min} > {minRange[1]}')
+        
+        if maxRange[0] is not None and self.max < maxRange[0]:
+            errors.append(f'max:{self.max} < {maxRange[0]}')
+        if maxRange[1] is not None and self.max > maxRange[1]:
+            errors.append(f'max:{self.max} > {maxRange[1]}')
+        
+        return ', '.join(errors)
+        
+    def __repr__(self):
+    
+        return '[{}, {}]'.format(self.min, self.max)
+        
+
+class Error(object):
+
+    """Error class for shipping error messages to the end user"""
+
+    def __init__(self, error=None, head=''):
+        
+        """Ctor
+        
+        Parameters:
+        :param error: string or Error
+        :error head: string ; ignored if error is an Error
+        """
+    
+        self.heads = list()
+        self.errors = list()
+        
+        if head or error:
+            self.append(error, head)
+        
+    def append(self, error, head=''):
+    
+        """Append an error to an other
+        
+        Parameters:
+        :param error: string or Error
+        :param head: string ; ignored if error is an Error
+        """
+        
+        if isinstance(error, self.__class__):
+            self.heads += error.heads
+            self.errors += error.errors
+        else:
+            self.heads.append(head)
+            self.errors.append(error)
+        
+    def __repr__(self):
+    
+        msgs = list()
+        prvHd = ''
+        for hd, err in zip(self.heads, self.errors):
+            msg = ''
+            if hd != prvHd and hd:
+                msg += hd + ' : '
+            msg += err
+            msgs.append(msg)
+            prvHd = hd
+        return ' & '.join(msgs)
+        
+    def __bool__(self):
+    
+        return any(err for err in self.errors)
+
+
+class DSOptimisation(object):
+    
+    """DSOptimisation (abstract) : A distance sampling analysis optimisation
+         possibly run in parallel with others, through an asynchronous "submit then getResults" scheme.
+    """
+    
+    def __init__(self, engine, sampleDataSet, name=None,
+                 executor=None, customData=None, error=None,
+                 expr2Optimise='chi2', minimiseExpr=False, **optimCoreOptions):
+        
+        """Ctor
+
+        Parameters:
+        :param engine: DS engine to use
+        :param sampleDataSet: data.SampleDataSet instance to use
+        :param name: used for prefixing run folders (sure to be automatically unique anyway),
+            analysis names, and so on, only for user-friendliness and deasier debugging ;
+            default: None => auto-generated from optimisation parameters
+        :param executor: Executor object to use for parallel execution of multiple optimisation instances
+             Note: Up to the caller to shut it down when no more needed (not owned).
+        :param customData: custom data for the run analyses to ship through
+        :param error: if an error occurred somewhere before, a string explaining it
+                      in order to prevent real submission, but still for keeping trace
+                      of unrun optimisations in optimiser results table :
+                      the optimisation then always returns at least a 1-row (empty/null) result + errors.
+        :param string expr2Optimise: Math. expression (python syntax) to optimise,
+                      using analyses results var. names inside (see derived classes for details)
+        :param minimiseExpr: True for minimisation of expr2Optimise, false for maximisation
+        :param optimCoreOptions: dict of specific options for the optimising core below
+        """
+        
+        self.engine = engine
+        self.executor = executor if executor is not None else Executor()
+        self.sampleDataSet = sampleDataSet
+        self.name = name
+        self.customData = customData
+        self.setupError = error
+        self.expr2Optimise = expr2Optimise
+        self.minimiseExpr = minimiseExpr
+        self.optimCoreOptions = optimCoreOptions
+        
+    def _run(self, times=1, onlyBest=None, *args, **kwargs):
+        
+        """Really do the optimisation work : run the optimiser for this
+        (this method is called by the executor thread/process that takes it from the submit queue)
+
+        Parameters:
+        :param times: Number of times to auto-run the optimisation (> 0 ; default = 1)
+        :param onlyBest: When multiple runs, number of best optimisations to retain (> 0 ; default = all runs)
+        :param *args, **kwargs: other _run params
+        :return: List of "solutions", each as a dict with target analysis params in the ctor order
+                 and then { expr2Optimise: analysis value }
+        """
+
+        raise NotImplementedError('DSOptimisation is an abstract class : implement _run in a derived class')
+    
+    def submit(self, times=1, onlyBest=None, error=None, *args, **kwargs):
+    
+        """Submit the optimisation, to run it possibly in parallel with others (through self.executor)
+        
+        :param times: Number of times to auto-run the optimisation (> 0 ; default = 1)
+        :param onlyBest: Number of best run results to keep (> 0 ; default None = all runs)
+        :param error: if an error occurred somewhere before since construction, a string explaining it
+                      in order to prevent real submission, but still for keeping trace
+                      of unrun optimisations in optimiser results table :
+                      the optimisation then always returns at least a 1-row (empty/null) result + errors.
+        :param args, kwargs: other _run arguments
+        """
+
+        # Submit optimisation work and return a Future object to ask from and wait for its results.
+        self.future = \
+            self.executor.submit(self._run, *args, 
+                                 **{'times': times, 'onlyBest': onlyBest, 'error': error, **kwargs})
+        
+        return self.future
+
+    def functionValue(self, anlysValue):
+    
+        """Analysis value to function (to optimise) value
+        
+        Inverse of functionValue (functionValue(analysisValue(x)) = x).
+        """
+        if pd.isnull(anlysValue):
+            funcValue = None
+        else:
+            funcValue = anlysValue if self.minimiseExpr else -anlysValue
+            
+        return funcValue
+    
+    def analysisValue(self, fnValue):
+    
+        """Function (to optimise) value to analysis value
+        
+        Inverse of functionValue (analysisValue(functionValue(x)) = x).
+        """
+    
+        return fnValue if self.minimiseExpr else -fnValue 
+    
+
+# class MCDSOptimisation(DSOptimisation):
+#
+#     """Optimisation for MCDS analyses ... stuff
+#     """
+#
+#     pass
+
+
+class MCDSTruncationOptimisation(DSOptimisation):
+
+    """Optimisation for MCDS analyses distance truncations
+    """
+    
+    EngineClass = MCDSEngine
+        
+    # Names of possible solution dimensions (the truncation parameter values we are searching for).
+    SolutionDimensionNames = ['minDist', 'maxDist', 'fitDistCuts', 'discrDistCuts']
+    
+    def __init__(self, engine, sampleDataSet, name=None, executor=None,
+                 distanceField='Distance', customData=None,
+                 logData=False, autoClean=True, error=None,
+                 estimKeyFn=EngineClass.EstKeyFnDef, estimAdjustFn=EngineClass.EstAdjustFnDef,
+                 estimCriterion=EngineClass.EstCriterionDef, cvInterval=EngineClass.EstCVIntervalDef,
+                 minDist=None, maxDist=None, fitDistCutsFctr=None, discrDistCutsFctr=None,
+                 fitDistCuts=None, discrDistCuts=None,
+                 expr2Optimise='chi2', minimiseExpr=False, **optimCoreOptions):
+
+        """Ctor
+        
+        Parameters (see base class, specific ones only here):
+        :param distanceField: Name of distance data column in sampleDataSet table
+        :param error: a string for explaining any pre-init error that'll prevent the optimisation from running,
+                      but will keep producing results (null of course) ;
+                      this is done for keeping trace of unrun optimisations in results table (1 empty/null row).
+
+        Fixed analysis parameters (see MCDSAnalysis):
+        :param estimKeyFn: 
+        :param estimAdjustFn: 
+        :param estimCriterion: 
+        :param cvInterval:  
+
+        Optimisation target parameters (at least one MUST be not None):
+        :param minDist: Min, max Interval() or 2-item list or tuple,
+                        or int / float constant for left truncation distance ;
+                        default: None => not optimised
+        :param maxDist: Idem, for right truncation distance ;
+                        default: None => not optimised
+        :param fitDistCutsFctr: Idem, for the mult. factor to apply to sqrt(nb of sightings)
+                        to get the number of distance cut intervals (for _model_fitting_) ;
+                        default: None => not optimised if not fitDistCuts
+        :param discrDistCutsFctr: Idem for the mult. factor to apply to sqrt(nb of sightings)
+                        for the number of distance cut intervals (for _distance_values_discretisation_) ;
+                        default: None => not optimised if not discrDistCuts
+        :param fitDistCuts: Idem for the absolute number of distance cut intervals
+                        (for _model_fitting_) if fitDistCutsFctr is None ;
+                        default: None => fitDistCuts not optimised if not fitDistCutsFctr
+        :param discrDistCuts: Idem for the number of distance cut intervals
+                        (for _distance_values_discretisation_) if discrDistCutsFctr is None ;
+                        default: None => not optimised if not discrDistCutsFctr
+        """
+
+        # Check engine
+        assert isinstance(engine, MCDSEngine), 'Engine must be an MCDSEngine'
+        
+        # Check and prepare analysis and optimisation params
+        moreError = Error()
+        if not (len(estimKeyFn) >= 2 and estimKeyFn in [kf[:len(estimKeyFn)] for kf in engine.EstKeyFns]):
+            moreError.append('Invalid estimate key function {}: should be in {} or at least 2-char abbreviations'
+                             .format(estimKeyFn, engine.EstKeyFns))
+        if not (len(estimAdjustFn) >= 2 and estimAdjustFn in [kf[:len(estimAdjustFn)] for kf in engine.EstAdjustFns]):
+            moreError.append('Invalid estimate adjust function {}: should be in {} or at least 2-char abbreviations'
+                             .format(estimAdjustFn, engine.EstAdjustFns))
+        if not (estimCriterion in engine.EstCriteria):
+            moreError.append('Invalid estimate criterion {}: should be in {}'
+                             .format(estimCriterion, engine.EstCriteria))
+        if not (0 < cvInterval < 100):
+            moreError.append('Invalid cvInterval {}% : should be in {}'.format(cvInterval, ']0%, 100%['))
+               
+        if not any(optPar is not None for optPar in [minDist, maxDist, fitDistCuts, discrDistCuts]):
+            moreError.append('At least 1 analysis parameter has to be optimised')
+            
+        self.dConstParams = dict()  # Params that won't be optimised : constants.
+        if minDist is not None and not isinstance(minDist, (int, float)):
+            minDist = Interval(minDist)
+        elif minDist is not None:
+            self.dConstParams['minDist'] = minDist
+            
+        if maxDist is not None and not isinstance(maxDist, (int, float)):
+            maxDist = Interval(maxDist)
+        elif maxDist is not None:
+            self.dConstParams['maxDist'] = maxDist
+            
+        if fitDistCutsFctr is not None:
+            fitDistCutsFctr = Interval(fitDistCutsFctr)
+        if fitDistCuts is not None and not isinstance(fitDistCuts, (int, float)):
+            fitDistCuts = Interval(fitDistCuts)
+        elif fitDistCuts is not None and fitDistCutsFctr is None:
+            self.dConstParams['fitDistCuts'] = fitDistCuts
+            
+        if discrDistCutsFctr is not None:
+            discrDistCutsFctr = Interval(discrDistCutsFctr)
+        if discrDistCuts is not None and not isinstance(discrDistCuts, (int, float)):
+            discrDistCuts = Interval(discrDistCuts)
+        elif discrDistCuts is not None and discrDistCutsFctr is None:
+            self.dConstParams['discrDistCuts'] = discrDistCuts
+
+        if self.dConstParams:
+            logger.info1(f'TrOptimisation({self.dConstParams})')
+        
+        if not (minDist is None or isinstance(minDist, (int, float)) or 0 <= minDist.min < minDist.max):
+            moreError.append('Invalid left truncation distance {}'.format(minDist))
+        if not (maxDist is None or isinstance(maxDist, (int, float)) or 0 <= maxDist.min < maxDist.max):
+            moreError.append('Invalid right truncation distance {}'.format(maxDist))
+        if not (minDist is None or isinstance(minDist, (int, float))
+                or maxDist is None or isinstance(maxDist, (int, float)) or minDist.max < maxDist.min):
+            moreError.append('Max left truncation distance {} must be lower than min right one {}'
+                             .format(minDist.max, maxDist.min))
+        
+        if not (fitDistCutsFctr is None or 0 <= fitDistCutsFctr.min < fitDistCutsFctr.max):
+            moreError.append('Invalid mult. factor for number of fitting distance cuts {}'
+                             .format(fitDistCutsFctr))
+        if not (discrDistCutsFctr is None or 0 <= discrDistCutsFctr.min < discrDistCutsFctr.max):
+            moreError.append('Invalid mult. factor number for distance discretisation cuts {}'
+                             .format(discrDistCutsFctr))
+        
+        if not (fitDistCutsFctr is None or fitDistCuts is None):
+            moreError.append('Can\'t specify both absolute value and mult. factor'
+                             ' for number of discretisation distance cuts')
+        if not (fitDistCuts is None or isinstance(fitDistCuts, (int, float))
+                or 2 <= fitDistCuts.min < fitDistCuts.max):
+            moreError.append('Invalid number of fitting distance cuts {}'.format(fitDistCuts))
+        if not (discrDistCutsFctr is None or discrDistCuts is None):
+            moreError.append('Can\'t specify both absolute value and mult. factor'
+                             ' for number of discretisation distance cuts')
+        if not (discrDistCuts is None or isinstance(discrDistCuts, (int, float))
+                or 2 <= discrDistCuts.min < discrDistCuts.max):
+            moreError.append('Invalid number of distance discretisation cuts {}'.format(discrDistCuts))
+        
+        # Build name from main params if not specified
+        if name is None:
+            fields = ['mcds'] + [p[:3].lower() for p in [estimKeyFn, estimAdjustFn]]
+            if estimCriterion != self.EngineClass.EstCriterionDef:
+                fields.append(estimCriterion.lower())
+            if cvInterval != self.EngineClass.EstCVIntervalDef:
+                fields.append(str(cvInterval))
+            name = '-'.join(fields)
+
+        # Show and merge errors if any.
+        if moreError:
+            logger.error('Check failed for optimisation params: ' + str(moreError))
+            if error:
+                error.append(moreError)
+            else:
+                error = moreError
+        
+        # Initialise base.
+        super().__init__(engine, sampleDataSet, name=name, 
+                         executor=executor, customData=customData, error=error,
+                         expr2Optimise=expr2Optimise, minimiseExpr=minimiseExpr, **optimCoreOptions)
+                
+        # Save / compute params.
+        # a. Analysis
+        self.logData = logData
+        self.autoClean = autoClean
+        self.estimKeyFn = estimKeyFn
+        self.estimAdjustFn = estimAdjustFn
+        self.estimCriterion = estimCriterion
+        self.cvInterval = cvInterval
+
+        # b. Analysis or optimisation (whether const or variant params)
+        self.minDist = minDist
+        self.maxDist = maxDist
+        if fitDistCutsFctr is not None or discrDistCutsFctr is not None:
+            sqrtNbSights = math.sqrt(len(sampleDataSet.dfData[distanceField].dropna()))
+        if fitDistCutsFctr is not None:
+            self.fitDistCuts = Interval(min=int(round(fitDistCutsFctr.min*sqrtNbSights)),
+                                        max=int(round(fitDistCutsFctr.max*sqrtNbSights)))
+        else:
+            self.fitDistCuts = fitDistCuts
+        if discrDistCutsFctr is not None:
+            self.discrDistCuts = Interval(min=int(round(discrDistCutsFctr.min*sqrtNbSights)),
+                                          max=int(round(discrDistCutsFctr.max*sqrtNbSights)))
+        else:
+            self.discrDistCuts = discrDistCuts
+            
+        # Other optimisation stuff.
+        fltSup = float('inf')  # sys.float_info.max
+        self.invalidFuncValue = fltSup if minimiseExpr else -fltSup
+
+        # Where to store analyses elapsed times before computing stats at the end.
+        self.elapsedTimes = list()
+
+    # Post-process analysis results (adapted from MCDSAnalysisResultsSet.postComputeColumns)
+    @staticmethod
+    def _postProcessAnalysisResults(sResults):
+
+        RSClass = MCDSAnalysisResultsSet  # ResultsSet class
+
+        # Chi2 test probability.
+        chi2AllColLbls = [col for col in RSClass.CLsChi2All if col in sResults.keys()]
+        sResults[RSClass.CLChi2] = RSClass._determineChi2Value(sResults[chi2AllColLbls])
+
+        # Combined quality indicators
+        # a. Make sure requested columns are there, and add them if not (NaN value)
+        miCompCols = RSClass.CLsQuaIndicSources
+        for miCol in miCompCols:
+            if miCol not in sResults.index:
+                sResults[miCol] = np.nan
+
+        # b. NaN value MUST kill down the indicators to compute => we have to enforce this
+        sResults.fillna({RSClass.CLNObs: RSClass.KilrNObs,
+                         RSClass.CLChi2: RSClass.KilrStaTest, RSClass.CLKS: RSClass.KilrStaTest,
+                         RSClass.CLCvMUw: RSClass.KilrStaTest, RSClass.CLCvMCw: RSClass.KilrStaTest,
+                         RSClass.CLDCv: RSClass.KilrDensCv,  # Usually considered good under 0.3
+                         RSClass.CLNTotObs: RSClass.KilrNTotObs,  # Should slap down _normObs whatever NObs
+                         RSClass.CLNAdjPars: RSClass.KilrNPars,  # Should slap down _normNAdjPars whatever NObs
+                         RSClass.CLNTotPars: RSClass.KilrNPars},
+                        inplace=True)
+
+        # c. Compute indicators at last !
+        aCombQuaData = np.expand_dims(sResults[miCompCols].values, axis=0)  # Series to 1 row results array.
+        sResults[RSClass.CLCmbQuaBal1] = RSClass._combinedQualityBalanced1(aCombQuaData)
+        for miCol, aIndic in zip(RSClass.CLsNewQuaIndics, RSClass._combinedQualityAll(aCombQuaData)):
+            sResults[miCol] = aIndic[0]
+
+        return sResults
+
+    # Alias and name / index (in analysis results) of results values available
+    # for use in analysis value computation expressions
+    # And the worst possible value for each, for (bad) default value when not present in results for some reason.
+    # Warning: Don't use np.nan for these worst values : at least zoopt doesn't like it !
+    RSClass = MCDSAnalysisResultsSet
+    AnlysResultsIndex = \
+        dict(chi2=(RSClass.CLChi2, RSClass.KilrStaTest), ks=(RSClass.CLKS, RSClass.KilrStaTest),
+             cvmuw=(RSClass.CLCvMUw, RSClass.KilrStaTest), cvmcw=(RSClass.CLCvMCw, RSClass.KilrStaTest),
+             dcv=(RSClass.CLDCv, RSClass.KilrDensCv),
+             balq1=(RSClass.CLCmbQuaBal1, RSClass.KilrBalQua), balq2=(RSClass.CLCmbQuaBal2, RSClass.KilrBalQua),
+             balq3=(RSClass.CLCmbQuaBal3, RSClass.KilrBalQua),
+             balqc2=(RSClass.CLCmbQuaChi2, RSClass.KilrBalQua), balqks=(RSClass.CLCmbQuaKS, RSClass.KilrBalQua),
+             balqcv=(RSClass.CLCmbQuaDCv, RSClass.KilrBalQua))
+
+    @classmethod
+    def _getAnalysisResultValue(cls, resultExpr, sResults, invalidValue):
+        
+        dLocals = {alias: sResults.get(name, worst) for alias, (name, worst) in cls.AnlysResultsIndex.items()}
+                                          
+        #logger.debug3('_getAnalysisResultValue: locals={}'.format(dLocals))
+        
+        try:
+            value = eval(resultExpr, None, dLocals)
+            if np.isnan(value):
+                value = invalidValue
+        except Exception as exc:
+            value = invalidValue
+            logger.warning('Failed to evaluate {} : {}'.format(resultExpr, exc))
+        
+        return value
+
+    def _runOneAnalysis(self, minDist=MCDSEngine.DistMinDef, maxDist=MCDSEngine.DistMaxDef, 
+                        fitDistCuts=MCDSEngine.DistFitCutsDef, discrDistCuts=MCDSEngine.DistDiscrCutsDef,
+                        valueExpr='chi2'):
+                              
+        """Run one analysis (among many others in the optimisation process) and compute its values to optimise
+           See MCDSAnalysis.__init__ for most parameters
+           :param string valueExpr: Math. expression (python syntax) for computing analysis value
+               (using result names from AnlysResultsIndex) (ex: chi2, chi2*ks, ...)
+        """
+
+        # Run analysis (Submit, and wait for end of execution) : parallelism taken care elsewhere.
+        startTime = pd.Timestamp.now()
+
+        dNameFlds = dict(l=minDist, r=maxDist, f=fitDistCuts, d=discrDistCuts)
+        nameSufx = ''.join(c+str(int(v)) for c, v in dNameFlds.items() if v is not None)
+
+        logger.debug2(f'Running analysis (minDist={minDist}, maxDist={maxDist},'
+                      f'fitDistCuts={fitDistCuts}, discrDistCuts={discrDistCuts}) ...')
+                      
+        anlys = MCDSAnalysis(engine=self.engine, sampleDataSet=self.sampleDataSet,
+                             name=self.name + '-' + nameSufx, logData=self.logData,
+                             estimKeyFn=self.estimKeyFn, estimAdjustFn=self.estimAdjustFn,
+                             estimCriterion=self.estimCriterion, cvInterval=self.cvInterval,
+                             minDist=minDist, maxDist=maxDist,
+                             fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts)
+
+        anlys.submit()
+        
+        sResults = anlys.getResults(postCleanup=self.autoClean)
+        #logger.debug3('Analysis results: {}'.format(sResults.to_dict()))
+
+        # Post-process results, and compute analysis values (_the_ values to optimise).
+        if anlys.success() or anlys.warnings():
+
+            sResults = self._postProcessAnalysisResults(sResults)
+
+            value = self._getAnalysisResultValue(valueExpr, sResults, self.invalidFuncValue)
+        
+        else:
+
+            value = self.invalidFuncValue
+
+        logger.debug1('Analysis result value: {} = {}'.format(valueExpr, value))
+
+        # Store elapsed time for this analysis, for later stats
+        self.elapsedTimes.append((pd.Timestamp.now() - startTime).total_seconds())
+
+        return value
+    
+    # Column names and translations
+    RunColumns = ['OptAbbrev', 'KeyFn', 'AdjSer', 'EstCrit', 'CVInt', 'OptCrit',
+                  'MinDist', 'MaxDist', 'FitDistCuts', 'DiscrDistCuts']
+    
+    DfRunColumnTrans = \
+        pd.DataFrame(index=RunColumns,
+                     data=dict(en=['Optim Abbrev', 'Mod Key Fn', 'Mod Adj Ser', 'Mod Chc Crit', 'Conf Interv',
+                                   'Optim Crit', 'Left Trunc Dist', 'Right Trunc Dist',
+                                   'Fit Dist Cuts', 'Discr Dist Cuts'],
+                               fr=['Abrév Optim', 'Fn Clé Mod', 'Sér Ajust Mod', 'Crit Chx Mod', 'Interv Conf',
+                                   'Crit Optim', 'Dist Tronc Gche', 'Dist Tronc Drte',
+                                   'Tranch Dist Mod', 'Tranch Dist Discr']))
+
+    def getResults(self):
+        
+        # Wait for availability (async) and get optimised results = target analysis parameters.
+        ldOptimResults = self.future.result()
+        
+        # Build header columns for all the optimisation results (same for all):
+        # actual (not default) analysis and optimisation params.
+        sHead = pd.Series(data=[self.name, self.estimKeyFn, self.estimAdjustFn,
+                                self.estimCriterion, self.cvInterval,
+                                '{}({})'.format('min' if self.minimiseExpr else 'max', self.expr2Optimise),
+                                str(self.minDist), str(self.maxDist),
+                                str(self.fitDistCuts), str(self.discrDistCuts)],
+                          index=self.RunColumns)
+        
+        # Build final table of optimisation results : header then results, for each optimisation.
+        dfOptimResults = pd.DataFrame(data=[sHead.append(pd.Series(optRes)) for optRes in ldOptimResults])
+        
+        # Done
+        return dfOptimResults
+
+
+class MCDSZerothOrderTruncationOptimisation(MCDSTruncationOptimisation):
+    
+    """Zero-order optimisation (no derivation used) for MCDS analyses distance truncations
+    """
+    
+    EngineClass = MCDSEngine
+        
+    # Column names and translations
+# TODO
+#    RunColumns = MCDSTruncationOptimisation.RunColumns + ['SetupStatus', 'SubmitStatus', 'NFunEvals', 'MeanFunElapd']
+#    
+#    DfRunColumnTrans = \
+#        pd.DataFrame(index=RunColumns,
+#                     data=dict(en=list(MCDSTruncationOptimisation.DfRunColumnTrans.en)
+#                                  + ['Setup Status', 'Submit Status', 'Num Fun Evals', 'Mean Fun Elapsed'],
+#                               fr=list(MCDSTruncationOptimisation.DfRunColumnTrans.fr)
+#                                  + ['Setup Status', 'Submit Status', 'Num Fun Evals', 'Mean Fun Elapd']))
+
+    @staticmethod
+    def zoopt(mxi=100, tv=None, a='racos', mxr=0):
+    
+        """Function for parsing optimisation specs:
+        * see optimiser.DSOptimiser._parseOptimCoreUserSpecs)
+        * see zoopt module for details
+        
+        Parameters:
+        :param mxi: max nb of iterations; default 100; 0 => no limit => use terminal value
+        :param tv: terminal value; default None = no terminal value check
+        :param a: algorithm to use: 'racos' (RACOS (AAAI'16) and Sequential RACOS (AAAI'17))
+                                    or 'poss' (Pareto Optimisation Subset Selection (NIPS'15))
+        :param mxr: max nb of retries on zoopt.Opt.min (exception); <= 0 => no retry
+        """
+    
+        dParms = dict(core='zoopt')
+        
+        mxi = max(0, mxi)
+        if mxi != 0:  # Default zoopt value
+            dParms.update(maxIters=mxi)
+
+        if tv is not None:  # Default zoopt value
+            dParms.update(termExprValue=tv)
+            
+        if a != 'racos':  # Default zoopt value
+            dParms.update(algorithm=a)
+            
+        mxr = max(0, mxr)
+        if mxr != 0:  # Default value
+            dParms.update(maxRetries=mxr)
+            
+        return dParms
+
+    CoreName = 'zoopt'
+    CoreParamNames = ['maxIters', 'termExprValue', 'algorithm', 'maxRetries']
+    CoreUserSpecParser = zoopt
+    
+    Parameter = ntuple('Parameter', ['name', 'interval', 'continuous', 'ordered'],
+                       defaults=['unknown', Interval(), True, True])
+    
+    def __init__(self, engine, sampleDataSet, name=None,
+                 distanceField='Distance', customData=None,
+                 executor=None, logData=False, autoClean=True, error=None,
+                 estimKeyFn=EngineClass.EstKeyFnDef, estimAdjustFn=EngineClass.EstAdjustFnDef, 
+                 estimCriterion=EngineClass.EstCriterionDef, cvInterval=EngineClass.EstCVIntervalDef,
+                 minDist=None, maxDist=None, fitDistCutsFctr=None, discrDistCutsFctr=None,
+                 fitDistCuts=None, discrDistCuts=None,
+                 expr2Optimise='chi2', minimiseExpr=False, 
+                 maxIters=100, termExprValue=None, algorithm='racos', maxRetries=0):  # CoreParamNames !
+
+        """Ctor
+        
+        Other parameters: See base class
+        
+        ZOOpt specific parameters:
+        :param algorithm: Zeroth Order optimisation algorithm to use
+                          (only 'racos' is suitable here, 'poss' is not, don't use))
+        :param maxRetries: Max number of retries on optim. core failure ; default: 0 => 0 retries = 1 try
+        :param maxIters: Number of iterations that stop optimisation algorithm when reached ; default: 0 => no limit
+        :param termExprValue: Value that stops optimisation algorithm when exceeded ;
+                          default: None => no such check done
+                          Note: when minimising, "exceeded" means that the function value becomes <= that termExprValue,
+                                and the other way round when maximising.
+        """
+        
+        # Initialise base.
+        super().__init__(engine, sampleDataSet, name=name,
+                         distanceField=distanceField, customData=customData,
+                         executor=executor, logData=logData, autoClean=autoClean, error=error,
+                         estimKeyFn=estimKeyFn, estimAdjustFn=estimAdjustFn,
+                         estimCriterion=estimCriterion, cvInterval=cvInterval,
+                         minDist=minDist, maxDist=maxDist,
+                         fitDistCutsFctr=fitDistCutsFctr, discrDistCutsFctr=discrDistCutsFctr,
+                         fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts,
+                         expr2Optimise=expr2Optimise, minimiseExpr=minimiseExpr,
+                         maxIters=maxIters, termExprValue=termExprValue, algorithm=algorithm, maxRetries=maxRetries)
+
+        # Prepare optimisation parameters.
+        self.dVariantParams = dict()  # Keys must be from SolutionDimensionNames
+        if isinstance(self.minDist, Interval):
+            self.dVariantParams.update(minDist=self.Parameter(name='MinDist', interval=self.minDist,
+                                                              continuous=True, ordered=True))
+        if isinstance(self.maxDist, Interval):
+            self.dVariantParams.update(maxDist=self.Parameter(name='MaxDist', interval=self.maxDist,
+                                                              continuous=True, ordered=True))
+        if isinstance(self.fitDistCuts, Interval):
+            self.dVariantParams.update(fitDistCuts=self.Parameter(name='FitDistCuts', interval=self.fitDistCuts,
+                                                                  continuous=False, ordered=True))
+        if isinstance(self.discrDistCuts, Interval):
+            self.dVariantParams.update(discrDistCuts=self.Parameter(name='DiscrDistCuts', interval=self.discrDistCuts,
+                                                                    continuous=False, ordered=True))
+        
+        assert all(name in self.SolutionDimensionNames for name in self.dVariantParams)
+        
+        logger.info1(f'ZOTrOptimisation({self.dVariantParams})')
+        
+        # Columns names for each optimisation result row (see _run).
+        self.resultsCols = ['SetupStatus', 'SubmitStatus', 'NFunEvals', 'MeanFunElapd'] \
+                           + list(self.dVariantParams.keys()) + [self.expr2Optimise]
+        
+        # zoopt optimiser initialisation.
+        self.zooptDims = \
+            zoopt.Dimension(size=len(self.dVariantParams),
+                            regs=[[param.interval.min, param.interval.max] for param in self.dVariantParams.values()],
+                            tys=[param.continuous for param in self.dVariantParams.values()],
+                            order=[param.ordered for param in self.dVariantParams.values()])
+
+        self.zooptObjtv = zoopt.Objective(func=self._function, dim=self.zooptDims)
+
+        self.zooptParams = zoopt.Parameter(budget=maxIters, algorithm=algorithm,
+                                           terminal_value=self.functionValue(termExprValue))
+
+        # Retry-on-failure stuff
+        self.maxRetries = max(maxRetries, 0)
+        self.retries = 0  # Just to keep track ...
+
+    def _function(self, solution):
+    
+        """The function to minimise : called as many times as needed by zoopt kernel.
+        :param solution: the zoopt "possible Solution" object to try and check if good enough
+        """
+
+        # Retrieve input parameters (to optimise)
+        dParams = dict(zip(self.dVariantParams.keys(), solution.get_x()))
+        dParams.update(self.dConstParams)
+        
+        # Run analysis and get value.
+        anlysValue = self._runOneAnalysis(valueExpr=self.expr2Optimise, **dParams)
+        
+        # One more function evaluated.
+        self.nFunEvals += 1
+
+        # Compute function value from analysis value.
+        return self.functionValue(anlysValue)
+    
+    def _optimize(self):
+
+        # Run the optimiser (for as many retries as requested in case of it fails)
+        nTriesLeft = maxTries = self.maxRetries + 1
+        while True:
+            try:
+                return zoopt.Opt.min(self.zooptObjtv, self.zooptParams)
+                # Done !
+            except Exception as exc:
+                nTriesLeft -= 1
+                if nTriesLeft > 0:
+                    self.retries += 1  # Just to keep track.
+                    logger.warning('zoopt.Opt.min retry #{} on {}'.format(maxTries - nTriesLeft, exc),
+                                   exc_info=True)
+                else:
+                    logger.warning('zoopt.Opt.min failed after #{} tries on {}'.format(maxTries, exc),
+                                   exc_info=True)
+                    return None
+
+    def _run(self, times=1, onlyBest=None, error=None, *args, **kwargs):
+        
+        """Really do the optimisation work (use the optimisation core for this).
+        (this method is called by the executor thread/process that takes it from the submit queue)
+        :return: List of "solutions", each as a dict with target analysis params in the ctor order
+                 preceded by { expr2Optimise: analysis value }
+        """
+        
+        # Number of evaluations of function to optimise.
+        self.nFunEvals = 0
+      
+        # When self.setupError or (submit) error, simply return a well-formed but empty results.
+        if self.setupError or error:
+            return [dict(zip(self.resultsCols, 
+                             [self.setupError, error, self.nFunEvals, 0.0] + [None]*(len(self.resultsCols) - 3)))]
+            
+        # Run the requested optimisations and get the solutions (ignore None ones).
+        solutions = [self._optimize() for _ in range(times)]
+        solutions = [sol for sol in solutions if sol is not None]
+        
+        # Keep only best solutions if requested.
+        if onlyBest is not None and len(solutions) >= onlyBest:
+            solutions = sorted(solutions, key=lambda sol: sol.get_value(), reverse=self.minimiseExpr)[:onlyBest]
+        
+        # Extract target results if any.
+        if not solutions:
+            return []
+        
+        nMeanFunEvals = int(round(self.nFunEvals / len(solutions)))
+        meanFunElapd = np.nan if not self.nFunEvals or not self.elapsedTimes \
+                              else sum(self.elapsedTimes) / self.nFunEvals
+        return [dict(zip(self.resultsCols, [None, None, nMeanFunEvals, meanFunElapd]
+                                           + sol.get_x() + [self.analysisValue(sol.get_value())]))
+                for sol in solutions]
```

### Comparing `pyaudisam-0.9.3/pyaudisam/optimiser.py` & `pyaudisam-1.0.1/pyaudisam/optimiser.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,1409 +1,1410 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "optimiser": Auto-determine analysis parameters values based on quality criteria
-
-import re
-
-import pathlib as pl
-
-from collections import namedtuple as ntuple
-
-import numpy as np
-import pandas as pd
-
-from . import log
-from .data import MonoCategoryDataSet, ResultsSet
-from .executor import Executor
-from .engine import MCDSEngine
-from .analyser import DSAnalyser, MCDSAnalyser
-from .optimisation import Interval, Error, DSOptimisation
-from .optimisation import MCDSTruncationOptimisation, MCDSZerothOrderTruncationOptimisation
-
-logger = log.logger('ads.opr')
-
-
-class OptimisationResultsSet(ResultsSet):
-    
-    """A specialized results set for DS analyses optimisations"""
-    
-    def __init__(self, optimisationClass, miCustomCols=None, dfCustomColTrans=None,
-                       dComputedCols=None, dfComputedColTrans=None, sortCols=[], sortAscend=[]):
-        
-        assert issubclass(optimisationClass, DSOptimisation), \
-               'optimisationClass must derive from DSOptimisation'
-        assert miCustomCols is None \
-               or (isinstance(miCustomCols, list) and len(miCustomCols) > 0
-                   and all(isinstance(col, str) for col in miCustomCols)), \
-               'customCols must be None or a list of strings'
-        
-        self.optimisationClass = optimisationClass
-        
-        # Optimisation results columns
-        miCols = optimisationClass.RunColumns
-        
-        # DataFrame for translating column names
-        dfColTrans = optimisationClass.DfRunColumnTrans
-        
-        # Initialize base class.
-        super().__init__(miCols=miCols, dfColTrans=dfColTrans,
-                         miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
-                         sortCols=sortCols, sortAscend=sortAscend, dropNACols=False)
-    
-    def copy(self, withData=True):
-    
-        """Clone function (shallow), with optional (deep) data copy"""
-    
-        # 1. Call ctor without computed columns stuff (we no more have initial data)
-        clone = OptimisationResultsSet(optimisationClass=self.optimisationClass,
-                                       miCustomCols=self.miCustomCols.copy(),
-                                       dfCustomColTrans=self.dfCustomColTrans.copy(),
-                                       sortCols=self.sortCols.copy(), sortAscend=self.sortAscend.copy())
-    
-        # 2. Complete clone initialisation.
-        # 3-level multi-index columns (module, statistic, figure)
-        clone.miCols = self.miCols.copy()
-        clone.computedCols = self.computedCols.copy()
-        
-        # DataFrames for translating columns names
-        clone.dfColTrans = self.dfColTrans.copy()
-        
-        # Copy data if needed.
-        if withData:
-            clone._dfData = self._dfData.copy()
-            clone.rightColOrder = self.rightColOrder
-            clone.postComputed = self.postComputed
-
-        return clone
-
-    def fromExcel(self, fileName, sheetName=None, specs=True, specSheetsPrfx='sp-'):
-
-        """Load (overwrite) data and optionally specs from an Excel worksheet (XLSX format),
-        assuming ctor params match with Excel sheet column names and list,
-        which can well be ensured by using the same ctor params as used for saving !
-        """
-        
-        super().fromExcel(fileName, sheetName=sheetName, header=0, skipRows=None, indexCol=None,
-                          specs=specs, specSheetsPrfx=specSheetsPrfx)
-
-    def fromOpenDoc(self, fileName, sheetName=None, specs=True, specSheetsPrfx='sp-'):
-
-        """Load (overwrite) data and optionally specs from an Open Document worksheet (ODS format),
-        assuming ctor params match with ODF sheet column names and list,
-        which can well be ensured by using the same ctor params as used for saving !
-        Notes: Needs odfpy module and pandas.version >= 0.25.1
-        """
-        
-        super().fromOpenDoc(fileName, sheetName=sheetName, header=0, skipRows=None, indexCol=None,
-                            specs=specs, specSheetsPrfx=specSheetsPrfx)
-
-    def fromFile(self, fileName, sheetName=None, specs=True, specSheetsPrfx='sp-'):
-
-        """Load (overwrite) data data and optionally specs from a given file,
-        (see ResultsSet.fromFile for supported formats, auto-detected from file extensions)
-        assuming ctor params match with the file contents,
-        which can well be ensured by using the same ctor params as used for saving !
-        """
-
-        super().fromFile(fileName, sheetName=sheetName, header=0, skipRows=None, indexCol=None,
-                         specs=specs, specSheetsPrfx=specSheetsPrfx)
-
-    def optimisationTargetColumns(self):
-    
-        """The names of the columns holding the real optimisation results
-        """
-    
-        return [col for col in self.optimisationClass.SolutionDimensionNames if col in self.columns]
-
-
-class DSParamsOptimiser(object):
-
-    """Run a bunch of DS analyses on samples extracted from a mono-category sightings data set,
-       according to a user-friendly set of analysis specs (first set of fixed params),
-       in order to determine best values for a second set of analysis params.
-       Abstract class.
-    """
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], 
-                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
-                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum', 
-                 distanceUnit='Meter', areaUnit='Hectare', 
-                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], 
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 workDir='.',
-                 defExpr2Optimise='chi2', defMinimiseExpr=False,
-                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),
-                 dDefOptimCoreParams=dict()):
-                       
-        """Ctor (don't use directly, abstract class)
-        
-        Parameters for input data to analyse:
-        :param pd.DataFrame dfMonoCatObs: mono-category data from FieldDataSet.monoCategorise() or individualise()
-        :param pd.DataFrame dfTransects: Transects infos with columns : transectPlaceCols (n), passIdCol (1),
-            effortCol (1) ; if None, auto generated from input sightings
-        :param effortConstVal: if dfTransects is None and effortCol not in source table, use this constant value
-        :param transectPlaceCols:  See above dfTransects description
-        :param passIdCol:  See above dfTransects description
-        :param effortCol: See above dfTransects description
-        :param sampleSelCols: sample identification = selection columns
-        :param sampleDecCols: Decimal columns among sighting columns
-        :param sampleDistCol: name of distance data column in run specs table
-        :param anlysSpecCustCols: Special columns from analysis specs to simply pass through in results
-        :param abbrevCol: Name of column to generate for abbreviating optimisation params, not sure really useful ...
-        :param abbrevBuilder: Function of explicit analysis params (as a Series) to generate abbreviated name
-        :param anlysIndCol: Name of column to generate for identifying analyses, unless already there in input data.
-        :param sampleIndCol: Name of column to generate for identifying samples, unless already there in input data.
-        
-        Parameters for DS engine:
-        :param distanceUnit: See DSEngine
-        :param areaUnit: See DSEngine
-
-        Parameters for optimisation output data:
-        :param resultsHeadCols: dict of list of column names (from dfMonoCatObs) to use in order
-            to build results header columns (left to results cols) ; 'sample' columns are sample selection columns.
-        
-        Parameters for evaluating analysis value to optimise
-            (when not fully specified in each optimisation parameters):
-        :param string defExpr2Optimise: Math. expression (python syntax) to optimise,
-               using analyses results var. names inside (see derived classes for details)
-        :param defMinimiseExpr: True for minimisation of expr2Optimise, false for maximisation
-
-        Parameters for optimisation submissions
-            (when not fully specified in each optimisation parameters):
-        :param defSubmitTimes: Number of times to auto-run each optimisation (> 0 ; default 1)
-        :param defSubmitOnlyBest: Number of best repetition results to keep
-                                  (> 0 ; default None = all repetitions)
-        :param dDefSubmitOtherParams: Other submission parameters
-
-        Other parameters:
-        :param dSurveyArea: dict of info about survey area (mainly name and surface) for DS analyses
-        :param workDir: target folder for intermediate computations and results files
-        :param dDefOptimCoreParams: Optimisation core specific params
-        """
-
-        # Save data.
-        self.dfMonoCatObs = dfMonoCatObs
-        self.sampleDistCol = sampleDistCol
-
-        self.resultsHeadCols = resultsHeadCols
-        self.abbrevCol = abbrevCol
-        self.abbrevBuilder = abbrevBuilder
-        self.anlysIndCol = anlysIndCol
-        self.sampleSelCols = sampleSelCols
-        self.sampleIndCol = sampleIndCol
-        self.anlysSpecCustCols = anlysSpecCustCols
-            
-        self.distanceUnit = distanceUnit
-        self.areaUnit = areaUnit
-
-        self.workDir = workDir
-        
-        # Default values for optimisation parameters.
-        # a. Expression to optimise
-        self.defExpr2Optimise = defExpr2Optimise
-        self.defMinimiseExpr = defMinimiseExpr
-        
-        # b. Optimisation core parameters.
-        self.dDefOptimCoreParams = dDefOptimCoreParams
-
-        # c. Optimisations submission (=run) parameters
-        self.defSubmitTimes = defSubmitTimes
-        self.defSubmitOnlyBest = defSubmitOnlyBest
-        self.dDefSubmitOtherParams = dDefSubmitOtherParams
-
-        # Mono-categorised data (all samples)
-        self._mcDataSet = \
-            MonoCategoryDataSet(dfMonoCatObs, dfTransects=dfTransects, effortConstVal=effortConstVal,
-                                dSurveyArea=dSurveyArea, transectPlaceCols=transectPlaceCols,
-                                passIdCol=passIdCol, effortCol=effortCol, sampleDecFields=sampleDecCols)
-                                
-        # Analysis engine and executor.
-        self._executor = None
-        self._engine = None
-
-        # Results.
-        self.results = None
-
-        # Computation specifications, for traceability only.
-        # For gathering copies of computations default parameter values, and stuff like that.
-        self.specs = dict()
-        self.updateSpecs(**{name: getattr(self, name) for name in ['distanceUnit', 'areaUnit']})
-        self.updateSpecs(**{name: getattr(self, name)
-                            for name in ['defExpr2Optimise', 'defMinimiseExpr', 'dDefOptimCoreParams',
-                                         'defSubmitTimes', 'defSubmitOnlyBest', 'dDefSubmitOtherParams']})
-
-    def updateSpecs(self, reset=False, overwrite=False, **specs):
-
-        if reset:
-            self.specs.clear()
-
-        if not overwrite:
-            assert all(name not in self.specs for name in specs), \
-                   "Unless specified, won't overwrite already present specs {}" \
-                   .format(', '.join(name for name in specs if name in self.specs))
-
-        self.specs.update(specs)
-
-    def flatSpecs(self):
-
-        # Flatten "in-line" 2nd level dicts if any (with 1st level name prefixing).
-        dFlatSpecs = dict()
-        for name, value in self.specs.items():
-            if isinstance(value, dict):
-                for n, v in value.items():
-                    dFlatSpecs[name + n[0].upper() + n[1:]] = v
-            else:
-                dFlatSpecs[name] = value
-
-        # Done.
-        return dFlatSpecs
-
-    # Optimiser internal parameter spec names, for which a match should be found (when one is needed)
-    # with user explicit optimisation specs used in run() calls.
-    IntSpecExpr2Optimise = 'Expr2Optimise'
-    IntSpecOptimisationCore = 'OptimisationCore'
-    IntSpecSubmitParams = 'SubmitParams'
-
-    # Possible regexps (values) for auto-detection of optimiser _internal_ parameter spec names (keys)
-    # from explicit _user_ spec columns
-    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
-    #  and case is ignored during searching).
-    Int2UserSpecREs = \
-        {IntSpecExpr2Optimise:    ['opt[a-z]*[\.\-_ ]*exp', 'exp[a-z2]*[\.\-_ ]*opt',
-                                   'opt[a-z]*[\.\-_ ]*cri', 'cri[a-z]*[\.\-_ ]*opt'],
-         IntSpecOptimisationCore: ['opt[a-z]*[\.\-_ ]*core', 'mot[a-z]*[\.\-_ ]*opt',
-                                   'noy[a-z]*[\.\-_ ]*opt'],
-         IntSpecSubmitParams:     ['sub[a-z]*[\.\-_ ]*par', 'par[a-z]*[\.\-_ ]*sou',
-                                   'run[a-z]*[\.\-_ ]*par', 'par[a-z]*[\.\-_ ]*ex',
-                                   'mul[a-z]*[\.\-_ ]*opt', 'opt[a-z]*[\.\-_ ]*mul']}
-
-    # Types for user specs parsing (see usage below)
-    class Auto(object):
-
-        def __repr__(self):
-            return 'Auto()'
-
-        def __eq__(self, other):
-            return isinstance(other, self.__class__)
-    
-    @classmethod
-    def _parseUserSpec(cls, spec, globals=dict(), locals=dict(),
-                       oneStrArg=False, nullOrEmpty=Error, errIfNotA=[]):
-                                  
-        """Parse parameter(s) user spec with python-like simple expression syntax from given rules 
-        
-        :param spec: None or np.nan or string spec to parse
-        :param globals: dict of globals for case-insensitive rules (we use eval function for parsing !)
-        :param locals: dict of locals for case-insensitive rules (we use eval function for parsing !)
-        :param oneStrArg: assume function call syntax with 1 single string argument
-                (ex: input "f(x,y)" is transformed to "f('x,y')" before calling eval)
-        :param nullOrEmpty: return value for null of empty spec ; not checked against errIfNotA
-            (default: Error => an instance of Error with error description inside)
-        :param errIfNotA: list of authorised output types ; empty => any type
-        
-        :return: a 2-value tuple : (None or an Error instance, parsed value or None in case of error)
-        """
-    
-        # Empty or Null cases
-        if pd.isnull(spec) or (isinstance(spec, str) and not spec.strip()):
-        
-            if nullOrEmpty is Error:
-                parsedValue = None
-                parseError = Error('Should be specified ; did you mean "auto" ?')
-            else:
-                parsedValue = nullOrEmpty
-                parseError = None
-        
-            return parseError, parsedValue
-
-        # Other cases.
-        spec = str(spec).lower()  # int and float cases, + lower case
-        if oneStrArg:
-            if "('" not in spec:
-                spec = spec.replace('(', "('")
-            if "')" not in spec:
-                spec = spec.replace(')', "')")
-        
-        try:
-            parsedValue = eval(spec, globals, locals)
-            if errIfNotA and not isinstance(parsedValue, tuple(errIfNotA)):
-                error = 'Not a {}'.format(', '.join(t.__name__ for t in errIfNotA))
-                parseError = Error(head=spec, error=error)
-                parsedValue = None
-            else:
-                parseError = None
-        except Exception as exc:
-            parsedValue = None
-            parseError = Error(head=spec, error=str(exc))
-        
-        return parseError, parsedValue
-    
-    # Types for parsing user spec
-    DistInterval = ntuple('DistInterval', ['dmin', 'dmax'], defaults=[0, -1])  # Distance interval
-    AbsInterval = ntuple('AbsInterval', ['min', 'max'], defaults=[0, -1])  # Interval for actual values
-    MultInterval = ntuple('MultInterval', ['kmin', 'kmax'])  # Interval for multipliers
-    OutliersMethod = ntuple('OutliersMethod', ['method', 'percent'])
-
-    @classmethod
-    def _parseDistTruncationUserSpec(cls, spec, errIfNotA=[]):
-    
-        """Parse user spec for one analysis optimised parameter
-        
-        Parameters:
-        :param spec: None or np.nan or string spec to parse
-        :param errIfNotA: list of authorised output types ; empty => any type
-        
-        :return: a 2-value tuple : (None or an Error instance, parsed value or None in case of error)
-                 Parsed value may result None or an instance of AbsInterval, MultInterval, OutliersMethod, Auto
-        """
-
-        # Defs for optimisation param. spec. mini-language
-        auto = cls.Auto()
-
-        def dist(dmin, dmax):
-            return cls.DistInterval(dmin, dmax)
-
-        def quant(pct):
-            return cls.OutliersMethod('quant', pct)
-
-        def tucquant(pct):
-            return cls.OutliersMethod('tucquant', pct)
-
-        def mult(kmin, kmax):
-            return cls.MultInterval(kmin, kmax)
-
-        def abs(min, max):
-            return cls.AbsInterval(min, max)
-            
-        # Parse spec.
-        return cls._parseUserSpec(spec, nullOrEmpty=None, errIfNotA=errIfNotA,
-                                  globals=dict(Auto=cls.Auto, DistInterval=cls.DistInterval,
-                                               AbsInterval=cls.AbsInterval, MultInterval=cls.MultInterval,
-                                               OutliersMethod=cls.OutliersMethod),
-                                  locals=dict(auto=auto, dist=dist, quant=quant, tucquant=tucquant,
-                                              mult=mult, abs=abs))
-
-    @classmethod
-    def _parseOptimCoreUserSpecs(cls, spec, globals=dict(), locals=dict(),
-                                 nullOrEmpty=Error, errIfNotA=[]):
-    
-        """Parse user spec for optimisation core parameters
-        
-        Parameters:
-        :param spec: the spec to parse
-        :param globals: dict of globals for rules (we use eval function for parsing !)
-        :param locals: dict of locals for rules (we use eval function for parsing !)
-        :param nullOrEmpty: return value for null of empty spec ; not checked against errIfNotA
-            (default: Error => an instance of Error with error description inside)
-        :param errIfNotA: list of authorised output types ; empty => any type
-        
-        :return: a 2-value tuple : (None or an Error instance, parsed value or None in case of error)
-                 Parsed value may result None or an instance of AbsInterval, MultInterval, OutliersMethod, Auto
-        """
-
-        if isinstance(spec, str) and spec.strip():
-        
-            # We don't care about case
-            spec = spec.lower()
-            
-            # A single string for the engine name is enough => add () at the end if so.
-            if '(' not in spec:
-                spec = spec + '()'
-                
-            # String parameters don't need quoting in spec, but python needs it : add it if needed
-            spec = re.sub('([a-z_]+) *= *([^=0-9,; ][^=,;\) ]*)', r"\1='\2'", spec)
-    
-        # Parse pythonified spec.
-        return cls._parseUserSpec(spec, nullOrEmpty=nullOrEmpty, errIfNotA=errIfNotA, 
-                                  oneStrArg=False, globals=globals, locals=locals)
-
-    # Optimisation object ctor parameter names (MUST match exactly: check in optimisation submodule !).
-    ParmExpr2Optimise = 'expr2Optimise'
-    ParmMinimiseExpr = 'minimiseExpr'
-
-    def getAnalysisOptimExprParams(self, sAnIntSpec):
-                                 
-        """Retrieve optimisation expression parameters, from user specs and default values.
-        
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-                           syntax: IntSpecExpr2Optimise => <min|max>(math. expr)
-
-        :return: None or an Error instance, dict(expr2Optimise=..., minimiseExpr=...) or None
-        
-        Ex: max(aic), min(1/aic/ks)"""
-
-        def _buildParsedValue(expr2Optimise, minimiseExpr):
-            return {self.ParmExpr2Optimise: expr2Optimise, self.ParmMinimiseExpr: minimiseExpr}
-
-        # Parse expression to optimise in sAnIntSpec if present.
-        if self.IntSpecExpr2Optimise in sAnIntSpec:
-        
-            # Retrieve
-            userOptExpr = sAnIntSpec[self.IntSpecExpr2Optimise]
-                
-            # Parse
-            def min(expr):
-                return _buildParsedValue(expr, True)
-
-            def max(expr):
-                return _buildParsedValue(expr, False)
-
-            parseError, parsedValue = \
-                 self._parseUserSpec(userOptExpr, globals=None, locals=dict(min=min, max=max),
-                                     oneStrArg=True, errIfNotA=[dict],
-                                     nullOrEmpty=_buildParsedValue(self.defExpr2Optimise, self.defMinimiseExpr))
-
-        # No way: fall back to default values.
-        else:
-            
-            parseError, parsedValue = \
-                None, _buildParsedValue(self.defExpr2Optimise, self.defMinimiseExpr)
-                 
-        # Done.
-        return parseError, parsedValue
-    
-    def getOptimisationCoreParams(self, sAnIntSpec):
-
-        """Retrieve optimisation core parameters, from user specs and default values.
-        
-        Ex: zoopt(mxi=50, tv=1, a=racos, mxr=2)
-
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-                           syntax: IntSpecExpr2Optimise => <opt. core name>(**{k:v})
-                           
-        :return: None or an Error instance, dict(core=..., **{key:value}) or None
-        """
-        
-        raise NotImplementedError('Abstract class: implement in a derived class')
-    
-    # Optimisation object ctor parameter names (MUST match exactly: check in optimisation submodule !).
-    ParmSubmTimes = 'times'
-    ParmSubmOnlyBest = 'onlyBest'
-
-    def getOptimisationSubmitParams(self, sAnIntSpec):
-    
-        """Retrieve optimisation submission parameters from user specs and default values.
-        
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-                           syntax: IntSpecSubmitParams => <times>([n=]<num>[, [b=]<num>])
-                           
-        :return: None or an Error instance, dict(=..., **{k:v}) or None
-        
-        Ex: dict(times=, onlyBest=, ...)"""
-
-        def _buildParsedValue(times, onlyBest):
-            return {self.ParmSubmTimes: times, self.ParmSubmOnlyBest: onlyBest}
-
-        # Parse expression to optimise in sAnIntSpec if present.
-        if self.IntSpecSubmitParams in sAnIntSpec:
-        
-            # Retrieve
-            userOptExpr = sAnIntSpec[self.IntSpecSubmitParams]
-                
-            # Parse
-            def times(n=1, b=None):
-                assert n > 0, 'Run times must be > 0'
-                assert b is None or b > 0, 'Number of best kept values must be > 0'
-                return _buildParsedValue(n, b)
-
-            parseError, parsedValue = \
-                 self._parseUserSpec(userOptExpr, globals=None, locals=dict(times=times),
-                                     nullOrEmpty=_buildParsedValue(self.defSubmitTimes, self.defSubmitOnlyBest),
-                                     errIfNotA=[dict])
-
-        # No way: fall back to default values.
-        else:
-            
-            parseError, parsedValue = \
-                None, _buildParsedValue(self.defSubmitTimes, self.defSubmitOnlyBest)
-                 
-        # Done.
-        return parseError, parsedValue
-        
-    def setupOptimisation(self, sampleDataSet, name=None, customData=None, setupError=None, **otherParams):
-
-        """Factory method for DSOptimisation derived classes
-        
-        Parameters:
-        :param sampleDataSet: SampleDataSet to analyse
-        :param name: of the optimisation (for clearer logging only, auto-generated and auto-unique anyway)
-        :param customData: pd.Series of custom data for heading optimisation results columns
-        :param setupError: if an error occurred during optimisation submit params user specs parsing,
-                 a string is here for explaining it, and to prevent any submit call of course !
-                 This is done for keeping trace of not run optimisations in results table (1 empty/null row)
-        :param otherParams: some room for derived classes
-        """
-
-        raise NotImplementedError('DSParamsOptimiser: Abstract class, implement setupOptimisation in derived one')
-
-    def explicitParamSpecs(self, implParamSpecs=None, dfExplParamSpecs=None, dropDupes=True, check=False):
-    
-        """Explicitate analysis and optimisation param. specs if not already done, and complete columns if needed ;
-        also automatically extract (regexps) columns which are really analysis parameters,
-        with their analyser-internal name, and also their "user" name.
-        
-        Can moreover check params specs for usability, if check is True :
-        * use it before calling analyser.run(implParamSpecs=..., dfExplParamSpecs=..., ...)
-          to check that everything's OK,
-        * or be sure that run() will fail at startup (because it itself will do it).
-        
-        Parameters:
-        :param implParamSpecs: Implicit analysis param specs, suitable for explicitation
-           through explicitVariantSpecs()
-        :param dfExplParamSpecs: Explicit analysis param specs, as a DataFrame
-           (generated through explicitVariantSpecs, as an example)
-        :param dropDupes: if True, drop duplicates (keep first)
-        :param check: if True, checks params for usability by run(),
-           and return a bool verdict and a list of strings explaining the negative (False) verdict
-
-        :return: a 3 or 5-item tuple :
-           * explicit specs as a DataFrame (input dfExplParamSpecs not modified: a new updated one is returned),
-           * list of analysis and optimisationparam. columns internal names,
-           * list of analysis and optimisationparam. columns user names,
-           if check, 2 more items in the return tuple :
-           * check verdict : True if everything went well, False otherwise,
-             * some columns from paramSpecCols could not be found in dfExplParamSpecs columns,
-             * some user columns could not be matched with some of the expected internal parameter names,
-             * some rows are not suitable for DS analysis (empty sample identification columns, ...).
-           * check failure reasons : list of strings explaining things that went bad.
-        """
-        
-        # Explicitate and complete
-        tplRslt = DSAnalyser._explicitParamSpecs(implParamSpecs, dfExplParamSpecs, self.Int2UserSpecREs,
-                                                 sampleSelCols=self.sampleSelCols, abbrevCol=self.abbrevCol,
-                                                 abbrevBuilder=self.abbrevBuilder, anlysIndCol=self.anlysIndCol,
-                                                 sampleIndCol=self.sampleIndCol,
-                                                 anlysSpecCustCols=self.anlysSpecCustCols, dropDupes=dropDupes)
-        
-        # Check if requested
-        if check:
-        
-            verdict = True
-            reasons = []
-     
-            dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = tplRslt
-            
-            # Check that an internal column name was found for every user spec column.
-            if len(unmUserParamSpecCols):
-                verdict = False
-                reasons.append('Failed to match some user spec. names with internal ones: {}'
-                               .format(', '.join(unmUserParamSpecCols)))
-
-            # Check that all rows are suitable for DS analysis (non empty sample identification columns, ...).
-            if dfExplParamSpecs[self.sampleSelCols].isnull().all(axis='columns').any():
-                verdict = False
-                reasons.append('Some rows have some null sample selection columns')
-
-            # Check done.
-            tplRslt += verdict, reasons
-
-        return tplRslt
-
-    def shutdown(self):
-    
-        """Shutdown engine and executor (only usefull if run() raises an exception and so fails to do it),
-        but keep the remainder of the object state as is.
-        """
-    
-        if self._engine:
-            self._engine.shutdown(executor=True)
-            self._engine = None
-        if self._executor:
-            self._executor.shutdown()
-            self._executor = None
-            
-#    def __del__(self):
-#    
-#        self.shutdown()
-
-
-class MCDSTruncationOptimiser(DSParamsOptimiser):
-
-    """Abstract class ; Run a bunch of MCDS truncation optimisations"""
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
-                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
-                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'],
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 workDir='.', runMethod='subprocess.run', runTimeOut=300,
-                 logData=False, logProgressEvery=5, backupEvery=50, autoClean=True,
-                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
-                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
-                 defExpr2Optimise='chi2', defMinimiseExpr=False,
-                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,
-                 defFitDistCutsFctr=dict(min=2/3, max=3/2), defDiscrDistCutsFctr=dict(min=1/3, max=1),
-                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),
-                 dDefOptimCoreParams=dict()):
-
-        """Ctor (don't use directly, abstract class)
-        
-        Parameters for MCDS engine:
-        :param surveyType: See MCDSEngine
-        :param distanceType: See MCDSEngine
-        :param clustering: See MCDSEngine
-
-        Parameters for auto-computing target intervals (when not fully specified in each optimisation parameters):
-        :param defOutliersMethod: Outliers estimation method when min/maxDist=auto
-               or auto(pct) but not auto(min, max): 
-                * 'quant' : Pure [P, 100-P] % quantiles
-                * 'tucquant' : Mixed P% quantiles & Tuckey method
-        :param defOutliersQuantCutPct: Outliers cut % value (= P in outliersMethod description above) 
-        :param defFitDistCutsFctr: Default factor multiplied to sqrt(nb of sightings)
-               to get min/max FitDistCuts when fitDistCutsFctr is auto with no parameters (min and max)
-        :param defDiscrDistCutsFctr: Factor multiplied to sqrt(nb of sightings)
-               to get min/max DiscrDistCuts when discrDistCutsFctr is auto with no parameters (min and max)
-
-        :param runMethod: for calling MCDS engine executable : 'os.system' or 'subprocess.run'
-        :param runTimeOut: engine call time limit (s) ; None => no limit ;
-                           WARNING: Not implemented (no way) for 'os.system' run method (no solution found)
-
-        Other parameters: See base class.
-        """
-
-        super().__init__(dfMonoCatObs, dfTransects=dfTransects,
-                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
-                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
-                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
-                         sampleDistCol=sampleDistCol, anlysSpecCustCols=anlysSpecCustCols,
-                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
-                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         resultsHeadCols=resultsHeadCols, workDir=workDir,
-                         defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,
-                         defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,
-                         dDefSubmitOtherParams=dDefSubmitOtherParams, dDefOptimCoreParams=dDefOptimCoreParams)
-
-        assert runTimeOut is None or runMethod != 'os.system', \
-               f"Can't care about {runTimeOut}s execution time limit with os.system run method (not implemented)"
-        assert defOutliersMethod in ['quant', 'tucquant'], f'Unsupported default outliers method {defOutliersMethod}'
-
-        self.surveyType = surveyType
-        self.distanceType = distanceType
-        self.clustering = clustering
-        
-        self.runMethod = runMethod
-        self.runTimeOut = runTimeOut
-        self.logData = logData
-        self.logProgressEvery = logProgressEvery
-        self.backupEvery = backupEvery
-        self.autoClean = autoClean
-        
-        self.defEstimKeyFn = defEstimKeyFn
-        self.defEstimAdjustFn = defEstimAdjustFn
-        self.defEstimCriterion = defEstimCriterion
-        self.defCVInterval = defCVInterval
-        self.defOutliersMethod = defOutliersMethod
-        self.defOutliersQuantCutPct = defOutliersQuantCutPct
-        self.defFitDistCutsFctr = \
-            defFitDistCutsFctr if defFitDistCutsFctr is None else Interval(defFitDistCutsFctr)
-        self.defDiscrDistCutsFctr = \
-            defDiscrDistCutsFctr if defDiscrDistCutsFctr is None else Interval(defDiscrDistCutsFctr)
-                         
-        self.updateSpecs(**{name: getattr(self, name)
-                            for name in ['runMethod', 'runTimeOut', 'surveyType', 'distanceType', 'clustering',
-                                         'defEstimKeyFn', 'defEstimAdjustFn', 'defEstimCriterion', 'defCVInterval']})
-        self.updateSpecs(**{name: getattr(self, name)
-                            for name in ['defOutliersMethod', 'defOutliersQuantCutPct',
-                                         'defFitDistCutsFctr', 'defDiscrDistCutsFctr']})
-
-    # Optimiser internal parameter spec names, for which a match should be found (when one is needed)
-    # with user explicit optimisation specs used in run() calls.
-    IntSpecEstimKeyFn = MCDSAnalyser.IntSpecEstimKeyFn
-    IntSpecEstimAdjustFn = MCDSAnalyser.IntSpecEstimAdjustFn
-    IntSpecEstimCriterion = MCDSAnalyser.IntSpecEstimCriterion
-    IntSpecCVInterval = MCDSAnalyser.IntSpecCVInterval
-    IntSpecMinDist = MCDSAnalyser.IntSpecMinDist  # Left truncation distance
-    IntSpecMaxDist = MCDSAnalyser.IntSpecMaxDist  # Right truncation distance
-    IntSpecFitDistCuts = MCDSAnalyser.IntSpecFitDistCuts
-    IntSpecDiscrDistCuts = MCDSAnalyser.IntSpecDiscrDistCuts
-    IntSpecOutliersMethod = 'OutliersMethod'
-
-    # Possible regexps (values) for auto-detection of optimiser _internal_ parameter spec names (keys)
-    # from explicit _user_ spec columns
-    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
-    #  and case is ignored during searching).
-    Int2UserSpecREs = \
-      dict(list(DSParamsOptimiser.Int2UserSpecREs.items())
-           + list(MCDSAnalyser.Int2UserSpecREs.items())
-           + [(IntSpecOutliersMethod, ['outl[a-z]*[\.\-_ ]*', 'me[a-z]*[\.\-_ ]*outl'])])
-
-    # Names of internal parameters which can be used as settings for optimising truncations
-    # and not only as const=fixed=pre-determined analysis parameters.
-    IntOptimParamSpecNames = \
-        [DSParamsOptimiser.IntSpecExpr2Optimise, DSParamsOptimiser.IntSpecOptimisationCore,
-         DSParamsOptimiser.IntSpecSubmitParams,
-         IntSpecMinDist, IntSpecMaxDist, IntSpecFitDistCuts, IntSpecDiscrDistCuts,
-         IntSpecOutliersMethod]
-         
-    @classmethod
-    def optimisationParamSpecUserNames(cls, userParamSpecCols, intParamSpecCols):
-    
-        """Extract user names of params spec. columns that may contain truncation optimisation parameters
-
-        Parameters:
-        :param userParamSpecCols: all user params spec column names to explore,
-        :param intParamSpecCols: the matching (1 by 1, same order) internal user params spec column names.
-        """
-    
-        # Internal column names to consider.
-        optimIntCols = [intCol for intCol in cls.IntOptimParamSpecNames if intCol in intParamSpecCols]
-        
-        # And the corresponding user names.
-        return [userCol for intCol, userCol in zip(intParamSpecCols, userParamSpecCols) if intCol in optimIntCols]
-
-    # Optimisation object ctor parameter names (MUST match exactly: check in optimisation submodule !).
-    ParmEstimKeyFn = 'estimKeyFn'
-    ParmEstimAdjustFn = 'estimAdjustFn'
-    ParmEstimCriterion = 'estimCriterion'
-    ParmCVInterval = 'cvInterval'
-    
-    def getAnalysisFixedParams(self, sAnIntSpec):
-    
-        """Retrieve analysis fixed parameters of an optimisation, from user specs and default values
-        
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-        
-        :return: a 2-value tuple (None or an Error instance,
-                                  dict(estimKeyFn=, estimAdjustFn=, estimCriterion=, cvInterval=) or None)
-        """
-        
-        estimKeyFn = sAnIntSpec.get(self.IntSpecEstimKeyFn, self.defEstimKeyFn)
-        if pd.isnull(estimKeyFn):
-            estimKeyFn = self.defEstimKeyFn
-        
-        estimAdjFn = sAnIntSpec.get(self.IntSpecEstimAdjustFn, self.defEstimAdjustFn)
-        if pd.isnull(estimAdjFn):
-            estimAdjFn = self.defEstimAdjustFn
-        
-        estimCrit = sAnIntSpec.get(self.IntSpecEstimCriterion, self.defEstimCriterion)
-        if pd.isnull(estimCrit):
-            estimCrit = self.defEstimCriterion
-        
-        cvInterv = sAnIntSpec.get(self.IntSpecCVInterval, self.defCVInterval)
-        if pd.isnull(cvInterv):
-            cvInterv = self.defCVInterval
-        
-        return None, {self.ParmEstimKeyFn: estimKeyFn, self.ParmEstimAdjustFn: estimAdjFn,
-                      self.ParmEstimCriterion: estimCrit, self.ParmCVInterval: cvInterv}
-
-    # From / to: Optimisation object ctor parameter names = Solution dimension names
-    # To / From: Optimiser internal param. spec names.
-    SolDim2IntSpecOptimTargetParamNames = \
-        dict(zip(MCDSTruncationOptimisation.SolutionDimensionNames,
-                 [IntSpecMinDist, IntSpecMaxDist, IntSpecFitDistCuts, IntSpecDiscrDistCuts]))
-    IntSpec2SolDimOptimTargetParamNames = {v:k for k,v in SolDim2IntSpecOptimTargetParamNames.items()}
-
-    def getAnalysisOptimedParams(self, sAnIntSpec, sSampleDists):
-    
-        """Compute optimisation intervals of an optimisation, from user specs and default parameters
-
-        Some checks for final values are done, may be resulting in an Error.
-        
-        Parameters:
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-                           syntax: sequence of <key>=<value> separated by ','
-        :param sSampleDists: sample sightings recorded distances
-        
-        :return: a 2-value tuple (None or an Error instance in case any parsing / check failed,
-                                  dict(minDist=, maxDist=, fitDistCuts=, discrDistCuts=) or None)
-        """
-        
-        # Parse found specs from strings (or so) to real objects
-        errMinDist, minDistSpec = \
-            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecMinDist, None),
-                                              errIfNotA=[int, float, self.Auto,
-                                                         self.DistInterval, self.OutliersMethod])
-        errMaxDist, maxDistSpec =  \
-            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecMaxDist, None),
-                                              errIfNotA=[int, float, self.Auto,
-                                                         self.DistInterval, self.OutliersMethod])
-        errFitDistCuts, fitDistCutsSpec = \
-            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecFitDistCuts, None),
-                                              errIfNotA=[int, float, self.Auto, 
-                                                         self.MultInterval, self.AbsInterval])
-        errDiscrDistCuts, discrDistCutsSpec = \
-            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecDiscrDistCuts, None),
-                                              errIfNotA=[int, float, self.Auto,
-                                                         self.MultInterval, self.AbsInterval])
-        errOutliersMethod, outliersMethodSpec = \
-            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecOutliersMethod, None),
-                                              errIfNotA=[self.Auto, self.OutliersMethod])
-
-        logger.info2('OptimedParams specs:' + str(dict(minDist=minDistSpec, maxDist=maxDistSpec,
-                                                       fitDistCuts=fitDistCutsSpec, discrDistCuts=discrDistCutsSpec)))
-
-        # Stop here if any parsing error.
-        finalErr = Error()
-        for err in [errMinDist, errMaxDist, errFitDistCuts, errDiscrDistCuts, errOutliersMethod]:
-            if err:
-                finalErr.append(err)
-        if finalErr:
-            return finalErr, None
-
-        # Compute or translate parameter values from objects.
-        
-        # A. outliers method: method ('quant' or 'tucquant') and cut % (% of outliers on _each_ side)
-        # - None or auto : use defOutliersMethod et defOutliersQuantCutPct,
-        # - <meth>(<cut pct>) : use <meth> method, and <cut pct> cut %
-        if outliersMethodSpec is None or isinstance(outliersMethodSpec, self.Auto):
-            outliersMethod = self.defOutliersMethod
-            outliersQuantCutPct = self.defOutliersQuantCutPct
-        elif isinstance(outliersMethodSpec, self.OutliersMethod):
-            outliersMethod = outliersMethodSpec.method
-            outliersQuantCutPct = outliersMethodSpec.percent
-        else:
-            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
-                            'Should not fall there (outliersMethod specs)')
-        assert outliersMethod in ['quant', 'tucquant'], f'Unsupported outliers method {outliersMethod}'
-
-        # B. minDist, maxDist :
-        # - None,
-        # - auto : mode full auto via colonne OutliersMethod, 
-        #                             ou alors defOutliersMethod et defOutliersQuantCutPct,
-        # - <meth>(5) : mode auto tq OutliersMethod (<meth> quant(5) ou tucquant) et outliersQuantCutPct(ici 5%) fixés
-        # - dist(20, 250) : mode fixé, dist min (20) et max (250) du domaine de variations fournies en dur
-
-        sDist = sSampleDists.dropna()
-        sqrNSights = np.sqrt(len(sDist))
-
-        # minDist specs
-        if minDistSpec is None or isinstance(minDistSpec, (int, float)):
-            minDist = minDistSpec
-        elif isinstance(minDistSpec, self.DistInterval):
-            minDist = Interval(min=minDistSpec.dmin, max=minDistSpec.dmax)
-        elif isinstance(minDistSpec, self.Auto):
-            maxMinDist = np.percentile(a=sDist, q=outliersQuantCutPct)
-            minDist = Interval(min=sDist.min(), max=maxMinDist)
-        elif isinstance(minDistSpec, self.OutliersMethod):
-            maxMinDist = np.percentile(a=sDist, q=minDistSpec.percent)
-            minDist = Interval(min=sDist.min(), max=maxMinDist)
-        else:
-            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
-                            'Should not fall there (minDist specs)')
-        
-        if isinstance(minDist, Interval) and minDist.min == minDist.max:
-            minDist = minDist.min
-
-        # maxDist specs
-        if maxDistSpec is None or isinstance(maxDistSpec, (int, float)):
-            maxDist = maxDistSpec
-        elif isinstance(maxDistSpec, self.DistInterval):
-            maxDist = Interval(min=maxDistSpec.dmin, max=maxDistSpec.dmax)
-        elif isinstance(maxDistSpec, (self.Auto, self.OutliersMethod)):
-            if isinstance(maxDistSpec, self.Auto):
-                d25, d75, d95, dPct = np.percentile(a=sDist, q=[25, 75, 95, 100-outliersQuantCutPct])
-            else:  # self.OutliersMethod
-                d25, d75, d95, dPct = np.percentile(a=sDist, q=[25, 75, 95, 100-maxDistSpec.percent])
-            if outliersMethod == 'quant':
-                minMaxDist = dPct
-            elif outliersMethod == 'tucquant':
-                minMaxDist = min(max(d95, d75 + 1.5*(d75 - d25)), dPct)
-            maxDist = Interval(min=minMaxDist, max=sDist.max())
-        else:
-            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
-                            'Should not fall there (maxDist specs)')
-
-        if isinstance(maxDist, Interval) and maxDist.min == maxDist.max:
-            minDist = maxDist.min
-
-        # C. fitDistCuts, discrDistCuts :
-        # - None,
-        # - auto : full automated mode through column outliersMethod,
-        #          or else defOutliersMethod and defOutliersQuantCutPct,
-        # - mult(1/3, 3/2) : "fixed" mode = hard-coded mult. factors min and max for sqrt(nb of sightings)
-        # - abs(5, 10) : fixed" mode = hard-coded mult. factors min and max for sqrt(nb of sightings)
-
-        # 1. fitDistCuts specs
-        if fitDistCutsSpec is None or isinstance(fitDistCutsSpec, (int, float)):
-            fitDistCuts = fitDistCutsSpec
-        elif isinstance(fitDistCutsSpec, self.AbsInterval):
-            fitDistCuts = Interval(min=max(2, fitDistCutsSpec.min), max=fitDistCutsSpec.max)
-        elif isinstance(fitDistCutsSpec, self.Auto):
-            fitDistCuts = Interval(min=max(2, int(round(self.defFitDistCutsFctr.min*sqrNSights))),
-                                   max=int(round(self.defFitDistCutsFctr.max*sqrNSights)))
-        elif isinstance(fitDistCutsSpec, self.MultInterval):
-            fitDistCuts = Interval(min=max(2, int(round(fitDistCutsSpec.kmin*sqrNSights))),
-                                   max=int(round(fitDistCutsSpec.kmax*sqrNSights)))
-        else:
-            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
-                            'Should not fall there (fitDistCuts specs)')
-            
-        if isinstance(fitDistCuts, Interval) and fitDistCuts.min == fitDistCuts.max:
-            fitDistCuts = fitDistCuts.min
-
-        # 2. discrDistCuts specs
-        if discrDistCutsSpec is None or isinstance(discrDistCutsSpec, (int, float)):
-            discrDistCuts = discrDistCutsSpec
-        elif isinstance(discrDistCutsSpec, self.AbsInterval):
-            discrDistCuts = Interval(min=max(2, discrDistCutsSpec.min), max=discrDistCutsSpec.max)
-        elif isinstance(discrDistCutsSpec, self.Auto):
-            discrDistCuts = Interval(min=max(2, int(round(self.defDiscrDistCutsFctr.min*sqrNSights))),
-                                     max=int(round(self.defDiscrDistCutsFctr.max*sqrNSights)))
-        elif isinstance(discrDistCutsSpec, self.MultInterval):
-            discrDistCuts = Interval(min=max(2, int(round(discrDistCutsSpec.kmin*sqrNSights))),
-                                     max=int(round(discrDistCutsSpec.kmax*sqrNSights)))
-        else:
-            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
-                            'Should not fall there (discrDistCuts specs)')
-            
-        if isinstance(discrDistCuts, Interval) and discrDistCuts.min == discrDistCuts.max:
-            discrDistCuts = discrDistCuts.min
-
-        # Final checks
-        finalErr = Error()
-        if minDist is not None:
-            minDistChk = minDist if isinstance(minDist, Interval) else Interval(minDist, minDist)
-            msg = minDistChk.check(order=True, minRange=(0, None), maxRange=(None, sDist.max()))
-            if msg:
-                finalErr.append(head='minDist', error=msg)
-
-        if maxDist is not None:
-            maxDistChk = maxDist if isinstance(maxDist, Interval) else Interval(maxDist, maxDist)
-            minMax = None if minDist is None else (minDist.max if isinstance(minDist, Interval) else minDist)
-            msg = maxDistChk.check(order=True, minRange=(minMax, None), maxRange=(None, sDist.max()))
-            if msg:
-                finalErr.append(head='maxDist', error=msg)
-
-        if fitDistCuts is not None:
-            fitDistCutsChk = \
-                fitDistCuts if isinstance(fitDistCuts, Interval) else Interval(fitDistCuts, fitDistCuts)
-            msg = fitDistCutsChk.check(order=True, minRange=(2, None))
-            if msg:
-                finalErr.append(head='fitDistCuts', error=msg)
-
-        if discrDistCuts is not None:
-            discrDistCutsChk = \
-                discrDistCuts if isinstance(discrDistCuts, Interval) else Interval(discrDistCuts, discrDistCuts)
-            msg = discrDistCutsChk.check(order=True, minRange=(2, None))
-            if msg:
-                finalErr.append(head='discrDistCuts', error=msg)
-
-        logger.info2(f'OptimedParams: {minDist=}, {maxDist=}, {fitDistCuts=}, {discrDistCuts=}')
-
-        return finalErr or None, \
-               {self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecMinDist]: minDist,
-                self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecMaxDist]: maxDist,
-                self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecFitDistCuts]: fitDistCuts,
-                self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecDiscrDistCuts]: discrDistCuts}
-                                      
-    # Supported truncation optimisation classes (=> engines = cores) (see submodule optimisation),
-    # all must be subclasses of MCDSTruncationOptimisation.
-    OptimisationClasses = [MCDSZerothOrderTruncationOptimisation]  #, MCDSGridBruteTruncationOptimisation]
-            
-    def getOptimisationCoreParams(self, sAnIntSpec):
-
-        """Retrieve optimisation core parameters, from user specs and default values.
-        
-        Ex: zoopt(mxi=0, tv=1, a=racos, mrx=3)
-
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-                           syntax: sequence of <key>=<value> separated by ','
-                           
-        :return: None or an Error instance, dict(core=..., **{key:value}) or None
-        """
-        
-        # Parse optimisation core params in sAnIntSpec if present.
-        if self.IntSpecOptimisationCore in sAnIntSpec:
-        
-            # Retrieve
-            userSpec = sAnIntSpec[self.IntSpecOptimisationCore]
-                
-            # Parse
-            parsers = {cls.CoreName: cls.CoreUserSpecParser for cls in self.OptimisationClasses}
-            parseError, parsedValue = \
-                 self._parseOptimCoreUserSpecs(userSpec, globals=None, locals=parsers,
-                                               nullOrEmpty=self.dDefOptimCoreParams,
-                                               errIfNotA=[dict])
-
-        # No way: fallback to default values.
-        else:
-            
-            parseError, parsedValue = None, self.dDefOptimCoreParams
-                 
-        # Done.
-        return parseError, parsedValue
-    
-    def getOptimisationSetupParams(self, sAnIntSpec, sSampleDists):
-                               
-        """Compute optimisation setup parameters from user specs and default values.
-        
-        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
-        :param sSampleDists: sample sightings recorded distances
-       
-        :return: a 2-value tuple (None or an Error instance,
-                          dict(minDist=, maxDist=, fitDistCuts=, discrDistCuts=) or None)
-
-        """
-        
-        # Get params from each of these sets.
-        dFinalParms = dict()
-        finalError = Error()
-        for err, dParms in [self.getAnalysisFixedParams(sAnIntSpec),
-                            self.getAnalysisOptimExprParams(sAnIntSpec),
-                            self.getAnalysisOptimedParams(sAnIntSpec, sSampleDists),
-                            self.getOptimisationCoreParams(sAnIntSpec)]:
-            if err is None:
-                dFinalParms.update(dParms)
-            else:
-                finalError.append(err)
-
-        # Any error => empty output params
-        if finalError:
-            logger.warning('Error(s) while parsing and computing setup params specs: {}'.format(finalError))
-         
-        return finalError, dFinalParms
-        
-    def setupOptimisation(self, sampleDataSet, name=None, customData=None, error=None,
-                          estimKeyFn=MCDSEngine.EstKeyFnDef, estimAdjustFn=MCDSEngine.EstAdjustFnDef, 
-                          estimCriterion=MCDSEngine.EstCriterionDef, cvInterval=MCDSEngine.EstCVIntervalDef,
-                          expr2Optimise='chi2', minimiseExpr=False,
-                          minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,
-                          **dCoreParams):
-                          
-        """Factory method for MCDSXXXTruncationOptimisation classes"""
-
-        # Search for optimisation class from core name dCoreParams['core']
-        # (default to 'zoopt' in case of any error parsing optim core params)
-        optimCore = dCoreParams.get('core', 'zoopt')
-        try:
-            OptimionClass = next(iter(cls for cls in self.OptimisationClasses if cls.CoreName == optimCore))
-        except StopIteration:
-            raise NotImplementedError('No such optimisation core "{}" in house'.format(optimCore))
-        
-        # Check core params.
-        invalidParams = [k for k in dCoreParams if k != 'core' and k not in OptimionClass.CoreParamNames]
-        assert not invalidParams, \
-               'No such parameter(s) {} for {} ctor'.format(','.join(invalidParams), OptimionClass.__name__)
-
-        # Build remainder of optimisation ctor params.
-        dOtherOptimParms = {k: v for k, v in dCoreParams.items() if k in OptimionClass.CoreParamNames}
-        
-        # Instantiate optimisation.
-        optimion = OptimionClass(self._engine, sampleDataSet, name=name,
-                                 customData=customData, error=error,
-                                 executor=self._executor, logData=self.logData, autoClean=self.autoClean,
-                                 estimKeyFn=estimKeyFn, estimAdjustFn=estimAdjustFn,
-                                 estimCriterion=estimCriterion, cvInterval=cvInterval,
-                                 minDist=minDist, maxDist=maxDist,
-                                 fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts,
-                                 expr2Optimise=expr2Optimise, minimiseExpr=minimiseExpr,
-                                 **dOtherOptimParms)
-                                 
-        return optimion
-
-    # Number of optimisation auto-backup files (alternating scheme)
-    BackupAltNum = 2
-
-    def _backupFileName(self, index=0):
-
-        return pl.Path(self.workDir) / f'optr-resbak-{index}.pickle.xz'
-
-    def setupResults(self, dfOptimParamSpecs=None, recover=False, loadFrom=None, sheetName=None):
-    
-        """Build an empty results objects and optionnaly pre-load results
-        * from the last auto-backup file, after a crash / interruption of the optimisations,
-        * or from the given file name.
-
-        Parameters:
-        :param dfOptimParamSpecs: if not None, optim. param. specs for the result object
-                                  (overwrite any specs loaded from file; see blow)
-        :param recover: if True, try and load the last backup file (see completeResults)
-        :param loadFrom: if specified, and not recover, try and load the specified file
-                         (see OptimisationResultsSet.fromFile for supported formats, auto-detected from file extensions)
-        :param sheetName: when loadFrom targets a workbook format file, the name of the sheet to load.
-        """
-    
-        # Build the empty result object
-        customCols = \
-            self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
-        dfCustColTrans = pd.DataFrame(index=customCols, data={lang: customCols for lang in ['fr', 'en']})
-
-        # TODO: optimisationClass=MCDSZerothOrderTruncationOptimisation,
-        self.results = OptimisationResultsSet(optimisationClass=MCDSTruncationOptimisation,
-                                              miCustomCols=customCols, dfCustomColTrans=dfCustColTrans)
-
-        # e. Load data from file if requested to.
-        if recover:
-
-            # Check that there will be an analysis index = Id column in results
-            assert self.anlysIndCol, "Can't recover optimisation results if no analysis index column specified !"
-
-            # Search for backup files and sort them by modification time in descending order.
-            bkupFileNames = [self._backupFileName(ind) for ind in range(self.BackupAltNum)]
-            bkupFileNames = [fpn for fpn in bkupFileNames if fpn.is_file()]
-            sBkupFileMTimes = pd.Series(index=[fpn.as_posix() for fpn in bkupFileNames], 
-                                        data=[fpn.stat().st_mtime for fpn in bkupFileNames])
-            assert not sBkupFileMTimes.empty, 'No such backup file found: {}'.format(self._backupFileName('*'))
-
-            sBkupFileMTimes.sort_values(inplace=True, ascending=False)
-
-            for bkupFpn in sBkupFileMTimes.index:
-                logger.info('Recovering optimisation results from auto-backup {} ...'.format(bkupFpn))
-                try:
-                    self.results.fromPickle(bkupFpn, acceptNewCols=True)
-                    break
-                except Exception as exc:
-                    if bkupFpn == sBkupFileMTimes.index[-1]:
-                        logger.error('... failed ; no other usable backup file, sorry.')
-                        raise
-                    else:
-                        logger.info('... failed ; trying next ...')
-
-            logger.info('... success !')
-
-            # Check that there IS really an analysis index = Id column in results
-            assert self.anlysIndCol in self.results.columns, \
-                   "Can't recover from backup results file because no analysis index/Id column found !"
-
-        elif loadFrom:
-
-            self.results.fromFile(fileName=loadFrom, sheetName=sheetName)
-
-        if dfOptimParamSpecs is not None:
-
-            # Set results specs for traceability.
-            self.results.updateSpecs(overwrite=True, optimiser=self.flatSpecs(), optimisations=dfOptimParamSpecs)
-        
-    def optimisationTargetColumnUserNames(self):
-    
-        """Determine the user names of the results optimisation target columns
-        """
-   
-        # Should not be called before run !
-        assert self.results is not None
-   
-        # From results column names to internal spec name to user spec name.
-        return [self.dInt2UserParamSpecNames[self.SolDim2IntSpecOptimTargetParamNames[solDimName]]
-                for solDimName in self.results.optimisationTargetColumns()]
-
-    def completeResults(self, dOptims):
-    
-        """Wait for and gather dOptims (MCDSOptimisation futures) results into a ResultsSet
-        
-        There should be no reason to specialise this for specific truncation optimisers, but ...
-        """
-    
-        # Start of elapsed time measurement (yes, starting the optimisations may take some time, but it is
-        # negligible when compared to optimisation time ; and better here for evaluating mean per optimisation).
-        optimStart = pd.Timestamp.now()
-        
-        # For each optimisation as it gets completed (first completed => first yielded)
-        nDone = 0
-        for optimFut in self._executor.asCompleted(dOptims):
-            
-            # Retrieve optimisation object from its associated future object
-            optim = dOptims[optimFut]
-            
-            # Get analysis results and optimisation target column names in results
-            dfResults = optim.getResults()
-
-            # Get custom header values, and set target index (= columns) for results
-            sCustomHead = optim.customData
-            sCustomHead.index = self.results.miCustomCols
-
-            # Save results (exact column list changes among optimisations objects so we must acceptNewCols)
-            self.results.append(dfResults, sCustomHead=sCustomHead, acceptNewCols=True)
-
-            # Backup raw results (with an alternate file scheme for safety) in case of unavoidable crash (or reboot).
-            # See setupResults for how these files can be reused later for recovery.
-            nDone += 1
-            if nDone % self.backupEvery == 0:
-                nBackupInd = (nDone // self.backupEvery) % self.BackupAltNum
-                self.results.toPickle(fileName=self._backupFileName(nBackupInd), raw=True)
-
-            # Report elapsed time and number of optimisations completed until now.
-            if nDone % self.logProgressEvery == 0 or nDone == len(dOptims):
-                now = pd.Timestamp.now()
-                elapsedTilNow = now - optimStart
-                if nDone < len(dOptims):
-                    expectedEnd = \
-                        now + pd.Timedelta(elapsedTilNow.value * (len(dOptims) - nDone) / nDone)
-                    expectedEnd = expectedEnd.strftime('%Y-%m-%d %H:%M:%S').replace(now.strftime('%Y-%m-%d '), '')
-                    endOfMsg = 'should end around ' + expectedEnd
-                else:
-                    endOfMsg = 'done'
-                logger.info1('{}/{} optimisations in {} (mean {:.2f}s): {}.'
-                             .format(nDone, len(dOptims), str(elapsedTilNow.round('S')).replace('0 days ', ''),
-                                     elapsedTilNow.total_seconds() / nDone, endOfMsg))
-
-        # Terminate analysis executor
-        self._executor.shutdown()
-
-        # Terminate analysis engine
-        self._engine.shutdown()
-        
-    def run(self, dfExplParamSpecs=None, implParamSpecs=None, threads=None, recover=False):
-   
-        """Optimise specified analyses
-        
-        Call checkUserSpecs(...) before this to make sure user specs are OK
-        
-        Parameters:
-        :param dfExplParamSpecs: optimisation params specs table, as a DataFrame
-        :param implParamSpecs: Implicit pd.DataFrame and optimisation param specs, suitable for explicitation
-                               through Analyser.explicitVariantSpecs
-        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
-        :param recover: Recover a previously interrupted run using last available auto-backup file
-        """
-    
-        # Executor for optimisations.
-        self._executor = Executor(threads=threads)
-
-        # MCDS analysis engine (a sequential one, because MCDSOptimisation does the parallel stuff itself,
-        # but an asynchronous one if execution time limit is to be enforced with os.system run method).
-
-        # Failed try: Seems we can't stack ThreadPoolExecutors, as optimisations get run sequentially
-        #             when using an Executor(threads=1) (means async) for self._engine ... 
-        # engineExor = None if self.runMethod != 'os.system' or self.runTimeOut is None else Executor(threads=1)
-        self._engine = MCDSEngine(workDir=self.workDir,  # executor=engineExor,
-                                  runMethod=self.runMethod, timeOut=self.runTimeOut,
-                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
-                                  surveyType=self.surveyType, distanceType=self.distanceType,
-                                  clustering=self.clustering)
-
-        # Custom columns for results.
-        customCols = \
-            self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
-        
-        # Explicitate and complete analysis and optimisation specs, and check for usability.
-        # (should be also done before calling run, to avoid failure).
-        dfExplParamSpecs, userParamSpecCols, intParamSpecCols, _, checkVerdict, checkErrors = \
-            self.explicitParamSpecs(implParamSpecs, dfExplParamSpecs, dropDupes=True, check=True)
-        assert checkVerdict, 'Error: Analysis / Optimisation params check failed: {}'.format('; '.join(checkErrors))
-        
-        # Build internal name => user name converter for spec. columns
-        self.dInt2UserParamSpecNames = dict(zip(intParamSpecCols, userParamSpecCols))
-        
-        # Results object construction
-        self.setupResults(recover=recover, dfOptimParamSpecs=dfExplParamSpecs)
-        recoveredOptims = [] if self.results.empty else list(self.results.dfRawData[self.anlysIndCol].unique())
-
-        # For each optimisation to run :
-        exptdWorkers = self._executor.expectedWorkers()
-        runHow = 'in sequence' if exptdWorkers <= 1 else f'at most {exptdWorkers} parallel threads'
-        logger.info('Running MCDS truncation optimisations for {} analyses specs ({}) ...'
-                    .format(len(dfExplParamSpecs), runHow))
-        if recoveredOptims:
-            logger.info("... but {} already done and recovered won't be again ...".format(len(recoveredOptims)))
-            logger.debug(f'{recoveredOptims=}')
-
-        dOptims = dict()
-        for optimInd, (_, sOptimSpec) in enumerate(dfExplParamSpecs.iterrows()):
-            
-            logger.info1('#{}/{}: {} (Id {})'.format(optimInd+1, len(dfExplParamSpecs),
-                                                    sOptimSpec[self.abbrevCol], sOptimSpec[self.anlysIndCol]))
-
-            # Skip optimisation if already in results (recovered).
-            if sOptimSpec[self.anlysIndCol] in recoveredOptims:
-                logger.info1('Skipping this one: already present in recovered results')
-                continue
-
-            # Select data sample to process (and skip if empty)
-            sds = self._mcDataSet.sampleDataSet(sOptimSpec[self.sampleSelCols])
-            if not sds:
-                continue
-
-            # Build optimisation params specs series with parameters internal names.
-            sOptimIntSpec = sOptimSpec[userParamSpecCols].set_axis(intParamSpecCols, inplace=False)
-            
-            # Compute optimisation setup parameters from user specs and default values.
-            setupError, dSetupParams = \
-                self.getOptimisationSetupParams(sOptimIntSpec, sds.dfData[self.sampleDistCol])
-            
-            # Create optimisation object
-            logger.info2('Optim. params: ' + ', '.join(f'{k}: {v}' for k, v in dSetupParams.items()))
-            optim = self.setupOptimisation(sampleDataSet=sds, name=sOptimSpec[self.abbrevCol],
-                                           customData=sOptimSpec[customCols].copy(),
-                                           error=setupError, **dSetupParams)
-
-            # Compute optimisation submission parameters from user specs and default values.
-            submitError, dSubmitParams = self.getOptimisationSubmitParams(sOptimIntSpec)
-                                               
-            # Submit optimisation (but don't wait for it's finished, go on with next, may run in parallel)
-            logger.info2('Submit params: ' + ', '.join([f'{k}: {v}' for k, v in dSubmitParams.items()]))
-            optimFut = optim.submit(error=submitError, **dSubmitParams)
-            
-            # Store optimisation object and associated "future" for later use (should be running soon or later).
-            dOptims[optimFut] = optim
-            
-            # Next analysis (loop).
-
-        if self._executor.isAsync():
-            logger.info('All optimisations started; now waiting for their end, and results ...')
-        else:
-            logger.info('All optimisations done; now collecting their results ...')
-
-        # Wait for and gather results of all analyses.
-        self.completeResults(dOptims)
-        
-        # Done.
-        logger.info('Optimisations completed ({} analyses => {} results).'
-                    .format(int(self.results.dfData.NFunEvals.sum()), len(self.results)))
-
-        return self.results
-
-
-class MCDSZerothOrderTruncationOptimiser(MCDSTruncationOptimiser):
-
-    """Run a bunch of Zeroth Order MCDS truncation optimisations"""
-
-    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
-                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
-                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
-                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
-                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
-                 distanceUnit='Meter', areaUnit='Hectare',
-                 surveyType='Point', distanceType='Radial', clustering=False,
-                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'],
-                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
-                 workDir='.', runMethod='subprocess.run', runTimeOut=300,
-                 logData=False, logProgressEvery=5, backupEvery=50, autoClean=True,
-                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
-                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
-                 defExpr2Optimise='chi2', defMinimiseExpr=False,
-                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,
-                 defFitDistCutsFctr=Interval(min=2/3, max=3/2),
-                 defDiscrDistCutsFctr=Interval(min=1/3, max=1),
-                 defSubmitTimes=1, defSubmitOnlyBest=None,
-                 defCoreMaxIters=100, defCoreTermExprValue=None, defCoreAlgorithm='racos', defCoreMaxRetries=0):
-
-        super().__init__(dfMonoCatObs=dfMonoCatObs, dfTransects=dfTransects, 
-                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
-                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
-                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
-                         sampleDistCol=sampleDistCol, anlysSpecCustCols=anlysSpecCustCols,
-                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
-                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,
-                         distanceUnit=distanceUnit, areaUnit=areaUnit,
-                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
-                         resultsHeadCols=resultsHeadCols,
-                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut, logData=logData,
-                         logProgressEvery=logProgressEvery, backupEvery=backupEvery, autoClean=autoClean,
-                         defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,
-                         defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,
-                         defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,
-                         defOutliersMethod=defOutliersMethod, defOutliersQuantCutPct=defOutliersQuantCutPct,
-                         defFitDistCutsFctr=defFitDistCutsFctr, defDiscrDistCutsFctr=defDiscrDistCutsFctr,
-                         defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,
-                         dDefOptimCoreParams=dict(core='zoopt', maxIters=defCoreMaxIters,
-                                                  termExprValue=defCoreTermExprValue,
-                                                  algorithm=defCoreAlgorithm, maxRetries=defCoreMaxRetries))
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "optimiser": Auto-determine analysis parameters values based on quality criteria
+
+import re
+
+import pathlib as pl
+
+from collections import namedtuple as ntuple
+
+import numpy as np
+import pandas as pd
+
+from . import log
+from .data import MonoCategoryDataSet, ResultsSet
+from .executor import Executor
+from .engine import MCDSEngine
+from .analyser import DSAnalyser, MCDSAnalyser
+from .optimisation import Interval, Error, DSOptimisation
+from .optimisation import MCDSTruncationOptimisation, MCDSZerothOrderTruncationOptimisation
+
+logger = log.logger('ads.opr')
+
+
+class OptimisationResultsSet(ResultsSet):
+    
+    """A specialized results set for DS analyses optimisations"""
+    
+    def __init__(self, optimisationClass, miCustomCols=None, dfCustomColTrans=None,
+                       dComputedCols=None, dfComputedColTrans=None, sortCols=[], sortAscend=[]):
+        
+        assert issubclass(optimisationClass, DSOptimisation), \
+               'optimisationClass must derive from DSOptimisation'
+        assert miCustomCols is None \
+               or (isinstance(miCustomCols, list) and len(miCustomCols) > 0
+                   and all(isinstance(col, str) for col in miCustomCols)), \
+               'customCols must be None or a list of strings'
+        
+        self.optimisationClass = optimisationClass
+        
+        # Optimisation results columns
+        miCols = optimisationClass.RunColumns
+        
+        # DataFrame for translating column names
+        dfColTrans = optimisationClass.DfRunColumnTrans
+        
+        # Initialize base class.
+        super().__init__(miCols=miCols, dfColTrans=dfColTrans,
+                         miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,
+                         sortCols=sortCols, sortAscend=sortAscend, dropNACols=False)
+    
+    def copy(self, withData=True):
+    
+        """Clone function (shallow), with optional (deep) data copy"""
+    
+        # 1. Call ctor without computed columns stuff (we no more have initial data)
+        clone = OptimisationResultsSet(optimisationClass=self.optimisationClass,
+                                       miCustomCols=self.miCustomCols.copy(),
+                                       dfCustomColTrans=self.dfCustomColTrans.copy(),
+                                       sortCols=self.sortCols.copy(), sortAscend=self.sortAscend.copy())
+    
+        # 2. Complete clone initialisation.
+        # 3-level multi-index columns (module, statistic, figure)
+        clone.miCols = self.miCols.copy()
+        clone.computedCols = self.computedCols.copy()
+        
+        # DataFrames for translating columns names
+        clone.dfColTrans = self.dfColTrans.copy()
+        
+        # Copy data if needed.
+        if withData:
+            clone._dfData = self._dfData.copy()
+            clone.rightColOrder = self.rightColOrder
+            clone.postComputed = self.postComputed
+
+        return clone
+
+    def fromExcel(self, fileName, sheetName=None, specs=True, specSheetsPrfx='sp-'):
+
+        """Load (overwrite) data and optionally specs from an Excel worksheet (XLSX format),
+        assuming ctor params match with Excel sheet column names and list,
+        which can well be ensured by using the same ctor params as used for saving !
+        """
+        
+        super().fromExcel(fileName, sheetName=sheetName, header=0, skipRows=None, indexCol=None,
+                          specs=specs, specSheetsPrfx=specSheetsPrfx)
+
+    def fromOpenDoc(self, fileName, sheetName=None, specs=True, specSheetsPrfx='sp-'):
+
+        """Load (overwrite) data and optionally specs from an Open Document worksheet (ODS format),
+        assuming ctor params match with ODF sheet column names and list,
+        which can well be ensured by using the same ctor params as used for saving !
+        Notes: Needs odfpy module and pandas.version >= 0.25.1
+        """
+        
+        super().fromOpenDoc(fileName, sheetName=sheetName, header=0, skipRows=None, indexCol=None,
+                            specs=specs, specSheetsPrfx=specSheetsPrfx)
+
+    def fromFile(self, fileName, sheetName=None, specs=True, specSheetsPrfx='sp-'):
+
+        """Load (overwrite) data data and optionally specs from a given file,
+        (see ResultsSet.fromFile for supported formats, auto-detected from file extensions)
+        assuming ctor params match with the file contents,
+        which can well be ensured by using the same ctor params as used for saving !
+        """
+
+        super().fromFile(fileName, sheetName=sheetName, header=0, skipRows=None, indexCol=None,
+                         specs=specs, specSheetsPrfx=specSheetsPrfx)
+
+    def optimisationTargetColumns(self):
+    
+        """The names of the columns holding the real optimisation results
+        """
+    
+        return [col for col in self.optimisationClass.SolutionDimensionNames if col in self.columns]
+
+
+class DSParamsOptimiser(object):
+
+    """Run a bunch of DS analyses on samples extracted from a mono-category sightings data set,
+       according to a user-friendly set of analysis specs (first set of fixed params),
+       in order to determine best values for a second set of analysis params.
+       Abstract class.
+    """
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], 
+                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
+                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum', 
+                 distanceUnit='Meter', areaUnit='Hectare', 
+                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], 
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 workDir='.',
+                 defExpr2Optimise='chi2', defMinimiseExpr=False,
+                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),
+                 dDefOptimCoreParams=dict()):
+                       
+        """Ctor (don't use directly, abstract class)
+        
+        Parameters for input data to analyse:
+        :param pd.DataFrame dfMonoCatObs: mono-category data from FieldDataSet.monoCategorise() or individualise()
+        :param pd.DataFrame dfTransects: Transects infos with columns : transectPlaceCols (n), passIdCol (1),
+            effortCol (1) ; if None, auto generated from input sightings
+        :param effortConstVal: if dfTransects is None and effortCol not in source table, use this constant value
+        :param transectPlaceCols:  See above dfTransects description
+        :param passIdCol:  See above dfTransects description
+        :param effortCol: See above dfTransects description
+        :param sampleSelCols: sample identification = selection columns
+        :param sampleDecCols: Decimal columns among sighting columns
+        :param sampleDistCol: name of distance data column in run specs table
+        :param anlysSpecCustCols: Special columns from analysis specs to simply pass through in results
+        :param abbrevCol: Name of column to generate for abbreviating optimisation params, not sure really useful ...
+        :param abbrevBuilder: Function of explicit analysis params (as a Series) to generate abbreviated name
+        :param anlysIndCol: Name of column to generate for identifying analyses, unless already there in input data.
+        :param sampleIndCol: Name of column to generate for identifying samples, unless already there in input data.
+        
+        Parameters for DS engine:
+        :param distanceUnit: See DSEngine
+        :param areaUnit: See DSEngine
+
+        Parameters for optimisation output data:
+        :param resultsHeadCols: dict of list of column names (from dfMonoCatObs) to use in order
+            to build results header columns (left to results cols) ; 'sample' columns are sample selection columns.
+        
+        Parameters for evaluating analysis value to optimise
+            (when not fully specified in each optimisation parameters):
+        :param string defExpr2Optimise: Math. expression (python syntax) to optimise,
+               using analyses results var. names inside (see derived classes for details)
+        :param defMinimiseExpr: True for minimisation of expr2Optimise, false for maximisation
+
+        Parameters for optimisation submissions
+            (when not fully specified in each optimisation parameters):
+        :param defSubmitTimes: Number of times to auto-run each optimisation (> 0 ; default 1)
+        :param defSubmitOnlyBest: Number of best repetition results to keep
+                                  (> 0 ; default None = all repetitions)
+        :param dDefSubmitOtherParams: Other submission parameters
+
+        Other parameters:
+        :param dSurveyArea: dict of info about survey area (mainly name and surface) for DS analyses
+        :param workDir: target folder for intermediate computations and results files
+        :param dDefOptimCoreParams: Optimisation core specific params
+        """
+
+        # Save data.
+        self.dfMonoCatObs = dfMonoCatObs
+        self.sampleDistCol = sampleDistCol
+
+        self.resultsHeadCols = resultsHeadCols
+        self.abbrevCol = abbrevCol
+        self.abbrevBuilder = abbrevBuilder
+        self.anlysIndCol = anlysIndCol
+        self.sampleSelCols = sampleSelCols
+        self.sampleIndCol = sampleIndCol
+        self.anlysSpecCustCols = anlysSpecCustCols
+            
+        self.distanceUnit = distanceUnit
+        self.areaUnit = areaUnit
+
+        self.workDir = workDir
+        
+        # Default values for optimisation parameters.
+        # a. Expression to optimise
+        self.defExpr2Optimise = defExpr2Optimise
+        self.defMinimiseExpr = defMinimiseExpr
+        
+        # b. Optimisation core parameters.
+        self.dDefOptimCoreParams = dDefOptimCoreParams
+
+        # c. Optimisations submission (=run) parameters
+        self.defSubmitTimes = defSubmitTimes
+        self.defSubmitOnlyBest = defSubmitOnlyBest
+        self.dDefSubmitOtherParams = dDefSubmitOtherParams
+
+        # Mono-categorised data (all samples)
+        self._mcDataSet = \
+            MonoCategoryDataSet(dfMonoCatObs, dfTransects=dfTransects, effortConstVal=effortConstVal,
+                                dSurveyArea=dSurveyArea, transectPlaceCols=transectPlaceCols,
+                                passIdCol=passIdCol, effortCol=effortCol, sampleDecFields=sampleDecCols)
+                                
+        # Analysis engine and executor.
+        self._executor = None
+        self._engine = None
+
+        # Results.
+        self.results = None
+
+        # Computation specifications, for traceability only.
+        # For gathering copies of computations default parameter values, and stuff like that.
+        self.specs = dict()
+        self.updateSpecs(**{name: getattr(self, name) for name in ['distanceUnit', 'areaUnit']})
+        self.updateSpecs(**{name: getattr(self, name)
+                            for name in ['defExpr2Optimise', 'defMinimiseExpr', 'dDefOptimCoreParams',
+                                         'defSubmitTimes', 'defSubmitOnlyBest', 'dDefSubmitOtherParams']})
+
+    def updateSpecs(self, reset=False, overwrite=False, **specs):
+
+        if reset:
+            self.specs.clear()
+
+        if not overwrite:
+            assert all(name not in self.specs for name in specs), \
+                   "Unless specified, won't overwrite already present specs {}" \
+                   .format(', '.join(name for name in specs if name in self.specs))
+
+        self.specs.update(specs)
+
+    def flatSpecs(self):
+
+        # Flatten "in-line" 2nd level dicts if any (with 1st level name prefixing).
+        dFlatSpecs = dict()
+        for name, value in self.specs.items():
+            if isinstance(value, dict):
+                for n, v in value.items():
+                    dFlatSpecs[name + n[0].upper() + n[1:]] = v
+            else:
+                dFlatSpecs[name] = value
+
+        # Done.
+        return dFlatSpecs
+
+    # Optimiser internal parameter spec names, for which a match should be found (when one is needed)
+    # with user explicit optimisation specs used in run() calls.
+    IntSpecExpr2Optimise = 'Expr2Optimise'
+    IntSpecOptimisationCore = 'OptimisationCore'
+    IntSpecSubmitParams = 'SubmitParams'
+
+    # Possible regexps (values) for auto-detection of optimiser _internal_ parameter spec names (keys)
+    # from explicit _user_ spec columns
+    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
+    #  and case is ignored during searching).
+    Int2UserSpecREs = \
+        {IntSpecExpr2Optimise:    ['opt[a-z]*[\.\-_ ]*exp', 'exp[a-z2]*[\.\-_ ]*opt',
+                                   'opt[a-z]*[\.\-_ ]*cri', 'cri[a-z]*[\.\-_ ]*opt'],
+         IntSpecOptimisationCore: ['opt[a-z]*[\.\-_ ]*core', 'mot[a-z]*[\.\-_ ]*opt',
+                                   'noy[a-z]*[\.\-_ ]*opt'],
+         IntSpecSubmitParams:     ['sub[a-z]*[\.\-_ ]*par', 'par[a-z]*[\.\-_ ]*sou',
+                                   'run[a-z]*[\.\-_ ]*par', 'par[a-z]*[\.\-_ ]*ex',
+                                   'mul[a-z]*[\.\-_ ]*opt', 'opt[a-z]*[\.\-_ ]*mul']}
+
+    # Types for user specs parsing (see usage below)
+    class Auto(object):
+
+        def __repr__(self):
+            return 'Auto()'
+
+        def __eq__(self, other):
+            return isinstance(other, self.__class__)
+    
+    @classmethod
+    def _parseUserSpec(cls, spec, globals=dict(), locals=dict(),
+                       oneStrArg=False, nullOrEmpty=Error, errIfNotA=[]):
+                                  
+        """Parse parameter(s) user spec with python-like simple expression syntax from given rules 
+        
+        :param spec: None or np.nan or string spec to parse
+        :param globals: dict of globals for case-insensitive rules (we use eval function for parsing !)
+        :param locals: dict of locals for case-insensitive rules (we use eval function for parsing !)
+        :param oneStrArg: assume function call syntax with 1 single string argument
+                (ex: input "f(x,y)" is transformed to "f('x,y')" before calling eval)
+        :param nullOrEmpty: return value for null of empty spec ; not checked against errIfNotA
+            (default: Error => an instance of Error with error description inside)
+        :param errIfNotA: list of authorised output types ; empty => any type
+        
+        :return: a 2-value tuple : (None or an Error instance, parsed value or None in case of error)
+        """
+    
+        # Empty or Null cases
+        if pd.isnull(spec) or (isinstance(spec, str) and not spec.strip()):
+        
+            if nullOrEmpty is Error:
+                parsedValue = None
+                parseError = Error('Should be specified ; did you mean "auto" ?')
+            else:
+                parsedValue = nullOrEmpty
+                parseError = None
+        
+            return parseError, parsedValue
+
+        # Other cases.
+        spec = str(spec).lower()  # int and float cases, + lower case
+        if oneStrArg:
+            if "('" not in spec:
+                spec = spec.replace('(', "('")
+            if "')" not in spec:
+                spec = spec.replace(')', "')")
+        
+        try:
+            parsedValue = eval(spec, globals, locals)
+            if errIfNotA and not isinstance(parsedValue, tuple(errIfNotA)):
+                error = 'Not a {}'.format(', '.join(t.__name__ for t in errIfNotA))
+                parseError = Error(head=spec, error=error)
+                parsedValue = None
+            else:
+                parseError = None
+        except Exception as exc:
+            parsedValue = None
+            parseError = Error(head=spec, error=str(exc))
+        
+        return parseError, parsedValue
+    
+    # Types for parsing user spec
+    DistInterval = ntuple('DistInterval', ['dmin', 'dmax'], defaults=[0, -1])  # Distance interval
+    AbsInterval = ntuple('AbsInterval', ['min', 'max'], defaults=[0, -1])  # Interval for actual values
+    MultInterval = ntuple('MultInterval', ['kmin', 'kmax'])  # Interval for multipliers
+    OutliersMethod = ntuple('OutliersMethod', ['method', 'percent'])
+
+    @classmethod
+    def _parseDistTruncationUserSpec(cls, spec, errIfNotA=[]):
+    
+        """Parse user spec for one analysis optimised parameter
+        
+        Parameters:
+        :param spec: None or np.nan or string spec to parse
+        :param errIfNotA: list of authorised output types ; empty => any type
+        
+        :return: a 2-value tuple : (None or an Error instance, parsed value or None in case of error)
+                 Parsed value may result None or an instance of AbsInterval, MultInterval, OutliersMethod, Auto
+        """
+
+        # Defs for optimisation param. spec. mini-language
+        auto = cls.Auto()
+
+        def dist(dmin, dmax):
+            return cls.DistInterval(dmin, dmax)
+
+        def quant(pct):
+            return cls.OutliersMethod('quant', pct)
+
+        def tucquant(pct):
+            return cls.OutliersMethod('tucquant', pct)
+
+        def mult(kmin, kmax):
+            return cls.MultInterval(kmin, kmax)
+
+        def abs(min, max):
+            return cls.AbsInterval(min, max)
+            
+        # Parse spec.
+        return cls._parseUserSpec(spec, nullOrEmpty=None, errIfNotA=errIfNotA,
+                                  globals=dict(Auto=cls.Auto, DistInterval=cls.DistInterval,
+                                               AbsInterval=cls.AbsInterval, MultInterval=cls.MultInterval,
+                                               OutliersMethod=cls.OutliersMethod),
+                                  locals=dict(auto=auto, dist=dist, quant=quant, tucquant=tucquant,
+                                              mult=mult, abs=abs))
+
+    @classmethod
+    def _parseOptimCoreUserSpecs(cls, spec, globals=dict(), locals=dict(),
+                                 nullOrEmpty=Error, errIfNotA=[]):
+    
+        """Parse user spec for optimisation core parameters
+        
+        Parameters:
+        :param spec: the spec to parse
+        :param globals: dict of globals for rules (we use eval function for parsing !)
+        :param locals: dict of locals for rules (we use eval function for parsing !)
+        :param nullOrEmpty: return value for null of empty spec ; not checked against errIfNotA
+            (default: Error => an instance of Error with error description inside)
+        :param errIfNotA: list of authorised output types ; empty => any type
+        
+        :return: a 2-value tuple : (None or an Error instance, parsed value or None in case of error)
+                 Parsed value may result None or an instance of AbsInterval, MultInterval, OutliersMethod, Auto
+        """
+
+        if isinstance(spec, str) and spec.strip():
+        
+            # We don't care about case
+            spec = spec.lower()
+            
+            # A single string for the engine name is enough => add () at the end if so.
+            if '(' not in spec:
+                spec = spec + '()'
+                
+            # String parameters don't need quoting in spec, but python needs it : add it if needed
+            spec = re.sub('([a-z_]+) *= *([^=0-9,; ][^=,;\) ]*)', r"\1='\2'", spec)
+    
+        # Parse pythonified spec.
+        return cls._parseUserSpec(spec, nullOrEmpty=nullOrEmpty, errIfNotA=errIfNotA, 
+                                  oneStrArg=False, globals=globals, locals=locals)
+
+    # Optimisation object ctor parameter names (MUST match exactly: check in optimisation submodule !).
+    ParmExpr2Optimise = 'expr2Optimise'
+    ParmMinimiseExpr = 'minimiseExpr'
+
+    def getAnalysisOptimExprParams(self, sAnIntSpec):
+                                 
+        """Retrieve optimisation expression parameters, from user specs and default values.
+        
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+                           syntax: IntSpecExpr2Optimise => <min|max>(math. expr)
+
+        :return: None or an Error instance, dict(expr2Optimise=..., minimiseExpr=...) or None
+        
+        Ex: max(aic), min(1/aic/ks)"""
+
+        def _buildParsedValue(expr2Optimise, minimiseExpr):
+            return {self.ParmExpr2Optimise: expr2Optimise, self.ParmMinimiseExpr: minimiseExpr}
+
+        # Parse expression to optimise in sAnIntSpec if present.
+        if self.IntSpecExpr2Optimise in sAnIntSpec:
+        
+            # Retrieve
+            userOptExpr = sAnIntSpec[self.IntSpecExpr2Optimise]
+                
+            # Parse
+            def min(expr):
+                return _buildParsedValue(expr, True)
+
+            def max(expr):
+                return _buildParsedValue(expr, False)
+
+            parseError, parsedValue = \
+                 self._parseUserSpec(userOptExpr, globals=None, locals=dict(min=min, max=max),
+                                     oneStrArg=True, errIfNotA=[dict],
+                                     nullOrEmpty=_buildParsedValue(self.defExpr2Optimise, self.defMinimiseExpr))
+
+        # No way: fall back to default values.
+        else:
+            
+            parseError, parsedValue = \
+                None, _buildParsedValue(self.defExpr2Optimise, self.defMinimiseExpr)
+                 
+        # Done.
+        return parseError, parsedValue
+    
+    def getOptimisationCoreParams(self, sAnIntSpec):
+
+        """Retrieve optimisation core parameters, from user specs and default values.
+        
+        Ex: zoopt(mxi=50, tv=1, a=racos, mxr=2)
+
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+                           syntax: IntSpecExpr2Optimise => <opt. core name>(**{k:v})
+                           
+        :return: None or an Error instance, dict(core=..., **{key:value}) or None
+        """
+        
+        raise NotImplementedError('Abstract class: implement in a derived class')
+    
+    # Optimisation object ctor parameter names (MUST match exactly: check in optimisation submodule !).
+    ParmSubmTimes = 'times'
+    ParmSubmOnlyBest = 'onlyBest'
+
+    def getOptimisationSubmitParams(self, sAnIntSpec):
+    
+        """Retrieve optimisation submission parameters from user specs and default values.
+        
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+                           syntax: IntSpecSubmitParams => <times>([n=]<num>[, [b=]<num>])
+                           
+        :return: None or an Error instance, dict(=..., **{k:v}) or None
+        
+        Ex: dict(times=, onlyBest=, ...)"""
+
+        def _buildParsedValue(times, onlyBest):
+            return {self.ParmSubmTimes: times, self.ParmSubmOnlyBest: onlyBest}
+
+        # Parse expression to optimise in sAnIntSpec if present.
+        if self.IntSpecSubmitParams in sAnIntSpec:
+        
+            # Retrieve
+            userOptExpr = sAnIntSpec[self.IntSpecSubmitParams]
+                
+            # Parse
+            def times(n=1, b=None):
+                assert n > 0, 'Run times must be > 0'
+                assert b is None or b > 0, 'Number of best kept values must be > 0'
+                return _buildParsedValue(n, b)
+
+            parseError, parsedValue = \
+                 self._parseUserSpec(userOptExpr, globals=None, locals=dict(times=times),
+                                     nullOrEmpty=_buildParsedValue(self.defSubmitTimes, self.defSubmitOnlyBest),
+                                     errIfNotA=[dict])
+
+        # No way: fall back to default values.
+        else:
+            
+            parseError, parsedValue = \
+                None, _buildParsedValue(self.defSubmitTimes, self.defSubmitOnlyBest)
+                 
+        # Done.
+        return parseError, parsedValue
+        
+    def setupOptimisation(self, sampleDataSet, name=None, customData=None, setupError=None, **otherParams):
+
+        """Factory method for DSOptimisation derived classes
+        
+        Parameters:
+        :param sampleDataSet: SampleDataSet to analyse
+        :param name: of the optimisation (for clearer logging only, auto-generated and auto-unique anyway)
+        :param customData: pd.Series of custom data for heading optimisation results columns
+        :param setupError: if an error occurred during optimisation submit params user specs parsing,
+                 a string is here for explaining it, and to prevent any submit call of course !
+                 This is done for keeping trace of not run optimisations in results table (1 empty/null row)
+        :param otherParams: some room for derived classes
+        """
+
+        raise NotImplementedError('DSParamsOptimiser: Abstract class, implement setupOptimisation in derived one')
+
+    def explicitParamSpecs(self, implParamSpecs=None, dfExplParamSpecs=None, dropDupes=True, check=False):
+    
+        """Explicitate analysis and optimisation param. specs if not already done, and complete columns if needed ;
+        also automatically extract (regexps) columns which are really analysis parameters,
+        with their analyser-internal name, and also their "user" name.
+        
+        Can moreover check params specs for usability, if check is True :
+        * use it before calling analyser.run(implParamSpecs=..., dfExplParamSpecs=..., ...)
+          to check that everything's OK,
+        * or be sure that run() will fail at startup (because it itself will do it).
+        
+        Parameters:
+        :param implParamSpecs: Implicit analysis param specs, suitable for explicitation
+           through explicitVariantSpecs()
+        :param dfExplParamSpecs: Explicit analysis param specs, as a DataFrame
+           (generated through explicitVariantSpecs, as an example)
+        :param dropDupes: if True, drop duplicates (keep first)
+        :param check: if True, checks params for usability by run(),
+           and return a bool verdict and a list of strings explaining the negative (False) verdict
+
+        :return: a 3 or 5-item tuple :
+           * explicit specs as a DataFrame (input dfExplParamSpecs not modified: a new updated one is returned),
+           * list of analysis and optimisationparam. columns internal names,
+           * list of analysis and optimisationparam. columns user names,
+           if check, 2 more items in the return tuple :
+           * check verdict : True if everything went well, False otherwise,
+             * some columns from paramSpecCols could not be found in dfExplParamSpecs columns,
+             * some user columns could not be matched with some of the expected internal parameter names,
+             * some rows are not suitable for DS analysis (empty sample identification columns, ...).
+           * check failure reasons : list of strings explaining things that went bad.
+        """
+        
+        # Explicitate and complete
+        tplRslt = DSAnalyser._explicitParamSpecs(implParamSpecs, dfExplParamSpecs, self.Int2UserSpecREs,
+                                                 sampleSelCols=self.sampleSelCols, abbrevCol=self.abbrevCol,
+                                                 abbrevBuilder=self.abbrevBuilder, anlysIndCol=self.anlysIndCol,
+                                                 sampleIndCol=self.sampleIndCol,
+                                                 anlysSpecCustCols=self.anlysSpecCustCols, dropDupes=dropDupes)
+        
+        # Check if requested
+        if check:
+        
+            verdict = True
+            reasons = []
+     
+            dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = tplRslt
+            
+            # Check that an internal column name was found for every user spec column.
+            if len(unmUserParamSpecCols):
+                verdict = False
+                reasons.append('Failed to match some user spec. names with internal ones: {}'
+                               .format(', '.join(unmUserParamSpecCols)))
+
+            # Check that all rows are suitable for DS analysis (non-empty sample
+            # identification columns, ...).
+            if dfExplParamSpecs[self.sampleSelCols].isnull().all(axis='columns').any():
+                verdict = False
+                reasons.append('Some rows have some null sample selection columns')
+
+            # Check done.
+            tplRslt += verdict, reasons
+
+        return tplRslt
+
+    def shutdown(self):
+    
+        """Shutdown engine and executor (only usefull if run() raises an exception and so fails to do it),
+        but keep the remainder of the object state as is.
+        """
+    
+        if self._engine:
+            self._engine.shutdown(executor=True)
+            self._engine = None
+        if self._executor:
+            self._executor.shutdown()
+            self._executor = None
+            
+#    def __del__(self):
+#    
+#        self.shutdown()
+
+
+class MCDSTruncationOptimiser(DSParamsOptimiser):
+
+    """Abstract class ; Run a bunch of MCDS truncation optimisations"""
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
+                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
+                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'],
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 workDir='.', runMethod='subprocess.run', runTimeOut=300,
+                 logData=False, logProgressEvery=5, backupEvery=50, autoClean=True,
+                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
+                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
+                 defExpr2Optimise='chi2', defMinimiseExpr=False,
+                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,
+                 defFitDistCutsFctr=dict(min=2/3, max=3/2), defDiscrDistCutsFctr=dict(min=1/3, max=1),
+                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),
+                 dDefOptimCoreParams=dict()):
+
+        """Ctor (don't use directly, abstract class)
+        
+        Parameters for MCDS engine:
+        :param surveyType: See MCDSEngine
+        :param distanceType: See MCDSEngine
+        :param clustering: See MCDSEngine
+
+        Parameters for auto-computing target intervals (when not fully specified in each optimisation parameters):
+        :param defOutliersMethod: Outliers estimation method when min/maxDist=auto
+               or auto(pct) but not auto(min, max): 
+                * 'quant' : Pure [P, 100-P] % quantiles
+                * 'tucquant' : Mixed P% quantiles & Tuckey method
+        :param defOutliersQuantCutPct: Outliers cut % value (= P in outliersMethod description above) 
+        :param defFitDistCutsFctr: Default factor multiplied to sqrt(nb of sightings)
+               to get min/max FitDistCuts when fitDistCutsFctr is auto with no parameters (min and max)
+        :param defDiscrDistCutsFctr: Factor multiplied to sqrt(nb of sightings)
+               to get min/max DiscrDistCuts when discrDistCutsFctr is auto with no parameters (min and max)
+
+        :param runMethod: for calling MCDS engine executable : 'os.system' or 'subprocess.run'
+        :param runTimeOut: engine call time limit (s) ; None => no limit ;
+                           WARNING: Not implemented (no way) for 'os.system' run method (no solution found)
+
+        Other parameters: See base class.
+        """
+
+        super().__init__(dfMonoCatObs, dfTransects=dfTransects,
+                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
+                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
+                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
+                         sampleDistCol=sampleDistCol, anlysSpecCustCols=anlysSpecCustCols,
+                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
+                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         resultsHeadCols=resultsHeadCols, workDir=workDir,
+                         defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,
+                         defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,
+                         dDefSubmitOtherParams=dDefSubmitOtherParams, dDefOptimCoreParams=dDefOptimCoreParams)
+
+        assert runTimeOut is None or runMethod != 'os.system', \
+               f"Can't care about {runTimeOut}s execution time limit with os.system run method (not implemented)"
+        assert defOutliersMethod in ['quant', 'tucquant'], f'Unsupported default outliers method {defOutliersMethod}'
+
+        self.surveyType = surveyType
+        self.distanceType = distanceType
+        self.clustering = clustering
+        
+        self.runMethod = runMethod
+        self.runTimeOut = runTimeOut
+        self.logData = logData
+        self.logProgressEvery = logProgressEvery
+        self.backupEvery = backupEvery
+        self.autoClean = autoClean
+        
+        self.defEstimKeyFn = defEstimKeyFn
+        self.defEstimAdjustFn = defEstimAdjustFn
+        self.defEstimCriterion = defEstimCriterion
+        self.defCVInterval = defCVInterval
+        self.defOutliersMethod = defOutliersMethod
+        self.defOutliersQuantCutPct = defOutliersQuantCutPct
+        self.defFitDistCutsFctr = \
+            defFitDistCutsFctr if defFitDistCutsFctr is None else Interval(defFitDistCutsFctr)
+        self.defDiscrDistCutsFctr = \
+            defDiscrDistCutsFctr if defDiscrDistCutsFctr is None else Interval(defDiscrDistCutsFctr)
+                         
+        self.updateSpecs(**{name: getattr(self, name)
+                            for name in ['runMethod', 'runTimeOut', 'surveyType', 'distanceType', 'clustering',
+                                         'defEstimKeyFn', 'defEstimAdjustFn', 'defEstimCriterion', 'defCVInterval']})
+        self.updateSpecs(**{name: getattr(self, name)
+                            for name in ['defOutliersMethod', 'defOutliersQuantCutPct',
+                                         'defFitDistCutsFctr', 'defDiscrDistCutsFctr']})
+
+    # Optimiser internal parameter spec names, for which a match should be found (when one is needed)
+    # with user explicit optimisation specs used in run() calls.
+    IntSpecEstimKeyFn = MCDSAnalyser.IntSpecEstimKeyFn
+    IntSpecEstimAdjustFn = MCDSAnalyser.IntSpecEstimAdjustFn
+    IntSpecEstimCriterion = MCDSAnalyser.IntSpecEstimCriterion
+    IntSpecCVInterval = MCDSAnalyser.IntSpecCVInterval
+    IntSpecMinDist = MCDSAnalyser.IntSpecMinDist  # Left truncation distance
+    IntSpecMaxDist = MCDSAnalyser.IntSpecMaxDist  # Right truncation distance
+    IntSpecFitDistCuts = MCDSAnalyser.IntSpecFitDistCuts
+    IntSpecDiscrDistCuts = MCDSAnalyser.IntSpecDiscrDistCuts
+    IntSpecOutliersMethod = 'OutliersMethod'
+
+    # Possible regexps (values) for auto-detection of optimiser _internal_ parameter spec names (keys)
+    # from explicit _user_ spec columns
+    # (regexps are re.search'ed : any match _anywhere_inside_ the column name is OK;
+    #  and case is ignored during searching).
+    Int2UserSpecREs = \
+      dict(list(DSParamsOptimiser.Int2UserSpecREs.items())
+           + list(MCDSAnalyser.Int2UserSpecREs.items())
+           + [(IntSpecOutliersMethod, ['outl[a-z]*[\.\-_ ]*', 'me[a-z]*[\.\-_ ]*outl'])])
+
+    # Names of internal parameters which can be used as settings for optimising truncations
+    # and not only as const=fixed=pre-determined analysis parameters.
+    IntOptimParamSpecNames = \
+        [DSParamsOptimiser.IntSpecExpr2Optimise, DSParamsOptimiser.IntSpecOptimisationCore,
+         DSParamsOptimiser.IntSpecSubmitParams,
+         IntSpecMinDist, IntSpecMaxDist, IntSpecFitDistCuts, IntSpecDiscrDistCuts,
+         IntSpecOutliersMethod]
+         
+    @classmethod
+    def optimisationParamSpecUserNames(cls, userParamSpecCols, intParamSpecCols):
+    
+        """Extract user names of params spec. columns that may contain truncation optimisation parameters
+
+        Parameters:
+        :param userParamSpecCols: all user params spec column names to explore,
+        :param intParamSpecCols: the matching (1 by 1, same order) internal user params spec column names.
+        """
+    
+        # Internal column names to consider.
+        optimIntCols = [intCol for intCol in cls.IntOptimParamSpecNames if intCol in intParamSpecCols]
+        
+        # And the corresponding user names.
+        return [userCol for intCol, userCol in zip(intParamSpecCols, userParamSpecCols) if intCol in optimIntCols]
+
+    # Optimisation object ctor parameter names (MUST match exactly: check in optimisation submodule !).
+    ParmEstimKeyFn = 'estimKeyFn'
+    ParmEstimAdjustFn = 'estimAdjustFn'
+    ParmEstimCriterion = 'estimCriterion'
+    ParmCVInterval = 'cvInterval'
+    
+    def getAnalysisFixedParams(self, sAnIntSpec):
+    
+        """Retrieve analysis fixed parameters of an optimisation, from user specs and default values
+        
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+        
+        :return: a 2-value tuple (None or an Error instance,
+                                  dict(estimKeyFn=, estimAdjustFn=, estimCriterion=, cvInterval=) or None)
+        """
+        
+        estimKeyFn = sAnIntSpec.get(self.IntSpecEstimKeyFn, self.defEstimKeyFn)
+        if pd.isnull(estimKeyFn):
+            estimKeyFn = self.defEstimKeyFn
+        
+        estimAdjFn = sAnIntSpec.get(self.IntSpecEstimAdjustFn, self.defEstimAdjustFn)
+        if pd.isnull(estimAdjFn):
+            estimAdjFn = self.defEstimAdjustFn
+        
+        estimCrit = sAnIntSpec.get(self.IntSpecEstimCriterion, self.defEstimCriterion)
+        if pd.isnull(estimCrit):
+            estimCrit = self.defEstimCriterion
+        
+        cvInterv = sAnIntSpec.get(self.IntSpecCVInterval, self.defCVInterval)
+        if pd.isnull(cvInterv):
+            cvInterv = self.defCVInterval
+        
+        return None, {self.ParmEstimKeyFn: estimKeyFn, self.ParmEstimAdjustFn: estimAdjFn,
+                      self.ParmEstimCriterion: estimCrit, self.ParmCVInterval: cvInterv}
+
+    # From / to: Optimisation object ctor parameter names = Solution dimension names
+    # To / From: Optimiser internal param. spec names.
+    SolDim2IntSpecOptimTargetParamNames = \
+        dict(zip(MCDSTruncationOptimisation.SolutionDimensionNames,
+                 [IntSpecMinDist, IntSpecMaxDist, IntSpecFitDistCuts, IntSpecDiscrDistCuts]))
+    IntSpec2SolDimOptimTargetParamNames = {v:k for k,v in SolDim2IntSpecOptimTargetParamNames.items()}
+
+    def getAnalysisOptimedParams(self, sAnIntSpec, sSampleDists):
+    
+        """Compute optimisation intervals of an optimisation, from user specs and default parameters
+
+        Some checks for final values are done, may be resulting in an Error.
+        
+        Parameters:
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+                           syntax: sequence of <key>=<value> separated by ','
+        :param sSampleDists: sample sightings recorded distances
+        
+        :return: a 2-value tuple (None or an Error instance in case any parsing / check failed,
+                                  dict(minDist=, maxDist=, fitDistCuts=, discrDistCuts=) or None)
+        """
+        
+        # Parse found specs from strings (or so) to real objects
+        errMinDist, minDistSpec = \
+            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecMinDist, None),
+                                              errIfNotA=[int, float, self.Auto,
+                                                         self.DistInterval, self.OutliersMethod])
+        errMaxDist, maxDistSpec =  \
+            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecMaxDist, None),
+                                              errIfNotA=[int, float, self.Auto,
+                                                         self.DistInterval, self.OutliersMethod])
+        errFitDistCuts, fitDistCutsSpec = \
+            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecFitDistCuts, None),
+                                              errIfNotA=[int, float, self.Auto, 
+                                                         self.MultInterval, self.AbsInterval])
+        errDiscrDistCuts, discrDistCutsSpec = \
+            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecDiscrDistCuts, None),
+                                              errIfNotA=[int, float, self.Auto,
+                                                         self.MultInterval, self.AbsInterval])
+        errOutliersMethod, outliersMethodSpec = \
+            self._parseDistTruncationUserSpec(sAnIntSpec.get(self.IntSpecOutliersMethod, None),
+                                              errIfNotA=[self.Auto, self.OutliersMethod])
+
+        logger.info2('OptimedParams specs:' + str(dict(minDist=minDistSpec, maxDist=maxDistSpec,
+                                                       fitDistCuts=fitDistCutsSpec, discrDistCuts=discrDistCutsSpec)))
+
+        # Stop here if any parsing error.
+        finalErr = Error()
+        for err in [errMinDist, errMaxDist, errFitDistCuts, errDiscrDistCuts, errOutliersMethod]:
+            if err:
+                finalErr.append(err)
+        if finalErr:
+            return finalErr, None
+
+        # Compute or translate parameter values from objects.
+        
+        # A. outliers method: method ('quant' or 'tucquant') and cut % (% of outliers on _each_ side)
+        # - None or auto : use defOutliersMethod et defOutliersQuantCutPct,
+        # - <meth>(<cut pct>) : use <meth> method, and <cut pct> cut %
+        if outliersMethodSpec is None or isinstance(outliersMethodSpec, self.Auto):
+            outliersMethod = self.defOutliersMethod
+            outliersQuantCutPct = self.defOutliersQuantCutPct
+        elif isinstance(outliersMethodSpec, self.OutliersMethod):
+            outliersMethod = outliersMethodSpec.method
+            outliersQuantCutPct = outliersMethodSpec.percent
+        else:
+            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
+                            'Should not fall there (outliersMethod specs)')
+        assert outliersMethod in ['quant', 'tucquant'], f'Unsupported outliers method {outliersMethod}'
+
+        # B. minDist, maxDist :
+        # - None,
+        # - auto : mode full auto via colonne OutliersMethod, 
+        #                             ou alors defOutliersMethod et defOutliersQuantCutPct,
+        # - <meth>(5) : mode auto tq OutliersMethod (<meth> quant(5) ou tucquant) et outliersQuantCutPct(ici 5%) fixés
+        # - dist(20, 250) : mode fixé, dist min (20) et max (250) du domaine de variations fournies en dur
+
+        sDist = sSampleDists.dropna()
+        sqrNSights = np.sqrt(len(sDist))
+
+        # minDist specs
+        if minDistSpec is None or isinstance(minDistSpec, (int, float)):
+            minDist = minDistSpec
+        elif isinstance(minDistSpec, self.DistInterval):
+            minDist = Interval(min=minDistSpec.dmin, max=minDistSpec.dmax)
+        elif isinstance(minDistSpec, self.Auto):
+            maxMinDist = np.percentile(a=sDist, q=outliersQuantCutPct)
+            minDist = Interval(min=sDist.min(), max=maxMinDist)
+        elif isinstance(minDistSpec, self.OutliersMethod):
+            maxMinDist = np.percentile(a=sDist, q=minDistSpec.percent)
+            minDist = Interval(min=sDist.min(), max=maxMinDist)
+        else:
+            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
+                            'Should not fall there (minDist specs)')
+        
+        if isinstance(minDist, Interval) and minDist.min == minDist.max:
+            minDist = minDist.min
+
+        # maxDist specs
+        if maxDistSpec is None or isinstance(maxDistSpec, (int, float)):
+            maxDist = maxDistSpec
+        elif isinstance(maxDistSpec, self.DistInterval):
+            maxDist = Interval(min=maxDistSpec.dmin, max=maxDistSpec.dmax)
+        elif isinstance(maxDistSpec, (self.Auto, self.OutliersMethod)):
+            if isinstance(maxDistSpec, self.Auto):
+                d25, d75, d95, dPct = np.percentile(a=sDist, q=[25, 75, 95, 100-outliersQuantCutPct])
+            else:  # self.OutliersMethod
+                d25, d75, d95, dPct = np.percentile(a=sDist, q=[25, 75, 95, 100-maxDistSpec.percent])
+            if outliersMethod == 'quant':
+                minMaxDist = dPct
+            elif outliersMethod == 'tucquant':
+                minMaxDist = min(max(d95, d75 + 1.5*(d75 - d25)), dPct)
+            maxDist = Interval(min=minMaxDist, max=sDist.max())
+        else:
+            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
+                            'Should not fall there (maxDist specs)')
+
+        if isinstance(maxDist, Interval) and maxDist.min == maxDist.max:
+            minDist = maxDist.min
+
+        # C. fitDistCuts, discrDistCuts :
+        # - None,
+        # - auto : full automated mode through column outliersMethod,
+        #          or else defOutliersMethod and defOutliersQuantCutPct,
+        # - mult(1/3, 3/2) : "fixed" mode = hard-coded mult. factors min and max for sqrt(nb of sightings)
+        # - abs(5, 10) : fixed" mode = hard-coded mult. factors min and max for sqrt(nb of sightings)
+
+        # 1. fitDistCuts specs
+        if fitDistCutsSpec is None or isinstance(fitDistCutsSpec, (int, float)):
+            fitDistCuts = fitDistCutsSpec
+        elif isinstance(fitDistCutsSpec, self.AbsInterval):
+            fitDistCuts = Interval(min=max(2, fitDistCutsSpec.min), max=fitDistCutsSpec.max)
+        elif isinstance(fitDistCutsSpec, self.Auto):
+            fitDistCuts = Interval(min=max(2, int(round(self.defFitDistCutsFctr.min*sqrNSights))),
+                                   max=int(round(self.defFitDistCutsFctr.max*sqrNSights)))
+        elif isinstance(fitDistCutsSpec, self.MultInterval):
+            fitDistCuts = Interval(min=max(2, int(round(fitDistCutsSpec.kmin*sqrNSights))),
+                                   max=int(round(fitDistCutsSpec.kmax*sqrNSights)))
+        else:
+            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
+                            'Should not fall there (fitDistCuts specs)')
+            
+        if isinstance(fitDistCuts, Interval) and fitDistCuts.min == fitDistCuts.max:
+            fitDistCuts = fitDistCuts.min
+
+        # 2. discrDistCuts specs
+        if discrDistCutsSpec is None or isinstance(discrDistCutsSpec, (int, float)):
+            discrDistCuts = discrDistCutsSpec
+        elif isinstance(discrDistCutsSpec, self.AbsInterval):
+            discrDistCuts = Interval(min=max(2, discrDistCutsSpec.min), max=discrDistCutsSpec.max)
+        elif isinstance(discrDistCutsSpec, self.Auto):
+            discrDistCuts = Interval(min=max(2, int(round(self.defDiscrDistCutsFctr.min*sqrNSights))),
+                                     max=int(round(self.defDiscrDistCutsFctr.max*sqrNSights)))
+        elif isinstance(discrDistCutsSpec, self.MultInterval):
+            discrDistCuts = Interval(min=max(2, int(round(discrDistCutsSpec.kmin*sqrNSights))),
+                                     max=int(round(discrDistCutsSpec.kmax*sqrNSights)))
+        else:
+            raise Exception('MCDSTruncationOptimiser.getAnalysisOptimedParams:'
+                            'Should not fall there (discrDistCuts specs)')
+            
+        if isinstance(discrDistCuts, Interval) and discrDistCuts.min == discrDistCuts.max:
+            discrDistCuts = discrDistCuts.min
+
+        # Final checks
+        finalErr = Error()
+        if minDist is not None:
+            minDistChk = minDist if isinstance(minDist, Interval) else Interval(minDist, minDist)
+            msg = minDistChk.check(order=True, minRange=(0, None), maxRange=(None, sDist.max()))
+            if msg:
+                finalErr.append(head='minDist', error=msg)
+
+        if maxDist is not None:
+            maxDistChk = maxDist if isinstance(maxDist, Interval) else Interval(maxDist, maxDist)
+            minMax = None if minDist is None else (minDist.max if isinstance(minDist, Interval) else minDist)
+            msg = maxDistChk.check(order=True, minRange=(minMax, None), maxRange=(None, sDist.max()))
+            if msg:
+                finalErr.append(head='maxDist', error=msg)
+
+        if fitDistCuts is not None:
+            fitDistCutsChk = \
+                fitDistCuts if isinstance(fitDistCuts, Interval) else Interval(fitDistCuts, fitDistCuts)
+            msg = fitDistCutsChk.check(order=True, minRange=(2, None))
+            if msg:
+                finalErr.append(head='fitDistCuts', error=msg)
+
+        if discrDistCuts is not None:
+            discrDistCutsChk = \
+                discrDistCuts if isinstance(discrDistCuts, Interval) else Interval(discrDistCuts, discrDistCuts)
+            msg = discrDistCutsChk.check(order=True, minRange=(2, None))
+            if msg:
+                finalErr.append(head='discrDistCuts', error=msg)
+
+        logger.info2(f'OptimedParams: {minDist=}, {maxDist=}, {fitDistCuts=}, {discrDistCuts=}')
+
+        return finalErr or None, \
+               {self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecMinDist]: minDist,
+                self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecMaxDist]: maxDist,
+                self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecFitDistCuts]: fitDistCuts,
+                self.IntSpec2SolDimOptimTargetParamNames[self.IntSpecDiscrDistCuts]: discrDistCuts}
+                                      
+    # Supported truncation optimisation classes (=> engines = cores) (see submodule optimisation),
+    # all must be subclasses of MCDSTruncationOptimisation.
+    OptimisationClasses = [MCDSZerothOrderTruncationOptimisation]  #, MCDSGridBruteTruncationOptimisation]
+            
+    def getOptimisationCoreParams(self, sAnIntSpec):
+
+        """Retrieve optimisation core parameters, from user specs and default values.
+        
+        Ex: zoopt(mxi=0, tv=1, a=racos, mrx=3)
+
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+                           syntax: sequence of <key>=<value> separated by ','
+                           
+        :return: None or an Error instance, dict(core=..., **{key:value}) or None
+        """
+        
+        # Parse optimisation core params in sAnIntSpec if present.
+        if self.IntSpecOptimisationCore in sAnIntSpec:
+        
+            # Retrieve
+            userSpec = sAnIntSpec[self.IntSpecOptimisationCore]
+                
+            # Parse
+            parsers = {cls.CoreName: cls.CoreUserSpecParser for cls in self.OptimisationClasses}
+            parseError, parsedValue = \
+                 self._parseOptimCoreUserSpecs(userSpec, globals=None, locals=parsers,
+                                               nullOrEmpty=self.dDefOptimCoreParams,
+                                               errIfNotA=[dict])
+
+        # No way: fallback to default values.
+        else:
+            
+            parseError, parsedValue = None, self.dDefOptimCoreParams
+                 
+        # Done.
+        return parseError, parsedValue
+    
+    def getOptimisationSetupParams(self, sAnIntSpec, sSampleDists):
+                               
+        """Compute optimisation setup parameters from user specs and default values.
+        
+        :param sAnIntSpec: analysis parameter user specs with internal names (indexed with IntSpecXXX)
+        :param sSampleDists: sample sightings recorded distances
+       
+        :return: a 2-value tuple (None or an Error instance,
+                          dict(minDist=, maxDist=, fitDistCuts=, discrDistCuts=) or None)
+
+        """
+        
+        # Get params from each of these sets.
+        dFinalParms = dict()
+        finalError = Error()
+        for err, dParms in [self.getAnalysisFixedParams(sAnIntSpec),
+                            self.getAnalysisOptimExprParams(sAnIntSpec),
+                            self.getAnalysisOptimedParams(sAnIntSpec, sSampleDists),
+                            self.getOptimisationCoreParams(sAnIntSpec)]:
+            if err is None:
+                dFinalParms.update(dParms)
+            else:
+                finalError.append(err)
+
+        # Any error => empty output params
+        if finalError:
+            logger.warning('Error(s) while parsing and computing setup params specs: {}'.format(finalError))
+         
+        return finalError, dFinalParms
+        
+    def setupOptimisation(self, sampleDataSet, name=None, customData=None, error=None,
+                          estimKeyFn=MCDSEngine.EstKeyFnDef, estimAdjustFn=MCDSEngine.EstAdjustFnDef, 
+                          estimCriterion=MCDSEngine.EstCriterionDef, cvInterval=MCDSEngine.EstCVIntervalDef,
+                          expr2Optimise='chi2', minimiseExpr=False,
+                          minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,
+                          **dCoreParams):
+                          
+        """Factory method for MCDSXXXTruncationOptimisation classes"""
+
+        # Search for optimisation class from core name dCoreParams['core']
+        # (default to 'zoopt' in case of any error parsing optim core params)
+        optimCore = dCoreParams.get('core', 'zoopt')
+        try:
+            OptimionClass = next(iter(cls for cls in self.OptimisationClasses if cls.CoreName == optimCore))
+        except StopIteration:
+            raise NotImplementedError('No such optimisation core "{}" in house'.format(optimCore))
+        
+        # Check core params.
+        invalidParams = [k for k in dCoreParams if k != 'core' and k not in OptimionClass.CoreParamNames]
+        assert not invalidParams, \
+               'No such parameter(s) {} for {} ctor'.format(','.join(invalidParams), OptimionClass.__name__)
+
+        # Build remainder of optimisation ctor params.
+        dOtherOptimParms = {k: v for k, v in dCoreParams.items() if k in OptimionClass.CoreParamNames}
+        
+        # Instantiate optimisation.
+        optimion = OptimionClass(self._engine, sampleDataSet, name=name,
+                                 customData=customData, error=error,
+                                 executor=self._executor, logData=self.logData, autoClean=self.autoClean,
+                                 estimKeyFn=estimKeyFn, estimAdjustFn=estimAdjustFn,
+                                 estimCriterion=estimCriterion, cvInterval=cvInterval,
+                                 minDist=minDist, maxDist=maxDist,
+                                 fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts,
+                                 expr2Optimise=expr2Optimise, minimiseExpr=minimiseExpr,
+                                 **dOtherOptimParms)
+                                 
+        return optimion
+
+    # Number of optimisation auto-backup files (alternating scheme)
+    BackupAltNum = 2
+
+    def _backupFileName(self, index=0):
+
+        return pl.Path(self.workDir) / f'optr-resbak-{index}.pickle.xz'
+
+    def setupResults(self, dfOptimParamSpecs=None, recover=False, loadFrom=None, sheetName=None):
+    
+        """Build an empty results objects and optionnaly pre-load results
+        * from the last auto-backup file, after a crash / interruption of the optimisations,
+        * or from the given file name.
+
+        Parameters:
+        :param dfOptimParamSpecs: if not None, optim. param. specs for the result object
+                                  (overwrite any specs loaded from file; see blow)
+        :param recover: if True, try and load the last backup file (see completeResults)
+        :param loadFrom: if specified, and not recover, try and load the specified file
+                         (see OptimisationResultsSet.fromFile for supported formats, auto-detected from file extensions)
+        :param sheetName: when loadFrom targets a workbook format file, the name of the sheet to load.
+        """
+    
+        # Build the empty result object
+        customCols = \
+            self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
+        dfCustColTrans = pd.DataFrame(index=customCols, data={lang: customCols for lang in ['fr', 'en']})
+
+        # TODO: optimisationClass=MCDSZerothOrderTruncationOptimisation,
+        self.results = OptimisationResultsSet(optimisationClass=MCDSTruncationOptimisation,
+                                              miCustomCols=customCols, dfCustomColTrans=dfCustColTrans)
+
+        # e. Load data from file if requested to.
+        if recover:
+
+            # Check that there will be an analysis index = Id column in results
+            assert self.anlysIndCol, "Can't recover optimisation results if no analysis index column specified !"
+
+            # Search for backup files and sort them by modification time in descending order.
+            bkupFileNames = [self._backupFileName(ind) for ind in range(self.BackupAltNum)]
+            bkupFileNames = [fpn for fpn in bkupFileNames if fpn.is_file()]
+            sBkupFileMTimes = pd.Series(index=[fpn.as_posix() for fpn in bkupFileNames], 
+                                        data=[fpn.stat().st_mtime for fpn in bkupFileNames])
+            assert not sBkupFileMTimes.empty, 'No such backup file found: {}'.format(self._backupFileName('*'))
+
+            sBkupFileMTimes.sort_values(inplace=True, ascending=False)
+
+            for bkupFpn in sBkupFileMTimes.index:
+                logger.info('Recovering optimisation results from auto-backup {} ...'.format(bkupFpn))
+                try:
+                    self.results.fromPickle(bkupFpn, acceptNewCols=True)
+                    break
+                except Exception as exc:
+                    if bkupFpn == sBkupFileMTimes.index[-1]:
+                        logger.error('... failed ; no other usable backup file, sorry.')
+                        raise
+                    else:
+                        logger.info('... failed ; trying next ...')
+
+            logger.info('... success !')
+
+            # Check that there IS really an analysis index = Id column in results
+            assert self.anlysIndCol in self.results.columns, \
+                   "Can't recover from backup results file because no analysis index/Id column found !"
+
+        elif loadFrom:
+
+            self.results.fromFile(fileName=loadFrom, sheetName=sheetName)
+
+        if dfOptimParamSpecs is not None:
+
+            # Set results specs for traceability.
+            self.results.updateSpecs(overwrite=True, optimiser=self.flatSpecs(), optimisations=dfOptimParamSpecs)
+        
+    def optimisationTargetColumnUserNames(self):
+    
+        """Determine the user names of the results optimisation target columns
+        """
+   
+        # Should not be called before run !
+        assert self.results is not None
+   
+        # From results column names to internal spec name to user spec name.
+        return [self.dInt2UserParamSpecNames[self.SolDim2IntSpecOptimTargetParamNames[solDimName]]
+                for solDimName in self.results.optimisationTargetColumns()]
+
+    def completeResults(self, dOptims):
+    
+        """Wait for and gather dOptims (MCDSOptimisation futures) results into a ResultsSet
+        
+        There should be no reason to specialise this for specific truncation optimisers, but ...
+        """
+    
+        # Start of elapsed time measurement (yes, starting the optimisations may take some time, but it is
+        # negligible when compared to optimisation time ; and better here for evaluating mean per optimisation).
+        optimStart = pd.Timestamp.now()
+        
+        # For each optimisation as it gets completed (first completed => first yielded)
+        nDone = 0
+        for optimFut in self._executor.asCompleted(dOptims):
+            
+            # Retrieve optimisation object from its associated future object
+            optim = dOptims[optimFut]
+            
+            # Get analysis results and optimisation target column names in results
+            dfResults = optim.getResults()
+
+            # Get custom header values, and set target index (= columns) for results
+            sCustomHead = optim.customData
+            sCustomHead.index = self.results.miCustomCols
+
+            # Save results (exact column list changes among optimisations objects so we must acceptNewCols)
+            self.results.append(dfResults, sCustomHead=sCustomHead, acceptNewCols=True)
+
+            # Backup raw results (with an alternate file scheme for safety) in case of unavoidable crash (or reboot).
+            # See setupResults for how these files can be reused later for recovery.
+            nDone += 1
+            if nDone % self.backupEvery == 0:
+                nBackupInd = (nDone // self.backupEvery) % self.BackupAltNum
+                self.results.toPickle(fileName=self._backupFileName(nBackupInd), raw=True)
+
+            # Report elapsed time and number of optimisations completed until now.
+            if nDone % self.logProgressEvery == 0 or nDone == len(dOptims):
+                now = pd.Timestamp.now()
+                elapsedTilNow = now - optimStart
+                if nDone < len(dOptims):
+                    expectedEnd = \
+                        now + pd.Timedelta(elapsedTilNow.value * (len(dOptims) - nDone) / nDone)
+                    expectedEnd = expectedEnd.strftime('%Y-%m-%d %H:%M:%S').replace(now.strftime('%Y-%m-%d '), '')
+                    endOfMsg = 'should end around ' + expectedEnd
+                else:
+                    endOfMsg = 'done'
+                logger.info1('{}/{} optimisations in {} (mean {:.2f}s): {}.'
+                             .format(nDone, len(dOptims), str(elapsedTilNow.round('S')).replace('0 days ', ''),
+                                     elapsedTilNow.total_seconds() / nDone, endOfMsg))
+
+        # Terminate analysis executor
+        self._executor.shutdown()
+
+        # Terminate analysis engine
+        self._engine.shutdown()
+        
+    def run(self, dfExplParamSpecs=None, implParamSpecs=None, threads=None, recover=False):
+   
+        """Optimise specified analyses
+        
+        Call checkUserSpecs(...) before this to make sure user specs are OK
+        
+        Parameters:
+        :param dfExplParamSpecs: optimisation params specs table, as a DataFrame
+        :param implParamSpecs: Implicit pd.DataFrame and optimisation param specs, suitable for explicitation
+                               through Analyser.explicitVariantSpecs
+        :param threads: Number of parallel threads to use (default None: no parallelism, no asynchronism)
+        :param recover: Recover a previously interrupted run using last available auto-backup file
+        """
+    
+        # Executor for optimisations.
+        self._executor = Executor(threads=threads)
+
+        # MCDS analysis engine (a sequential one, because MCDSOptimisation does the parallel stuff itself,
+        # but an asynchronous one if execution time limit is to be enforced with os.system run method).
+
+        # Failed try: Seems we can't stack ThreadPoolExecutors, as optimisations get run sequentially
+        #             when using an Executor(threads=1) (means async) for self._engine ... 
+        # engineExor = None if self.runMethod != 'os.system' or self.runTimeOut is None else Executor(threads=1)
+        self._engine = MCDSEngine(workDir=self.workDir,  # executor=engineExor,
+                                  runMethod=self.runMethod, timeOut=self.runTimeOut,
+                                  distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,
+                                  surveyType=self.surveyType, distanceType=self.distanceType,
+                                  clustering=self.clustering)
+
+        # Custom columns for results.
+        customCols = \
+            self.resultsHeadCols['before'] + self.resultsHeadCols['sample'] + self.resultsHeadCols['after']
+        
+        # Explicitate and complete analysis and optimisation specs, and check for usability.
+        # (should be also done before calling run, to avoid failure).
+        dfExplParamSpecs, userParamSpecCols, intParamSpecCols, _, checkVerdict, checkErrors = \
+            self.explicitParamSpecs(implParamSpecs, dfExplParamSpecs, dropDupes=True, check=True)
+        assert checkVerdict, 'Error: Analysis / Optimisation params check failed: {}'.format('; '.join(checkErrors))
+        
+        # Build internal name => user name converter for spec. columns
+        self.dInt2UserParamSpecNames = dict(zip(intParamSpecCols, userParamSpecCols))
+        
+        # Results object construction
+        self.setupResults(recover=recover, dfOptimParamSpecs=dfExplParamSpecs)
+        recoveredOptims = [] if self.results.empty else list(self.results.dfRawData[self.anlysIndCol].unique())
+
+        # For each optimisation to run :
+        exptdWorkers = self._executor.expectedWorkers()
+        runHow = 'in sequence' if exptdWorkers <= 1 else f'at most {exptdWorkers} parallel threads'
+        logger.info('Running MCDS truncation optimisations for {} analyses specs ({}) ...'
+                    .format(len(dfExplParamSpecs), runHow))
+        if recoveredOptims:
+            logger.info("... but {} already done and recovered won't be again ...".format(len(recoveredOptims)))
+            logger.debug(f'{recoveredOptims=}')
+
+        dOptims = dict()
+        for optimInd, (_, sOptimSpec) in enumerate(dfExplParamSpecs.iterrows()):
+            
+            logger.info1('#{}/{}: {} (Id {})'.format(optimInd+1, len(dfExplParamSpecs),
+                                                    sOptimSpec[self.abbrevCol], sOptimSpec[self.anlysIndCol]))
+
+            # Skip optimisation if already in results (recovered).
+            if sOptimSpec[self.anlysIndCol] in recoveredOptims:
+                logger.info1('Skipping this one: already present in recovered results')
+                continue
+
+            # Select data sample to process (and skip if empty)
+            sds = self._mcDataSet.sampleDataSet(sOptimSpec[self.sampleSelCols])
+            if not sds:
+                continue
+
+            # Build optimisation params specs series with parameters internal names.
+            sOptimIntSpec = sOptimSpec[userParamSpecCols].set_axis(intParamSpecCols, inplace=False)
+            
+            # Compute optimisation setup parameters from user specs and default values.
+            setupError, dSetupParams = \
+                self.getOptimisationSetupParams(sOptimIntSpec, sds.dfData[self.sampleDistCol])
+            
+            # Create optimisation object
+            logger.info2('Optim. params: ' + ', '.join(f'{k}: {v}' for k, v in dSetupParams.items()))
+            optim = self.setupOptimisation(sampleDataSet=sds, name=sOptimSpec[self.abbrevCol],
+                                           customData=sOptimSpec[customCols].copy(),
+                                           error=setupError, **dSetupParams)
+
+            # Compute optimisation submission parameters from user specs and default values.
+            submitError, dSubmitParams = self.getOptimisationSubmitParams(sOptimIntSpec)
+                                               
+            # Submit optimisation (but don't wait for it's finished, go on with next, may run in parallel)
+            logger.info2('Submit params: ' + ', '.join([f'{k}: {v}' for k, v in dSubmitParams.items()]))
+            optimFut = optim.submit(error=submitError, **dSubmitParams)
+            
+            # Store optimisation object and associated "future" for later use (should be running soon or later).
+            dOptims[optimFut] = optim
+            
+            # Next analysis (loop).
+
+        if self._executor.isAsync():
+            logger.info('All optimisations started; now waiting for their end, and results ...')
+        else:
+            logger.info('All optimisations done; now collecting their results ...')
+
+        # Wait for and gather results of all analyses.
+        self.completeResults(dOptims)
+        
+        # Done.
+        logger.info('Optimisations completed ({} analyses => {} results).'
+                    .format(int(self.results.dfData.NFunEvals.sum()), len(self.results)))
+
+        return self.results
+
+
+class MCDSZerothOrderTruncationOptimiser(MCDSTruncationOptimiser):
+
+    """Run a bunch of Zeroth Order MCDS truncation optimisations"""
+
+    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), 
+                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',
+                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'],
+                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],
+                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',
+                 distanceUnit='Meter', areaUnit='Hectare',
+                 surveyType='Point', distanceType='Radial', clustering=False,
+                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'],
+                                      sample=['Species', 'Pass', 'Adult', 'Duration']),
+                 workDir='.', runMethod='subprocess.run', runTimeOut=300,
+                 logData=False, logProgressEvery=5, backupEvery=50, autoClean=True,
+                 defEstimKeyFn=MCDSEngine.EstKeyFnDef, defEstimAdjustFn=MCDSEngine.EstAdjustFnDef,
+                 defEstimCriterion=MCDSEngine.EstCriterionDef, defCVInterval=MCDSEngine.EstCVIntervalDef,
+                 defExpr2Optimise='chi2', defMinimiseExpr=False,
+                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,
+                 defFitDistCutsFctr=Interval(min=2/3, max=3/2),
+                 defDiscrDistCutsFctr=Interval(min=1/3, max=1),
+                 defSubmitTimes=1, defSubmitOnlyBest=None,
+                 defCoreMaxIters=100, defCoreTermExprValue=None, defCoreAlgorithm='racos', defCoreMaxRetries=0):
+
+        super().__init__(dfMonoCatObs=dfMonoCatObs, dfTransects=dfTransects, 
+                         effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, 
+                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,
+                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,
+                         sampleDistCol=sampleDistCol, anlysSpecCustCols=anlysSpecCustCols,
+                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,
+                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,
+                         distanceUnit=distanceUnit, areaUnit=areaUnit,
+                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,
+                         resultsHeadCols=resultsHeadCols,
+                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut, logData=logData,
+                         logProgressEvery=logProgressEvery, backupEvery=backupEvery, autoClean=autoClean,
+                         defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,
+                         defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,
+                         defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,
+                         defOutliersMethod=defOutliersMethod, defOutliersQuantCutPct=defOutliersQuantCutPct,
+                         defFitDistCutsFctr=defFitDistCutsFctr, defDiscrDistCutsFctr=defDiscrDistCutsFctr,
+                         defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,
+                         dDefOptimCoreParams=dict(core='zoopt', maxIters=defCoreMaxIters,
+                                                  termExprValue=defCoreTermExprValue,
+                                                  algorithm=defCoreAlgorithm, maxRetries=defCoreMaxRetries))
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-arrow-left-hover.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-left-hover.svg`

 * *Files 22% similar despite different names*

```diff
@@ -15,19 +15,19 @@
 000000e0: 352d 2e34 2033 342e 334c 3133 362e 3620  5-.4 34.3L136.6 
 000000f0: 3231 3648 3432 3463 3133 2e33 2030 2032  216H424c13.3 0 2
 00000100: 3420 3130 2e37 2032 3420 3234 7633 3263  4 10.7 24 24v32c
 00000110: 3020 3133 2e33 2d31 302e 3720 3234 2d32  0 13.3-10.7 24-2
 00000120: 3420 3234 4831 3336 2e36 6c31 3230 2e35  4 24H136.6l120.5
 00000130: 2031 3134 2e38 6339 2e38 2039 2e33 2031   114.8c9.8 9.3 1
 00000140: 3020 3234 2e38 2e34 2033 342e 337a 222f  0 24.8.4 34.3z"/
-00000150: 3e3c 2f73 7667 3e0d 0a3c 212d 2d0d 0a46  ></svg>..<!--..F
-00000160: 6f6e 7420 4177 6573 6f6d 6520 4672 6565  ont Awesome Free
-00000170: 2035 2e31 2e31 2062 7920 4066 6f6e 7461   5.1.1 by @fonta
-00000180: 7765 736f 6d65 202d 2068 7474 7073 3a2f  wesome - https:/
-00000190: 2f66 6f6e 7461 7765 736f 6d65 2e63 6f6d  /fontawesome.com
-000001a0: 0d0a 4c69 6365 6e73 6520 2d20 6874 7470  ..License - http
-000001b0: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
-000001c0: 636f 6d2f 6c69 6365 6e73 6520 2849 636f  com/license (Ico
-000001d0: 6e73 3a20 4343 2042 5920 342e 302c 2046  ns: CC BY 4.0, F
-000001e0: 6f6e 7473 3a20 5349 4c20 4f46 4c20 312e  onts: SIL OFL 1.
-000001f0: 312c 2043 6f64 653a 204d 4954 204c 6963  1, Code: MIT Lic
-00000200: 656e 7365 290d 0a2d 2d3e                 ense)..-->
+00000150: 3e3c 2f73 7667 3e0a 3c21 2d2d 0a46 6f6e  ></svg>.<!--.Fon
+00000160: 7420 4177 6573 6f6d 6520 4672 6565 2035  t Awesome Free 5
+00000170: 2e31 2e31 2062 7920 4066 6f6e 7461 7765  .1.1 by @fontawe
+00000180: 736f 6d65 202d 2068 7474 7073 3a2f 2f66  some - https://f
+00000190: 6f6e 7461 7765 736f 6d65 2e63 6f6d 0a4c  ontawesome.com.L
+000001a0: 6963 656e 7365 202d 2068 7474 7073 3a2f  icense - https:/
+000001b0: 2f66 6f6e 7461 7765 736f 6d65 2e63 6f6d  /fontawesome.com
+000001c0: 2f6c 6963 656e 7365 2028 4963 6f6e 733a  /license (Icons:
+000001d0: 2043 4320 4259 2034 2e30 2c20 466f 6e74   CC BY 4.0, Font
+000001e0: 733a 2053 494c 204f 464c 2031 2e31 2c20  s: SIL OFL 1.1, 
+000001f0: 436f 6465 3a20 4d49 5420 4c69 6365 6e73  Code: MIT Licens
+00000200: 6529 0a2d 2d3e                           e).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-arrow-left.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-left.svg`

 * *Files 22% similar despite different names*

```diff
@@ -15,19 +15,19 @@
 000000e0: 352d 2e34 2033 342e 334c 3133 362e 3620  5-.4 34.3L136.6 
 000000f0: 3231 3648 3432 3463 3133 2e33 2030 2032  216H424c13.3 0 2
 00000100: 3420 3130 2e37 2032 3420 3234 7633 3263  4 10.7 24 24v32c
 00000110: 3020 3133 2e33 2d31 302e 3720 3234 2d32  0 13.3-10.7 24-2
 00000120: 3420 3234 4831 3336 2e36 6c31 3230 2e35  4 24H136.6l120.5
 00000130: 2031 3134 2e38 6339 2e38 2039 2e33 2031   114.8c9.8 9.3 1
 00000140: 3020 3234 2e38 2e34 2033 342e 337a 222f  0 24.8.4 34.3z"/
-00000150: 3e3c 2f73 7667 3e0d 0a3c 212d 2d0d 0a46  ></svg>..<!--..F
-00000160: 6f6e 7420 4177 6573 6f6d 6520 4672 6565  ont Awesome Free
-00000170: 2035 2e31 2e31 2062 7920 4066 6f6e 7461   5.1.1 by @fonta
-00000180: 7765 736f 6d65 202d 2068 7474 7073 3a2f  wesome - https:/
-00000190: 2f66 6f6e 7461 7765 736f 6d65 2e63 6f6d  /fontawesome.com
-000001a0: 0d0a 4c69 6365 6e73 6520 2d20 6874 7470  ..License - http
-000001b0: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
-000001c0: 636f 6d2f 6c69 6365 6e73 6520 2849 636f  com/license (Ico
-000001d0: 6e73 3a20 4343 2042 5920 342e 302c 2046  ns: CC BY 4.0, F
-000001e0: 6f6e 7473 3a20 5349 4c20 4f46 4c20 312e  onts: SIL OFL 1.
-000001f0: 312c 2043 6f64 653a 204d 4954 204c 6963  1, Code: MIT Lic
-00000200: 656e 7365 290d 0a2d 2d3e                 ense)..-->
+00000150: 3e3c 2f73 7667 3e0a 3c21 2d2d 0a46 6f6e  ></svg>.<!--.Fon
+00000160: 7420 4177 6573 6f6d 6520 4672 6565 2035  t Awesome Free 5
+00000170: 2e31 2e31 2062 7920 4066 6f6e 7461 7765  .1.1 by @fontawe
+00000180: 736f 6d65 202d 2068 7474 7073 3a2f 2f66  some - https://f
+00000190: 6f6e 7461 7765 736f 6d65 2e63 6f6d 0a4c  ontawesome.com.L
+000001a0: 6963 656e 7365 202d 2068 7474 7073 3a2f  icense - https:/
+000001b0: 2f66 6f6e 7461 7765 736f 6d65 2e63 6f6d  /fontawesome.com
+000001c0: 2f6c 6963 656e 7365 2028 4963 6f6e 733a  /license (Icons:
+000001d0: 2043 4320 4259 2034 2e30 2c20 466f 6e74   CC BY 4.0, Font
+000001e0: 733a 2053 494c 204f 464c 2031 2e31 2c20  s: SIL OFL 1.1, 
+000001f0: 436f 6465 3a20 4d49 5420 4c69 6365 6e73  Code: MIT Licens
+00000200: 6529 0a2d 2d3e                           e).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-arrow-right-hover.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-right-hover.svg`

 * *Files 4% similar despite different names*

```diff
@@ -15,19 +15,19 @@
 000000e0: 2d32 3520 2e34 2d33 342e 334c 3331 312e  -25 .4-34.3L311.
 000000f0: 3420 3239 3648 3234 632d 3133 2e33 2030  4 296H24c-13.3 0
 00000100: 2d32 342d 3130 2e37 2d32 342d 3234 762d  -24-10.7-24-24v-
 00000110: 3332 6330 2d31 332e 3320 3130 2e37 2d32  32c0-13.3 10.7-2
 00000120: 3420 3234 2d32 3468 3238 372e 344c 3139  4 24-24h287.4L19
 00000130: 302e 3920 3130 312e 3263 2d39 2e38 2d39  0.9 101.2c-9.8-9
 00000140: 2e33 2d31 302d 3234 2e38 2d2e 342d 3334  .3-10-24.8-.4-34
-00000150: 2e33 7a22 2f3e 3c2f 7376 673e 0d0a 3c21  .3z"/></svg>..<!
-00000160: 2d2d 0d0a 466f 6e74 2041 7765 736f 6d65  --..Font Awesome
-00000170: 2046 7265 6520 352e 312e 3120 6279 2040   Free 5.1.1 by @
-00000180: 666f 6e74 6177 6573 6f6d 6520 2d20 6874  fontawesome - ht
-00000190: 7470 733a 2f2f 666f 6e74 6177 6573 6f6d  tps://fontawesom
-000001a0: 652e 636f 6d0d 0a4c 6963 656e 7365 202d  e.com..License -
-000001b0: 2068 7474 7073 3a2f 2f66 6f6e 7461 7765   https://fontawe
-000001c0: 736f 6d65 2e63 6f6d 2f6c 6963 656e 7365  some.com/license
-000001d0: 2028 4963 6f6e 733a 2043 4320 4259 2034   (Icons: CC BY 4
-000001e0: 2e30 2c20 466f 6e74 733a 2053 494c 204f  .0, Fonts: SIL O
-000001f0: 464c 2031 2e31 2c20 436f 6465 3a20 4d49  FL 1.1, Code: MI
-00000200: 5420 4c69 6365 6e73 6529 0d0a 2d2d 3e    T License)..-->
+00000150: 2e33 7a22 2f3e 3c2f 7376 673e 0a3c 212d  .3z"/></svg>.<!-
+00000160: 2d0a 466f 6e74 2041 7765 736f 6d65 2046  -.Font Awesome F
+00000170: 7265 6520 352e 312e 3120 6279 2040 666f  ree 5.1.1 by @fo
+00000180: 6e74 6177 6573 6f6d 6520 2d20 6874 7470  ntawesome - http
+00000190: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
+000001a0: 636f 6d0a 4c69 6365 6e73 6520 2d20 6874  com.License - ht
+000001b0: 7470 733a 2f2f 666f 6e74 6177 6573 6f6d  tps://fontawesom
+000001c0: 652e 636f 6d2f 6c69 6365 6e73 6520 2849  e.com/license (I
+000001d0: 636f 6e73 3a20 4343 2042 5920 342e 302c  cons: CC BY 4.0,
+000001e0: 2046 6f6e 7473 3a20 5349 4c20 4f46 4c20   Fonts: SIL OFL 
+000001f0: 312e 312c 2043 6f64 653a 204d 4954 204c  1.1, Code: MIT L
+00000200: 6963 656e 7365 290a 2d2d 3e              icense).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-arrow-right.svg` & `pyaudisam-1.0.1/pyaudisam/report/fa-arrow-right-hover.svg`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 00000000: 3c73 7667 2078 6d6c 6e73 3d22 6874 7470  <svg xmlns="http
 00000010: 3a2f 2f77 7777 2e77 332e 6f72 672f 3230  ://www.w3.org/20
 00000020: 3030 2f73 7667 2220 7669 6577 426f 783d  00/svg" viewBox=
 00000030: 2230 2030 2034 3438 2035 3132 223e 3c70  "0 0 448 512"><p
-00000040: 6174 6820 6669 6c6c 3d22 2332 3434 6330  ath fill="#244c0
-00000050: 6322 2064 3d22 4d31 3930 2e35 2036 362e  c" d="M190.5 66.
+00000040: 6174 6820 6669 6c6c 3d22 2362 6435 6133  ath fill="#bd5a3
+00000050: 3522 2064 3d22 4d31 3930 2e35 2036 362e  5" d="M190.5 66.
 00000060: 396c 3232 2e32 2d32 322e 3263 392e 342d  9l22.2-22.2c9.4-
 00000070: 392e 3420 3234 2e36 2d39 2e34 2033 332e  9.4 24.6-9.4 33.
 00000080: 3920 304c 3434 3120 3233 3963 392e 3420  9 0L441 239c9.4 
 00000090: 392e 3420 392e 3420 3234 2e36 2030 2033  9.4 9.4 24.6 0 3
 000000a0: 332e 394c 3234 362e 3620 3436 372e 3363  3.9L246.6 467.3c
 000000b0: 2d39 2e34 2039 2e34 2d32 342e 3620 392e  -9.4 9.4-24.6 9.
 000000c0: 342d 3333 2e39 2030 6c2d 3232 2e32 2d32  4-33.9 0l-22.2-2
@@ -15,19 +15,19 @@
 000000e0: 2d32 3520 2e34 2d33 342e 334c 3331 312e  -25 .4-34.3L311.
 000000f0: 3420 3239 3648 3234 632d 3133 2e33 2030  4 296H24c-13.3 0
 00000100: 2d32 342d 3130 2e37 2d32 342d 3234 762d  -24-10.7-24-24v-
 00000110: 3332 6330 2d31 332e 3320 3130 2e37 2d32  32c0-13.3 10.7-2
 00000120: 3420 3234 2d32 3468 3238 372e 344c 3139  4 24-24h287.4L19
 00000130: 302e 3920 3130 312e 3263 2d39 2e38 2d39  0.9 101.2c-9.8-9
 00000140: 2e33 2d31 302d 3234 2e38 2d2e 342d 3334  .3-10-24.8-.4-34
-00000150: 2e33 7a22 2f3e 3c2f 7376 673e 0d0a 3c21  .3z"/></svg>..<!
-00000160: 2d2d 0d0a 466f 6e74 2041 7765 736f 6d65  --..Font Awesome
-00000170: 2046 7265 6520 352e 312e 3120 6279 2040   Free 5.1.1 by @
-00000180: 666f 6e74 6177 6573 6f6d 6520 2d20 6874  fontawesome - ht
-00000190: 7470 733a 2f2f 666f 6e74 6177 6573 6f6d  tps://fontawesom
-000001a0: 652e 636f 6d0d 0a4c 6963 656e 7365 202d  e.com..License -
-000001b0: 2068 7474 7073 3a2f 2f66 6f6e 7461 7765   https://fontawe
-000001c0: 736f 6d65 2e63 6f6d 2f6c 6963 656e 7365  some.com/license
-000001d0: 2028 4963 6f6e 733a 2043 4320 4259 2034   (Icons: CC BY 4
-000001e0: 2e30 2c20 466f 6e74 733a 2053 494c 204f  .0, Fonts: SIL O
-000001f0: 464c 2031 2e31 2c20 436f 6465 3a20 4d49  FL 1.1, Code: MI
-00000200: 5420 4c69 6365 6e73 6529 0d0a 2d2d 3e    T License)..-->
+00000150: 2e33 7a22 2f3e 3c2f 7376 673e 0a3c 212d  .3z"/></svg>.<!-
+00000160: 2d0a 466f 6e74 2041 7765 736f 6d65 2046  -.Font Awesome F
+00000170: 7265 6520 352e 312e 3120 6279 2040 666f  ree 5.1.1 by @fo
+00000180: 6e74 6177 6573 6f6d 6520 2d20 6874 7470  ntawesome - http
+00000190: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
+000001a0: 636f 6d0a 4c69 6365 6e73 6520 2d20 6874  com.License - ht
+000001b0: 7470 733a 2f2f 666f 6e74 6177 6573 6f6d  tps://fontawesom
+000001c0: 652e 636f 6d2f 6c69 6365 6e73 6520 2849  e.com/license (I
+000001d0: 636f 6e73 3a20 4343 2042 5920 342e 302c  cons: CC BY 4.0,
+000001e0: 2046 6f6e 7473 3a20 5349 4c20 4f46 4c20   Fonts: SIL OFL 
+000001f0: 312e 312c 2043 6f64 653a 204d 4954 204c  1.1, Code: MIT L
+00000200: 6963 656e 7365 290a 2d2d 3e              icense).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-arrow-up-hover.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-up-hover.svg`

 * *Files 24% similar despite different names*

```diff
@@ -15,19 +15,19 @@
 000000e0: 2e33 2d33 342e 332d 2e34 4c32 3634 2031  .3-34.3-.4L264 1
 000000f0: 3638 2e36 5634 3536 6330 2031 332e 332d  68.6V456c0 13.3-
 00000100: 3130 2e37 2032 342d 3234 2032 3468 2d33  10.7 24-24 24h-3
 00000110: 3263 2d31 332e 3320 302d 3234 2d31 302e  2c-13.3 0-24-10.
 00000120: 372d 3234 2d32 3456 3136 382e 364c 3639  7-24-24V168.6L69
 00000130: 2e32 2032 3839 2e31 632d 392e 3320 392e  .2 289.1c-9.3 9.
 00000140: 382d 3234 2e38 2031 302d 3334 2e33 2e34  8-24.8 10-34.3.4
-00000150: 7a22 2f3e 3c2f 7376 673e 0d0a 3c21 2d2d  z"/></svg>..<!--
-00000160: 0d0a 466f 6e74 2041 7765 736f 6d65 2046  ..Font Awesome F
-00000170: 7265 6520 352e 312e 3120 6279 2040 666f  ree 5.1.1 by @fo
-00000180: 6e74 6177 6573 6f6d 6520 2d20 6874 7470  ntawesome - http
-00000190: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
-000001a0: 636f 6d0d 0a4c 6963 656e 7365 202d 2068  com..License - h
-000001b0: 7474 7073 3a2f 2f66 6f6e 7461 7765 736f  ttps://fontaweso
-000001c0: 6d65 2e63 6f6d 2f6c 6963 656e 7365 2028  me.com/license (
-000001d0: 4963 6f6e 733a 2043 4320 4259 2034 2e30  Icons: CC BY 4.0
-000001e0: 2c20 466f 6e74 733a 2053 494c 204f 464c  , Fonts: SIL OFL
-000001f0: 2031 2e31 2c20 436f 6465 3a20 4d49 5420   1.1, Code: MIT 
-00000200: 4c69 6365 6e73 6529 0d0a 2d2d 3e         License)..-->
+00000150: 7a22 2f3e 3c2f 7376 673e 0a3c 212d 2d0a  z"/></svg>.<!--.
+00000160: 466f 6e74 2041 7765 736f 6d65 2046 7265  Font Awesome Fre
+00000170: 6520 352e 312e 3120 6279 2040 666f 6e74  e 5.1.1 by @font
+00000180: 6177 6573 6f6d 6520 2d20 6874 7470 733a  awesome - https:
+00000190: 2f2f 666f 6e74 6177 6573 6f6d 652e 636f  //fontawesome.co
+000001a0: 6d0a 4c69 6365 6e73 6520 2d20 6874 7470  m.License - http
+000001b0: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
+000001c0: 636f 6d2f 6c69 6365 6e73 6520 2849 636f  com/license (Ico
+000001d0: 6e73 3a20 4343 2042 5920 342e 302c 2046  ns: CC BY 4.0, F
+000001e0: 6f6e 7473 3a20 5349 4c20 4f46 4c20 312e  onts: SIL OFL 1.
+000001f0: 312c 2043 6f64 653a 204d 4954 204c 6963  1, Code: MIT Lic
+00000200: 656e 7365 290a 2d2d 3e                   ense).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-arrow-up.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-arrow-up.svg`

 * *Files 23% similar despite different names*

```diff
@@ -15,19 +15,19 @@
 000000e0: 2e33 2d33 342e 332d 2e34 4c32 3634 2031  .3-34.3-.4L264 1
 000000f0: 3638 2e36 5634 3536 6330 2031 332e 332d  68.6V456c0 13.3-
 00000100: 3130 2e37 2032 342d 3234 2032 3468 2d33  10.7 24-24 24h-3
 00000110: 3263 2d31 332e 3320 302d 3234 2d31 302e  2c-13.3 0-24-10.
 00000120: 372d 3234 2d32 3456 3136 382e 364c 3639  7-24-24V168.6L69
 00000130: 2e32 2032 3839 2e31 632d 392e 3320 392e  .2 289.1c-9.3 9.
 00000140: 382d 3234 2e38 2031 302d 3334 2e33 2e34  8-24.8 10-34.3.4
-00000150: 7a22 2f3e 3c2f 7376 673e 0d0a 3c21 2d2d  z"/></svg>..<!--
-00000160: 0d0a 466f 6e74 2041 7765 736f 6d65 2046  ..Font Awesome F
-00000170: 7265 6520 352e 312e 3120 6279 2040 666f  ree 5.1.1 by @fo
-00000180: 6e74 6177 6573 6f6d 6520 2d20 6874 7470  ntawesome - http
-00000190: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
-000001a0: 636f 6d0d 0a4c 6963 656e 7365 202d 2068  com..License - h
-000001b0: 7474 7073 3a2f 2f66 6f6e 7461 7765 736f  ttps://fontaweso
-000001c0: 6d65 2e63 6f6d 2f6c 6963 656e 7365 2028  me.com/license (
-000001d0: 4963 6f6e 733a 2043 4320 4259 2034 2e30  Icons: CC BY 4.0
-000001e0: 2c20 466f 6e74 733a 2053 494c 204f 464c  , Fonts: SIL OFL
-000001f0: 2031 2e31 2c20 436f 6465 3a20 4d49 5420   1.1, Code: MIT 
-00000200: 4c69 6365 6e73 6529 0d0a 2d2d 3e         License)..-->
+00000150: 7a22 2f3e 3c2f 7376 673e 0a3c 212d 2d0a  z"/></svg>.<!--.
+00000160: 466f 6e74 2041 7765 736f 6d65 2046 7265  Font Awesome Fre
+00000170: 6520 352e 312e 3120 6279 2040 666f 6e74  e 5.1.1 by @font
+00000180: 6177 6573 6f6d 6520 2d20 6874 7470 733a  awesome - https:
+00000190: 2f2f 666f 6e74 6177 6573 6f6d 652e 636f  //fontawesome.co
+000001a0: 6d0a 4c69 6365 6e73 6520 2d20 6874 7470  m.License - http
+000001b0: 733a 2f2f 666f 6e74 6177 6573 6f6d 652e  s://fontawesome.
+000001c0: 636f 6d2f 6c69 6365 6e73 6520 2849 636f  com/license (Ico
+000001d0: 6e73 3a20 4343 2042 5920 342e 302c 2046  ns: CC BY 4.0, F
+000001e0: 6f6e 7473 3a20 5349 4c20 4f46 4c20 312e  onts: SIL OFL 1.
+000001f0: 312c 2043 6f64 653a 204d 4954 204c 6963  1, Code: MIT Lic
+00000200: 656e 7365 290a 2d2d 3e                   ense).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-feather-alt.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-feather-alt.svg`

 * *Files 5% similar despite different names*

```diff
@@ -24,20 +24,19 @@
 00000170: 382d 3536 2e35 3448 3235 352e 3734 6c31  8-56.54H255.74l1
 00000180: 3436 2e37 392d 3438 2e38 3863 3131 2e32  46.79-48.88c11.2
 00000190: 352d 3134 2e38 3920 3231 2e33 372d 3330  5-14.89 21.37-30
 000001a0: 2e37 3120 3330 2e34 352d 3437 2e31 3268  .71 30.45-47.12h
 000001b0: 2d38 312e 3134 6c31 3036 2e35 342d 3533  -81.14l106.54-53
 000001c0: 2e32 3143 3530 302e 3239 2031 3332 2e38  .21C500.29 132.8
 000001d0: 3620 3531 302e 3139 2032 362e 3236 2035  6 510.19 26.26 5
-000001e0: 3132 2030 7a22 2f3e 3c2f 7376 673e 0d0a  12 0z"/></svg>..
-000001f0: 3c21 2d2d 0d0a 466f 6e74 2041 7765 736f  <!--..Font Aweso
-00000200: 6d65 2046 7265 6520 352e 312e 3120 6279  me Free 5.1.1 by
-00000210: 2040 666f 6e74 6177 6573 6f6d 6520 2d20   @fontawesome - 
-00000220: 6874 7470 733a 2f2f 666f 6e74 6177 6573  https://fontawes
-00000230: 6f6d 652e 636f 6d0d 0a4c 6963 656e 7365  ome.com..License
-00000240: 202d 2068 7474 7073 3a2f 2f66 6f6e 7461   - https://fonta
-00000250: 7765 736f 6d65 2e63 6f6d 2f6c 6963 656e  wesome.com/licen
-00000260: 7365 2028 4963 6f6e 733a 2043 4320 4259  se (Icons: CC BY
-00000270: 2034 2e30 2c20 466f 6e74 733a 2053 494c   4.0, Fonts: SIL
-00000280: 204f 464c 2031 2e31 2c20 436f 6465 3a20   OFL 1.1, Code: 
-00000290: 4d49 5420 4c69 6365 6e73 6529 0d0a 2d2d  MIT License)..--
-000002a0: 3e                                       >
+000001e0: 3132 2030 7a22 2f3e 3c2f 7376 673e 0a3c  12 0z"/></svg>.<
+000001f0: 212d 2d0a 466f 6e74 2041 7765 736f 6d65  !--.Font Awesome
+00000200: 2046 7265 6520 352e 312e 3120 6279 2040   Free 5.1.1 by @
+00000210: 666f 6e74 6177 6573 6f6d 6520 2d20 6874  fontawesome - ht
+00000220: 7470 733a 2f2f 666f 6e74 6177 6573 6f6d  tps://fontawesom
+00000230: 652e 636f 6d0a 4c69 6365 6e73 6520 2d20  e.com.License - 
+00000240: 6874 7470 733a 2f2f 666f 6e74 6177 6573  https://fontawes
+00000250: 6f6d 652e 636f 6d2f 6c69 6365 6e73 6520  ome.com/license 
+00000260: 2849 636f 6e73 3a20 4343 2042 5920 342e  (Icons: CC BY 4.
+00000270: 302c 2046 6f6e 7473 3a20 5349 4c20 4f46  0, Fonts: SIL OF
+00000280: 4c20 312e 312c 2043 6f64 653a 204d 4954  L 1.1, Code: MIT
+00000290: 204c 6963 656e 7365 290a 2d2d 3e          License).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-file-excel-hover.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-file-excel-hover.svg`

 * *Files 3% similar despite different names*

```diff
@@ -36,20 +36,19 @@
 00000230: 3135 2e39 2032 382e 3620 3537 2e37 2032  15.9 28.6 57.7 2
 00000240: 2e31 2033 2e39 2036 2e32 2036 2e33 2031  .1 3.9 6.2 6.3 1
 00000250: 302e 3620 362e 3348 3236 3063 392e 3320  0.6 6.3H260c9.3 
 00000260: 3020 3135 2d31 3020 3130 2e34 2d31 384c  0 15-10 10.4-18L
 00000270: 3232 3420 3332 3063 2e37 2d31 2e31 2033  224 320c.7-1.1 3
 00000280: 302e 332d 3530 2e35 2034 362e 332d 3738  0.3-50.5 46.3-78
 00000290: 2034 2e37 2d38 2d31 2e31 2d31 382d 3130   4.7-8-1.1-18-10
-000002a0: 2e33 2d31 387a 222f 3e3c 2f73 7667 3e0d  .3-18z"/></svg>.
-000002b0: 0a3c 212d 2d0d 0a46 6f6e 7420 4177 6573  .<!--..Font Awes
-000002c0: 6f6d 6520 4672 6565 2035 2e31 2e31 2062  ome Free 5.1.1 b
-000002d0: 7920 4066 6f6e 7461 7765 736f 6d65 202d  y @fontawesome -
-000002e0: 2068 7474 7073 3a2f 2f66 6f6e 7461 7765   https://fontawe
-000002f0: 736f 6d65 2e63 6f6d 0d0a 4c69 6365 6e73  some.com..Licens
-00000300: 6520 2d20 6874 7470 733a 2f2f 666f 6e74  e - https://font
-00000310: 6177 6573 6f6d 652e 636f 6d2f 6c69 6365  awesome.com/lice
-00000320: 6e73 6520 2849 636f 6e73 3a20 4343 2042  nse (Icons: CC B
-00000330: 5920 342e 302c 2046 6f6e 7473 3a20 5349  Y 4.0, Fonts: SI
-00000340: 4c20 4f46 4c20 312e 312c 2043 6f64 653a  L OFL 1.1, Code:
-00000350: 204d 4954 204c 6963 656e 7365 290d 0a2d   MIT License)..-
-00000360: 2d3e                                     ->
+000002a0: 2e33 2d31 387a 222f 3e3c 2f73 7667 3e0a  .3-18z"/></svg>.
+000002b0: 3c21 2d2d 0a46 6f6e 7420 4177 6573 6f6d  <!--.Font Awesom
+000002c0: 6520 4672 6565 2035 2e31 2e31 2062 7920  e Free 5.1.1 by 
+000002d0: 4066 6f6e 7461 7765 736f 6d65 202d 2068  @fontawesome - h
+000002e0: 7474 7073 3a2f 2f66 6f6e 7461 7765 736f  ttps://fontaweso
+000002f0: 6d65 2e63 6f6d 0a4c 6963 656e 7365 202d  me.com.License -
+00000300: 2068 7474 7073 3a2f 2f66 6f6e 7461 7765   https://fontawe
+00000310: 736f 6d65 2e63 6f6d 2f6c 6963 656e 7365  some.com/license
+00000320: 2028 4963 6f6e 733a 2043 4320 4259 2034   (Icons: CC BY 4
+00000330: 2e30 2c20 466f 6e74 733a 2053 494c 204f  .0, Fonts: SIL O
+00000340: 464c 2031 2e31 2c20 436f 6465 3a20 4d49  FL 1.1, Code: MI
+00000350: 5420 4c69 6365 6e73 6529 0a2d 2d3e       T License).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/fa-file-excel.svg` & `pyaudisam-1.0.1/docs/how-it-works/common/fa-file-excel.svg`

 * *Files 3% similar despite different names*

```diff
@@ -36,20 +36,19 @@
 00000230: 3135 2e39 2032 382e 3620 3537 2e37 2032  15.9 28.6 57.7 2
 00000240: 2e31 2033 2e39 2036 2e32 2036 2e33 2031  .1 3.9 6.2 6.3 1
 00000250: 302e 3620 362e 3348 3236 3063 392e 3320  0.6 6.3H260c9.3 
 00000260: 3020 3135 2d31 3020 3130 2e34 2d31 384c  0 15-10 10.4-18L
 00000270: 3232 3420 3332 3063 2e37 2d31 2e31 2033  224 320c.7-1.1 3
 00000280: 302e 332d 3530 2e35 2034 362e 332d 3738  0.3-50.5 46.3-78
 00000290: 2034 2e37 2d38 2d31 2e31 2d31 382d 3130   4.7-8-1.1-18-10
-000002a0: 2e33 2d31 387a 222f 3e3c 2f73 7667 3e0d  .3-18z"/></svg>.
-000002b0: 0a3c 212d 2d0d 0a46 6f6e 7420 4177 6573  .<!--..Font Awes
-000002c0: 6f6d 6520 4672 6565 2035 2e31 2e31 2062  ome Free 5.1.1 b
-000002d0: 7920 4066 6f6e 7461 7765 736f 6d65 202d  y @fontawesome -
-000002e0: 2068 7474 7073 3a2f 2f66 6f6e 7461 7765   https://fontawe
-000002f0: 736f 6d65 2e63 6f6d 0d0a 4c69 6365 6e73  some.com..Licens
-00000300: 6520 2d20 6874 7470 733a 2f2f 666f 6e74  e - https://font
-00000310: 6177 6573 6f6d 652e 636f 6d2f 6c69 6365  awesome.com/lice
-00000320: 6e73 6520 2849 636f 6e73 3a20 4343 2042  nse (Icons: CC B
-00000330: 5920 342e 302c 2046 6f6e 7473 3a20 5349  Y 4.0, Fonts: SI
-00000340: 4c20 4f46 4c20 312e 312c 2043 6f64 653a  L OFL 1.1, Code:
-00000350: 204d 4954 204c 6963 656e 7365 290d 0a2d   MIT License)..-
-00000360: 2d3e                                     ->
+000002a0: 2e33 2d31 387a 222f 3e3c 2f73 7667 3e0a  .3-18z"/></svg>.
+000002b0: 3c21 2d2d 0a46 6f6e 7420 4177 6573 6f6d  <!--.Font Awesom
+000002c0: 6520 4672 6565 2035 2e31 2e31 2062 7920  e Free 5.1.1 by 
+000002d0: 4066 6f6e 7461 7765 736f 6d65 202d 2068  @fontawesome - h
+000002e0: 7474 7073 3a2f 2f66 6f6e 7461 7765 736f  ttps://fontaweso
+000002f0: 6d65 2e63 6f6d 0a4c 6963 656e 7365 202d  me.com.License -
+00000300: 2068 7474 7073 3a2f 2f66 6f6e 7461 7765   https://fontawe
+00000310: 736f 6d65 2e63 6f6d 2f6c 6963 656e 7365  some.com/license
+00000320: 2028 4963 6f6e 733a 2043 4320 4259 2034   (Icons: CC BY 4
+00000330: 2e30 2c20 466f 6e74 733a 2053 494c 204f  .0, Fonts: SIL O
+00000340: 464c 2031 2e31 2c20 436f 6465 3a20 4d49  FL 1.1, Code: MI
+00000350: 5420 4c69 6365 6e73 6529 0a2d 2d3e       T License).-->
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report/report.css` & `pyaudisam-1.0.1/docs/how-it-works/common/report.css`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,259 +1,259 @@
-/* PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-   Copyright (C) 2021 Jean-Philippe Meuret
-   
-   This program is free software: you can redistribute it and/or modify it under the terms
-   of the GNU General Public License as published by the Free Software Foundation,
-   either version 3 of the License, or (at your option) any later version.
-   This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-   without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-   See the GNU General Public License for more details.
-   You should have received a copy of the GNU General Public License along with this program.
-   If not, see https://www.gnu.org/licenses/.
-*/
-
-html, body  {
- /* font-family: Arial, Helvetica, sans-serif;*/
- font-family: "Liberation Serif", Georgia, serif
- margin: 0;
- padding: 0;
- width: 100%;
-}
-div {
- margin: 0;
- padding: 0;
-}
-body {
- background-color: #e8efd1;
- font-size: 100%;
-}
-h1 {
- font-size: 480%;
- color: #244c0c;
- text-align: center;
-}
-h2 {
- font-size: 240%;
- color: #244c0c;
- margin-bottom: 20px;
-}
-h3 {
- font-size: 160%;
- color: #244c0c;
- margin-bottom: 10px;
-}
-h4 {
- font-size: 120%;
- color: #244c0c;
- margin-bottom: 0;
-}
-h5 {
- font-size: 100%;
- color: #244c0c;
- margin-bottom: 0;
-}
-h6 {
- font-size: 80%;
- color: #244c0c;
- margin-bottom: 0;
-}
-p {
- color: #244c0c;
-}
-ul,ol,li,td {
- color: #244c0c;
-}
-a:link {
- color: #2f7404;
- font-weight: bold;
- text-decoration:underline;
-}
-a:visited {
- color: #379000;
- font-weight:bold;
- text-decoration:underline;
-}
-a:active,
-a:hover {
- color: #bd5a35;
- text-decoration:underline;
-}
-
-table a:link {
- color: #244c0c;
- font-weight: bold;
- text-decoration:none;
-}
-table a:visited {
- color: #546122;
- font-weight:bold;
- text-decoration:none;
-}
-table a:active,
-table a:hover {
- color: #bd5a35;
- text-decoration:underline;
-}
-
-table {
- color:#244c0c;
- text-shadow: 1px 1px 0px #fff;
- margin: 8px 4px 0 4px;
- border: #ccc 1px solid;
-
- -moz-border-radius:3px;
- -webkit-border-radius:3px;
- border-radius:3px;
-
- -moz-box-shadow: 0 1px 2px #d1d1d1;
- -webkit-box-shadow: 0 1px 2px #d1d1d1;
- box-shadow: 0 1px 2px #d1d1d1;
- 
- white-space: nowrap;
-}
-table th {
- text-align: left;
- padding: 4px 4px 2px 5px;
- border-top: 1px solid #f9fbf3;
- border-bottom: 1px solid #dee5ca;
-
- background: #bcc380;
- background: -webkit-gradient(linear, left top, left bottom, from(#bcc380), to(#e4eac8));
- background: -moz-linear-gradient(top, #bcc380, #e4eac8);
-
- white-space: normal;
-}
-table th:first-child {
- text-align: left;
- padding-left: 5px;
-}
-table tr:first-child th:first-child {
- -moz-border-radius-topleft:3px;
- -webkit-border-top-left-radius:3px;
- border-top-left-radius:3px;
-}
-table tr:first-child th:last-child {
- -moz-border-radius-topright:3px;
- -webkit-border-top-right-radius:3px;
- border-top-right-radius:3px;
-}
-table tr {
- padding: 0 5px 0 0;
- }
-table td:first-child {
- border-left: 0;
-}
-table td {
- text-align: right;
- padding: 4px 4px 2px 5px;
- border-top: 1px solid #ffffff;
- border-bottom: 1px solid #dee5ca;
- border-left: 1px solid #dee5ca;
- vertical-align: top;
-}
-table tr.even td {
- background: #f6f6f6;
- background: -webkit-gradient(linear, left top, left bottom, from(#f8f8f8), to(#f6f6f6));
- background: -moz-linear-gradient(top,  #f8f8f8,  #f6f6f6);
-}
-/*table tr:last-child td {
- border-bottom:0;
-}*/
-table tr:last-child td:first-child {
- -moz-border-radius-bottomleft:3px;
- -webkit-border-bottom-left-radius:3px;
- border-bottom-left-radius:3px;
-}
-table tr:last-child td:last-child {
- -moz-border-radius-bottomright:3px;
- -webkit-border-bottom-right-radius:3px;
- border-bottom-right-radius:3px;
-}
-table tr:hover td {
- border-top: 1px solid #e8efd1;
- border-bottom: 1px solid #99b484;
- border-left: 1px solid #99b484;
-}
-
-/* Tables embedded in tables */
-table td table {
- all: initial;
-}
-table td table th {
- all: initial;
- font-weight: bold;
-}
-
-#toTopBtn {
-  display: none;
-  position: fixed;
-  bottom: 15px;
-  right: 15px;
-  z-index: 99;
-  border: none;
-  border-radius: 10px;
-  outline: none;
-  opacity: .25;
-  background-color: white;
-  cursor: pointer;
-}
-#toTopBtn:hover {
-  opacity: .75;
-}
-
-div.chapter {
-  -moz-border-radius:8px;
-  -webkit-border-radius:8px;
-  border-radius:8px;
-  padding: 1px 5px 1px 5px;
-}
-div.chapter:hover {
- background:#e3eac9;
-}
-
-.top-vert-align {
-  vertical-align: top;
-}
-
-.center {
-  display: block;
-  margin-left: auto;
-  margin-right: auto;
-}
-
-.shrinkable {
-  max-width: 100%;
-}
-
-.audio-player {
- min-width: 384px;
- max-width: 100%;
-}
-
-#title {
- all: initial;
- display: table;
- margin-left: auto;
- margin-right: auto;
- text-shadow: 1px 1px 0px #fff;
-}
-#title tbody {
- all: initial;
-}
-#title th {
- all: initial;
- display: table-cell;
-}
-#title tr {
- all: initial;
- display: table-row;
- color: #244c0c
-}
-#title td {
- all: initial;
- /* font-family: Arial, Helvetica, sans-serif;*/
- font-family: "Liberation Serif", Georgia, serif;
- display: table-cell;
- padding-bottom: 15px;
- color: #244c0c
-}
+/* PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+   Copyright (C) 2021 Jean-Philippe Meuret
+   
+   This program is free software: you can redistribute it and/or modify it under the terms
+   of the GNU General Public License as published by the Free Software Foundation,
+   either version 3 of the License, or (at your option) any later version.
+   This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+   without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+   See the GNU General Public License for more details.
+   You should have received a copy of the GNU General Public License along with this program.
+   If not, see https://www.gnu.org/licenses/.
+*/
+
+html, body  {
+ /* font-family: Arial, Helvetica, sans-serif;*/
+ font-family: "Liberation Serif", Georgia, serif
+ margin: 0;
+ padding: 0;
+ width: 100%;
+}
+div {
+ margin: 0;
+ padding: 0;
+}
+body {
+ background-color: #e8efd1;
+ font-size: 100%;
+}
+h1 {
+ font-size: 480%;
+ color: #244c0c;
+ text-align: center;
+}
+h2 {
+ font-size: 240%;
+ color: #244c0c;
+ margin-bottom: 20px;
+}
+h3 {
+ font-size: 160%;
+ color: #244c0c;
+ margin-bottom: 10px;
+}
+h4 {
+ font-size: 120%;
+ color: #244c0c;
+ margin-bottom: 0;
+}
+h5 {
+ font-size: 100%;
+ color: #244c0c;
+ margin-bottom: 0;
+}
+h6 {
+ font-size: 80%;
+ color: #244c0c;
+ margin-bottom: 0;
+}
+p {
+ color: #244c0c;
+}
+ul,ol,li,td {
+ color: #244c0c;
+}
+a:link {
+ color: #2f7404;
+ font-weight: bold;
+ text-decoration:underline;
+}
+a:visited {
+ color: #379000;
+ font-weight:bold;
+ text-decoration:underline;
+}
+a:active,
+a:hover {
+ color: #bd5a35;
+ text-decoration:underline;
+}
+
+table a:link {
+ color: #244c0c;
+ font-weight: bold;
+ text-decoration:none;
+}
+table a:visited {
+ color: #546122;
+ font-weight:bold;
+ text-decoration:none;
+}
+table a:active,
+table a:hover {
+ color: #bd5a35;
+ text-decoration:underline;
+}
+
+table {
+ color:#244c0c;
+ text-shadow: 1px 1px 0px #fff;
+ margin: 8px 4px 0 4px;
+ border: #ccc 1px solid;
+
+ -moz-border-radius:3px;
+ -webkit-border-radius:3px;
+ border-radius:3px;
+
+ -moz-box-shadow: 0 1px 2px #d1d1d1;
+ -webkit-box-shadow: 0 1px 2px #d1d1d1;
+ box-shadow: 0 1px 2px #d1d1d1;
+ 
+ white-space: nowrap;
+}
+table th {
+ text-align: left;
+ padding: 4px 4px 2px 5px;
+ border-top: 1px solid #f9fbf3;
+ border-bottom: 1px solid #dee5ca;
+
+ background: #bcc380;
+ background: -webkit-gradient(linear, left top, left bottom, from(#bcc380), to(#e4eac8));
+ background: -moz-linear-gradient(top, #bcc380, #e4eac8);
+
+ white-space: normal;
+}
+table th:first-child {
+ text-align: left;
+ padding-left: 5px;
+}
+table tr:first-child th:first-child {
+ -moz-border-radius-topleft:3px;
+ -webkit-border-top-left-radius:3px;
+ border-top-left-radius:3px;
+}
+table tr:first-child th:last-child {
+ -moz-border-radius-topright:3px;
+ -webkit-border-top-right-radius:3px;
+ border-top-right-radius:3px;
+}
+table tr {
+ padding: 0 5px 0 0;
+ }
+table td:first-child {
+ border-left: 0;
+}
+table td {
+ text-align: right;
+ padding: 4px 4px 2px 5px;
+ border-top: 1px solid #ffffff;
+ border-bottom: 1px solid #dee5ca;
+ border-left: 1px solid #dee5ca;
+ vertical-align: top;
+}
+table tr.even td {
+ background: #f6f6f6;
+ background: -webkit-gradient(linear, left top, left bottom, from(#f8f8f8), to(#f6f6f6));
+ background: -moz-linear-gradient(top,  #f8f8f8,  #f6f6f6);
+}
+/*table tr:last-child td {
+ border-bottom:0;
+}*/
+table tr:last-child td:first-child {
+ -moz-border-radius-bottomleft:3px;
+ -webkit-border-bottom-left-radius:3px;
+ border-bottom-left-radius:3px;
+}
+table tr:last-child td:last-child {
+ -moz-border-radius-bottomright:3px;
+ -webkit-border-bottom-right-radius:3px;
+ border-bottom-right-radius:3px;
+}
+table tr:hover td {
+ border-top: 1px solid #e8efd1;
+ border-bottom: 1px solid #99b484;
+ border-left: 1px solid #99b484;
+}
+
+/* Tables embedded in tables */
+table td table {
+ all: initial;
+}
+table td table th {
+ all: initial;
+ font-weight: bold;
+}
+
+#toTopBtn {
+  display: none;
+  position: fixed;
+  bottom: 15px;
+  right: 15px;
+  z-index: 99;
+  border: none;
+  border-radius: 10px;
+  outline: none;
+  opacity: .25;
+  background-color: white;
+  cursor: pointer;
+}
+#toTopBtn:hover {
+  opacity: .75;
+}
+
+div.chapter {
+  -moz-border-radius:8px;
+  -webkit-border-radius:8px;
+  border-radius:8px;
+  padding: 1px 5px 1px 5px;
+}
+div.chapter:hover {
+ background:#e3eac9;
+}
+
+.top-vert-align {
+  vertical-align: top;
+}
+
+.center {
+  display: block;
+  margin-left: auto;
+  margin-right: auto;
+}
+
+.shrinkable {
+  max-width: 100%;
+}
+
+.audio-player {
+ min-width: 384px;
+ max-width: 100%;
+}
+
+#title {
+ all: initial;
+ display: table;
+ margin-left: auto;
+ margin-right: auto;
+ text-shadow: 1px 1px 0px #fff;
+}
+#title tbody {
+ all: initial;
+}
+#title th {
+ all: initial;
+ display: table-cell;
+}
+#title tr {
+ all: initial;
+ display: table-row;
+ color: #244c0c
+}
+#title td {
+ all: initial;
+ /* font-family: Arial, Helvetica, sans-serif;*/
+ font-family: "Liberation Serif", Georgia, serif;
+ display: table-cell;
+ padding-bottom: 15px;
+ color: #244c0c
+}
```

### Comparing `pyaudisam-0.9.3/pyaudisam/report.py` & `pyaudisam-1.0.1/pyaudisam/report.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,2240 +1,2248 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-#
-# Copyright (C) 2021 Jean-Philippe Meuret
-#
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Submodule "report": HTML and Excel report generation from DS results
-
-import sys
-import os
-import shutil
-import re
-import pathlib as pl
-from packaging import version as pkgver
-import copy
-
-import datetime as dt
-import codecs
-
-import math
-import numpy as np
-import pandas as pd
-
-import jinja2
-import matplotlib.pyplot as plt
-import matplotlib.ticker as pltt
-# import seaborn as sb
-
-from . import log, runtime, __version__
-from .executor import Executor
-from .analyser import MCDSAnalysisResultsSet
-
-runtime.update(matplotlib=sys.modules['matplotlib'].__version__, jinja2=jinja2.__version__)
-
-logger = log.logger('ads.rep')
-
-# Actual package install dir.
-KInstDirPath = pl.Path(__file__).parent.resolve()
-
-
-def _mergeTransTables(base, update):
-
-    """Merge an 'update' translation table into a 'base' one ('update' completes or overwrites 'base').
-
-    Note: Trans. tables are dict(<lang>=dict(<source>: <translation>))
-    """
-
-    final = copy.deepcopy(base)
-    for lang in update.keys():
-        if lang not in final:
-            final[lang] = dict()
-        final[lang].update(update[lang])
-    return final
-
-
-# Base for results reports classes (abstract)
-class ResultsReport(object):
-
-    # Translation table for output documents (specialized in derived classes, merged with custom instance one).
-    DTrans = dict(en={}, fr={})
-
-    def __init__(self, resultsSet, title, subTitle, description, keywords,
-                 dCustomTrans=None, lang='en', pySources=[], tgtFolder='.', tgtPrefix='results'):
-    
-        """Ctor
-        
-        Parameters:
-        :param resultsSet: source results
-        :param title: main page title (and <title> tag in HTML header)
-        :param subTitle: main page sub-title (under the title, lower font size)
-        :param description: main page description text (under the sub-title, lower font size)
-        :param keywords: for HTML header <meta name="keywords" ...>
-        :param dCustomTrans: custom translations to complete the report standard ones,
-                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
-        :param lang: Target language for translation (only 'en' and 'fr' supported)
-        :param pySources: path-name of source files to copy in report folder and link in report
-        :param tgtFolder: target folder for the report (for _all_ generated files)
-        :param tgtPrefix: default target file name for the report
-        """
-
-        assert len(resultsSet) > 0, 'Can\'t build reports with nothing inside'
-        assert os.path.isdir(tgtFolder), 'Target folder {} doesn\'t seem to exist ...'.format(tgtFolder)
-        assert lang in ['en', 'fr'], 'Only en and fr translation supported for the moment'
-        
-        self.resultsSet = resultsSet
-        
-        self.trRunFolderCol = resultsSet.dfColTrans.loc[resultsSet.analysisClass.RunFolderColumn, lang]
-        self.dfEnColTrans = None  # EN to other languages column name translation
-
-        self.lang = lang
-        self.title = title
-        self.subTitle = subTitle
-        self.description = description
-        self.keywords = keywords
-        self.pySources = pySources
-        self.dTrans = _mergeTransTables(self.DTrans, dict() if dCustomTrans is None else dCustomTrans)
-        
-        self.tgtPrefix = tgtPrefix
-        self.tgtFolder = tgtFolder
-        
-        self.tmplEnv = None
-    
-    @staticmethod
-    def _libVersions():
-        return {'Python':sys.version.split()[0],
-                'NumPy': runtime['numpy'],
-                'Pandas': runtime['pandas'],
-                'ZOOpt': runtime['zoopt'],
-                'Matplotlib': runtime['matplotlib'],
-                'Jinja': runtime['jinja2']}
-
-    # Translate string
-    def tr(self, s):
-        return self.dTrans[self.lang].get(s, s)
-    
-    def trEnColNames(self, colNames, startsWith=None):
-        
-        """Translate EN-translated column(s) name(s) to self.lang one
-
-        Parameters:
-        :param colNames: str (1), list(N) or dict (N keys) of column names
-        :param startsWith: if not None or '', only columns starting with it will get processed
-        """
-
-        # 1. Build translation function (and translation table if needed)
-        # a. If target language is English, no translation needed
-        if self.lang == 'en':
-            def _trEnColName(cn):
-                return cn
-
-        # b. If target language is NOT English, translation is NEEDED
-        else:
-            if self.dfEnColTrans is None:
-                self.dfEnColTrans = self.resultsSet.transTable()
-                self.dfEnColTrans.set_index('en', inplace=True, drop=True)
-
-            def _trEnColName(cn):  # Assuming there's only 1 match !
-                return self.dfEnColTrans.at[cn, self.lang]
-        
-        # 2. Translate !
-        if isinstance(colNames, str):
-            trColNames = _trEnColName(colNames) \
-                         if not startsWith or colNames.startswith(startsWith) else None
-        elif isinstance(colNames, list):
-            trColNames = [_trEnColName(colName) for colName in colNames
-                          if not startsWith or colName.startswith(startsWith)]
-        elif isinstance(colNames, dict):
-            trColNames = {_trEnColName(colName): value for colName, value in colNames.items()
-                          if not startsWith or colName.startswith(startsWith)}
-        else:
-            raise NotImplementedError(f'Unsupported type {type(colNames)} for en-column names to translate spec')
-        
-        # Done
-        return trColNames
-        
-    # Output file pathname generation.
-    def targetFilePathName(self, suffix, prefix=None, tgtFolder=None):
-        
-        return os.path.join(tgtFolder or self.tgtFolder, (prefix or self.tgtPrefix) + suffix)
-    
-    def relativeRunFolderUrl(self, runFolderPath):
-
-        return os.path.relpath(runFolderPath, self.tgtFolder).replace(os.sep, '/')
-    
-    # Install needed attached files for HTML report.
-    def installAttFiles(self, attFiles):
-        
-        # Given attached files.
-        for fn in attFiles:
-            shutil.copy(KInstDirPath / 'report' / fn, self.tgtFolder)
-            
-        # Python source files.
-        for fpn in self.pySources:
-            shutil.copy(fpn, self.tgtFolder)
-    
-    # Get Jinja2 template environment for HTML reports.
-    def getTemplateEnv(self):
-        
-        # Build and configure jinja2 environment if not already done.
-        if self.tmplEnv is None:
-            self.tmplEnv = jinja2.Environment(loader=jinja2.FileSystemLoader([KInstDirPath]),
-                                              trim_blocks=True, lstrip_blocks=True)
-            # self.tmplEnv.filters.update(trace=_jcfPrint2StdOut)  # Template debugging ...
-
-        return self.tmplEnv
-    
-    def asWorkbook(self, subset=None, rebuild=False):
-
-        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
-        where each item is a named worksheet
-
-        Parameters:
-        :param subset: Selected list of data categories to include ; None = [] = all
-                       (categories in {'specs'})
-        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
-                        (not used here)
-        """
-        
-        logger.debug(f'ResultsReport.asWorkbook({subset=}, {rebuild=})')
-
-        ddfWbk = dict()
-
-        # Specs (no need to check if 'specs' in subset: we have nothing else than results specs here.
-        for spName, dfSpData in self.resultsSet.specs2Tables().items():
-            logger.info1(f'* {spName} ...')
-            ddfWbk[self.tr(spName)] = (dfSpData, True)
-
-        # Done
-        return ddfWbk
- 
-    def toExcel(self, fileName=None, engine='openpyxl', ext='.xlsx', rebuild=False):
-
-        """Export to a workbook file format (Excel, ODF, ...)
-
-        Parameters:
-        :param fileName: target file path name ; None => self.tgtFolder/self.tgtPrefix + ext
-        :param engine: Python module to use for exporting (openpyxl=default, or odf)
-        :param ext: extension of target file, if not specified in filename
-        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
-        """
-        
-        logger.debug(f'ResultsReport.toExcel({fileName=}, {ext=}, {rebuild=})')
-
-        fileName = fileName or os.path.join(self.tgtFolder, self.tgtPrefix + ext)
-        
-        with pd.ExcelWriter(fileName, engine=engine) as xlsxWriter:
-            for wstName, (dfWstData, wstIndex) in self.asWorkbook(rebuild=rebuild).items():
-                dfWstData.to_excel(xlsxWriter, sheet_name=wstName, index=wstIndex)
-            logger.info(f'Writing report to {fileName} ...')
-
-        logger.info('... done.')
-
-        return fileName
-
-    def toOpenDoc(self, fileName=None, rebuild=False):
-
-        """Export report to Open Doc worksheet format
-
-        Parameters:
-        :param fileName: target file path name ; None => self.tgtFolder/self.tgtPrefix + ext
-        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
-        """
-    
-        assert pkgver.parse(pd.__version__).release >= (1, 1), \
-               'Don\'t know how to write to OpenDoc format before Pandas 1.1'
-        
-        return self.toExcel(fileName, engine='odf', ext='.ods', rebuild=rebuild)
-
-    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
-    # in the "one analysis at a time" case.
-    # (sort, convert units, round values, and style).
-    # To be specialized in derived classes (here, we do nothing) !
-    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
-    # Return a pd.DataFrame.Styler
-    def finalFormatEachAnalysisData(self, dfTrData, sort=True, convert=True, round_=True, style=True):
-        
-        return dfTrData.style  # Nothing done here, specialize in derived class if needed.
-
-    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
-    # in the "all analyses at once" case.
-    # (sort, convert units, round values, and style).
-    # To be specialized in derived classes (here, we do nothing) !
-    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
-    # Return a pd.DataFrame.Styler
-    def finalFormatAllAnalysesData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
-        
-        logger.debug(f'ResultsReport.finalFormatAllAnalysesData({sort=}, {indexer=}, {convert=}, {round_=}, {style=})')
-
-        return dfTrData.style  # Nothing done here, specialize in derived class if needed.
-
-
-# DS results reports class (Excel and HTML, targeting similar layout as in Distance 6+)
-class DSResultsDistanceReport(ResultsReport):
-
-    # Translation table.
-    DTrans = _mergeTransTables(base=ResultsReport.DTrans,
-        update=dict(en={'RunFolder': 'Analysis', 'Synthesis': 'Synthesis',
-                        'Details': 'Details', 'Traceability': 'Traceability',
-                        'Table of contents': 'Table of contents',
-                        'Click on analysis # for details': 'Click on analysis number to get to detailed report',
-                        'Main results': 'Results: main figures',
-                        'Detailed results': 'Results: all details',
-                        'Analysis': 'Analysis',
-                        'Download Excel': 'Download as Excel(TM) file',
-                        'Summary computation log': 'Summary computation log',
-                        'Detailed computation log': 'Detailed computation log',
-                        'Previous analysis': 'Previous analysis', 'Next analysis': 'Next analysis',
-                        'Back to top': 'Back to global report',
-                        'Observation model': 'Observations (fitted)',
-                        'Real observations': 'Observations (sampled)',
-                        'Fixed bin distance histograms': 'Fixed bin distance histograms',
-                        'Distance': 'Distance', 'Number of observations': 'Number of observations',
-                        'Page generated': 'Generated', 'with': 'with',
-                        'with icons from': 'with icons from',
-                        'and': 'and', 'in': 'in', 'sources': 'sources', 'on': 'on',
-                        'Point': 'Point transect', 'Line': 'Line transect',
-                        'Radial': 'Radial distance', 'Perpendicular': 'Perpendicular distance',
-                        'Radial & Angle': 'Radial distance & Angle',
-                        'Clustering': 'With clustering', 'No clustering': 'No clustering',
-                        'Meter': 'Meter', 'Kilometer': 'Kilometer', 'Mile': 'Mile',
-                        'Inch': 'Inch', 'Feet': 'Feet', 'Yard': 'Yard', 'Nautical mile': 'Nautical mile',
-                        'Hectare': 'Hectare', 'Acre': 'Acre', 'Sq. Meter': 'Sq. Meter',
-                        'Sq. Kilometer': 'Sq. Kilometer', 'Sq. Mile': 'Sq. Mile',
-                        'Sq. Inch': 'Sq. Inch', 'Sq. Feet': 'Sq. Feet', 'Sq. Yard': 'Sq. Yard',
-                        'Sq. Nautical mile': 'Sq. Nautical mile',
-                        'Traceability tech. details':
-                          'Traceability data (more technical details on how this report was produced)',
-                        'Order': 'Order', 'Qual Bal': 'Qual Bal', 'Pre-selection': 'Pre-selection'},
-                    fr={'DossierExec': 'Analyse', 'Synthesis': 'Synthèse',
-                        'Details': 'Détails', 'Traceability': 'Traçabilité',
-                        'Table of contents': 'Table des matières',
-                        'Click on analysis # for details':
-                          "Cliquer sur le numéro de l'analyse pour accéder au rapport détaillé",
-                        'Main results': 'Résultats : indicateurs principaux',
-                        'Detailed results': 'Résultats : tous les détails',
-                        'Analysis': 'Analyse',
-                        'Download Excel': 'Télécharger le classeur Excel (TM)',
-                        'Summary computation log': 'Résumé des calculs',
-                        'Detailed computation log': 'Détail des calculs',
-                        'Previous analysis': 'Analyse précédente', 'Next analysis': 'Analyse suivante',
-                        'Back to top': 'Retour au rapport global',
-                        'Observation model': 'Observations (fitted)',  # No actual translation for plots
-                        'Real observations': 'Observations (sampled)',  # Idem
-                        'Fixed bin distance histograms': 'Fixed bin distance histograms',  # Idem
-                        'Distance': 'Distance',  # Idem
-                        'Number of observations': 'Number of observations',  # Idem
-                        'Page generated': 'Généré', 'with': 'avec',
-                        'with icons from': 'avec les pictogrammes de',
-                        'and': 'et', 'in': 'dans', 'sources': 'sources', 'on': 'le',
-                        'Point': 'Point fixe', 'Line': 'Transect',
-                        'Radial': 'Distance radiale', 'Perpendicular': 'Distance perpendiculaire',
-                        'Radial & Angle': 'Distance radiale & Angle',
-                        'Clustering': 'Avec clustering', 'No clustering': 'Sans clustering',
-                        'Meter': 'Mètre', 'Kilometer': 'Kilomètre', 'Mile': 'Mile',
-                        'Inch': 'Pouce', 'Feet': 'Pied', 'Yard': 'Yard', 'Nautical mile': 'Mille marin',
-                        'Hectare': 'Hectare', 'Acre': 'Acre', 'Sq. Meter': 'Mètre carré',
-                        'Sq. Kilometer': 'Kilomètre carré', 'Sq. Mile': 'Mile carré',
-                        'Sq. Inch': 'Pouce carré', 'Sq. Feet': 'Pied carré', 'Sq. Yard': 'Yard carré',
-                        'Sq. Nautical mile': 'Mille marin carré',
-                        'Traceability tech. details':
-                          'Données de traçabilité (autres détails techniques sur comment ce rapport a été produit)',
-                        'Order': 'Ordre', 'Qual Bal': 'Qual Equi', 'Pre-selection': 'Pré-selection'}))
-
-    @staticmethod
-    def noDupColumns(cols, log=True, head='Results cols'):
-
-        """Drop duplicates from a column list, and possibly warn about which"""
-
-        dups = None
-        if isinstance(cols, list):
-            dups = [col for ind, col in enumerate(cols) if col in cols[:ind]]
-            if len(dups) > 0:
-                cols = [col for ind, col in enumerate(cols) if col not in cols[:ind]]
-
-        elif isinstance(cols, pd.MultiIndex):
-            dups = cols[cols.duplicated()]
-            if len(dups) > 0:
-                cols = cols.drop_duplicates()
-
-        if log and dups is not None and len(dups) > 0:
-            logger.warning(head + ': Dropped {} duplicate(s) {}'
-                                  .format(len(dups), ', '.join(str(dup) for dup in dups)))
-
-        return cols
-
-    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                 synthCols=None, sortCols=None, sortAscend=None, dCustomTrans=None, lang='en',
-                 plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
-                 plotLineWidth=2, plotDotWidth=6, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
-                 pySources=[], tgtFolder='.', tgtPrefix='results', logProgressEvery=5):
-                       
-        """Ctor
-        
-        Parameters:
-        :param resultsSet: source results
-        :param title: main page title (and <title> tag in HTML header)
-        :param subTitle: main page sub-title (under the title, lower font size)
-        :param description: main page description text (under the sub-title, lower font size)
-        :param anlysSubTitle: analysis pages title
-        :param keywords: for HTML header <meta name="keywords" ...>
-        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
-        :param sortCols: sorting columns for report tables
-        :param sortAscend: sorting order for report tables, as a bool or list of bools, of len(synthCols)
-        :param dCustomTrans: custom translations to complete the report standard ones,
-                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
-        :param lang: Target language for translation
-        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
-        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
-        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
-        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
-        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
-        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
-        :param pySources: path-name of source files to copy in report folder and link in report
-        :param tgtFolder: target folder for the report (for _all_ generated files)
-        :param tgtPrefix: default target file name for the report
-        :param logProgressEvery: every such nb of details pages, log some elapsed time stats
-                                 and end of generation forecast
-        """
-    
-        assert synthCols is None or isinstance(synthCols, list) or isinstance(synthCols, pd.MultiIndex), \
-               'Synthesis columns must be specified as None (all), or as a list of tuples, or as a pandas.MultiIndex'
-        
-        assert logProgressEvery > 0, 'logProgressEvery must be positive'
-
-        super().__init__(resultsSet, title, subTitle, description, keywords,
-                         dCustomTrans=dCustomTrans, lang=lang, pySources=pySources,
-                         tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
-        
-        self.synthCols = self.noDupColumns(synthCols, head='Synthesis columns')
-        self.sortCols = self.noDupColumns(sortCols, head='Sorting columns')
-        self.sortAscend = sortAscend
-
-        assert sortAscend is None or isinstance(sortAscend, bool) or len(sortAscend) == len(self.sortCols), \
-               'Some duplicated sort columns were removed, or sortAscend is too long or short, ' \
-               'such that sortAscend and sortCols are not compatible => please fix these params'
-        
-        self.plotImgFormat = plotImgFormat
-        self.plotImgSize = plotImgSize
-        self.plotImgQuality = plotImgQuality
-        self.plotLineWidth = plotLineWidth
-        self.plotDotWidth = plotDotWidth
-        self.plotFontSizes = plotFontSizes
-
-        self.anlysSubTitle = anlysSubTitle
-
-        self.logProgressEvery = logProgressEvery
-        
-    # Static attached files for HTML report.
-    AttachedFiles = ['report.css', 'fa-feather-alt.svg', 'fa-angle-up.svg', 'fa-file-excel.svg',
-                     'fa-file-excel-hover.svg', 'fa-arrow-left-hover.svg', 'fa-arrow-left.svg',
-                     'fa-arrow-right-hover.svg', 'fa-arrow-right.svg',
-                     'fa-arrow-up-hover.svg', 'fa-arrow-up.svg']
-    
-    # Plot ... data to be plot, and draw resulting figure to image files.
-    PlotImgPrfxQqPlot = 'qqplot'
-    PlotImgPrfxDetProb = 'detprob'
-    PlotImgPrfxProbDens = 'probdens'
-    PlotImgPrfxDistHist = 'disthist'
-    StripPlotAlpha, StripPlotJitter = 0.5, 0.3
-    RefDistHistBinWidths = [10, 20, 40]  # unit = Distance unit
-    HistBinWithRefDist = 600  # unit = Distance unit
-    
-    def generatePlots(self, plotsData, tgtFolder, rebuild=True, sDistances=None, lang='en',
-                      imgFormat='png', imgSize=(640, 400), imgQuality=90, grid=True, transparent=False,
-                      colors=dict(background='#f9fbf3', histograms='blue',
-                                  multihistograms=['blue', 'green', 'red'], curves='red', dots='green'),
-                      widths=dict(lines=2, dots=6), fontSizes=dict(title=11, axes=10, ticks=9, legend=10)):
-        
-        dPlots = dict()
-
-        # For each plot from extracted plotsData, 
-        for title, pld in plotsData.items():
-            
-            # a. Determine target image file name, and if rebuild not forced, don't regenerate it if already there
-            if 'Qq-plot' in title:
-                tgtFileName = self.PlotImgPrfxQqPlot
-            elif 'Detection Probability' in title:
-                sufx = title.split(' ')[-1]  # Assume last "word" is the plot number
-                sufx = sufx if sufx.isnumeric() else ''  # But when only 1, there's no number.
-                tgtFileName = self.PlotImgPrfxDetProb + sufx
-            elif 'Pdf' in title:
-                sufx = title.split(' ')[-1]  # Assume last "word" is the plot number
-                sufx = sufx if sufx.isnumeric() else ''  # But when only 1, there's no number.
-                tgtFileName = self.PlotImgPrfxProbDens + sufx
-            else:
-                raise NotImplementedError(f'Unsupported plot "{title}" found in loaded plot data')
-            tgtFileName = tgtFileName + '.' + imgFormat.lower()
-
-            dPlots[title] = tgtFileName  # Save image URL
-
-            tgtFilePathName = os.path.join(tgtFolder, tgtFileName)                
-            if not rebuild and os.path.isfile(tgtFilePathName):
-                continue  # Already done, and not to be done again.
-
-            # b. Create the target figure and one-only subplot (note: QQ plots with forced height square shape).
-            figHeight = imgSize[1] / plt.rcParams['figure.dpi']
-            figWidth = figHeight if 'Qq-plot' in title else imgSize[0] / plt.rcParams['figure.dpi']
-
-            fig = plt.figure(figsize=(figWidth, figHeight))
-            axes = fig.subplots()
-            
-            # c. Plot a figure from the plot data (3 possible types, from title).
-            if 'Qq-plot' in title:
-                
-                n = len(pld['dataRows'])
-                df2Plot = pd.DataFrame(data=pld['dataRows'],
-                                       columns=[self.tr(s) for s in ['Observation model', 'Real observations']],
-                                       index=np.linspace(0.5/n, 1.0-0.5/n, n))
-                
-                df2Plot.plot(ax=axes, zorder=10, color=[colors['histograms'], colors['curves']],
-                             linewidth=widths['lines'], grid=grid,
-                             xlim=(pld['xMin'], pld['xMax']), ylim=(pld['yMin'], pld['yMax']))
-
-            elif 'Detection Probability' in title:
-                
-                # if sDistances is not None:
-                #     axes2 = axes.twinx()
-                #     sb.stripplot(ax=axes2, zorder=5, x=sDistances, color=colors['dots'], size=widths['dots'],
-                #                  alpha=self.StripPlotAlpha, jitter=self.StripPlotJitter)
-
-                df2Plot = pd.DataFrame(data=pld['dataRows'], 
-                                       columns=[pld['xLabel'], pld['yLabel'] + ' (sampled)',
-                                                pld['yLabel'] + ' (fitted)'])
-                df2Plot.set_index(pld['xLabel'], inplace=True)
-                
-                df2Plot.plot(ax=axes, zorder=10, color=[colors['histograms'], colors['curves']],
-                             linewidth=widths['lines'], grid=grid,
-                             xlim=(pld['xMin'], pld['xMax']), ylim=(pld['yMin'], pld['yMax']))
-                
-                aMTicks = axes.get_xticks()
-                axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))
-                axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)
-                axes.grid(True, which='minor', zorder=0)
-
-            elif 'Pdf' in title:
-                
-                # if sDistances is not None:
-                #     axes2 = axes.twinx()
-                #     sb.stripplot(ax=axes2, zorder=5, x=sDistances, color=colors['dots'], size=widths['dots'],
-                #                  alpha=self.StripPlotAlpha, jitter=self.StripPlotJitter)
-
-                df2Plot = pd.DataFrame(data=pld['dataRows'], 
-                                       columns=[pld['xLabel'], pld['yLabel'] + ' (sampled)',
-                                                pld['yLabel'] + ' (fitted)'])
-                df2Plot.set_index(pld['xLabel'], inplace=True)
-                
-                df2Plot.plot(ax=axes, zorder=10, color=[colors['histograms'], colors['curves']],
-                             linewidth=widths['lines'], grid=grid,
-                             xlim=(pld['xMin'], pld['xMax']), ylim=(pld['yMin'], pld['yMax']))
-
-                aMTicks = axes.get_xticks()
-                axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))
-                axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)
-                axes.grid(True, which='minor', zorder=0)
-
-            else:
-                raise NotImplementedError(f'Unsupported plot "{title}" found in loaded plot data')
-
-            # d. Finish plotting.
-            axes.legend(df2Plot.columns, fontsize=fontSizes['legend'])
-            axes.set_title(label=pld['title'] + ' : ' + pld['subTitle'],
-                           fontdict=dict(fontsize=fontSizes['title']), pad=10)
-            axes.set_xlabel(pld['xLabel'], fontsize=fontSizes['axes'])
-            axes.set_ylabel(pld['yLabel'], fontsize=fontSizes['axes'])
-            axes.tick_params(axis='both', labelsize=fontSizes['ticks'])
-            axes.grid(True, which='major', zorder=0)
-            if not transparent:
-                axes.set_facecolor(colors['background'])
-                fig.patch.set_facecolor(colors['background'])
-                
-            # e. Generate an image file for the plot figure (forcing the specified patch background color).
-            fig.tight_layout()
-            pilArgs = dict(quality=imgQuality) if imgFormat == 'jpg' else dict()
-            fig.savefig(tgtFilePathName, bbox_inches='tight', transparent=transparent,
-                        facecolor=axes.figure.get_facecolor(), edgecolor='none', pil_kwargs=pilArgs)
-
-            # g. Memory cleanup (does not work in interactive mode ... but OK thanks to plt.ioff above)
-            axes.clear()
-            fig.clear()
-            plt.close(fig)
-
-        # Standard fixed-bin-width super-imposed histograms (multiple bin width, scaled with distMax)
-        if sDistances is not None:
-
-            # a. Determine target image file name, and if rebuild not forced, don't regenerate it if already there
-            title = 'Standard Distance Histograms'
-            tgtFileName = self.PlotImgPrfxDistHist
-            tgtFileName = tgtFileName + '.' + imgFormat.lower()
-
-            dPlots[title] = tgtFileName  # Save image URL
-
-            tgtFilePathName = os.path.join(tgtFolder, tgtFileName)                
-            if rebuild or not os.path.isfile(tgtFilePathName):  # Do it only if forced to do or not already done.
-
-                # b. Create the target figure and one-only subplot
-                figHeight = imgSize[1] / plt.rcParams['figure.dpi']
-                figWidth = imgSize[0] / plt.rcParams['figure.dpi']
-
-                fig = plt.figure(figsize=(figWidth, figHeight))
-                axes = fig.subplots()
-                    
-                # c. Plot the figure from the distance data
-                # axes2 = axes.twinx()
-                # sb.stripplot(ax=axes2, zorder=5, x=sDistances, color=colors['dots'], size=widths['dots'],
-                #              alpha=self.StripPlotAlpha, jitter=self.StripPlotJitter)
-
-                distMax = sDistances.max()
-
-                distHistBinWidths = np.array(self.RefDistHistBinWidths, dtype=float)
-                distHistBinWidths *= 2 ** int(distMax / self.HistBinWithRefDist)
-
-                for binWidthInd, binWidth in enumerate(distHistBinWidths):
-                
-                    aDistBins = np.linspace(start=0, stop=binWidth * int(distMax / binWidth),
-                                            num=1 + int(distMax / binWidth)).tolist()
-                    if aDistBins[-1] < distMax:
-                        aDistBins.append(distMax)
-
-                    binWidthRInd = len(distHistBinWidths) - binWidthInd - 1
-                    sDistances.plot.hist(ax=axes, bins=aDistBins, fill=None, linewidth=1,
-                                         zorder=10*(1+binWidthRInd), rwidth=1-0.15*binWidthRInd,
-                                         edgecolor=colors['multihistograms'][binWidthInd])
-
-                axes.set_xlim((0, distMax))
-                axes.grid(True, which='minor', zorder=0)
-                aMTicks = axes.get_xticks()
-                axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)
-                axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))
-                axes.yaxis.set_major_locator(pltt.MaxNLocator(integer=True))
-
-                # d. Finish plotting.
-                axes.legend([self.tr('Real observations') + ' ' + str(int(binWidth))
-                             for binWidth in distHistBinWidths], fontsize=fontSizes['legend'])
-                axes.set_title(label=self.tr('Fixed bin distance histograms'),
-                               fontdict=dict(fontsize=fontSizes['title']), pad=10)
-                axes.set_xlabel(self.tr('Distance'), fontsize=fontSizes['axes'])
-                axes.set_ylabel(self.tr('Number of observations'), fontsize=fontSizes['axes'])
-                axes.tick_params(axis='both', labelsize=fontSizes['ticks'])
-                axes.grid(True, which='major', zorder=0)
-                if not transparent:
-                    axes.set_facecolor(colors['background'])
-                    fig.patch.set_facecolor(colors['background'])
-                    
-                # e. Generate an image file for the plot figure (forcing the specified patch background color).
-                fig.tight_layout()
-                pilArgs = dict(quality=imgQuality) if imgFormat == 'jpg' else dict()
-                fig.savefig(tgtFilePathName, bbox_inches='tight', transparent=transparent,
-                            facecolor=axes.figure.get_facecolor(), edgecolor='none', pil_kwargs=pilArgs)
-
-                # f. Memory cleanup (does not work in interactive mode ... but OK thanks to plt.ioff above)
-                axes.clear()
-                fig.clear()
-                plt.close(fig)
-
-        return dPlots
-    
-    # Top page
-    def toHtmlAllAnalyses(self, rebuild=False):
-        
-        logger.debug(f'DSResultsDistanceReport.toHtmlAllAnalyses({rebuild=})')
-        logger.info('Top page ...')
-        
-        # 1. Generate post-processed and translated synthesis table.
-        # a. Add run folder column if not selected (will serve to generate the link to the analysis detailed report)
-        synCols = self.synthCols
-        if self.resultsSet.analysisClass.RunFolderColumn not in synCols:
-            synCols += [self.resultsSet.analysisClass.RunFolderColumn]
-        dfSyn = self.resultsSet.dfTransData(self.lang, columns=synCols)
-        
-        # b. Links to each analysis detailed report.
-        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSyn))), 1))
-        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
-
-        def numNavLink(sAnlys):
-            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
-       
-        # c. Post-format as specified in actual class.
-        dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=numNavLink,
-                                                 convert=True, round_=True, style=True)
-
-        # 2. Generate post-processed and translated detailed table.
-        dfDet = self.resultsSet.dfTransData(self.lang)
-
-        # a. Add run folder column if not there (will serve to generate the link to the analysis detailed report)
-        detTrCols = list(dfDet.columns)
-        if self.trRunFolderCol not in detTrCols:
-            detTrCols += [self.trRunFolderCol]
-        dfDet = dfDet.reindex(columns=detTrCols)
-       
-        # b. Links to each analysis detailed report.
-        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=numNavLink,
-                                                 convert=False, round_=False, style=True)
-
-        # 3. Generate traceability infos parts (results specs).
-        ddfTrc = self.asWorkbook(subset=['specs'])
-
-        # 4.Generate top report page.
-        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
-        tmpl = self.getTemplateEnv().get_template('mcds/top.htpl')
-        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
-        html = tmpl.render(synthesis=dfsSyn.render(),  # escape=False, index=False),
-                           details=dfsDet.render(),  # escape=False, index=False),
-                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
-                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
-                           title=self.title, subtitle=self.subTitle,
-                           description=self.description, keywords=self.keywords,
-                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang], 
-                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
-                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(),
-                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
-                           areaUnit=self.tr(self.resultsSet.areaUnit),
-                           surveyType=self.tr(self.resultsSet.surveyType),
-                           distanceType=self.tr(self.resultsSet.distanceType),
-                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'))
-        html = re.sub('(?:[ \t]*\\\n){2,}', '\n'*2, html)  # Cleanup blank line series to one only.
-
-        # Write top HTML to file.
-        htmlPathName = self.targetFilePathName(suffix='.html')
-        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
-            tgtFile.write(html)
-
-        return htmlPathName
-    
-    def getRawTransData(self, **kwargs):
-
-        """Retrieve input translated raw data for HTML pages specific to each analysis
-
-        Parameters:
-        :param kwargs: Args relevant to derived classes (none here).
-
-        :return: 2 dataFrames, for synthesis (synthCols) and detailed (all) column sets,
-                 + None (other implementations may use this place for something relevant)
-        """
-
-        # Generate translated synthesis table.
-        synthCols = self.synthCols
-        if self.resultsSet.analysisClass.RunFolderColumn not in synthCols:
-            synthCols += [self.resultsSet.analysisClass.RunFolderColumn]
-        dfSynthRes = self.resultsSet.dfTransData(self.lang, columns=synthCols)
-
-        # Generate translated detailed table.
-        dfDetRes = self.resultsSet.dfTransData(self.lang)
-
-        # Side check as soon as possible : Are all report-needed columns available ?
-        self.checkNeededColumns()
-
-        return dfSynthRes, dfDetRes, None
-
-    def toHtmlEachAnalysis(self, rebuild=False, generators=0, topSuffix='.html', **kwargs):
-        
-        """Generate HTML page specific to each analysis
-
-        Parameters:
-        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
-                        otherwise, use any cached data or existing plot image file
-        :param generators: Number of parallel (process) generators to use :
-                           - 0 => auto-number, based on the actual number of CPUs onboard,
-                           - > 0 => the actual number to use
-        :param kwargs: Other args relevant to derived classes.
-        """
-
-        # Get source translated raw data to format
-        dfSynthRes, dfDetRes, _ = self.getRawTransData(rebuild=rebuild, **kwargs)
-
-        # Generate translated synthesis and detailed tables.
-        logger.info(f'Analyses pages ({len(dfSynthRes)}) ...')
-        executor = Executor(processes=generators)
-        nExpWorkers = executor.expectedWorkers()
-        if nExpWorkers > 1:
-            logger.info(f'... through at most {nExpWorkers} parallel generators ...')
-
-        # 1. 1st pass : Generate previous / next list (for navigation buttons) with the sorted order if any
-        dfSynthRes = self.finalformatEachAnalysisData(dfSynthRes, sort=True, indexer=True,
-                                                      convert=False, round_=False, style=False).data
-        sCurrUrl = dfSynthRes[self.trRunFolderCol]
-        sCurrUrl = sCurrUrl.apply(lambda path: self.targetFilePathName(tgtFolder=path, prefix='index', suffix='.html'))
-        sCurrUrl = sCurrUrl.apply(lambda path: os.path.relpath(path, self.tgtFolder).replace(os.sep, '/'))
-        dfAnlysUrls = pd.DataFrame(dict(current=sCurrUrl, previous=np.roll(sCurrUrl, 1), next=np.roll(sCurrUrl, -1)))
-
-        # And don't forget to sort & index detailed results the same way as synthesis ones.
-        dfDetRes = self.finalformatEachAnalysisData(dfDetRes, sort=True, indexer=True,
-                                                    convert=False, round_=False, style=False).data
-
-        # 2. 2nd pass : Generate
-        # a. Stops heavy Matplotlib.pyplot memory leak in generatePlots (WTF !?)
-        wasInter = plt.isinteractive()
-        if wasInter:
-            plt.ioff()
-
-        # b. Generate analysis detailed HTML page, for each analysis, parallely.
-        topHtmlPathName = self.targetFilePathName(suffix=topSuffix)
-        trCustCols = [col for col in self.resultsSet.transCustomColumns(self.lang) if col in dfDetRes.columns]
-        
-        # i. Start generation of all pages in parallel (unless specified not)
-        genStart = pd.Timestamp.now()  # Start of elapsed time measurement.
-        pages = dict()
-        for lblRes in dfSynthRes.index:
-            
-            logger.info1(f'#{lblRes}/{len(dfSynthRes)}: '
-                         + ' '.join(f'{k}={v}' for k, v in dfDetRes.loc[lblRes, trCustCols].iteritems()))
-
-            pgFut = executor.submit(self._toHtmlAnalysis, lblRes, dfSynthRes.loc[lblRes],
-                                    dfDetRes.loc[lblRes], dfAnlysUrls.loc[lblRes],
-                                    topHtmlPathName, trCustCols, rebuild=rebuild)
-                                    
-            pages[pgFut] = lblRes
-        
-        if executor.isParallel():
-            logger.info1(f'Waiting for generators results ...')
-        
-        # ii. Wait for end of generation of each page, as it comes first.
-        nDone = 0
-        for pgFut in executor.asCompleted(pages):
-
-            # If there, it's because it's done (or crashed) !
-            exc = pgFut.exception()
-            if exc:
-                logger.error(f'#{pages[pgFut]}: Exception: {exc}')
-            elif executor.isParallel():
-                logger.info1(f'#{pages[pgFut]}: Done.')
-
-            # Report elapsed time and number of pages completed until now (once per self.logProgressEvery pages).
-            nDone += 1
-            if nDone % self.logProgressEvery == 0 or nDone == len(pages):
-                now = pd.Timestamp.now()
-                elapsedTilNow = now - genStart
-                expectedEnd = now
-                if nDone < len(pages):
-                    expectedEnd += pd.Timedelta(elapsedTilNow.value * (len(pages) - nDone) / nDone)
-                logger.info1('{}/{} pages in {} (mean {:.2f}s){}'
-                             .format(nDone, len(pages), str(elapsedTilNow.round('S')).replace('0 days ', ''),
-                                     elapsedTilNow.total_seconds() / nDone,
-                                     ': done.' if nDone == len(pages)
-                                               else ': should end around ' + expectedEnd.strftime('%Y-%m-%d %H:%M:%S')
-                                                                             .replace(now.strftime('%Y-%m-%d '), '')))
-
-        # iii. Terminate parallel executor.
-        executor.shutdown()
-
-        # c. Restore Matplotlib.pyplot interactive mode as it was before.
-        if wasInter:
-            plt.ion()
-
-    def _toHtmlAnalysis(self, lblRes, sSynthRes, sDetRes, sResNav, topHtmlPathName, trCustCols, rebuild=True):
-
-        # Postprocess synthesis table :
-        dfSyn = pd.DataFrame([sSynthRes])
-        idxFmt = '{{:0{}d}}'.format(1+max(int(math.log10(len(dfSyn))), 1))
-        dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-        dfSyn.index = dfSyn.index.map(lambda n: idxFmt.format(n))
-        dfsSyn = self.finalformatEachAnalysisData(dfSyn, sort=False, indexer=None,
-                                                  convert=True, round_=True, style=True)
-        
-        # Post-process detailed table :
-        dfDet = pd.DataFrame([sDetRes])
-        dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-        dfDet.index = dfDet.index.map(lambda n: idxFmt.format(n))
-        dfsDet = self.finalformatEachAnalysisData(dfDet, sort=False, indexer=None,
-                                                  convert=False, round_=False, style=True)
-        
-        # Generate analysis report page.
-        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
-        subTitle = '{} {} : {{}}'.format(self.tr('Analysis'), idxFmt).format(lblRes, self.anlysSubTitle)
-        engineClass = self.resultsSet.engineClass
-        anlysFolder = sDetRes[self.trRunFolderCol]
-        tmpl = self.getTemplateEnv().get_template('mcds/anlys.htpl')
-        html = tmpl.render(synthesis=dfsSyn.render(),
-                           details=dfsDet.render(),
-                           log=engineClass.decodeLog(anlysFolder),
-                           output=engineClass.decodeOutput(anlysFolder),
-                           plots=self.generatePlots(plotsData=engineClass.decodePlots(anlysFolder), 
-                                                    sDistances=engineClass.loadDataFile(anlysFolder).DISTANCE,
-                                                    tgtFolder=anlysFolder, lang='en',  # No translation.
-                                                    imgFormat=self.plotImgFormat, imgSize=self.plotImgSize,
-                                                    imgQuality=self.plotImgQuality,
-                                                    widths=dict(lines=self.plotLineWidth, dots=self.plotDotWidth),
-                                                    colors=dict(background='#f9fbf3', histograms='blue',
-                                                                multihistograms=['blue', 'green', 'red'],
-                                                                curves='red', dots='green'),
-                                                    fontSizes=self.plotFontSizes,
-                                                    rebuild=rebuild),
-                           title=self.title, subtitle=subTitle, keywords=self.keywords,
-                           navUrls=dict(prevAnlys='../' + sResNav.previous,
-                                        nextAnlys='../' + sResNav.next,
-                                        back2Top='../' + os.path.basename(topHtmlPathName)),
-                           tr=self.dTrans[self.lang], pySources=[pl.Path(fpn).name for fpn in self.pySources],
-                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(), 
-                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
-                           areaUnit=self.tr(self.resultsSet.areaUnit),
-                           surveyType=self.tr(self.resultsSet.surveyType),
-                           distanceType=self.tr(self.resultsSet.distanceType),
-                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'))
-        html = re.sub('(?:[ \t]*\\\n){2,}', '\n'*2, html)  # Cleanup blank line series to one only.
-
-        # Write analysis HTML to file.
-        htmlPathName = self.targetFilePathName(tgtFolder=anlysFolder, prefix='index', suffix='.html')
-        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
-            tgtFile.write(html)
-
-    def toHtml(self, rebuild=False):
-    
-        """HTML report generation.
-
-        Parameters:
-        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
-                        otherwise, use any cached data or existing plot image file
-
-        Note: Parallelism does not work for this class, hence the absence of the generators parameter.
-        """
-
-        logger.debug(f'DSResultsDistanceReport.toHtml({rebuild=})')
-
-        # Install needed attached files.
-        self.installAttFiles(self.AttachedFiles)
-            
-        # Generate synthesis report page (all analyses in one page).
-        topHtmlPathName = self.toHtmlAllAnalyses(rebuild=rebuild)
-
-        # Generate detailed report pages (one page for each analysis)
-        # Note: For some obscure reason, parallelism does not work here (while it does for derived classes !).
-        generators = 1
-        self.toHtmlEachAnalysis(rebuild=rebuild, generators=generators)
-
-        logger.info('... done.')
-        
-        return topHtmlPathName
-
-    def asWorkbook(self, subset=None, rebuild=False):
-
-        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
-        where each item is a named worksheet
-
-        Parameters:
-        :param subset: Selected list of data categories to include ; None = [] = all
-                       (categories in {'specs'})
-        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
-                        (not used here)
-        """
-        
-        logger.debug(f'DSResultsDistanceReport.asWorkbook({subset=}, {rebuild=})')
-
-        ddfWbk = dict()
-
-        # Build results worksheets if specified in subset
-        if not subset or 'results' in subset:
-
-            # Synthesis
-            logger.info1('* synthesis ...')
-            synCols = self.synthCols
-            if self.resultsSet.analysisClass.RunFolderColumn not in synCols:
-                synCols += [self.resultsSet.analysisClass.RunFolderColumn]
-            dfSyn = self.resultsSet.dfTransData(self.lang, columns=synCols)
-            dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-            
-            # ... Convert run folder columns to hyperlink
-            def toHyperlink(path):
-                return '=HYPERLINK("file:///{path}", "{path}")'.format(path=path)
-            dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(toHyperlink)
-            
-            dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=True,
-                                                     convert=True, round_=True, style=True)
-
-            ddfWbk[self.tr('Synthesis')] = (dfsSyn, True)
-
-            # Details
-            logger.info1('* details ...')
-            dfDet = self.resultsSet.dfTransData(self.lang)
-            dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-            dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(toHyperlink)
-            
-            dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
-                                                     convert=False, round_=False, style=True)
-
-            ddfWbk[self.tr('Details')] = (dfsDet, True)
-
-        # Append inherited worksheets.
-        ddfWbk.update(super().asWorkbook(subset=subset, rebuild=rebuild))
-
-        # Done
-        return ddfWbk
- 
-
-# A specialized report for MCDS analyses, targetting similar layout as in Distance 6+, with actual output formatting.
-class MCDSResultsDistanceReport(DSResultsDistanceReport):
-
-    DTrans = _mergeTransTables(base=DSResultsDistanceReport.DTrans,
-        update=dict(en={'Study type:': "<strong>Study type</strong>:",
-                        'Units used:': "<strong>Units used</strong>:",
-                        'for distances': 'for distances',
-                        'for areas': 'for areas',
-                        'Estimator selection criterion:': '<strong>Adjustment term selection criterion</strong>:',
-                        'Confidence value interval:': '<strong>Confidence value interval</strong>:',
-                        'If not listed in table below, please': 'If not listed in table below, please',
-                        'BE AWARE that different values have been used among analyses':
-                          '<strong>BE AWARE</strong> that different values have been used among analyses',
-                        'note that all analyses have been run with the same value':
-                          'note that <strong>ALL</strong> analyses have been run with the same value',
-                        'see detailed table below': 'see detailed table below',
-                        'see details for each analysis': 'see details for each analysis',
-                        'Note: Some figures rounded, but not converted':
-                           "<strong>Note</strong>: Most figures have been rounded for readability,"
-                           " but 'CoefVar Density' have been further modified : converted to %",
-                        'Note: All figures untouched, as output by MCDS':
-                           "<strong>Note</strong>: All values have been left untouched,"
-                           " as output by MCDS (no rounding, no conversion)",
-                        'samples': 'Samples', 'analyses': 'Analyses', 'models': 'Models',
-                        'analyser': 'Analyser', 'runtime': 'Computing platform'},
-                    fr={'Study type:': "<strong>Type d'étude</strong>:",
-                        'Units used:': "<strong>Unités utilisées</strong>:",
-                        'for distances': 'pour les distances',
-                        'for areas': 'pour les surfaces',
-                        'Estimator selection criterion:':
-                          "<strong>Critère de sélection des termes d'ajustement</strong>:",
-                        'Confidence value interval:': '<strong>Intervalle de confiance</strong>:',
-                        'If not listed in table below, please': 'Si non présent dans la table ci-dessous,',
-                        'BE AWARE that different values have been used among analyses':
-                          'faites <strong>ATTENTION</strong>,'
-                          ' différentes valeurs ont été utilisés suivant les analyses',
-                        'note that all analyses have been run with the same value':
-                          'notez que <strong>TOUTES</strong> les analyses ont été faites avec la même valeur',
-                        'see detailed table below': 'voir table de détails ci-dessous',
-                        'see details for each analysis': 'voir détails de chaque analyse',
-                        'Note: Some figures rounded, but not converted':
-                           "<strong>N.B.</strong> Presque toutes les valeurs ont été arrondies pour la lisibilité,"
-                           " mais seul 'CoefVar Densité' a été autrement modifié : converti en %",
-                        'Note: All figures untouched, as output by MCDS':
-                           "<strong>N.B.</strong> Aucune valeur n'a été convertie ou arrondie,"
-                           " elles sont toutes telles que produites par MCDS",
-                        'samples': 'Echantillons', 'analyses': 'Analyses', 'models': 'Modèles',
-                        'analyser': 'Analyseur', 'runtime': 'Plateforme de calcul'}))
-    
-    RightTruncCol = ('encounter rate', 'right truncation distance (w)', 'Value')
-
-    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                 synthCols=None, sortCols=None, sortAscend=None, dCustomTrans=None, lang='en',
-                 plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
-                 plotLineWidth=2, plotDotWidth=5, plotFontSizes=dict(title=12, axes=10, ticks=9, legend=10),
-                 pySources=[], tgtFolder='.', tgtPrefix='results'):
-
-        """
-        Parameters:
-        :param resultsSet: source results
-        :param title: main page title (and <title> tag in HTML header)
-        :param subTitle: main page sub-title (under the title, lower font size)
-        :param description: main page description text (under the sub-title, lower font size)
-        :param anlysSubTitle: analysis pages title
-        :param keywords: for HTML header <meta name="keywords" ...>
-        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
-        :param sortCols: sorting columns for report tables
-        :param sortAscend: sorting order for report tables, as a bool or list of bools, of len(synthCols)
-        :param dCustomTrans: custom translations to complete the report standard ones,
-                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
-        :param lang: Target language for translation
-        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
-        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
-        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
-        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
-        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
-        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
-        :param pySources: path-name of source files to copy in report folder and link in report
-        :param tgtFolder: target folder for the report (for _all_ generated files)
-        :param tgtPrefix: default target file name for the report
-         """
-    
-        assert isinstance(resultsSet, MCDSAnalysisResultsSet), 'resultsSet must be a MCDSAnalysisResultsSet'
-
-        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                         synthCols=synthCols, sortCols=sortCols, sortAscend=sortAscend,
-                         dCustomTrans=dCustomTrans, lang=lang,
-                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
-                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
-                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
-        
-    # Styling colors
-    CChrGray = '#869074'
-    CBckGreen, CBckGray = '#e0ef8c', '#dae3cb'
-    CSclGreen, CSclOrange, CSclRed = '#cbef8c', '#f9da56', '#fe835a'
-    CChrInvis = '#e8efd1'  # body background
-    ScaledColors = [CSclGreen, CSclOrange, CSclRed]
-    ScaledColorsRvd = list(reversed(ScaledColors))
-    
-    DExCodeColors = dict(zip([1, 2, 3], ScaledColors))
-    
-    @classmethod
-    def colorExecCodes(cls, sCodes):
-        return ['background-color: ' + cls.DExCodeColors.get(c, cls.DExCodeColors[3]) for c in sCodes]
-    
-    @classmethod
-    def scaledColorV(cls, v, thresholds, colors):  # len(thresholds) == len(colors) - 1
-        if pd.isnull(v):
-            return cls.CBckGray
-        for ind, thresh in enumerate(thresholds):
-            if v > thresh:
-                return colors[ind]
-        return colors[-1]
-    
-    @classmethod
-    def scaledColorS(cls, sValues, thresholds, colors):
-        return ['background-color: ' + cls.scaledColorV(v, thresholds, colors) for v in sValues]
-    
-    @staticmethod
-    def isNull(o):
-        return not isinstance(o, list) and pd.isnull(o)
-    
-    @staticmethod
-    def shortenDistCuts(distCuts):
-        if isinstance(distCuts, int) or isinstance(distCuts, float):
-            return distCuts
-        short = str(distCuts)
-        if short in ['None', 'nan']:
-            return None
-        else:
-            return short.translate(str.maketrans({c: '' for c in '[] '})).replace('.0,', ',')
-
-    @staticmethod
-    def _roundNumber(v, ndigits=0):
-        """round number with built-in round function, unless NaN, which fails to int() if ndigits=0"""
-        return v if pd.isnull(v) else round(v, ndigits)
-
-    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
-    # in the "all analyses at once" case.
-    # (sort, generate index, convert units, round values, and style).
-    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
-    # Return a pd.DataFrame.Styler
-    def finalFormatAllAnalysesData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
-        
-        logger.debug(f'MCDSResultsDistanceReport.finalFormatAllAnalysesData'
-                     f'({sort=}, {indexer=}, {convert=}, {round_=}, {style=})')
-
-        # Sorting
-        df = dfTrData
-        if sort:
-        
-            # If no sorting order was specified, generate one simple one,
-            # through a temporarily sample num. column and Delta AIC column (so ... it MUST be there)
-            # (assuming analyses have been run as grouped by sample)
-            if not self.sortCols:
-            
-                # Note: Ignoring all-NaN sample id columns, for a working groupby
-                sampleIdCols = [col for col in self.resultsSet.transSampleColumns(self.lang)
-                                if col in df.columns and not df[col].isna().all()]
-                df.insert(0, column='#Sample#', value=df.groupby(sampleIdCols, sort=False).ngroup())
-
-                sortCols = ['#Sample#'] + [col for col in self.trEnColNames(['Delta AIC']) if col in df.columns]
-                sortAscend = True
-                
-            # Otherwise, use the one specified.
-            else:
-            
-                # ... after some cleaning up in case some sort columns are not present.
-                sortCols = list()
-                sortAscend = list() if isinstance(self.sortAscend, list) else self.sortAscend
-                for ind, col in enumerate(self.resultsSet.transColumns(self.sortCols, self.lang)):
-                    if col in df.columns:
-                        sortCols.append(col)
-                        if isinstance(self.sortAscend, list):
-                            sortAscend.append(self.sortAscend[ind])
-                assert not isinstance(sortAscend, list) or len(sortCols) == len(sortAscend)
-
-            # Sort
-            df.sort_values(by=sortCols, ascending=sortAscend, inplace=True)
-            
-            # Remove temporary sample num. column if no sorting order was specified
-            if not self.sortCols:
-                df.drop(columns=['#Sample#'], inplace=True)
-
-        # Standard 1 to N index + optional post-formatting (ex. for synthesis <=> details navigation).
-        if indexer:
-            df.index = range(1, len(df) + 1)
-            if callable(indexer):
-                df.index = df.apply(indexer, axis='columns')
-
-        # Converting to other units, or so.
-        if convert:
-            
-            # for col in self.trEnColNames(['Density', 'Min Density', 'Max Density']): # 'CoefVar Density',
-            #     if col in df.columns:
-            #         df[col] *= 1000000 / 10000 # ha => km2
-            
-            col = self.trEnColNames('CoefVar Density')
-            if col in df.columns:
-                kVarDens = 100.0
-                df[col] *= kVarDens  # [0, 1] => %
-            
-            for col in self.trEnColNames(['Fit Dist Cuts', 'Discr Dist Cuts']):
-                if col in df.columns:
-                    df[col] = df[col].apply(self.shortenDistCuts)
-            
-        # Reducing float precision
-        if round_:
-            
-            # Use built-in round for more accurate rounding than np.round
-            # a. Fixed list of columns: enumerate their English names.
-            dColDecimals = {**{col: 4 for col in ['Delta CoefVar Density']},
-                            **{col: 3 for col in ['Effort', 'PDetec', 'Min PDetec', 'Max PDetec']},
-                            **{col: 2 for col in ['Delta AIC', 'Chi2 P', 'KS P', 'CvM Uw P', 'CvM Cw P',
-                                                  'Density', 'Min Density', 'Max Density',
-                                                  'Qual Chi2+', 'Qual KS+', 'Qual DCv+']},
-                            **{col: 1 for col in ['AIC', 'EDR/ESW', 'Min EDR/ESW', 'Max EDR/ESW',
-                                                  'Number', 'Min Number', 'Max Number',
-                                                  'CoefVar Density', 'CoefVar Number', 'Obs Rate']},
-                            **{col: 0 for col in ['Left Trunc Dist', 'Right Trunc Dist',
-                                                  'Left Trunc', 'Right Trunc']}}
-                                                     
-            for col, dec in self.trEnColNames(dColDecimals).items():
-                if col in df.columns:
-                    df[col] = df[col].apply(self._roundNumber, ndigits=dec)
-
-            # b. Dynamic lists of columns: select their names through a startswith criterion.
-            for col in df.columns:
-                if col.startswith(self.tr('Qual Bal')):
-                    df[col] = df[col].apply(self._roundNumber, ndigits=2)
-                if col.startswith(self.tr('Order')) or col.startswith(self.tr('Pre-selection')):
-                    df[col] = df[col].apply(self._roundNumber, ndigits=0)
-            
-            # Don't use df.round ... because it does not work, at least with pandas 1.0.x up to 1.1.2 !?!?!?
-            # df = df.round(decimals={ col: dec for col, dec in self.trEnColNames(dColDecimals).items() \
-            #                                   if col in df.columns })
-            
-        # Styling
-        return self.styleAllAnalysesData(df, convert=convert, round_=round_, style=style)
-
-    @staticmethod
-    def _trimTrailingZeroesFormat(v):
-        return '' if pd.isnull(v) else format(v, 'g') if pd.api.types.is_numeric_dtype(v) else v
-
-    def styleAllAnalysesData(self, df, convert=True, round_=True, style=True):
-    
-        dfs = df.style
-        
-        if style:
-        
-            # Remove trailing (useless) zeroes in floats when rounding requested.
-            if round_:
-                dfs.format(self._trimTrailingZeroesFormat,
-                           subset=[col for col in df.columns if df[col].dtype is np.dtype('float')])
-
-            # Left align all-text columns
-            cols = [col for col in df.columns
-                    if df[col].dropna().apply(lambda v: isinstance(v, str)).all()]
-            if cols:
-                dfs.set_properties(subset=cols, **{'text-align': 'left'})
-
-            # Green background for the 0-value Delta AIC rows
-            col = self.trEnColNames('Delta AIC')
-            if col in df.columns and df[col].max() > 0:  # if all delta AIC == 0, no need to stress it.
-                dfs.set_properties(subset=pd.IndexSlice[df[df[col] == 0].index, :],
-                                   **{'background-color': self.CBckGreen})
-               
-            # Red/Orange/Green color code for exec. codes for normal codes
-            col = self.trEnColNames('ExCod')
-            if col in df.columns:
-                dfs.apply(self.colorExecCodes, subset=[col], axis='columns')
-            
-            # Red/Orange/Green color code for DCV based on thresholds
-            col = self.trEnColNames('CoefVar Density')
-            if col in df.columns:
-                kVarDens = 100.0 if convert else 1.0
-                dfs.apply(self.scaledColorS, subset=[col], axis='columns',
-                          thresholds=[v * kVarDens for v in [0.3, 0.2]], colors=self.ScaledColorsRvd)
-            
-            # Red/Orange/Green color code for DCV based on thresholds
-            col = self.trEnColNames('KS P')
-            if col in df.columns:
-                dfs.apply(self.scaledColorS, subset=[col], axis='columns',
-                          thresholds=[0.7, 0.2], colors=self.ScaledColors)
-            
-            # Red/Orange/Green color code for DCV based on thresholds
-            col = self.trEnColNames('Chi2 P')
-            if col in df.columns:
-                dfs.apply(self.scaledColorS, subset=[col], axis='columns',
-                          thresholds=[0.7, 0.2], colors=self.ScaledColors)
-            
-            # Greyed foreground for rows with bad exec codes
-            col = self.trEnColNames('ExCod')
-            if col in df.columns:
-                dfs.set_properties(subset=pd.IndexSlice[df[~df[col].isin([1, 2])].index, :],
-                                   **{'color': self.CChrGray})
-            
-            # NaN cells are set to transparent foreground / no shadow (to hide NaNs).
-            dfs.where(self.isNull, 'color: transparent').where(self.isNull, 'text-shadow: none')
-        
-        return dfs
-
-    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
-    # in the "all analyses at once" case.
-    # (sort, convert units, round values, and style).
-    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
-    # Return a pd.DataFrame.Styler
-    def finalformatEachAnalysisData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
-    
-        return self.finalFormatAllAnalysesData(dfTrData, sort=sort, indexer=indexer,
-                                               convert=convert, round_=round_, style=style)
-
-    @staticmethod
-    def float2str(v):  # Workaround to_html non transparent default float format (!?)
-        return format(v, 'g')
-
-    @staticmethod
-    def series2VertTable(ser):
-        return re.sub('\\\n *', '',  ser.to_frame().to_html(header=False,
-                                                            float_format=MCDSResultsDistanceReport.float2str,
-                                                            na_rep=''))
-    
-    def plotImageHtmlElement(self, runFolder, plotImgPrfx, plotHeight):
-        
-        if plotImgPrfx in [self.PlotImgPrfxQqPlot, self.PlotImgPrfxDistHist]:
-            plotFileName = '{}.{}'.format(plotImgPrfx, self.plotImgFormat)
-            if os.path.isfile(os.path.join(runFolder, plotFileName)):
-                return '<img src="./{}/{}" style="height: {}px" />' \
-                       .format(self.relativeRunFolderUrl(runFolder), plotFileName, plotHeight)
-        else:
-            for plotInd in range(3, 0, -1):
-                plotFileName = '{}{}.{}'.format(plotImgPrfx, plotInd, self.plotImgFormat)
-                if os.path.isfile(os.path.join(runFolder, plotFileName)):
-                    return '<img src="./{}/{}" style="height: {}px" />' \
-                           .format(self.relativeRunFolderUrl(runFolder), plotFileName, plotHeight)
-            plotFileName = '{}.{}'.format(plotImgPrfx, self.plotImgFormat)
-            if os.path.isfile(os.path.join(runFolder, plotFileName)):
-                return '<img src="./{}/{}" style="height: {}px" />' \
-                       .format(self.relativeRunFolderUrl(runFolder), plotFileName, plotHeight)
-        
-        return f'No {plotImgPrfx} plot produced'
-        
-    def asWorkbook(self, subset=None, rebuild=False):
-
-        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
-        where each item is a named worksheet
-
-        Parameters:
-        :param subset: Selected list of data categories to include ; None = [] = all
-                       (categories in {'specs'})
-        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
-                        (not used here)
-        """
-        
-        logger.debug(f'MCDSResultsDistanceReport.asWorkbook({subset=}, {rebuild=})')
-
-        ddfWbk = dict()
-
-        baseWbk = super().asWorkbook(subset=subset, rebuild=rebuild)
-
-        # Build results worksheets if specified in subset
-        if not subset or 'samples' in subset:
-
-            # Format sample list if not already there.
-            if self.tr('samples') not in baseWbk:
-
-                logger.info1('* samples ...')
-
-                # But first, relocate synthesis and details sheets before the future 'samples' sheet if present.
-                synthShName = self.tr('Synthesis')
-                if synthShName in baseWbk:
-                    ddfWbk[synthShName] = baseWbk.pop(synthShName)
-
-                detShName = self.tr('Details')
-                if detShName in baseWbk:
-                    ddfWbk[detShName] = baseWbk.pop(detShName)
-
-                # Build this missing 'samples' sheet.
-                dfSamples = self.resultsSet.listSamples().copy()
-                dfSamples.reset_index(inplace=True)
-                dfSamples.columns = self.resultsSet.transColumns(dfSamples.columns, self.lang)
-                dfSamples.set_index(self.resultsSet.transColumn(self.resultsSet.sampleIndCol, self.lang),
-                                    inplace=True)
-
-                # Add it at the end of the workbook
-                ddfWbk[self.tr('samples')] = (dfSamples, True)
-
-        # Append inherited worksheets at the end.
-        ddfWbk.update(baseWbk)
-
-        # Done
-        return ddfWbk
-
-    def toHtml(self, rebuild=False, generators=0):
-        
-        """HTML report generation.
-
-        Parameters:
-        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
-                        otherwise, use any cached data or existing plot image file
-        :param generators: Number of parallel (process) generators to use :
-                           - 0 => auto-number, based on the actual number of CPUs onboard,
-                           - > 0 => the actual number to use
-                           Note: Parallelism works well for this class, hence the default 0.
-        """
-
-        logger.debug(f'MCDSResultsDistanceReport.toHtml({rebuild=}, {generators=})')
-
-        # Install needed attached files.
-        self.installAttFiles(self.AttachedFiles)
-            
-        # Generate full report detailed pages (one for each analysis)
-        # (done first to have plot image files generated for top report page generation right below).
-        self.toHtmlEachAnalysis(rebuild=rebuild, generators=generators)
-        
-        # Generate top = main report page (one for all analyses).
-        topHtmlPathName = self.toHtmlAllAnalyses(rebuild=rebuild)
-
-        logger.info('... done.')
-                
-        return topHtmlPathName
-
-
-class MCDSResultsPreReport(MCDSResultsDistanceReport):
-
-    """A specialized pre-report for MCDS analyses, with actual output formatting.
-
-    HTML mode gives a specialized main page layout, with a super-synthesis table (with plots)
-    in place of the synthesis and detailed tables of MCDSResultsDistanceReport ;
-    detailed (linked) pages unchanged from MCDSResultsDistanceReport.
-
-    Designed for showing results of fully automatic pre-analyses, in order to give the user
-    hints about which actual analyses are to be done, and with what parameter values.
-    """
-
-    # Translation table.
-    DTrans = _mergeTransTables(base=MCDSResultsDistanceReport.DTrans,
-        update=dict(en={'Quick-view results': 'Results: the essence',
-                        'SampleParams': 'Sample & Model',
-                        'Results1': 'Results (1/2)', 'Results2': 'Results (2/2)',
-                        'DistHist': 'Standard distance histogram',
-                        'ProbDens': 'Detection probability density (PDF)',
-                        'DetProb': 'Detection probability'},
-                    fr={'Quick-view results': 'Résultats : l\'essentiel',
-                        'SampleParams': 'Echantillon & Modèle',
-                        'Results1': 'Résultats (1/2)', 'Results2': 'Résultats (2/2)',
-                        'DistHist': 'Histogramme standard des distances',
-                        'ProbDens': 'Densité de probabilité de détection (DdP)',
-                        'DetProb': 'Probabilité de détection'}))
-
-    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords, 
-                 sampleCols, paramCols, resultCols, synthCols=None,
-                 sortCols=None, sortAscend=None, dCustomTrans=None, lang='en',
-                 superSynthPlotsHeight=288, plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
-                 plotLineWidth=1, plotDotWidth=4, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
-                 pySources=[], tgtFolder='.', tgtPrefix='results'):
-
-        """Ctor
-        
-        Parameters:
-        :param resultsSet: source results
-        :param title: main page title (and <title> tag in HTML header)
-        :param subTitle: main page sub-title (under the title, lower font size)
-        :param description: main page description text (under the sub-title, lower font size)
-        :param anlysSubTitle: analysis pages title
-        :param keywords: for HTML header <meta name="keywords" ...>
-        :param sampleCols: for main page table, 1st column (top)
-        :param paramCols: for main page table, 1st column (bottom)
-        :param resultCols: for main page table, 2nd and 3rd columns
-        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
-        :param dCustomTrans: custom translations to complete the report standard ones,
-                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
-        :param lang: Target language for translation
-        :param superSynthPlotsHeight: Display height (in pixels) of the super-synthesis table plots
-        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
-        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
-        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
-        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
-        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
-        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
-        :param pySources: path-name of source files to copy in report folder and link in report
-        :param tgtFolder: target folder for the report (for _all_ generated files)
-        :param tgtPrefix: default target file name for the report
-        """
-
-        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                         synthCols=synthCols, sortCols=sortCols, sortAscend=sortAscend,
-                         dCustomTrans=dCustomTrans, lang=lang,
-                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
-                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
-                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
-        
-        self.sampleCols = self.noDupColumns(sampleCols, head='Sample columns')
-        self.paramCols = self.noDupColumns(paramCols, head='Parameter columns')
-        self.resultCols = self.noDupColumns(resultCols, head='Result columns')
-        self.superSynthPlotsHeight = superSynthPlotsHeight
-
-    def checkNeededColumns(self):
-
-        """Side check as soon as possible : Are all report needed columns available ?
-        (now that computed columns have been ... post-computed through self.resultsSet.dfFilSorData calls)
-
-        :raise: AssertionError if not the case
-        """
-
-        assert all(col in self.resultsSet.columns for col in self.sampleCols), \
-               'Missing super-synthesis sample columns in resultsSet: {}' \
-               .format(', '.join('|'.join(col) for col in self.sampleCols if col not in self.resultsSet.columns))
-        assert all(col in self.resultsSet.columns for col in self.paramCols), \
-               'Missing super-synthesis parameters columns in resultsSet: {}' \
-               .format(', '.join('|'.join(col) for col in self.paramCols if col not in self.resultsSet.columns))
-        assert all(col in self.resultsSet.columns for col in self.resultCols), \
-               'Missing super-synthesis results columns in resultsSet: {}' \
-               .format(', '.join('|'.join(col) for col in self.resultCols if col not in self.resultsSet.columns))
-
-    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
-    # in the "all analyses at once" case.
-    # (sort, convert units, round values, and style).
-    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
-    # Return a pd.DataFrame.Styler
-    def finalFormatAllAnalysesData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
-        
-        logger.debug(f'MCDSResultsPreReport.finalFormatAllAnalysesData'
-                     f'({sort=}, {indexer=}, {convert=}, {round_=}, {style=})')
-
-        df = dfTrData
-
-        # Sorting
-        if sort:
-            
-            # If no sorting order was specified, generate one simple one, through a temporarily sample num. column
-            # (assuming analyses have been run as grouped by sample)
-            if not self.sortCols:
-            
-                # Note: Ignoring all-NaN sample id columns, for a working groupby
-                sampleIdCols = [col for col in self.resultsSet.transSampleColumns(self.lang)
-                                if col in df.columns and not df[col].isna().all()]
-                df.insert(0, column='#Sample#', value=df.groupby(sampleIdCols, sort=False).ngroup())
-
-                sortCols = ['#Sample#']
-                sortAscend = True
-                
-            # Otherwise, use the one specified.
-            else:
-            
-                # ... after some cleaning up in case some sort columns are not present.
-                sortCols = list()
-                sortAscend = list() if isinstance(self.sortAscend, list) else self.sortAscend
-                for ind, col in enumerate(self.resultsSet.transColumns(self.sortCols, self.lang)):
-                    if col in df.columns:
-                        sortCols.append(col)
-                        if isinstance(self.sortAscend, list):
-                            sortAscend.append(self.sortAscend[ind])
-                assert not isinstance(sortAscend, list) or len(sortCols) == len(sortAscend)
-                assert len(sortCols) > 0
-
-            # Sort
-            df.sort_values(by=sortCols, ascending=sortAscend, inplace=True)
-            
-            # Remove temporary sample num. column if no sorting order was specified
-            if not self.sortCols:
-                df.drop(columns=['#Sample#'], inplace=True)
-        
-        # Standard 1 to N index for synthesis <=> details navigation.
-        if indexer:
-            df.index = range(1, len(df) + 1)
-
-        # Converting to other units, or so.
-        if convert:
-            
-            col = self.trEnColNames('CoefVar Density')
-            if col in df.columns:
-                kVarDens = 100.0
-                df[col] *= kVarDens  # [0, 1] => %
-            
-        # Reducing float precision
-        if round_:
-            
-            # Use built-in round for more accurate rounding than np.round
-            # a. Fixed list of columns: simply enumerate their English names.
-            dColDecimals = {**{col: 3 for col in ['PDetec', 'Min PDetec', 'Max PDetec']},
-                            **{col: 2 for col in ['Delta AIC', 'Chi2 P', 'KS P', 'CvM Uw P', 'CvM Cw P',
-                                                  'Density', 'Min Density', 'Max Density',
-                                                  'Qual Chi2+', 'Qual KS+', 'Qual DCv+']},
-                            **{col: 1 for col in ['AIC', 'EDR/ESW', 'Min EDR/ESW', 'Max EDR/ESW',
-                                                  'Number', 'Min Number', 'Max Number',
-                                                  'CoefVar Density', 'CoefVar Number', 'Obs Rate']},
-                            **{col: 0 for col in ['Left Trunc', 'Right Trunc']}}
-            for col, dec in self.trEnColNames(dColDecimals).items():
-                if col in df.columns:
-                    df[col] = df[col].apply(self._roundNumber, ndigits=dec)
-
-            # b. Dynamic lists of columns: select their names through a startswith criterion.
-            for col in df.columns:
-                if col.startswith(self.tr('Qual Bal')):
-                    df[col] = df[col].apply(self._roundNumber, ndigits=2)
-
-            # Don't use df.round ... because it does nothing, at least with pandas to 1.1.2 !?!?!?
-            # df = df.round(decimals={col: dec for col, dec in self.trEnColNames(dColDecimals).items()
-            #                         if col in df.columns})
-
-        # Styling
-        return self.styleAllAnalysesData(df, convert=convert, round_=round_, style=style)
-
-    # Top page
-    def toHtmlAllAnalyses(self, rebuild=False):
-        
-        logger.info('Top page ...')
-        logger.debug(f'MCDSResultsPreReport.toHtmlAllAnalyses({rebuild=})')
-
-        # Generate the table to display from raw results 
-        # (index + 5 columns : sample, params, results, ProbDens plot, DetProb plot)
-        # 1. Get translated detailed results
-        dfDet = self.resultsSet.dfTransData(self.lang)
-
-        # 2. List estimator selection criterion and CV interval values used
-        #    (these values MUST be notified in the report, even if not in the selected columns to show)
-        estimSelCrits = dfDet[self.trEnColNames('Mod Chc Crit')].unique().tolist()
-        confIntervals = dfDet[self.trEnColNames('Conf Interv')].unique().tolist()
-
-        # 3. Post-format results (styling not used later, so don't do it).
-        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
-                                                 convert=True, round_=True, style=False)
-        dfDet = dfsDet.data
-
-        # 4. Translate sample, parameter and result columns
-        sampleTrCols = self.resultsSet.transColumns(self.sampleCols, self.lang)
-        paramTrCols = self.resultsSet.transColumns(self.paramCols, self.lang)
-        result1TrCols = self.resultsSet.transColumns(self.resultCols, self.lang)
-
-        midResInd = len(result1TrCols) // 2 + len(result1TrCols) % 2
-        result2TrCols = result1TrCols[midResInd:]
-        result1TrCols = result1TrCols[:midResInd]
-        
-        # 5. Fill target table index and columns
-        dfSupSyn = pd.DataFrame(dict(SampleParams=dfDet[sampleTrCols + paramTrCols].apply(self.series2VertTable,
-                                                                                          axis='columns'),
-                                     Results1=dfDet[result1TrCols].apply(self.series2VertTable, axis='columns'),
-                                     Results2=dfDet[result2TrCols].apply(self.series2VertTable, axis='columns'),
-                                     DistHist=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                               plotImgPrfx=self.PlotImgPrfxDistHist,
-                                                                               plotHeight=self.superSynthPlotsHeight),
-                                     ProbDens=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                               plotImgPrfx=self.PlotImgPrfxProbDens,
-                                                                               plotHeight=self.superSynthPlotsHeight),
-                                     DetProb=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                              plotImgPrfx=self.PlotImgPrfxDetProb,
-                                                                              plotHeight=self.superSynthPlotsHeight)))
-        
-        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSupSyn))), 1))
-        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
-
-        def numNavLink(sAnlys):
-            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
-        dfSupSyn.index = dfDet.apply(numNavLink, axis='columns')
-        
-        # 6. Translate table columns.
-        dfSupSyn.columns = [self.tr(col) for col in dfSupSyn.columns]
-
-        # 7. Generate traceability infos parts (results specs).
-        ddfTrc = self.asWorkbook(subset=['specs', 'samples'])
-
-        # 8. Generate top report page.
-        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
-        tmpl = self.getTemplateEnv().get_template('mcds/pretop.htpl')
-        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
-        html = tmpl.render(supersynthesis=dfSupSyn.to_html(escape=False),
-                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
-                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
-                           title=self.title, subtitle=self.subTitle,
-                           description=self.description, keywords=self.keywords,
-                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang],
-                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
-                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(), 
-                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
-                           areaUnit=self.tr(self.resultsSet.areaUnit),
-                           surveyType=self.tr(self.resultsSet.surveyType),
-                           distanceType=self.tr(self.resultsSet.distanceType),
-                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'),
-                           estimSelCrits=estimSelCrits, confIntervals=[str(v) for v in confIntervals])
-        html = re.sub('(?:[ \t]*\\\n){2,}', '\n'*2, html)  # Cleanup blank lines series to one only
-
-        # 9. Write top HTML to file.
-        htmlPathName = self.targetFilePathName(suffix='.html')
-        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
-            tgtFile.write(html)
-
-        return htmlPathName
-
-
-class MCDSResultsFullReport(MCDSResultsDistanceReport):
-
-    """A specialized full report for MCDS analyses, with actual output formatting.
-
-    HTML mode gives a mix of Distance and PreReport main page layout,
-    with a super-synthesis table (with plots), a synthesis table, and a detailed table ;
-    detailed table unchanged from MCDSResultsDistanceReport
-    detailed (linked) pages unchanged from MCDSResultsDistanceReport.
-    """
-
-    # Translation table.
-    DTrans = _mergeTransTables(base=MCDSResultsDistanceReport.DTrans,
-        update=dict(en={'Quick-view results': 'Results: the essence',
-                        'SampleParams': 'Sample & Model',
-                        'Results1': 'Results (1/2)', 'Results2': 'Results (2/2)',
-                        'QqPlot': 'Quantile-quantile plot',
-                        'ProbDens': 'Detection probability density (PDF)',
-                        'DetProb': 'Detection probability'},
-                    fr={'Quick-view results': 'Résultats : l\'essentiel',
-                        'SampleParams': 'Echantillon & Modèle',
-                        'Results1': 'Résultats (1/2)', 'Results2': 'Résultats (2/2)',
-                        'QqPlot': 'Diagramme quantile-quantile',
-                        'ProbDens': 'Densité de probabilité de détection (DdP)',
-                        'DetProb': 'Probabilité de détection'}))
-    
-    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords, 
-                 sampleCols, paramCols, resultCols, synthCols=None, sortCols=None, sortAscend=None,
-                 dCustomTrans=None, lang='en',
-                 superSynthPlotsHeight=288, plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
-                 plotLineWidth=1, plotDotWidth=4, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
-                 pySources=[], tgtFolder='.', tgtPrefix='results'):
-
-        """Ctor
-        
-        Parameters:
-        :param resultsSet: source results
-        :param title: main page title (and <title> tag in HTML header)
-        :param subTitle: main page sub-title (under the title, lower font size)
-        :param description: main page description text (under the sub-title, lower font size)
-        :param anlysSubTitle: analysis pages title
-        :param keywords: for HTML header <meta name="keywords" ...>
-        :param sampleCols: for main page table, 1st column (top)
-        :param paramCols: for main page table, 1st column (bottom)
-        :param resultCols: for main page table, 2nd and 3rd columns
-        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
-        :param sortCols: sorting columns for report tables
-        :param sortAscend: sorting order for report tables, as a bool or list of bools, of len(synthCols)
-        :param dCustomTrans: custom translations to complete the report standard ones,
-                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
-        :param lang: Target language for translation
-        :param superSynthPlotsHeight: Display height (in pixels) of the super-synthesis table plots
-        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
-        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
-        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
-        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
-        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
-        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
-        :param pySources: path-name of source files to copy in report folder and link in report
-        :param tgtFolder: target folder for the report (for _all_ generated files)
-        :param tgtPrefix: default target file name for the report
-        """
-
-        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                         synthCols=synthCols, sortCols=sortCols, sortAscend=sortAscend, 
-                         dCustomTrans=dCustomTrans, lang=lang,
-                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
-                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
-                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
-        
-        self.sampleCols = self.noDupColumns(sampleCols, head='Sample columns')
-        self.paramCols = self.noDupColumns(paramCols, head='Parameter columns')
-        self.resultCols = self.noDupColumns(resultCols, head='Result columns')
-        self.superSynthPlotsHeight = superSynthPlotsHeight
-
-    def checkNeededColumns(self):
-
-        """Side check as soon as possible : Are all report needed columns available ?
-        (now that computed columns have been ... post-computed through self.resultsSet.dfFilSorData calls)
-
-        :raise: AssertionError if not the case
-        """
-
-        assert all(col in self.resultsSet.columns for col in self.sampleCols), \
-               'Missing super-synthesis sample columns in resultsSet: {}' \
-               .format(', '.join('|'.join(col) for col in self.sampleCols if col not in self.resultsSet.columns))
-        assert all(col in self.resultsSet.columns for col in self.paramCols), \
-               'Missing super-synthesis parameters columns in resultsSet: {}' \
-               .format(', '.join('|'.join(col) for col in self.paramCols if col not in self.resultsSet.columns))
-        assert all(col in self.resultsSet.columns for col in self.resultCols), \
-               'Missing super-synthesis results columns in resultsSet: {}' \
-               .format(', '.join('|'.join(col) for col in self.resultCols if col not in self.resultsSet.columns))
-
-    # Top page (based on results.dfTransData).
-    def toHtmlAllAnalyses(self, rebuild=False):
-        
-        logger.info('Top page ...')
-        
-        # 1. Get source translated raw data to format
-        dfSynRes, dfDetRes, _ = self.getRawTransData(rebuild=rebuild)
-
-        # 2. List estimator selection criterion and CV interval values used
-        #    (these values MUST be notified in the report, even if not in the selected columns to show)
-        estimSelCrits = dfDetRes[self.trEnColNames('Mod Chc Crit')].unique().tolist()
-        confIntervals = dfDetRes[self.trEnColNames('Conf Interv')].unique().tolist()
-
-        # 3. Super-synthesis: Generate post-processed and translated table.
-        #    (index + 5 columns : sample + params, results, Qq plot, ProbDens plot, DetProb plot)
-        # a. Get translated and post-formatted detailed results
-        dfDet = dfDetRes.copy()  # Also needed as is below.
-        
-        # b. Styling not used for super-synthesis, so don't do it.
-        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
-                                                 convert=True, round_=True, style=False)
-        dfDet = dfsDet.data
-
-        # c. Translate sample, parameter and result columns
-        sampleTrCols = self.resultsSet.transColumns(self.sampleCols, self.lang)
-        paramTrCols = self.resultsSet.transColumns(self.paramCols, self.lang)
-        result1TrCols = self.resultsSet.transColumns(self.resultCols, self.lang)
-
-        midResInd = len(result1TrCols) // 2 + len(result1TrCols) % 2
-        result2TrCols = result1TrCols[midResInd:]
-        result1TrCols = result1TrCols[:midResInd]
-        
-        # d. Fill target table index and columns
-        dfSupSyn = pd.DataFrame(dict(SampleParams=dfDet[sampleTrCols + paramTrCols].apply(self.series2VertTable,
-                                                                                          axis='columns'),
-                                     Results1=dfDet[result1TrCols].apply(self.series2VertTable, axis='columns'),
-                                     Results2=dfDet[result2TrCols].apply(self.series2VertTable, axis='columns'),
-                                     QqPlot=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                             plotImgPrfx=self.PlotImgPrfxQqPlot,
-                                                                             plotHeight=self.superSynthPlotsHeight),
-                                     ProbDens=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                               plotImgPrfx=self.PlotImgPrfxProbDens,
-                                                                               plotHeight=self.superSynthPlotsHeight),
-                                     DetProb=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                              plotImgPrfx=self.PlotImgPrfxDetProb,
-                                                                              plotHeight=self.superSynthPlotsHeight)))
-        
-        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSupSyn))), 1))
-        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
-
-        def numNavLink(sAnlys):
-            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
-        dfSupSyn.index = dfDet.apply(numNavLink, axis='columns')
-        
-        # e. Translate table columns.
-        dfSupSyn.columns = [self.tr(col) for col in dfSupSyn.columns]
-
-        # 4. Synthesis: Generate post-processed and translated table.
-        # a. Add run folder column if not selected (will serve to generate the link to the analysis detailed report)
-        dfSyn = dfSynRes
-        dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-        
-        # b. Links to each analysis detailed report.
-        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSyn))), 1))
-        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
-
-        def numNavLink(sAnlys):
-            return numNavLinkFmt.format(p=sAnlys[self.trRunFolderCol], n=sAnlys.name)
-       
-        # c. Post-format as specified in actual class.
-        dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=numNavLink,
-                                                 convert=True, round_=True, style=True)
-
-        # 5. Details: Generate post-processed and translated table.
-        dfDet = dfDetRes
-
-        # a. Add run folder column if not there (will serve to generate the link to the analysis detailed report)
-        detTrCols = list(dfDet.columns)
-        if self.trRunFolderCol not in detTrCols:
-            detTrCols += [self.trRunFolderCol]
-        dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-        dfDet = dfDet.reindex(columns=detTrCols)
-       
-        # b. Links to each analysis detailed report.
-        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=numNavLink,
-                                                 convert=False, round_=False, style=True)
-
-        # 6. Generate traceability infos parts (results specs).
-        ddfTrc = self.asWorkbook(subset=['specs', 'samples'])
-
-        # Generate top report page.
-        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
-        tmpl = self.getTemplateEnv().get_template('mcds/fulltop.htpl')
-        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
-        html = tmpl.render(supersynthesis=dfSupSyn.to_html(escape=False),
-                           synthesis=dfsSyn.render(),  # escape=False, index=False),
-                           details=dfsDet.render(),  # escape=False, index=False),
-                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
-                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
-                           title=self.title, subtitle=self.subTitle,
-                           description=self.description, keywords=self.keywords,
-                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang],
-                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
-                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(),
-                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
-                           areaUnit=self.tr(self.resultsSet.areaUnit),
-                           surveyType=self.tr(self.resultsSet.surveyType),
-                           distanceType=self.tr(self.resultsSet.distanceType),
-                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'),
-                           estimSelCrits=estimSelCrits, confIntervals=[str(v) for v in confIntervals])
-        html = re.sub('(?:[ \t]*\\\n){2,}', '\n'*2, html)  # Cleanup blank lines series to one only
-
-        # 7. Write top HTML to file.
-        htmlPathName = self.targetFilePathName(suffix='.html')
-        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
-            tgtFile.write(html)
-
-        return htmlPathName
-
-
-# A specialized filtered and sorted report for MCDS analyses, with actual output formatting ...
-# and above all auto-filtered and sorted results aiming at showing the few best results to the user
-# among which s(he)'ll (manually) select THE best (for each sample)
-# (HTML mode gives a mix of Distance and PreReport main page layout,
-#  with a plots + super-synthesis table, a synthesis table, and a detailed table,
-#  but only showing the filtered part (rows) of the results set ;
-#  detailed pages unchanged from MCDSResultsDistanceReport).
-class MCDSResultsFilterSortReport(MCDSResultsFullReport):
-
-    """A specialized filtered and sorted full report for MCDS analyses, with actual output formatting
-    and above all auto-filtered and sorted results aiming at showing the few best results to the user
-    among which to (manually) select THE best (for each sample).
-
-    Just like MCDSResultsFullReport, but for filtered and sorted results for 1-only scheme.
-    """
-
-    # Translation table.
-    DTrans = _mergeTransTables(base=MCDSResultsFullReport.DTrans,
-        update=dict(en={'Scheme': 'Scheme', 'Step': 'Step',
-                        'Property': 'Property', 'Value': 'Value',
-                        'AFS': 'AFS', 'AFSM': 'AFSM', 'Steps': 'Steps',
-                        'Filter & Sort steps': 'Filter and sort steps'},
-                    fr={'Scheme': 'Méthode', 'Step': 'Etape',
-                        'Property': 'Propriété', 'Value': 'Valeur',
-                        'AFS': 'FTA', 'AFSM': 'MFTA', 'Steps': 'Etapes',
-                        'Filter & Sort steps': 'Etapes de filtrage et tri'}))
-
-    ResClass = MCDSAnalysisResultsSet
-
-    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                 sampleCols, paramCols, resultCols, synthCols=None, sortCols=None, sortAscend=None,
-                 dCustomTrans=None, lang='en',
-                 filSorSchemes=[dict(method=ResClass.filterSortOnExecCode)],
-                 superSynthPlotsHeight=288, plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
-                 plotLineWidth=1, plotDotWidth=4, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
-                 pySources=[], tgtFolder='.', tgtPrefix='results'):
-
-        """Ctor
-        
-        Parameters:
-        :param resultsSet: source results (an instance of MCDSAnalysisResultsSet, or subclass,
-                                           named ResClass or R below)
-        :param title: main page title (and <title> tag in HTML header) for the HTML report (1 scheme only)
-        :param subTitle: main page sub-title (under the title, lower font size) for the HTML report (1 scheme only) ;
-                         any {fsMeth} placeholder will get replaced by the method name of the used filter sort scheme 
-        :param description: main page description text (under the sub-title, lower font size)
-                            for the HTML report (1 scheme only) ; any {fsMeth} placeholder formatted as in subTitle
-        :param anlysSubTitle: analysis pages title
-        :param keywords: for HTML header <meta name="keywords" ...>
-        :param sampleCols: for main page table, 1st column (top)
-        :param paramCols: for main page table, 1st column (bottom)
-        :param resultCols: for main page table, 2nd and 3rd columns
-        :param synthCols: Subset and order of columns to keep at the end (before translation)
-                          as the synthesis table of each filter-sort sub-report (None = [] = all)
-                          Warning: No need to specify here pre-selection and final selection columns,
-                                   as they'll be added automatically, and relocated at a non-customisable place.
-        :param sortCols: sorting columns for report tables ??? which ones ???
-        :param sortAscend: sorting order for report tables, as a bool or list of bools,
-                           of len(synthCols) ??? which ones ???
-        :param dCustomTrans: custom translations to complete the report standard ones,
-                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
-        :param lang: Target language for translation
-        :param filSorSchemes: filter and sort schemes to apply in order to generate each sub-report
-                 as a list of dict(method= <results set class>.filterSortOnXXX method to use,
-                                   deduplicate= dict(dupSubset=, dDupRounds=) of deduplication params
-                                       (if not or partially given, see RCLS.filterSortOnXXX defaults)
-                                   filterSort= dict of other <method>-specific params,
-                                   preselCols= target columns for generating auto-preselection ones,
-                                               containing [1, preselNum] ranks ; default: []
-                                   preselAscs= Rank direction to use for each column (list),
-                                               or all (single bool) ; default: True
-                                               (True means that lower values are "better" ones)
-                                   preselThrhs= Eliminating threshold for each column (list),
-                                                or all (single number) ; default: 0.2
-                                                (eliminated above if preselAscs True, below otherwise)
-                                   preselNum= number of (best) pre-selections to keep for each sample) ;
-                                              default: 5
-                 example: [dict(method=R.filterSortOnExecCode,  # let R = MCDSTruncOptanalysisResultsSet
-                                preselCols=[R.CLCmbQuaBal1, R.CLCmbQuaBal2], preselAscs=False,
-                                preselThrhs=0.2, preselNum=5),
-                           dict(method=R.filterSortOnExCAicMulQua,
-                                deduplicate=dict(dupSubset=[R.CLNObs, R.CLEffort, R.CLDeltaAic, R.CLChi2,
-                                                            R.CLKS, R.CLCvMUw, R.CLCvMCw, R.CLDCv]),
-                                                 dDupRounds={R.CLDeltaAic: 1, R.CLChi2: 2, R.CLKS: 2,
-                                                             R.CLCvMUw: 2, R.CLCvMCw: 2, R.CLDCv: 2})
-                                filterSort=dict(sightRate=92.5, nBestAIC=3, nBestQua=1,
-                                                whichBestQua=[R.CLGrpOrdClTrChi2KSDCv, R.CLGrpOrdClTrQuaBal3],
-                                                nFinalRes=12, whichFinalQua=R.CLCmbQuaBal3, ascFinalQua=False),
-                                preselCols=[R.CLCmbQuaBal1, R.CLDCv], preselAscs=[False, True],
-                                preselThrhs=[0.2, 0.5], preselNum=3)]        
-        :param superSynthPlotsHeight: Display height (in pixels) of the super-synthesis table plots
-        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
-        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
-        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
-        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
-        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
-        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
-        :param pySources: path-name of source files to copy in report folder and link in report
-        :param tgtFolder: target folder for the report (for _all_ generated files)
-        :param tgtPrefix: default target file name for the report
-        """
-
-        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
-                         sampleCols=sampleCols, paramCols=paramCols, resultCols=resultCols, synthCols=synthCols,
-                         sortCols=sortCols, sortAscend=sortAscend, dCustomTrans=dCustomTrans, lang=lang,
-                         superSynthPlotsHeight=superSynthPlotsHeight,
-                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
-                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
-                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
-        
-        self.filSorSchemes = filSorSchemes
-
-    def asWorkbook(self, subset=None, rebuild=False):
-
-        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
-        where each item is a named worksheet
-
-        Parameters:
-        :param subset: Selected list of data categories to include ; None = [] = all
-                       (categories in {'specs', 'samples', results'})
-        :param rebuild: If True, force rebuild of filtered and sorted sub-report
-                        => prevent use of / reset results set filter & sort cache
-        """
-        
-        logger.debug(f'MCDSResultsFilterSortReport.asWorkbook({subset=}, {rebuild=})')
-
-        ddfWbk = dict()
-
-        logger.info('Formatting FilterSort sub-reports as a workbook ...')
-
-        # Build results worksheets if specified in subset
-        if not subset or 'results' in subset:
-
-            # TODO: Add better formatting (color, ... etc)
-
-            # For each filter and sort scheme:
-            repLog = list()
-            for scheme in self.filSorSchemes:
-
-                logger.info1('* filter & sort "{}" scheme ...'
-                             .format(self.resultsSet.filSorSchemeId(scheme)))
-
-                # Apply it
-                filSorSchId, dfFilSorRes, filSorSteps = \
-                    self.resultsSet.dfFilSorData(scheme=scheme, rebuild=rebuild,
-                                                 columns=self.synthCols, lang=self.lang)
-
-                # Store results in workbook
-                ddfWbk['-'.join([self.tr('AFSM'), filSorSchId])] = (dfFilSorRes, False)
-
-                # Update all-scheme log
-                repLog += filSorSteps
-
-            # Log of opérations, for traceability.
-            logger.info1('* filter & sort steps ...')
-
-            indexCols = [self.tr(col) for col in ['Scheme', 'Step']]
-            dataCols = [self.tr(col) for col in ['Property', 'Value']]
-            dfFilSorHist = pd.DataFrame(repLog, columns=indexCols + dataCols)
-            dfFilSorHist.set_index(indexCols, inplace=True)
-
-            ddfWbk['-'.join([self.tr('AFS'), self.tr('Steps')])] = (dfFilSorHist, True)
-
-        # Append inherited worksheets.
-        ddfWbk.update(super().asWorkbook(subset=subset, rebuild=rebuild))
-
-        # Done
-        logger.info('... done.')
-
-        return ddfWbk
-
-    def getRawTransData(self, filSorScheme=dict(method=ResClass.filterSortOnExecCode),
-                        rebuild=False):
-
-        """Retrieve input translated raw data to be formatted
-
-        :return: 2 dataFrames, for synthesis (synCols) and detailed (all) column sets,
-                 + the id of the applied scheme and the log of its application. 
-        """
-
-        # Generate translated synthesis table.
-        synthCols = self.synthCols
-        if self.resultsSet.analysisClass.RunFolderColumn not in synthCols:
-            synthCols += [self.resultsSet.analysisClass.RunFolderColumn]
-        filSorSchId, dfFilSorSynRes, filSorSteps = \
-            self.resultsSet.dfFilSorData(scheme=filSorScheme, rebuild=rebuild, columns=synthCols, lang=self.lang)
-
-        # Generate translated detailed table.
-        _, dfFilSorDetRes, _ = \
-            self.resultsSet.dfFilSorData(scheme=filSorScheme, rebuild=rebuild, lang=self.lang)
-
-        # Side check as soon as possible : Are all report needed columns available ?
-        self.checkNeededColumns()
-
-        return dfFilSorSynRes, dfFilSorDetRes, (filSorSchId, filSorSteps)
-
-    def toHtmlAllAnalyses(self, filSorScheme=dict(method=ResClass.filterSortOnExecCode),
-                          rebuild=False):
-
-        """Top page for a given scheme.
-        """
-
-        logger.info('Top page ...')
-        
-        # 1. Get source translated raw data to format (post-compute, filter and sort, extract)
-        dfSynRes, dfDetRes, (filSorSchId, filSorSteps) = \
-            self.getRawTransData(filSorScheme=filSorScheme, rebuild=rebuild)
-
-        # 2. List estimator selection criterion and CV interval values used
-        #    (these values MUST be notified in the report, even if not in the selected columns to show)
-        estimSelCrits = dfDetRes[self.trEnColNames('Mod Chc Crit')].unique().tolist()
-        confIntervals = dfDetRes[self.trEnColNames('Conf Interv')].unique().tolist()
-
-        # 3. Super-synthesis: Format filtered and translated data.
-        #    (index + 5 columns : sample + params, results, Qq plot, ProbDens plot, DetProb plot)
-        dfDet = dfDetRes.copy()
-
-        # a. Styling not used for super-synthesis, so don't do it.
-        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
-                                                 convert=True, round_=True, style=False)
-        dfDet = dfsDet.data
-
-        # b. Translate sample, parameter and result columns
-        sampleTrCols = self.resultsSet.transColumns(self.sampleCols, self.lang)
-        paramTrCols = self.resultsSet.transColumns(self.paramCols, self.lang)
-        result1TrCols = self.resultsSet.transColumns(self.resultCols, self.lang)
-
-        midResInd = len(result1TrCols) // 2 + len(result1TrCols) % 2
-        result2TrCols = result1TrCols[midResInd:]
-        result1TrCols = result1TrCols[:midResInd]
-        
-        # c. Fill target table index and columns
-        dfSupSyn = \
-            pd.DataFrame(dict(SampleParams=dfDet[sampleTrCols + paramTrCols].apply(self.series2VertTable,
-                                                                                   axis='columns'),
-                              Results1=dfDet[result1TrCols].apply(self.series2VertTable, axis='columns'),
-                              Results2=dfDet[result2TrCols].apply(self.series2VertTable, axis='columns'),
-                              QqPlot=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                      plotImgPrfx=self.PlotImgPrfxQqPlot,
-                                                                      plotHeight=self.superSynthPlotsHeight),
-                              ProbDens=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                        plotImgPrfx=self.PlotImgPrfxProbDens,
-                                                                        plotHeight=self.superSynthPlotsHeight),
-                              DetProb=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
-                                                                       plotImgPrfx=self.PlotImgPrfxDetProb,
-                                                                       plotHeight=self.superSynthPlotsHeight)))
-        
-        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSupSyn))), 1))
-        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
-
-        def numNavLink(sAnlys):
-            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
-        dfSupSyn.index = dfDet.apply(numNavLink, axis='columns')
-        
-        # d. Translate table columns.
-        dfSupSyn.columns = [self.tr(col) for col in dfSupSyn.columns]
-
-        # 4. Synthesis: Format filtered and translated data.
-        # a. Add run folder column if not selected (will serve to generate the link to the analysis detailed report)
-        dfSyn = dfSynRes
-        dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-        
-        # b. Links to each analysis detailed report.
-        idxFmt = '{{n:0{}d}}'.format(1 + max(int(math.log10(len(dfSyn))), 1))
-        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
-
-        def numNavLink(sAnlys):
-            return numNavLinkFmt.format(p=sAnlys[self.trRunFolderCol], n=sAnlys.name)
-       
-        # c. Post-format as specified in actual class.
-        dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=numNavLink,
-                                                 convert=True, round_=True, style=True)
-
-        # 5. Details: Format filtered and translated data.
-        dfDet = dfDetRes
-
-        # a. Add run folder column if not there (will serve to generate the link to the analysis detailed report)
-        detTrCols = list(dfDet.columns)
-        if self.trRunFolderCol not in detTrCols:
-            detTrCols += [self.trRunFolderCol]
-        dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
-        dfDet = dfDet.reindex(columns=detTrCols)
-       
-        # b. Links to each analysis detailed report.
-        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=numNavLink,
-                                                 convert=False, round_=False, style=True)
-
-        # 6. Generate traceability infos section.
-        # a. Log of opérations.
-        indexCols = [self.tr(col) for col in ['Scheme', 'Step']]
-        dataCols = [self.tr(col) for col in ['Property', 'Value']]
-        dfFilSorHist = pd.DataFrame(filSorSteps, columns=indexCols + dataCols)
-        dfFilSorHist.set_index(indexCols, inplace=True)
-        ddfTrc = {self.tr('Filter & Sort steps'): (dfFilSorHist, True)}
-
-        # b. Results specs
-        ddfTrc.update(self.asWorkbook(subset=['specs', 'samples']))
-
-        # 7. Generate top report page.
-        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
-        tmpl = self.getTemplateEnv().get_template('mcds/fulltop.htpl')
-        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
-        html = tmpl.render(supersynthesis=dfSupSyn.to_html(escape=False),
-                           synthesis=dfsSyn.render(),  # escape=False, index=False),
-                           details=dfsDet.render(),  # escape=False, index=False),
-                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
-                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
-                           title=self.title, keywords=self.keywords,
-                           subtitle=self.subTitle.format(fsId=filSorSchId.split('@')[0]),
-                           description=self.description.format(fsId=filSorSchId.split('@')[0]),
-                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang],
-                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
-                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(), 
-                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
-                           areaUnit=self.tr(self.resultsSet.areaUnit),
-                           surveyType=self.tr(self.resultsSet.surveyType),
-                           distanceType=self.tr(self.resultsSet.distanceType),
-                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'),
-                           estimSelCrits=estimSelCrits, confIntervals=[str(v) for v in confIntervals])
-        html = re.sub('(?:[ \t]*\\\n){2,}', '\n'*2, html)  # Cleanup blank lines series to one only
-
-        # 8. Write top HTML to file.
-        filSorSchId = self.resultsSet.filSorSchemeId(filSorScheme)
-        htmlPathName = self.targetFilePathName(suffix=f'.{filSorSchId}.html')
-        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
-            tgtFile.write(html)
-
-        return htmlPathName
-
-    def toHtml(self, filSorScheme=dict(method=ResClass.filterSortOnExecCode),
-               rebuild=False):
-
-        """HTML report generation for a given scheme.
-
-        Parameters:
-        :param filSorScheme: the 1 and only) scheme to use for building the report (see ctor)
-        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
-                        otherwise, use any cached data or existing plot image files
-
-        Note: Parallelism does not work for this class (WTF ?), hence the absence of the 'generators' parameter.
-              Actually, it seems to work only the first time, and only when rebuild == False
-              (and may be no matplotlib drawing actually done) ;
-              but then, Exception: Can't pickle <function sync_do_first ... :-(
-        """
-
-        generators = None
-
-        # Install needed attached files.
-        self.installAttFiles(self.AttachedFiles)
-            
-        # Generate full report detailed pages (one for each analysis)
-        # (done first to have plot image files generated for top report page generation right below).
-        filSorSchId = self.resultsSet.filSorSchemeId(filSorScheme)
-        self.toHtmlEachAnalysis(filSorScheme=filSorScheme, rebuild=rebuild, generators=generators,
-                                topSuffix=f'.{filSorSchId}.html')
-        
-        # Generate top = main report page (one for all analyses).
-        topHtmlPathName = self.toHtmlAllAnalyses(filSorScheme=filSorScheme, rebuild=rebuild)
-
-        logger.info('... done.')
-                
-        return topHtmlPathName
-
-
-if __name__ == '__main__':
-
-    sys.exit(0)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+#
+# Copyright (C) 2021 Jean-Philippe Meuret
+#
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Submodule "report": HTML and Excel report generation from DS results
+
+import sys
+import os
+import shutil
+import re
+import pathlib as pl
+from packaging import version as pkgver
+import copy
+
+import datetime as dt
+import codecs
+
+import math
+import numpy as np
+import pandas as pd
+
+import jinja2
+import matplotlib.pyplot as plt
+import matplotlib.ticker as pltt
+# import seaborn as sb
+
+from . import log, runtime, __version__
+from .executor import Executor
+from .analyser import MCDSAnalysisResultsSet
+
+runtime.update(matplotlib=sys.modules['matplotlib'].__version__, jinja2=jinja2.__version__)
+
+logger = log.logger('ads.rep')
+
+# Actual package install dir.
+KInstDirPath = pl.Path(__file__).parent.resolve()
+
+
+def _mergeTransTables(base, update):
+
+    """Merge an 'update' translation table into a 'base' one ('update' completes or overwrites 'base').
+
+    Note: Trans. tables are dict(<lang>=dict(<source>: <translation>))
+    """
+
+    final = copy.deepcopy(base)
+    for lang in update.keys():
+        if lang not in final:
+            final[lang] = dict()
+        final[lang].update(update[lang])
+    return final
+
+
+class ResultsReport:
+
+    """Base for results reports classes (abstract)"""
+
+    # Translation table for output documents (specialized in derived classes, merged with custom instance one).
+    DTrans = dict(en={}, fr={})
+
+    def __init__(self, resultsSet, title, subTitle, description, keywords,
+                 dCustomTrans=None, lang='en', pySources=[], tgtFolder='.', tgtPrefix='results'):
+    
+        """Ctor
+        
+        Parameters:
+        :param resultsSet: source results
+        :param title: main page title (and <title> tag in HTML header)
+        :param subTitle: main page sub-title (under the title, lower font size)
+        :param description: main page description text (under the sub-title, lower font size)
+        :param keywords: for HTML header <meta name="keywords" ...>
+        :param dCustomTrans: custom translations to complete the report standard ones,
+                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
+        :param lang: Target language for translation (only 'en' and 'fr' supported)
+        :param pySources: path-name of source files to copy in report folder and link in report
+        :param tgtFolder: target folder for the report (for _all_ generated files)
+        :param tgtPrefix: default target file name for the report
+        """
+
+        assert len(resultsSet) > 0, 'Can\'t build reports with nothing inside'
+        assert os.path.isdir(tgtFolder), 'Target folder {} doesn\'t seem to exist ...'.format(tgtFolder)
+        assert lang in ['en', 'fr'], 'Only en and fr translation supported for the moment'
+        
+        self.resultsSet = resultsSet
+        
+        self.trRunFolderCol = resultsSet.dfColTrans.loc[resultsSet.analysisClass.RunFolderColumn, lang]
+        self.dfEnColTrans = None  # EN to other languages column name translation
+
+        self.lang = lang
+        self.title = title
+        self.subTitle = subTitle
+        self.description = description
+        self.keywords = keywords
+        self.pySources = pySources
+        self.dTrans = _mergeTransTables(self.DTrans, dict() if dCustomTrans is None else dCustomTrans)
+        
+        self.tgtPrefix = tgtPrefix
+        self.tgtFolder = tgtFolder
+        
+        self.tmplEnv = None
+    
+    @staticmethod
+    def _libVersions():
+        return {'Python': sys.version.split()[0],
+                'NumPy': runtime['numpy'],
+                'Pandas': runtime['pandas'],
+                'ZOOpt': runtime['zoopt'],
+                'Matplotlib': runtime['matplotlib'],
+                'Jinja': runtime['jinja2']}
+
+    # Translate string
+    def tr(self, s):
+        return self.dTrans[self.lang].get(s, s)
+    
+    def trEnColNames(self, colNames, startsWith=None):
+        
+        """Translate EN-translated column(s) name(s) to self.lang one
+
+        Parameters:
+        :param colNames: str (1), list(N) or dict (N keys) of column names
+        :param startsWith: if not None or '', only columns starting with it will get processed
+        """
+
+        # 1. Build translation function (and translation table if needed)
+        # a. If target language is English, no translation needed
+        if self.lang == 'en':
+            def _trEnColName(cn):
+                return cn
+
+        # b. If target language is NOT English, translation is NEEDED
+        else:
+            if self.dfEnColTrans is None:
+                self.dfEnColTrans = self.resultsSet.transTable()
+                self.dfEnColTrans.set_index('en', inplace=True, drop=True)
+
+            def _trEnColName(cn):  # Assuming there's only 1 match !
+                return self.dfEnColTrans.at[cn, self.lang]
+        
+        # 2. Translate !
+        if isinstance(colNames, str):
+            trColNames = _trEnColName(colNames) \
+                         if not startsWith or colNames.startswith(startsWith) else None
+        elif isinstance(colNames, list):
+            trColNames = [_trEnColName(colName) for colName in colNames
+                          if not startsWith or colName.startswith(startsWith)]
+        elif isinstance(colNames, dict):
+            trColNames = {_trEnColName(colName): value for colName, value in colNames.items()
+                          if not startsWith or colName.startswith(startsWith)}
+        else:
+            raise NotImplementedError(f'Unsupported type {type(colNames)} for en-column names to translate spec')
+        
+        # Done
+        return trColNames
+        
+    # Output file pathname generation.
+    def targetFilePathName(self, suffix, prefix=None, tgtFolder=None):
+        
+        return os.path.join(tgtFolder or self.tgtFolder, (prefix or self.tgtPrefix) + suffix)
+    
+    def relativeRunFolderUrl(self, runFolderPath):
+
+        return os.path.relpath(runFolderPath, self.tgtFolder).replace(os.sep, '/')
+    
+    # Install needed attached files for HTML report.
+    def installAttFiles(self, attFiles):
+        
+        # Given attached files.
+        for fn in attFiles:
+            shutil.copy(KInstDirPath / 'report' / fn, self.tgtFolder)
+            
+        # Python source files.
+        for fpn in self.pySources:
+            shutil.copy(fpn, self.tgtFolder)
+    
+    # Get Jinja2 template environment for HTML reports.
+    def getTemplateEnv(self):
+        
+        # Build and configure jinja2 environment if not already done.
+        if self.tmplEnv is None:
+            self.tmplEnv = jinja2.Environment(loader=jinja2.FileSystemLoader([KInstDirPath]),
+                                              trim_blocks=True, lstrip_blocks=True)
+            # self.tmplEnv.filters.update(trace=_jcfPrint2StdOut)  # Template debugging ...
+
+        return self.tmplEnv
+    
+    def asWorkbook(self, subset=None, rebuild=False):
+
+        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
+        where each item is a named worksheet
+
+        Parameters:
+        :param subset: Selected list of data categories to include ; None = [] = all
+                       (categories in {'specs'})
+        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
+                        (not used here)
+        """
+        
+        logger.debug(f'ResultsReport.asWorkbook({subset=}, {rebuild=})')
+
+        ddfWbk = dict()
+
+        # Specs (no need to check if 'specs' in subset: we have nothing else than results specs here).
+        for spName, dfSpData in self.resultsSet.specs2Tables().items():
+            logger.info1(f'* {spName} ...')
+            ddfWbk[self.tr(spName)] = (dfSpData, True)
+
+        # Done
+        return ddfWbk
+ 
+    def toExcel(self, fileName=None, engine='openpyxl', ext='.xlsx', rebuild=False):
+
+        """Export to a workbook file format (Excel, ODF, ...)
+
+        Parameters:
+        :param fileName: target file path name ; None => self.tgtFolder/self.tgtPrefix + ext
+        :param engine: Python module to use for exporting (openpyxl=default, or odf)
+        :param ext: extension of target file, if not specified in filename
+        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
+        """
+        
+        logger.debug(f'ResultsReport.toExcel({fileName=}, {ext=}, {rebuild=})')
+
+        fileName = fileName or os.path.join(self.tgtFolder, self.tgtPrefix + ext)
+        
+        with pd.ExcelWriter(fileName, engine=engine) as xlsxWriter:
+            logger.info(f'Building workbook report {fileName} ...')
+            for wstName, (dfWstData, wstIndex) in self.asWorkbook(rebuild=rebuild).items():
+                dfWstData.to_excel(xlsxWriter, sheet_name=wstName, index=wstIndex)
+
+        logger.info('... done.')
+
+        return fileName
+
+    def toOpenDoc(self, fileName=None, rebuild=False):
+
+        """Export report to Open Doc worksheet format
+
+        Parameters:
+        :param fileName: target file path name ; None => self.tgtFolder/self.tgtPrefix + ext
+        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
+        """
+    
+        assert pkgver.parse(pd.__version__).release >= (1, 1), \
+               'Don\'t know how to write to OpenDoc format before Pandas 1.1'
+        
+        return self.toExcel(fileName, engine='odf', ext='.ods', rebuild=rebuild)
+
+    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
+    # in the "one analysis at a time" case.
+    # (sort, convert units, round values, and style).
+    # To be specialized in derived classes (here, we do nothing) !
+    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
+    # Return a pd.DataFrame.Styler
+    def finalFormatEachAnalysisData(self, dfTrData, sort=True, convert=True, round_=True, style=True):
+        
+        return dfTrData.style  # Nothing done here, specialize in derived class if needed.
+
+    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
+    # in the "all analyses at once" case.
+    # (sort, convert units, round values, and style).
+    # To be specialized in derived classes (here, we do nothing) !
+    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
+    # Return a pd.DataFrame.Styler
+    def finalFormatAllAnalysesData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
+        
+        logger.debug(f'ResultsReport.finalFormatAllAnalysesData({sort=}, {indexer=}, {convert=}, {round_=}, {style=})')
+
+        return dfTrData.style  # Nothing done here, specialize in derived class if needed.
+
+
+class DSResultsDistanceReport(ResultsReport):
+
+    """DS results reports class (Excel and HTML, targeting similar layout as in Distance 6+)"""
+
+    # Translation table.
+    DTrans = _mergeTransTables(base=ResultsReport.DTrans,
+        update=dict(en={'RunFolder': 'Analysis', 'Synthesis': 'Synthesis',
+                        'Details': 'Details', 'Traceability': 'Traceability',
+                        'Table of contents': 'Table of contents',
+                        'Click on analysis # for details': 'Click on analysis number to get to detailed report',
+                        'Main results': 'Results: main figures',
+                        'Detailed results': 'Results: all details',
+                        'Analysis': 'Analysis',
+                        'Download Excel': 'Download as Excel(TM) file',
+                        'Summary computation log': 'Summary computation log',
+                        'Detailed computation log': 'Detailed computation log',
+                        'Previous analysis': 'Previous analysis', 'Next analysis': 'Next analysis',
+                        'Back to top': 'Back to global report',
+                        'Observation model': 'Observations (fitted)',
+                        'Real observations': 'Observations (sampled)',
+                        'Fixed bin distance histograms': 'Fixed bin distance histograms',
+                        'Distance': 'Distance', 'Number of observations': 'Number of observations',
+                        'Page generated': 'Generated', 'with': 'with',
+                        'with icons from': 'with icons from',
+                        'and': 'and', 'in': 'in', 'sources': 'sources', 'on': 'on',
+                        'Point': 'Point transect', 'Line': 'Line transect',
+                        'Radial': 'Radial distance', 'Perpendicular': 'Perpendicular distance',
+                        'Radial & Angle': 'Radial distance & Angle',
+                        'Clustering': 'With clustering', 'No clustering': 'No clustering',
+                        'Meter': 'Meter', 'Kilometer': 'Kilometer', 'Mile': 'Mile',
+                        'Inch': 'Inch', 'Feet': 'Feet', 'Yard': 'Yard', 'Nautical mile': 'Nautical mile',
+                        'Hectare': 'Hectare', 'Acre': 'Acre', 'Sq. Meter': 'Sq. Meter',
+                        'Sq. Kilometer': 'Sq. Kilometer', 'Sq. Mile': 'Sq. Mile',
+                        'Sq. Inch': 'Sq. Inch', 'Sq. Feet': 'Sq. Feet', 'Sq. Yard': 'Sq. Yard',
+                        'Sq. Nautical mile': 'Sq. Nautical mile',
+                        'Traceability tech. details':
+                          'Traceability data (more technical details on how this report was produced)',
+                        'Order': 'Order', 'Qual Bal': 'Qual Bal', 'Pre-selection': 'Pre-selection'},
+                    fr={'DossierExec': 'Analyse', 'Synthesis': 'Synthèse',
+                        'Details': 'Détails', 'Traceability': 'Traçabilité',
+                        'Table of contents': 'Table des matières',
+                        'Click on analysis # for details':
+                          "Cliquer sur le numéro de l'analyse pour accéder au rapport détaillé",
+                        'Main results': 'Résultats : indicateurs principaux',
+                        'Detailed results': 'Résultats : tous les détails',
+                        'Analysis': 'Analyse',
+                        'Download Excel': 'Télécharger le classeur Excel (TM)',
+                        'Summary computation log': 'Résumé des calculs',
+                        'Detailed computation log': 'Détail des calculs',
+                        'Previous analysis': 'Analyse précédente', 'Next analysis': 'Analyse suivante',
+                        'Back to top': 'Retour au rapport global',
+                        'Observation model': 'Observations (fitted)',  # No actual translation for plots
+                        'Real observations': 'Observations (sampled)',  # Idem
+                        'Fixed bin distance histograms': 'Fixed bin distance histograms',  # Idem
+                        'Distance': 'Distance',  # Idem
+                        'Number of observations': 'Number of observations',  # Idem
+                        'Page generated': 'Généré', 'with': 'avec',
+                        'with icons from': 'avec les pictogrammes de',
+                        'and': 'et', 'in': 'dans', 'sources': 'sources', 'on': 'le',
+                        'Point': 'Point fixe', 'Line': 'Transect',
+                        'Radial': 'Distance radiale', 'Perpendicular': 'Distance perpendiculaire',
+                        'Radial & Angle': 'Distance radiale & Angle',
+                        'Clustering': 'Avec clustering', 'No clustering': 'Sans clustering',
+                        'Meter': 'Mètre', 'Kilometer': 'Kilomètre', 'Mile': 'Mile',
+                        'Inch': 'Pouce', 'Feet': 'Pied', 'Yard': 'Yard', 'Nautical mile': 'Mille marin',
+                        'Hectare': 'Hectare', 'Acre': 'Acre', 'Sq. Meter': 'Mètre carré',
+                        'Sq. Kilometer': 'Kilomètre carré', 'Sq. Mile': 'Mile carré',
+                        'Sq. Inch': 'Pouce carré', 'Sq. Feet': 'Pied carré', 'Sq. Yard': 'Yard carré',
+                        'Sq. Nautical mile': 'Mille marin carré',
+                        'Traceability tech. details':
+                          'Données de traçabilité (autres détails techniques sur comment ce rapport a été produit)',
+                        'Order': 'Ordre', 'Qual Bal': 'Qual Equi', 'Pre-selection': 'Pré-selection'}))
+
+    @staticmethod
+    def noDupColumns(cols, log=True, head='Results cols'):
+
+        """Drop duplicates from a column list, and possibly warn about which"""
+
+        dups = None
+        if isinstance(cols, list):
+            dups = [col for ind, col in enumerate(cols) if col in cols[:ind]]
+            if len(dups) > 0:
+                cols = [col for ind, col in enumerate(cols) if col not in cols[:ind]]
+
+        elif isinstance(cols, pd.MultiIndex):
+            dups = cols[cols.duplicated()]
+            if len(dups) > 0:
+                cols = cols.drop_duplicates()
+
+        if log and dups is not None and len(dups) > 0:
+            logger.warning(head + ': Dropped {} duplicate(s) {}'
+                                  .format(len(dups), ', '.join(str(dup) for dup in dups)))
+
+        return cols
+
+    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                 synthCols=None, sortCols=None, sortAscend=None, dCustomTrans=None, lang='en',
+                 plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
+                 plotLineWidth=2, plotDotWidth=6, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
+                 pySources=[], tgtFolder='.', tgtPrefix='results', logProgressEvery=5):
+                       
+        """Ctor
+        
+        Parameters:
+        :param resultsSet: source results
+        :param title: main page title (and <title> tag in HTML header)
+        :param subTitle: main page sub-title (under the title, lower font size)
+        :param description: main page description text (under the sub-title, lower font size)
+        :param anlysSubTitle: analysis pages title
+        :param keywords: for HTML header <meta name="keywords" ...>
+        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
+        :param sortCols: sorting columns for report tables
+        :param sortAscend: sorting order for report tables, as a bool or list of bools, of len(synthCols)
+        :param dCustomTrans: custom translations to complete the report standard ones,
+                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
+        :param lang: Target language for translation
+        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
+        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
+        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
+        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
+        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
+        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
+        :param pySources: path-name of source files to copy in report folder and link in report
+        :param tgtFolder: target folder for the report (for _all_ generated files)
+        :param tgtPrefix: default target file name for the report
+        :param logProgressEvery: every such nb of details pages, log some elapsed time stats
+                                 and end of generation forecast
+        """
+    
+        assert synthCols is None or isinstance(synthCols, list) or isinstance(synthCols, pd.MultiIndex), \
+               'Synthesis columns must be specified as None (all), or as a list of tuples, or as a pandas.MultiIndex'
+        
+        assert logProgressEvery > 0, 'logProgressEvery must be positive'
+
+        super().__init__(resultsSet, title, subTitle, description, keywords,
+                         dCustomTrans=dCustomTrans, lang=lang, pySources=pySources,
+                         tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
+        
+        self.synthCols = self.noDupColumns(synthCols, head='Synthesis columns')
+        self.sortCols = self.noDupColumns(sortCols, head='Sorting columns')
+        self.sortAscend = sortAscend
+
+        assert sortAscend is None or isinstance(sortAscend, bool) or len(sortAscend) == len(self.sortCols), \
+               'Some duplicated sort columns were removed, or sortAscend is too long or short, ' \
+               'such that sortAscend and sortCols are not compatible => please fix these params'
+        
+        self.plotImgFormat = plotImgFormat
+        self.plotImgSize = plotImgSize
+        self.plotImgQuality = plotImgQuality
+        self.plotLineWidth = plotLineWidth
+        self.plotDotWidth = plotDotWidth
+        self.plotFontSizes = plotFontSizes
+
+        self.anlysSubTitle = anlysSubTitle
+
+        self.logProgressEvery = logProgressEvery
+        
+    def checkNeededColumns(self):
+
+        """Side check as soon as possible : Are all report needed columns available ?
+        (now that computed columns have been ... post-computed through self.resultsSet.dfFilSorData calls)
+
+        :raise: AssertionError if not the case
+        """
+        raise NotImplementedError('Abstract method DSResultsDistanceReport.checkNeededColumns must not be called')
+
+    # Static attached files for HTML report.
+    AttachedFiles = ['report.css', 'fa-feather-alt.svg', 'fa-angle-up.svg', 'fa-file-excel.svg',
+                     'fa-file-excel-hover.svg', 'fa-arrow-left-hover.svg', 'fa-arrow-left.svg',
+                     'fa-arrow-right-hover.svg', 'fa-arrow-right.svg',
+                     'fa-arrow-up-hover.svg', 'fa-arrow-up.svg']
+    
+    # Plot ... data to be plotted, and draw resulting figure to image files.
+    PlotImgPrfxQqPlot = 'qqplot'
+    PlotImgPrfxDetProb = 'detprob'
+    PlotImgPrfxProbDens = 'probdens'
+    PlotImgPrfxDistHist = 'disthist'
+    StripPlotAlpha, StripPlotJitter = 0.5, 0.3
+    RefDistHistBinWidths = [10, 20, 40]  # unit = Distance unit
+    HistBinWithRefDist = 600  # unit = Distance unit
+    
+    def generatePlots(self, plotsData, tgtFolder, rebuild=True, sDistances=None, lang='en',
+                      imgFormat='png', imgSize=(640, 400), imgQuality=90, grid=True, transparent=False,
+                      colors=dict(background='#f9fbf3', histograms='blue',
+                                  multihistograms=['blue', 'green', 'red'], curves='red', dots='green'),
+                      widths=dict(lines=2, dots=6), fontSizes=dict(title=11, axes=10, ticks=9, legend=10)):
+        
+        dPlots = dict()
+
+        # For each plot from extracted plotsData, 
+        for title, pld in plotsData.items():
+            
+            # a. Determine target image file name, and if rebuild not forced, don't regenerate it if already there
+            if 'Qq-plot' in title:
+                tgtFileName = self.PlotImgPrfxQqPlot
+            elif 'Detection Probability' in title:
+                sufx = title.split(' ')[-1]  # Assume last "word" is the plot number
+                sufx = sufx if sufx.isnumeric() else ''  # But when only 1, there's no number.
+                tgtFileName = self.PlotImgPrfxDetProb + sufx
+            elif 'Pdf' in title:
+                sufx = title.split(' ')[-1]  # Assume last "word" is the plot number
+                sufx = sufx if sufx.isnumeric() else ''  # But when only 1, there's no number.
+                tgtFileName = self.PlotImgPrfxProbDens + sufx
+            else:
+                raise NotImplementedError(f'Unsupported plot "{title}" found in loaded plot data')
+            tgtFileName = tgtFileName + '.' + imgFormat.lower()
+
+            dPlots[title] = tgtFileName  # Save image URL
+
+            tgtFilePathName = os.path.join(tgtFolder, tgtFileName)                
+            if not rebuild and os.path.isfile(tgtFilePathName):
+                continue  # Already done, and not to be done again.
+
+            # b. Create the target figure and one-only subplot (note: QQ plots with forced height square shape).
+            figHeight = imgSize[1] / plt.rcParams['figure.dpi']
+            figWidth = figHeight if 'Qq-plot' in title else imgSize[0] / plt.rcParams['figure.dpi']
+
+            fig = plt.figure(figsize=(figWidth, figHeight))
+            axes = fig.subplots()
+            
+            # c. Plot a figure from the plot data (3 possible types, from title).
+            if 'Qq-plot' in title:
+                
+                n = len(pld['dataRows'])
+                df2Plot = pd.DataFrame(data=pld['dataRows'],
+                                       columns=[self.tr(s) for s in ['Observation model', 'Real observations']],
+                                       index=np.linspace(0.5/n, 1.0-0.5/n, n))
+                
+                df2Plot.plot(ax=axes, zorder=10, color=[colors['histograms'], colors['curves']],
+                             linewidth=widths['lines'], grid=grid,
+                             xlim=(pld['xMin'], pld['xMax']), ylim=(pld['yMin'], pld['yMax']))
+
+            elif 'Detection Probability' in title:
+                
+                # if sDistances is not None:
+                #     axes2 = axes.twinx()
+                #     sb.stripplot(ax=axes2, zorder=5, x=sDistances, color=colors['dots'], size=widths['dots'],
+                #                  alpha=self.StripPlotAlpha, jitter=self.StripPlotJitter)
+
+                df2Plot = pd.DataFrame(data=pld['dataRows'], 
+                                       columns=[pld['xLabel'], pld['yLabel'] + ' (sampled)',
+                                                pld['yLabel'] + ' (fitted)'])
+                df2Plot.set_index(pld['xLabel'], inplace=True)
+                
+                df2Plot.plot(ax=axes, zorder=10, color=[colors['histograms'], colors['curves']],
+                             linewidth=widths['lines'], grid=grid,
+                             xlim=(pld['xMin'], pld['xMax']), ylim=(pld['yMin'], pld['yMax']))
+                
+                aMTicks = axes.get_xticks()
+                axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))
+                axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)
+                axes.grid(True, which='minor', zorder=0)
+
+            elif 'Pdf' in title:
+                
+                # if sDistances is not None:
+                #     axes2 = axes.twinx()
+                #     sb.stripplot(ax=axes2, zorder=5, x=sDistances, color=colors['dots'], size=widths['dots'],
+                #                  alpha=self.StripPlotAlpha, jitter=self.StripPlotJitter)
+
+                df2Plot = pd.DataFrame(data=pld['dataRows'], 
+                                       columns=[pld['xLabel'], pld['yLabel'] + ' (sampled)',
+                                                pld['yLabel'] + ' (fitted)'])
+                df2Plot.set_index(pld['xLabel'], inplace=True)
+                
+                df2Plot.plot(ax=axes, zorder=10, color=[colors['histograms'], colors['curves']],
+                             linewidth=widths['lines'], grid=grid,
+                             xlim=(pld['xMin'], pld['xMax']), ylim=(pld['yMin'], pld['yMax']))
+
+                aMTicks = axes.get_xticks()
+                axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))
+                axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)
+                axes.grid(True, which='minor', zorder=0)
+
+            else:
+                raise NotImplementedError(f'Unsupported plot "{title}" found in loaded plot data')
+
+            # d. Finish plotting.
+            axes.legend(df2Plot.columns, fontsize=fontSizes['legend'])
+            axes.set_title(label=pld['title'] + ' : ' + pld['subTitle'],
+                           fontdict=dict(fontsize=fontSizes['title']), pad=10)
+            axes.set_xlabel(pld['xLabel'], fontsize=fontSizes['axes'])
+            axes.set_ylabel(pld['yLabel'], fontsize=fontSizes['axes'])
+            axes.tick_params(axis='both', labelsize=fontSizes['ticks'])
+            axes.grid(True, which='major', zorder=0)
+            if not transparent:
+                axes.set_facecolor(colors['background'])
+                fig.patch.set_facecolor(colors['background'])
+                
+            # e. Generate an image file for the plot figure (forcing the specified patch background color).
+            fig.tight_layout()
+            pilArgs = dict(quality=imgQuality) if imgFormat == 'jpg' else dict()
+            fig.savefig(tgtFilePathName, bbox_inches='tight', transparent=transparent,
+                        facecolor=axes.figure.get_facecolor(), edgecolor='none', pil_kwargs=pilArgs)
+
+            # g. Memory cleanup (does not work in interactive mode ... but OK thanks to plt.ioff above)
+            axes.clear()
+            fig.clear()
+            plt.close(fig)
+
+        # Standard fixed-bin-width super-imposed histograms (multiple bin width, scaled with distMax)
+        if sDistances is not None:
+
+            # a. Determine target image file name, and if rebuild not forced, don't regenerate it if already there
+            title = 'Standard Distance Histograms'
+            tgtFileName = self.PlotImgPrfxDistHist
+            tgtFileName = tgtFileName + '.' + imgFormat.lower()
+
+            dPlots[title] = tgtFileName  # Save image URL
+
+            tgtFilePathName = os.path.join(tgtFolder, tgtFileName)                
+            if rebuild or not os.path.isfile(tgtFilePathName):  # Do it only if forced to do or not already done.
+
+                # b. Create the target figure and one-only subplot
+                figHeight = imgSize[1] / plt.rcParams['figure.dpi']
+                figWidth = imgSize[0] / plt.rcParams['figure.dpi']
+
+                fig = plt.figure(figsize=(figWidth, figHeight))
+                axes = fig.subplots()
+                    
+                # c. Plot the figure from the distance data
+                # axes2 = axes.twinx()
+                # sb.stripplot(ax=axes2, zorder=5, x=sDistances, color=colors['dots'], size=widths['dots'],
+                #              alpha=self.StripPlotAlpha, jitter=self.StripPlotJitter)
+
+                distMax = sDistances.max()
+
+                distHistBinWidths = np.array(self.RefDistHistBinWidths, dtype=float)
+                distHistBinWidths *= 2 ** int(distMax / self.HistBinWithRefDist)
+
+                for binWidthInd, binWidth in enumerate(distHistBinWidths):
+                
+                    aDistBins = np.linspace(start=0, stop=binWidth * int(distMax / binWidth),
+                                            num=1 + int(distMax / binWidth)).tolist()
+                    if aDistBins[-1] < distMax:
+                        aDistBins.append(distMax)
+
+                    binWidthRInd = len(distHistBinWidths) - binWidthInd - 1
+                    sDistances.plot.hist(ax=axes, bins=aDistBins, fill=None, linewidth=1,
+                                         zorder=10*(1+binWidthRInd), rwidth=1-0.15*binWidthRInd,
+                                         edgecolor=colors['multihistograms'][binWidthInd])
+
+                axes.set_xlim((0, distMax))
+                axes.grid(True, which='minor', zorder=0)
+                aMTicks = axes.get_xticks()
+                axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)
+                axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))
+                axes.yaxis.set_major_locator(pltt.MaxNLocator(integer=True))
+
+                # d. Finish plotting.
+                axes.legend([self.tr('Real observations') + ' ' + str(int(binWidth))
+                             for binWidth in distHistBinWidths], fontsize=fontSizes['legend'])
+                axes.set_title(label=self.tr('Fixed bin distance histograms'),
+                               fontdict=dict(fontsize=fontSizes['title']), pad=10)
+                axes.set_xlabel(self.tr('Distance'), fontsize=fontSizes['axes'])
+                axes.set_ylabel(self.tr('Number of observations'), fontsize=fontSizes['axes'])
+                axes.tick_params(axis='both', labelsize=fontSizes['ticks'])
+                axes.grid(True, which='major', zorder=0)
+                if not transparent:
+                    axes.set_facecolor(colors['background'])
+                    fig.patch.set_facecolor(colors['background'])
+                    
+                # e. Generate an image file for the plot figure (forcing the specified patch background color).
+                fig.tight_layout()
+                pilArgs = dict(quality=imgQuality) if imgFormat == 'jpg' else dict()
+                fig.savefig(tgtFilePathName, bbox_inches='tight', transparent=transparent,
+                            facecolor=axes.figure.get_facecolor(), edgecolor='none', pil_kwargs=pilArgs)
+
+                # f. Memory cleanup (does not work in interactive mode ... but OK thanks to plt.ioff above)
+                axes.clear()
+                fig.clear()
+                plt.close(fig)
+
+        return dPlots
+    
+    # Top page
+    def toHtmlAllAnalyses(self, rebuild=False):
+        
+        logger.debug(f'DSResultsDistanceReport.toHtmlAllAnalyses({rebuild=})')
+        logger.info('Top page ...')
+        
+        # 1. Generate post-processed and translated synthesis table.
+        # a. Add run folder column if not selected (will serve to generate the link to the analysis detailed report)
+        synCols = self.synthCols
+        if self.resultsSet.analysisClass.RunFolderColumn not in synCols:
+            synCols += [self.resultsSet.analysisClass.RunFolderColumn]
+        dfSyn = self.resultsSet.dfTransData(self.lang, columns=synCols)
+        
+        # b. Links to each analysis detailed report.
+        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSyn))), 1))
+        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
+
+        def numNavLink(sAnlys):
+            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
+       
+        # c. Post-format as specified in actual class.
+        dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=numNavLink,
+                                                 convert=True, round_=True, style=True)
+
+        # 2. Generate post-processed and translated detailed table.
+        dfDet = self.resultsSet.dfTransData(self.lang)
+
+        # a. Add run folder column if not there (will serve to generate the link to the analysis detailed report)
+        detTrCols = list(dfDet.columns)
+        if self.trRunFolderCol not in detTrCols:
+            detTrCols += [self.trRunFolderCol]
+        dfDet = dfDet.reindex(columns=detTrCols)
+       
+        # b. Links to each analysis detailed report.
+        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=numNavLink,
+                                                 convert=False, round_=False, style=True)
+
+        # 3. Generate traceability infos parts (results specs).
+        ddfTrc = self.asWorkbook(subset=['specs'])
+
+        # 4.Generate top report page.
+        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
+        tmpl = self.getTemplateEnv().get_template('mcds/top.htpl')
+        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
+        html = tmpl.render(synthesis=dfsSyn.render(),  # escape=False, index=False),
+                           details=dfsDet.render(),  # escape=False, index=False),
+                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
+                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
+                           title=self.title, subtitle=self.subTitle,
+                           description=self.description, keywords=self.keywords,
+                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang], 
+                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
+                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(),
+                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
+                           areaUnit=self.tr(self.resultsSet.areaUnit),
+                           surveyType=self.tr(self.resultsSet.surveyType),
+                           distanceType=self.tr(self.resultsSet.distanceType),
+                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'))
+        html = re.sub('(?:[ \t]*\n){2,}', '\n'*2, html)  # Cleanup blank line series to one only.
+
+        # Write top HTML to file.
+        htmlPathName = self.targetFilePathName(suffix='.html')
+        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
+            tgtFile.write(html)
+
+        return htmlPathName
+    
+    def getRawTransData(self, **kwargs):
+
+        """Retrieve input translated raw data for HTML pages specific to each analysis
+
+        Parameters:
+        :param kwargs: Args relevant to derived classes (none here).
+
+        :return: 2 dataFrames, for synthesis (synthCols) and detailed (all) column sets,
+                 + None (other implementations may use this place for something relevant)
+        """
+
+        # Generate translated synthesis table.
+        synthCols = self.synthCols
+        if self.resultsSet.analysisClass.RunFolderColumn not in synthCols:
+            synthCols += [self.resultsSet.analysisClass.RunFolderColumn]
+        dfSynthRes = self.resultsSet.dfTransData(self.lang, columns=synthCols)
+
+        # Generate translated detailed table.
+        dfDetRes = self.resultsSet.dfTransData(self.lang)
+
+        # Side check as soon as possible : Are all report-needed columns available ?
+        self.checkNeededColumns()
+
+        return dfSynthRes, dfDetRes, None
+
+    def toHtmlEachAnalysis(self, rebuild=False, generators=0, topSuffix='.html', **kwargs):
+        
+        """Generate HTML page specific to each analysis
+
+        Parameters:
+        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
+                        otherwise, use any cached data or existing plot image file
+        :param generators: Number of parallel (process) generators to use :
+                           - 0 => auto-number, based on the actual number of CPUs onboard,
+                           - > 0 => the actual number to use
+        :param kwargs: Other args relevant to derived classes.
+        """
+
+        # Get source translated raw data to format
+        dfSynthRes, dfDetRes, _ = self.getRawTransData(rebuild=rebuild, **kwargs)
+
+        # Generate translated synthesis and detailed tables.
+        logger.info(f'Analyses pages ({len(dfSynthRes)}) ...')
+        executor = Executor(processes=generators)
+        nExpWorkers = executor.expectedWorkers()
+        if nExpWorkers > 1:
+            logger.info(f'... through at most {nExpWorkers} parallel generators ...')
+
+        # 1. 1st pass : Generate previous / next list (for navigation buttons) with the sorted order if any
+        dfSynthRes = self.finalformatEachAnalysisData(dfSynthRes, sort=True, indexer=True,
+                                                      convert=False, round_=False, style=False).data
+        sCurrUrl = dfSynthRes[self.trRunFolderCol]
+        sCurrUrl = sCurrUrl.apply(lambda path: self.targetFilePathName(tgtFolder=path, prefix='index', suffix='.html'))
+        sCurrUrl = sCurrUrl.apply(lambda path: os.path.relpath(path, self.tgtFolder).replace(os.sep, '/'))
+        dfAnlysUrls = pd.DataFrame(dict(current=sCurrUrl, previous=np.roll(sCurrUrl, 1), next=np.roll(sCurrUrl, -1)))
+
+        # And don't forget to sort & index detailed results the same way as synthesis ones.
+        dfDetRes = self.finalformatEachAnalysisData(dfDetRes, sort=True, indexer=True,
+                                                    convert=False, round_=False, style=False).data
+
+        # 2. 2nd pass : Generate
+        # a. Stops heavy Matplotlib.pyplot memory leak in generatePlots (WTF !?)
+        wasInter = plt.isinteractive()
+        if wasInter:
+            plt.ioff()
+
+        # b. Generate analysis detailed HTML page, for each analysis, parallely.
+        topHtmlPathName = self.targetFilePathName(suffix=topSuffix)
+        trCustCols = [col for col in self.resultsSet.transCustomColumns(self.lang) if col in dfDetRes.columns]
+        
+        # i. Start generation of all pages in parallel (unless specified not)
+        genStart = pd.Timestamp.now()  # Start of elapsed time measurement.
+        pages = dict()
+        for lblRes in dfSynthRes.index:
+            
+            logger.info1(f'#{lblRes}/{len(dfSynthRes)}: '
+                         + ' '.join(f'{k}={v}' for k, v in dfDetRes.loc[lblRes, trCustCols].iteritems()))
+
+            pgFut = executor.submit(self._toHtmlAnalysis, lblRes, dfSynthRes.loc[lblRes],
+                                    dfDetRes.loc[lblRes], dfAnlysUrls.loc[lblRes],
+                                    topHtmlPathName, trCustCols, rebuild=rebuild)
+                                    
+            pages[pgFut] = lblRes
+        
+        if executor.isParallel():
+            logger.info1(f'Waiting for generators results ...')
+        
+        # ii. Wait for end of generation of each page, as it comes first.
+        nDone = 0
+        for pgFut in executor.asCompleted(pages):
+
+            # If there, it's because it's done (or crashed) !
+            exc = pgFut.exception()
+            if exc:
+                logger.error(f'#{pages[pgFut]}: Exception: {exc}')
+            elif executor.isParallel():
+                logger.info1(f'#{pages[pgFut]}: Done.')
+
+            # Report elapsed time and number of pages completed until now (once per self.logProgressEvery pages).
+            nDone += 1
+            if nDone % self.logProgressEvery == 0 or nDone == len(pages):
+                now = pd.Timestamp.now()
+                elapsedTilNow = now - genStart
+                expectedEnd = now
+                if nDone < len(pages):
+                    expectedEnd += pd.Timedelta(elapsedTilNow.value * (len(pages) - nDone) / nDone)
+                logger.info1('{}/{} pages in {} (mean {:.2f}s){}'
+                             .format(nDone, len(pages), str(elapsedTilNow.round('S')).replace('0 days ', ''),
+                                     elapsedTilNow.total_seconds() / nDone,
+                                     ': done.' if nDone == len(pages)
+                                               else ': should end around ' + expectedEnd.strftime('%Y-%m-%d %H:%M:%S')
+                                                                             .replace(now.strftime('%Y-%m-%d '), '')))
+
+        # iii. Terminate parallel executor.
+        executor.shutdown()
+
+        # c. Restore Matplotlib.pyplot interactive mode as it was before.
+        if wasInter:
+            plt.ion()
+
+    def _toHtmlAnalysis(self, lblRes, sSynthRes, sDetRes, sResNav, topHtmlPathName, trCustCols, rebuild=True):
+
+        # Postprocess synthesis table :
+        dfSyn = pd.DataFrame([sSynthRes])
+        idxFmt = '{{:0{}d}}'.format(1+max(int(math.log10(len(dfSyn))), 1))
+        dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+        dfSyn.index = dfSyn.index.map(lambda n: idxFmt.format(n))
+        dfsSyn = self.finalformatEachAnalysisData(dfSyn, sort=False, indexer=None,
+                                                  convert=True, round_=True, style=True)
+        
+        # Post-process detailed table :
+        dfDet = pd.DataFrame([sDetRes])
+        dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+        dfDet.index = dfDet.index.map(lambda n: idxFmt.format(n))
+        dfsDet = self.finalformatEachAnalysisData(dfDet, sort=False, indexer=None,
+                                                  convert=False, round_=False, style=True)
+        
+        # Generate analysis report page.
+        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
+        subTitle = '{} {} : {{}}'.format(self.tr('Analysis'), idxFmt).format(lblRes, self.anlysSubTitle)
+        engineClass = self.resultsSet.engineClass
+        anlysFolder = sDetRes[self.trRunFolderCol]
+        tmpl = self.getTemplateEnv().get_template('mcds/anlys.htpl')
+        html = tmpl.render(synthesis=dfsSyn.render(),
+                           details=dfsDet.render(),
+                           log=engineClass.decodeLog(anlysFolder),
+                           output=engineClass.decodeOutput(anlysFolder),
+                           plots=self.generatePlots(plotsData=engineClass.decodePlots(anlysFolder), 
+                                                    sDistances=engineClass.loadDataFile(anlysFolder).DISTANCE,
+                                                    tgtFolder=anlysFolder, lang='en',  # No translation.
+                                                    imgFormat=self.plotImgFormat, imgSize=self.plotImgSize,
+                                                    imgQuality=self.plotImgQuality,
+                                                    widths=dict(lines=self.plotLineWidth, dots=self.plotDotWidth),
+                                                    colors=dict(background='#f9fbf3', histograms='blue',
+                                                                multihistograms=['blue', 'green', 'red'],
+                                                                curves='red', dots='green'),
+                                                    fontSizes=self.plotFontSizes,
+                                                    rebuild=rebuild),
+                           title=self.title, subtitle=subTitle, keywords=self.keywords,
+                           navUrls=dict(prevAnlys='../' + sResNav.previous,
+                                        nextAnlys='../' + sResNav.next,
+                                        back2Top='../' + os.path.basename(topHtmlPathName)),
+                           tr=self.dTrans[self.lang], pySources=[pl.Path(fpn).name for fpn in self.pySources],
+                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(), 
+                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
+                           areaUnit=self.tr(self.resultsSet.areaUnit),
+                           surveyType=self.tr(self.resultsSet.surveyType),
+                           distanceType=self.tr(self.resultsSet.distanceType),
+                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'))
+        html = re.sub('(?:[ \t]*\n){2,}', '\n'*2, html)  # Cleanup blank line series to one only.
+
+        # Write analysis HTML to file.
+        htmlPathName = self.targetFilePathName(tgtFolder=anlysFolder, prefix='index', suffix='.html')
+        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
+            tgtFile.write(html)
+
+    def toHtml(self, rebuild=False):
+    
+        """HTML report generation.
+
+        Parameters:
+        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
+                        otherwise, use any cached data or existing plot image file
+
+        Note: Parallelism does not work for this class, hence the absence of the generators parameter.
+        """
+
+        logger.debug(f'DSResultsDistanceReport.toHtml({rebuild=})')
+
+        # Install needed attached files.
+        self.installAttFiles(self.AttachedFiles)
+            
+        # Generate synthesis report page (all analyses in one page).
+        topHtmlPathName = self.toHtmlAllAnalyses(rebuild=rebuild)
+
+        # Generate detailed report pages (one page for each analysis)
+        # Note: For some obscure reason, parallelism does not work here (while it does for derived classes !).
+        generators = 1
+        self.toHtmlEachAnalysis(rebuild=rebuild, generators=generators)
+
+        logger.info('... done.')
+        
+        return topHtmlPathName
+
+    def asWorkbook(self, subset=None, rebuild=False):
+
+        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
+        where each item is a named worksheet
+
+        Parameters:
+        :param subset: Selected list of data categories to include ; None = [] = all
+                       (categories in {'specs'})
+        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
+                        (not used here)
+        """
+        
+        logger.debug(f'DSResultsDistanceReport.asWorkbook({subset=}, {rebuild=})')
+
+        ddfWbk = dict()
+
+        # Build results worksheets if specified in subset
+        if not subset or 'results' in subset:
+
+            # Synthesis
+            logger.info1('* synthesis ...')
+            synCols = self.synthCols
+            if self.resultsSet.analysisClass.RunFolderColumn not in synCols:
+                synCols += [self.resultsSet.analysisClass.RunFolderColumn]
+            dfSyn = self.resultsSet.dfTransData(self.lang, columns=synCols)
+            dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+            
+            # ... Convert run folder columns to hyperlink
+            def toHyperlink(path):
+                return '=HYPERLINK("file:///{path}", "{path}")'.format(path=path)
+            dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(toHyperlink)
+            
+            dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=True,
+                                                     convert=True, round_=True, style=True)
+
+            ddfWbk[self.tr('Synthesis')] = (dfsSyn, True)
+
+            # Details
+            logger.info1('* details ...')
+            dfDet = self.resultsSet.dfTransData(self.lang)
+            dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+            dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(toHyperlink)
+            
+            dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
+                                                     convert=False, round_=False, style=True)
+
+            ddfWbk[self.tr('Details')] = (dfsDet, True)
+
+        # Append inherited worksheets.
+        ddfWbk.update(super().asWorkbook(subset=subset, rebuild=rebuild))
+
+        # Done
+        return ddfWbk
+ 
+
+class MCDSResultsDistanceReport(DSResultsDistanceReport):
+
+    """A specialized report for MCDS analyses, targeting similar layout as in Distance 6+,
+    with actual output formatting."""
+
+    DTrans = _mergeTransTables(base=DSResultsDistanceReport.DTrans,
+        update=dict(en={'Study type:': "<strong>Study type</strong>:",
+                        'Units used:': "<strong>Units used</strong>:",
+                        'for distances': 'for distances',
+                        'for areas': 'for areas',
+                        'Estimator selection criterion:': '<strong>Adjustment term selection criterion</strong>:',
+                        'Confidence value interval:': '<strong>Confidence value interval</strong>:',
+                        'If not listed in table below, please': 'If not listed in table below, please',
+                        'BE AWARE that different values have been used among analyses':
+                          '<strong>BE AWARE</strong> that different values have been used among analyses',
+                        'note that all analyses have been run with the same value':
+                          'note that <strong>ALL</strong> analyses have been run with the same value',
+                        'see detailed table below': 'see detailed table below',
+                        'see details for each analysis': 'see details for each analysis',
+                        'Note: Some figures rounded, but not converted':
+                           "<strong>Note</strong>: Most figures have been rounded for readability,"
+                           " but 'CoefVar Density' have been further modified : converted to %",
+                        'Note: All figures untouched, as output by MCDS':
+                           "<strong>Note</strong>: All values have been left untouched,"
+                           " as output by MCDS (no rounding, no conversion)",
+                        'samples': 'Samples', 'analyses': 'Analyses', 'models': 'Models',
+                        'analyser': 'Analyser', 'runtime': 'Computing platform'},
+                    fr={'Study type:': "<strong>Type d'étude</strong>:",
+                        'Units used:': "<strong>Unités utilisées</strong>:",
+                        'for distances': 'pour les distances',
+                        'for areas': 'pour les surfaces',
+                        'Estimator selection criterion:':
+                          "<strong>Critère de sélection des termes d'ajustement</strong>:",
+                        'Confidence value interval:': '<strong>Intervalle de confiance</strong>:',
+                        'If not listed in table below, please': 'Si non présent dans la table ci-dessous,',
+                        'BE AWARE that different values have been used among analyses':
+                          'faites <strong>ATTENTION</strong>,'
+                          ' différentes valeurs ont été utilisés suivant les analyses',
+                        'note that all analyses have been run with the same value':
+                          'notez que <strong>TOUTES</strong> les analyses ont été faites avec la même valeur',
+                        'see detailed table below': 'voir table de détails ci-dessous',
+                        'see details for each analysis': 'voir détails de chaque analyse',
+                        'Note: Some figures rounded, but not converted':
+                           "<strong>N.B.</strong> Presque toutes les valeurs ont été arrondies pour la lisibilité,"
+                           " mais seul 'CoefVar Densité' a été autrement modifié : converti en %",
+                        'Note: All figures untouched, as output by MCDS':
+                           "<strong>N.B.</strong> Aucune valeur n'a été convertie ou arrondie,"
+                           " elles sont toutes telles que produites par MCDS",
+                        'samples': 'Echantillons', 'analyses': 'Analyses', 'models': 'Modèles',
+                        'analyser': 'Analyseur', 'runtime': 'Plateforme de calcul'}))
+    
+    RightTruncCol = ('encounter rate', 'right truncation distance (w)', 'Value')
+
+    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                 synthCols=None, sortCols=None, sortAscend=None, dCustomTrans=None, lang='en',
+                 plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
+                 plotLineWidth=2, plotDotWidth=5, plotFontSizes=dict(title=12, axes=10, ticks=9, legend=10),
+                 pySources=[], tgtFolder='.', tgtPrefix='results'):
+
+        """
+        Parameters:
+        :param resultsSet: source results
+        :param title: main page title (and <title> tag in HTML header)
+        :param subTitle: main page sub-title (under the title, lower font size)
+        :param description: main page description text (under the sub-title, lower font size)
+        :param anlysSubTitle: analysis pages title
+        :param keywords: for HTML header <meta name="keywords" ...>
+        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
+        :param sortCols: sorting columns for report tables
+        :param sortAscend: sorting order for report tables, as a bool or list of bools, of len(synthCols)
+        :param dCustomTrans: custom translations to complete the report standard ones,
+                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
+        :param lang: Target language for translation
+        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
+        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
+        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
+        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
+        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
+        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
+        :param pySources: path-name of source files to copy in report folder and link in report
+        :param tgtFolder: target folder for the report (for _all_ generated files)
+        :param tgtPrefix: default target file name for the report
+         """
+    
+        assert isinstance(resultsSet, MCDSAnalysisResultsSet), 'resultsSet must be a MCDSAnalysisResultsSet'
+
+        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                         synthCols=synthCols, sortCols=sortCols, sortAscend=sortAscend,
+                         dCustomTrans=dCustomTrans, lang=lang,
+                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
+                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
+                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
+        
+    # Styling colors
+    CChrGray = '#869074'
+    CBckGreen, CBckGray = '#e0ef8c', '#dae3cb'
+    CSclGreen, CSclOrange, CSclRed = '#cbef8c', '#f9da56', '#fe835a'
+    CChrInvis = '#e8efd1'  # body background
+    ScaledColors = [CSclGreen, CSclOrange, CSclRed]
+    ScaledColorsRvd = list(reversed(ScaledColors))
+    
+    DExCodeColors = dict(zip([1, 2, 3], ScaledColors))
+    
+    @classmethod
+    def colorExecCodes(cls, sCodes):
+        return ['background-color: ' + cls.DExCodeColors.get(c, cls.DExCodeColors[3]) for c in sCodes]
+    
+    @classmethod
+    def scaledColorV(cls, v, thresholds, colors):  # len(thresholds) == len(colors) - 1
+        if pd.isnull(v):
+            return cls.CBckGray
+        for ind, thresh in enumerate(thresholds):
+            if v > thresh:
+                return colors[ind]
+        return colors[-1]
+    
+    @classmethod
+    def scaledColorS(cls, sValues, thresholds, colors):
+        return ['background-color: ' + cls.scaledColorV(v, thresholds, colors) for v in sValues]
+    
+    @staticmethod
+    def isNull(o):
+        return not isinstance(o, list) and pd.isnull(o)
+    
+    @staticmethod
+    def shortenDistCuts(distCuts):
+        if isinstance(distCuts, int) or isinstance(distCuts, float):
+            return distCuts
+        short = str(distCuts)
+        if short in ['None', 'nan']:
+            return None
+        else:
+            return short.translate(str.maketrans({c: '' for c in '[] '})).replace('.0,', ',')
+
+    @staticmethod
+    def _roundNumber(v, ndigits=0):
+        """round number with built-in round function, unless NaN, which fails to int() if ndigits=0"""
+        return v if pd.isnull(v) else round(v, ndigits)
+
+    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
+    # in the "all analyses at once" case.
+    # (sort, generate index, convert units, round values, and style).
+    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
+    # Return a pd.DataFrame.Styler
+    def finalFormatAllAnalysesData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
+        
+        logger.debug(f'MCDSResultsDistanceReport.finalFormatAllAnalysesData'
+                     f'({sort=}, {indexer=}, {convert=}, {round_=}, {style=})')
+
+        # Sorting
+        df = dfTrData
+        if sort:
+        
+            # If no sorting order was specified, generate one simple one,
+            # through a temporarily sample num. column and Delta AIC column (so ... it MUST be there)
+            # (assuming analyses have been run as grouped by sample)
+            if not self.sortCols:
+            
+                # Note: Ignoring all-NaN sample id columns, for a working groupby
+                sampleIdCols = [col for col in self.resultsSet.transSampleColumns(self.lang)
+                                if col in df.columns and not df[col].isna().all()]
+                df.insert(0, column='#Sample#', value=df.groupby(sampleIdCols, sort=False).ngroup())
+
+                sortCols = ['#Sample#'] + [col for col in self.trEnColNames(['Delta AIC']) if col in df.columns]
+                sortAscend = True
+                
+            # Otherwise, use the one specified.
+            else:
+            
+                # ... after some cleaning up in case some sort columns are not present.
+                sortCols = list()
+                sortAscend = list() if isinstance(self.sortAscend, list) else self.sortAscend
+                for ind, col in enumerate(self.resultsSet.transColumns(self.sortCols, self.lang)):
+                    if col in df.columns:
+                        sortCols.append(col)
+                        if isinstance(self.sortAscend, list):
+                            sortAscend.append(self.sortAscend[ind])
+                assert not isinstance(sortAscend, list) or len(sortCols) == len(sortAscend)
+
+            # Sort
+            df.sort_values(by=sortCols, ascending=sortAscend, inplace=True)
+            
+            # Remove temporary sample num. column if no sorting order was specified
+            if not self.sortCols:
+                df.drop(columns=['#Sample#'], inplace=True)
+
+        # Standard 1 to N index + optional post-formatting (ex. for synthesis <=> details navigation).
+        if indexer:
+            df.index = range(1, len(df) + 1)
+            if callable(indexer):
+                df.index = df.apply(indexer, axis='columns')
+
+        # Converting to other units, or so.
+        if convert:
+            
+            # for col in self.trEnColNames(['Density', 'Min Density', 'Max Density']): # 'CoefVar Density',
+            #     if col in df.columns:
+            #         df[col] *= 1000000 / 10000 # ha => km2
+            
+            col = self.trEnColNames('CoefVar Density')
+            if col in df.columns:
+                kVarDens = 100.0
+                df[col] *= kVarDens  # [0, 1] => %
+            
+            for col in self.trEnColNames(['Fit Dist Cuts', 'Discr Dist Cuts']):
+                if col in df.columns:
+                    df[col] = df[col].apply(self.shortenDistCuts)
+            
+        # Reducing float precision
+        if round_:
+            
+            # Use built-in round for more accurate rounding than np.round
+            # a. Fixed list of columns: enumerate their English names.
+            dColDecimals = {**{col: 4 for col in ['Delta CoefVar Density']},
+                            **{col: 3 for col in ['Effort', 'PDetec', 'Min PDetec', 'Max PDetec']},
+                            **{col: 2 for col in ['Delta AIC', 'Chi2 P', 'KS P', 'CvM Uw P', 'CvM Cw P',
+                                                  'Density', 'Min Density', 'Max Density',
+                                                  'Qual Chi2+', 'Qual KS+', 'Qual DCv+']},
+                            **{col: 1 for col in ['AIC', 'EDR/ESW', 'Min EDR/ESW', 'Max EDR/ESW',
+                                                  'Number', 'Min Number', 'Max Number',
+                                                  'CoefVar Density', 'CoefVar Number', 'Obs Rate']},
+                            **{col: 0 for col in ['Left Trunc Dist', 'Right Trunc Dist',
+                                                  'Left Trunc', 'Right Trunc']}}
+                                                     
+            for col, dec in self.trEnColNames(dColDecimals).items():
+                if col in df.columns:
+                    df[col] = df[col].apply(self._roundNumber, ndigits=dec)
+
+            # b. Dynamic lists of columns: select their names through a startswith criterion.
+            for col in df.columns:
+                if col.startswith(self.tr('Qual Bal')):
+                    df[col] = df[col].apply(self._roundNumber, ndigits=2)
+                if col.startswith(self.tr('Order')) or col.startswith(self.tr('Pre-selection')):
+                    df[col] = df[col].apply(self._roundNumber, ndigits=0)
+            
+            # Don't use df.round ... because it does not work, at least with pandas 1.0.x up to 1.1.2 !?!?!?
+            # df = df.round(decimals={ col: dec for col, dec in self.trEnColNames(dColDecimals).items() \
+            #                                   if col in df.columns })
+            
+        # Styling
+        return self.styleAllAnalysesData(df, convert=convert, round_=round_, style=style)
+
+    @staticmethod
+    def _trimTrailingZeroesFormat(v):
+        return '' if pd.isnull(v) else format(v, 'g') if pd.api.types.is_numeric_dtype(v) else v
+
+    def styleAllAnalysesData(self, df, convert=True, round_=True, style=True):
+    
+        dfs = df.style
+        
+        if style:
+        
+            # Remove trailing (useless) zeroes in floats when rounding requested.
+            if round_:
+                dfs.format(self._trimTrailingZeroesFormat,
+                           subset=[col for col in df.columns if df[col].dtype is np.dtype('float')])
+
+            # Left align all-text columns
+            cols = [col for col in df.columns
+                    if df[col].dropna().apply(lambda v: isinstance(v, str)).all()]
+            if cols:
+                dfs.set_properties(subset=cols, **{'text-align': 'left'})
+
+            # Green background for the 0-value Delta AIC rows
+            col = self.trEnColNames('Delta AIC')
+            if col in df.columns and df[col].max() > 0:  # if all delta AIC == 0, no need to stress it.
+                dfs.set_properties(subset=pd.IndexSlice[df[df[col] == 0].index, :],
+                                   **{'background-color': self.CBckGreen})
+               
+            # Red/Orange/Green color code for exec. codes for normal codes
+            col = self.trEnColNames('ExCod')
+            if col in df.columns:
+                dfs.apply(self.colorExecCodes, subset=[col], axis='columns')
+            
+            # Red/Orange/Green color code for DCV based on thresholds
+            col = self.trEnColNames('CoefVar Density')
+            if col in df.columns:
+                kVarDens = 100.0 if convert else 1.0
+                dfs.apply(self.scaledColorS, subset=[col], axis='columns',
+                          thresholds=[v * kVarDens for v in [0.3, 0.2]], colors=self.ScaledColorsRvd)
+            
+            # Red/Orange/Green color code for DCV based on thresholds
+            col = self.trEnColNames('KS P')
+            if col in df.columns:
+                dfs.apply(self.scaledColorS, subset=[col], axis='columns',
+                          thresholds=[0.7, 0.2], colors=self.ScaledColors)
+            
+            # Red/Orange/Green color code for DCV based on thresholds
+            col = self.trEnColNames('Chi2 P')
+            if col in df.columns:
+                dfs.apply(self.scaledColorS, subset=[col], axis='columns',
+                          thresholds=[0.7, 0.2], colors=self.ScaledColors)
+            
+            # Greyed foreground for rows with bad exec codes
+            col = self.trEnColNames('ExCod')
+            if col in df.columns:
+                dfs.set_properties(subset=pd.IndexSlice[df[~df[col].isin([1, 2])].index, :],
+                                   **{'color': self.CChrGray})
+            
+            # NaN cells are set to transparent foreground / no shadow (to hide NaNs).
+            dfs.where(self.isNull, 'color: transparent').where(self.isNull, 'text-shadow: none')
+        
+        return dfs
+
+    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
+    # in the "all analyses at once" case.
+    # (sort, convert units, round values, and style).
+    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
+    # Return a pd.DataFrame.Styler
+    def finalformatEachAnalysisData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
+    
+        return self.finalFormatAllAnalysesData(dfTrData, sort=sort, indexer=indexer,
+                                               convert=convert, round_=round_, style=style)
+
+    @staticmethod
+    def float2str(v):  # Workaround to_html non-transparent default float format (!?)
+        return format(v, 'g')
+
+    @staticmethod
+    def series2VertTable(ser):
+        return re.sub('\n *', '',  ser.to_frame().to_html(header=False,
+                                                          float_format=MCDSResultsDistanceReport.float2str,
+                                                          na_rep=''))
+    
+    def plotImageHtmlElement(self, runFolder, plotImgPrfx, plotHeight):
+        
+        if plotImgPrfx in [self.PlotImgPrfxQqPlot, self.PlotImgPrfxDistHist]:
+            plotFileName = '{}.{}'.format(plotImgPrfx, self.plotImgFormat)
+            if os.path.isfile(os.path.join(runFolder, plotFileName)):
+                return '<img src="./{}/{}" style="height: {}px" />' \
+                       .format(self.relativeRunFolderUrl(runFolder), plotFileName, plotHeight)
+        else:
+            for plotInd in range(3, 0, -1):
+                plotFileName = '{}{}.{}'.format(plotImgPrfx, plotInd, self.plotImgFormat)
+                if os.path.isfile(os.path.join(runFolder, plotFileName)):
+                    return '<img src="./{}/{}" style="height: {}px" />' \
+                           .format(self.relativeRunFolderUrl(runFolder), plotFileName, plotHeight)
+            plotFileName = '{}.{}'.format(plotImgPrfx, self.plotImgFormat)
+            if os.path.isfile(os.path.join(runFolder, plotFileName)):
+                return '<img src="./{}/{}" style="height: {}px" />' \
+                       .format(self.relativeRunFolderUrl(runFolder), plotFileName, plotHeight)
+        
+        return f'No {plotImgPrfx} plot produced'
+        
+    def asWorkbook(self, subset=None, rebuild=False):
+
+        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
+        where each item is a named worksheet
+
+        Parameters:
+        :param subset: Selected list of data categories to include ; None = [] = all
+                       (categories in {'specs'})
+        :param rebuild: if True force rebuild of report = prevent use of / reset any cache
+                        (not used here)
+        """
+        
+        logger.debug(f'MCDSResultsDistanceReport.asWorkbook({subset=}, {rebuild=})')
+
+        ddfWbk = dict()
+
+        baseWbk = super().asWorkbook(subset=subset, rebuild=rebuild)
+
+        # Build results worksheets if specified in subset
+        if not subset or 'samples' in subset:
+
+            # Format sample list if not already there.
+            if self.tr('samples') not in baseWbk:
+
+                logger.info1('* samples ...')
+
+                # But first, relocate synthesis and details sheets before the future 'samples' sheet if present.
+                synthShName = self.tr('Synthesis')
+                if synthShName in baseWbk:
+                    ddfWbk[synthShName] = baseWbk.pop(synthShName)
+
+                detShName = self.tr('Details')
+                if detShName in baseWbk:
+                    ddfWbk[detShName] = baseWbk.pop(detShName)
+
+                # Build this missing 'samples' sheet.
+                dfSamples = self.resultsSet.listSamples().copy()
+                dfSamples.reset_index(inplace=True)
+                dfSamples.columns = self.resultsSet.transColumns(dfSamples.columns, self.lang)
+                dfSamples.set_index(self.resultsSet.transColumn(self.resultsSet.sampleIndCol, self.lang),
+                                    inplace=True)
+
+                # Add it at the end of the workbook
+                ddfWbk[self.tr('samples')] = (dfSamples, True)
+
+        # Append inherited worksheets at the end.
+        ddfWbk.update(baseWbk)
+
+        # Done
+        return ddfWbk
+
+    def toHtml(self, rebuild=False, generators=0):
+        
+        """HTML report generation.
+
+        Parameters:
+        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
+                        otherwise, use any cached data or existing plot image file
+        :param generators: Number of parallel (process) generators to use :
+                           - 0 => auto-number, based on the actual number of CPUs onboard,
+                           - > 0 => the actual number to use
+                           Note: Parallelism works well for this class, hence the default 0.
+        """
+
+        logger.debug(f'MCDSResultsDistanceReport.toHtml({rebuild=}, {generators=})')
+
+        # Install needed attached files.
+        self.installAttFiles(self.AttachedFiles)
+            
+        # Generate full report detailed pages (one for each analysis)
+        # (done first to have plot image files generated for top report page generation right below).
+        self.toHtmlEachAnalysis(rebuild=rebuild, generators=generators)
+        
+        # Generate top = main report page (one for all analyses).
+        topHtmlPathName = self.toHtmlAllAnalyses(rebuild=rebuild)
+
+        logger.info('... done.')
+                
+        return topHtmlPathName
+
+
+class MCDSResultsPreReport(MCDSResultsDistanceReport):
+
+    """A specialized pre-report for MCDS analyses, with actual output formatting.
+
+    HTML mode gives a specialized main page layout, with a super-synthesis table (with plots)
+    in place of the synthesis and detailed tables of MCDSResultsDistanceReport ;
+    detailed (linked) pages unchanged from MCDSResultsDistanceReport.
+
+    Designed for showing results of fully automatic pre-analyses, in order to give the user
+    hints about which actual analyses are to be done, and with what parameter values.
+    """
+
+    # Translation table.
+    DTrans = _mergeTransTables(base=MCDSResultsDistanceReport.DTrans,
+        update=dict(en={'Quick-view results': 'Results: the essence',
+                        'SampleParams': 'Sample & Model',
+                        'Results1': 'Results (1/2)', 'Results2': 'Results (2/2)',
+                        'DistHist': 'Standard distance histogram',
+                        'ProbDens': 'Detection probability density (PDF)',
+                        'DetProb': 'Detection probability'},
+                    fr={'Quick-view results': 'Résultats : l\'essentiel',
+                        'SampleParams': 'Echantillon & Modèle',
+                        'Results1': 'Résultats (1/2)', 'Results2': 'Résultats (2/2)',
+                        'DistHist': 'Histogramme standard des distances',
+                        'ProbDens': 'Densité de probabilité de détection (DdP)',
+                        'DetProb': 'Probabilité de détection'}))
+
+    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords, 
+                 sampleCols, paramCols, resultCols, synthCols=None,
+                 sortCols=None, sortAscend=None, dCustomTrans=None, lang='en',
+                 superSynthPlotsHeight=288, plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
+                 plotLineWidth=1, plotDotWidth=4, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
+                 pySources=[], tgtFolder='.', tgtPrefix='results'):
+
+        """Ctor
+        
+        Parameters:
+        :param resultsSet: source results
+        :param title: main page title (and <title> tag in HTML header)
+        :param subTitle: main page sub-title (under the title, lower font size)
+        :param description: main page description text (under the sub-title, lower font size)
+        :param anlysSubTitle: analysis pages title
+        :param keywords: for HTML header <meta name="keywords" ...>
+        :param sampleCols: for main page table, 1st column (top)
+        :param paramCols: for main page table, 1st column (bottom)
+        :param resultCols: for main page table, 2nd and 3rd columns
+        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
+        :param dCustomTrans: custom translations to complete the report standard ones,
+                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
+        :param lang: Target language for translation
+        :param superSynthPlotsHeight: Display height (in pixels) of the super-synthesis table plots
+        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
+        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
+        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
+        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
+        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
+        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
+        :param pySources: path-name of source files to copy in report folder and link in report
+        :param tgtFolder: target folder for the report (for _all_ generated files)
+        :param tgtPrefix: default target file name for the report
+        """
+
+        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                         synthCols=synthCols, sortCols=sortCols, sortAscend=sortAscend,
+                         dCustomTrans=dCustomTrans, lang=lang,
+                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
+                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
+                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
+        
+        self.sampleCols = self.noDupColumns(sampleCols, head='Sample columns')
+        self.paramCols = self.noDupColumns(paramCols, head='Parameter columns')
+        self.resultCols = self.noDupColumns(resultCols, head='Result columns')
+        self.superSynthPlotsHeight = superSynthPlotsHeight
+
+    def checkNeededColumns(self):
+
+        """Side check as soon as possible : Are all report needed columns available ?
+        (now that computed columns have been ... post-computed through self.resultsSet.dfFilSorData calls)
+
+        :raise: AssertionError if not the case
+        """
+
+        assert all(col in self.resultsSet.columns for col in self.sampleCols), \
+               'Missing super-synthesis sample columns in resultsSet: {}' \
+               .format(', '.join('|'.join(col) for col in self.sampleCols if col not in self.resultsSet.columns))
+        assert all(col in self.resultsSet.columns for col in self.paramCols), \
+               'Missing super-synthesis parameters columns in resultsSet: {}' \
+               .format(', '.join('|'.join(col) for col in self.paramCols if col not in self.resultsSet.columns))
+        assert all(col in self.resultsSet.columns for col in self.resultCols), \
+               'Missing super-synthesis results columns in resultsSet: {}' \
+               .format(', '.join('|'.join(col) for col in self.resultCols if col not in self.resultsSet.columns))
+
+    # Final formatting of translated data tables, for HTML or SpreadSheet rendering
+    # in the "all analyses at once" case.
+    # (sort, convert units, round values, and style).
+    # Note: Use trEnColNames method to pass from EN-translated columns names to self.lang-ones
+    # Return a pd.DataFrame.Styler
+    def finalFormatAllAnalysesData(self, dfTrData, sort=True, indexer=None, convert=True, round_=True, style=True):
+        
+        logger.debug(f'MCDSResultsPreReport.finalFormatAllAnalysesData'
+                     f'({sort=}, {indexer=}, {convert=}, {round_=}, {style=})')
+
+        df = dfTrData
+
+        # Sorting
+        if sort:
+            
+            # If no sorting order was specified, generate one simple one, through a temporarily sample num. column
+            # (assuming analyses have been run as grouped by sample)
+            if not self.sortCols:
+            
+                # Note: Ignoring all-NaN sample id columns, for a working groupby
+                sampleIdCols = [col for col in self.resultsSet.transSampleColumns(self.lang)
+                                if col in df.columns and not df[col].isna().all()]
+                df.insert(0, column='#Sample#', value=df.groupby(sampleIdCols, sort=False).ngroup())
+
+                sortCols = ['#Sample#']
+                sortAscend = True
+                
+            # Otherwise, use the one specified.
+            else:
+            
+                # ... after some cleaning up in case some sort columns are not present.
+                sortCols = list()
+                sortAscend = list() if isinstance(self.sortAscend, list) else self.sortAscend
+                for ind, col in enumerate(self.resultsSet.transColumns(self.sortCols, self.lang)):
+                    if col in df.columns:
+                        sortCols.append(col)
+                        if isinstance(self.sortAscend, list):
+                            sortAscend.append(self.sortAscend[ind])
+                assert not isinstance(sortAscend, list) or len(sortCols) == len(sortAscend)
+                assert len(sortCols) > 0
+
+            # Sort
+            df.sort_values(by=sortCols, ascending=sortAscend, inplace=True)
+            
+            # Remove temporary sample num. column if no sorting order was specified
+            if not self.sortCols:
+                df.drop(columns=['#Sample#'], inplace=True)
+        
+        # Standard 1 to N index for synthesis <=> details navigation.
+        if indexer:
+            df.index = range(1, len(df) + 1)
+
+        # Converting to other units, or so.
+        if convert:
+            
+            col = self.trEnColNames('CoefVar Density')
+            if col in df.columns:
+                kVarDens = 100.0
+                df[col] *= kVarDens  # [0, 1] => %
+            
+        # Reducing float precision
+        if round_:
+            
+            # Use built-in round for more accurate rounding than np.round
+            # a. Fixed list of columns: simply enumerate their English names.
+            dColDecimals = {**{col: 3 for col in ['PDetec', 'Min PDetec', 'Max PDetec']},
+                            **{col: 2 for col in ['Delta AIC', 'Chi2 P', 'KS P', 'CvM Uw P', 'CvM Cw P',
+                                                  'Density', 'Min Density', 'Max Density',
+                                                  'Qual Chi2+', 'Qual KS+', 'Qual DCv+']},
+                            **{col: 1 for col in ['AIC', 'EDR/ESW', 'Min EDR/ESW', 'Max EDR/ESW',
+                                                  'Number', 'Min Number', 'Max Number',
+                                                  'CoefVar Density', 'CoefVar Number', 'Obs Rate']},
+                            **{col: 0 for col in ['Left Trunc', 'Right Trunc']}}
+            for col, dec in self.trEnColNames(dColDecimals).items():
+                if col in df.columns:
+                    df[col] = df[col].apply(self._roundNumber, ndigits=dec)
+
+            # b. Dynamic lists of columns: select their names through a startswith criterion.
+            for col in df.columns:
+                if col.startswith(self.tr('Qual Bal')):
+                    df[col] = df[col].apply(self._roundNumber, ndigits=2)
+
+            # Don't use df.round ... because it does nothing, at least with pandas to 1.1.2 !?!?!?
+            # df = df.round(decimals={col: dec for col, dec in self.trEnColNames(dColDecimals).items()
+            #                         if col in df.columns})
+
+        # Styling
+        return self.styleAllAnalysesData(df, convert=convert, round_=round_, style=style)
+
+    # Top page
+    def toHtmlAllAnalyses(self, rebuild=False):
+        
+        logger.info('Top page ...')
+        logger.debug(f'MCDSResultsPreReport.toHtmlAllAnalyses({rebuild=})')
+
+        # Generate the table to display from raw results 
+        # (index + 5 columns : sample, params, results, ProbDens plot, DetProb plot)
+        # 1. Get translated detailed results
+        dfDet = self.resultsSet.dfTransData(self.lang)
+
+        # 2. List estimator selection criterion and CV interval values used
+        #    (these values MUST be notified in the report, even if not in the selected columns to show)
+        estimSelCrits = dfDet[self.trEnColNames('Mod Chc Crit')].unique().tolist()
+        confIntervals = dfDet[self.trEnColNames('Conf Interv')].unique().tolist()
+
+        # 3. Post-format results (styling not used later, so don't do it).
+        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
+                                                 convert=True, round_=True, style=False)
+        dfDet = dfsDet.data
+
+        # 4. Translate sample, parameter and result columns
+        sampleTrCols = self.resultsSet.transColumns(self.sampleCols, self.lang)
+        paramTrCols = self.resultsSet.transColumns(self.paramCols, self.lang)
+        result1TrCols = self.resultsSet.transColumns(self.resultCols, self.lang)
+
+        midResInd = len(result1TrCols) // 2 + len(result1TrCols) % 2
+        result2TrCols = result1TrCols[midResInd:]
+        result1TrCols = result1TrCols[:midResInd]
+        
+        # 5. Fill target table index and columns
+        dfSupSyn = pd.DataFrame(dict(SampleParams=dfDet[sampleTrCols + paramTrCols].apply(self.series2VertTable,
+                                                                                          axis='columns'),
+                                     Results1=dfDet[result1TrCols].apply(self.series2VertTable, axis='columns'),
+                                     Results2=dfDet[result2TrCols].apply(self.series2VertTable, axis='columns'),
+                                     DistHist=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                               plotImgPrfx=self.PlotImgPrfxDistHist,
+                                                                               plotHeight=self.superSynthPlotsHeight),
+                                     ProbDens=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                               plotImgPrfx=self.PlotImgPrfxProbDens,
+                                                                               plotHeight=self.superSynthPlotsHeight),
+                                     DetProb=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                              plotImgPrfx=self.PlotImgPrfxDetProb,
+                                                                              plotHeight=self.superSynthPlotsHeight)))
+        
+        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSupSyn))), 1))
+        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
+
+        def numNavLink(sAnlys):
+            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
+        dfSupSyn.index = dfDet.apply(numNavLink, axis='columns')
+        
+        # 6. Translate table columns.
+        dfSupSyn.columns = [self.tr(col) for col in dfSupSyn.columns]
+
+        # 7. Generate traceability infos parts (results specs).
+        ddfTrc = self.asWorkbook(subset=['specs', 'samples'])
+
+        # 8. Generate top report page.
+        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
+        tmpl = self.getTemplateEnv().get_template('mcds/pretop.htpl')
+        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
+        html = tmpl.render(supersynthesis=dfSupSyn.to_html(escape=False),
+                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
+                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
+                           title=self.title, subtitle=self.subTitle,
+                           description=self.description, keywords=self.keywords,
+                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang],
+                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
+                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(), 
+                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
+                           areaUnit=self.tr(self.resultsSet.areaUnit),
+                           surveyType=self.tr(self.resultsSet.surveyType),
+                           distanceType=self.tr(self.resultsSet.distanceType),
+                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'),
+                           estimSelCrits=estimSelCrits, confIntervals=[str(v) for v in confIntervals])
+        html = re.sub('(?:[ \t]*\n){2,}', '\n'*2, html)  # Cleanup blank lines series to one only
+
+        # 9. Write top HTML to file.
+        htmlPathName = self.targetFilePathName(suffix='.html')
+        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
+            tgtFile.write(html)
+
+        return htmlPathName
+
+
+class MCDSResultsFullReport(MCDSResultsDistanceReport):
+
+    """A specialized full report for MCDS analyses, with actual output formatting.
+
+    HTML mode gives a mix of Distance and PreReport main page layout,
+    with a super-synthesis table (with plots), a synthesis table, and a detailed table ;
+    detailed table unchanged from MCDSResultsDistanceReport
+    detailed (linked) pages unchanged from MCDSResultsDistanceReport.
+    """
+
+    # Translation table.
+    DTrans = _mergeTransTables(base=MCDSResultsDistanceReport.DTrans,
+        update=dict(en={'Quick-view results': 'Results: the essence',
+                        'SampleParams': 'Sample & Model',
+                        'Results1': 'Results (1/2)', 'Results2': 'Results (2/2)',
+                        'QqPlot': 'Quantile-quantile plot',
+                        'ProbDens': 'Detection probability density (PDF)',
+                        'DetProb': 'Detection probability'},
+                    fr={'Quick-view results': 'Résultats : l\'essentiel',
+                        'SampleParams': 'Echantillon & Modèle',
+                        'Results1': 'Résultats (1/2)', 'Results2': 'Résultats (2/2)',
+                        'QqPlot': 'Diagramme quantile-quantile',
+                        'ProbDens': 'Densité de probabilité de détection (DdP)',
+                        'DetProb': 'Probabilité de détection'}))
+    
+    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords, 
+                 sampleCols, paramCols, resultCols, synthCols=None, sortCols=None, sortAscend=None,
+                 dCustomTrans=None, lang='en',
+                 superSynthPlotsHeight=288, plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
+                 plotLineWidth=1, plotDotWidth=4, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
+                 pySources=[], tgtFolder='.', tgtPrefix='results'):
+
+        """Ctor
+        
+        Parameters:
+        :param resultsSet: source results
+        :param title: main page title (and <title> tag in HTML header)
+        :param subTitle: main page sub-title (under the title, lower font size)
+        :param description: main page description text (under the sub-title, lower font size)
+        :param anlysSubTitle: analysis pages title
+        :param keywords: for HTML header <meta name="keywords" ...>
+        :param sampleCols: for main page table, 1st column (top)
+        :param paramCols: for main page table, 1st column (bottom)
+        :param resultCols: for main page table, 2nd and 3rd columns
+        :param synthCols: for synthesis table (Excel format only, "Synthesis" tab)
+        :param sortCols: sorting columns for report tables
+        :param sortAscend: sorting order for report tables, as a bool or list of bools, of len(synthCols)
+        :param dCustomTrans: custom translations to complete the report standard ones,
+                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
+        :param lang: Target language for translation
+        :param superSynthPlotsHeight: Display height (in pixels) of the super-synthesis table plots
+        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
+        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
+        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
+        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
+        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
+        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
+        :param pySources: path-name of source files to copy in report folder and link in report
+        :param tgtFolder: target folder for the report (for _all_ generated files)
+        :param tgtPrefix: default target file name for the report
+        """
+
+        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                         synthCols=synthCols, sortCols=sortCols, sortAscend=sortAscend, 
+                         dCustomTrans=dCustomTrans, lang=lang,
+                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
+                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
+                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
+        
+        self.sampleCols = self.noDupColumns(sampleCols, head='Sample columns')
+        self.paramCols = self.noDupColumns(paramCols, head='Parameter columns')
+        self.resultCols = self.noDupColumns(resultCols, head='Result columns')
+        self.superSynthPlotsHeight = superSynthPlotsHeight
+
+    def checkNeededColumns(self):
+
+        """Side check as soon as possible : Are all report needed columns available ?
+        (now that computed columns have been ... post-computed through self.resultsSet.dfFilSorData calls)
+
+        :raise: AssertionError if not the case
+        """
+
+        assert all(col in self.resultsSet.columns for col in self.sampleCols), \
+               'Missing super-synthesis sample columns in resultsSet: {}' \
+               .format(', '.join('|'.join(col) for col in self.sampleCols if col not in self.resultsSet.columns))
+        assert all(col in self.resultsSet.columns for col in self.paramCols), \
+               'Missing super-synthesis parameters columns in resultsSet: {}' \
+               .format(', '.join('|'.join(col) for col in self.paramCols if col not in self.resultsSet.columns))
+        assert all(col in self.resultsSet.columns for col in self.resultCols), \
+               'Missing super-synthesis results columns in resultsSet: {}' \
+               .format(', '.join('|'.join(col) for col in self.resultCols if col not in self.resultsSet.columns))
+
+    # Top page (based on results.dfTransData).
+    def toHtmlAllAnalyses(self, rebuild=False):
+        
+        logger.info('Top page ...')
+        
+        # 1. Get source translated raw data to format
+        dfSynRes, dfDetRes, _ = self.getRawTransData(rebuild=rebuild)
+
+        # 2. List estimator selection criterion and CV interval values used
+        #    (these values MUST be notified in the report, even if not in the selected columns to show)
+        estimSelCrits = dfDetRes[self.trEnColNames('Mod Chc Crit')].unique().tolist()
+        confIntervals = dfDetRes[self.trEnColNames('Conf Interv')].unique().tolist()
+
+        # 3. Super-synthesis: Generate post-processed and translated table.
+        #    (index + 5 columns : sample + params, results, Qq plot, ProbDens plot, DetProb plot)
+        # a. Get translated and post-formatted detailed results
+        dfDet = dfDetRes.copy()  # Also needed as is below.
+        
+        # b. Styling not used for super-synthesis, so don't do it.
+        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
+                                                 convert=True, round_=True, style=False)
+        dfDet = dfsDet.data
+
+        # c. Translate sample, parameter and result columns
+        sampleTrCols = self.resultsSet.transColumns(self.sampleCols, self.lang)
+        paramTrCols = self.resultsSet.transColumns(self.paramCols, self.lang)
+        result1TrCols = self.resultsSet.transColumns(self.resultCols, self.lang)
+
+        midResInd = len(result1TrCols) // 2 + len(result1TrCols) % 2
+        result2TrCols = result1TrCols[midResInd:]
+        result1TrCols = result1TrCols[:midResInd]
+        
+        # d. Fill target table index and columns
+        dfSupSyn = pd.DataFrame(dict(SampleParams=dfDet[sampleTrCols + paramTrCols].apply(self.series2VertTable,
+                                                                                          axis='columns'),
+                                     Results1=dfDet[result1TrCols].apply(self.series2VertTable, axis='columns'),
+                                     Results2=dfDet[result2TrCols].apply(self.series2VertTable, axis='columns'),
+                                     QqPlot=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                             plotImgPrfx=self.PlotImgPrfxQqPlot,
+                                                                             plotHeight=self.superSynthPlotsHeight),
+                                     ProbDens=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                               plotImgPrfx=self.PlotImgPrfxProbDens,
+                                                                               plotHeight=self.superSynthPlotsHeight),
+                                     DetProb=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                              plotImgPrfx=self.PlotImgPrfxDetProb,
+                                                                              plotHeight=self.superSynthPlotsHeight)))
+        
+        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSupSyn))), 1))
+        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
+
+        def numNavLink(sAnlys):
+            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
+        dfSupSyn.index = dfDet.apply(numNavLink, axis='columns')
+        
+        # e. Translate table columns.
+        dfSupSyn.columns = [self.tr(col) for col in dfSupSyn.columns]
+
+        # 4. Synthesis: Generate post-processed and translated table.
+        # a. Add run folder column if not selected (will serve to generate the link to the analysis detailed report)
+        dfSyn = dfSynRes
+        dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+        
+        # b. Links to each analysis detailed report.
+        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSyn))), 1))
+        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
+
+        def numNavLink(sAnlys):
+            return numNavLinkFmt.format(p=sAnlys[self.trRunFolderCol], n=sAnlys.name)
+       
+        # c. Post-format as specified in actual class.
+        dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=numNavLink,
+                                                 convert=True, round_=True, style=True)
+
+        # 5. Details: Generate post-processed and translated table.
+        dfDet = dfDetRes
+
+        # a. Add run folder column if not there (will serve to generate the link to the analysis detailed report)
+        detTrCols = list(dfDet.columns)
+        if self.trRunFolderCol not in detTrCols:
+            detTrCols += [self.trRunFolderCol]
+        dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+        dfDet = dfDet.reindex(columns=detTrCols)
+       
+        # b. Links to each analysis detailed report.
+        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=numNavLink,
+                                                 convert=False, round_=False, style=True)
+
+        # 6. Generate traceability infos parts (results specs).
+        ddfTrc = self.asWorkbook(subset=['specs', 'samples'])
+
+        # Generate top report page.
+        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
+        tmpl = self.getTemplateEnv().get_template('mcds/fulltop.htpl')
+        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
+        html = tmpl.render(supersynthesis=dfSupSyn.to_html(escape=False),
+                           synthesis=dfsSyn.render(),  # escape=False, index=False),
+                           details=dfsDet.render(),  # escape=False, index=False),
+                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
+                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
+                           title=self.title, subtitle=self.subTitle,
+                           description=self.description, keywords=self.keywords,
+                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang],
+                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
+                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(),
+                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
+                           areaUnit=self.tr(self.resultsSet.areaUnit),
+                           surveyType=self.tr(self.resultsSet.surveyType),
+                           distanceType=self.tr(self.resultsSet.distanceType),
+                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'),
+                           estimSelCrits=estimSelCrits, confIntervals=[str(v) for v in confIntervals])
+        html = re.sub('(?:[ \t]*\n){2,}', '\n'*2, html)  # Cleanup blank lines series to one only
+
+        # 7. Write top HTML to file.
+        htmlPathName = self.targetFilePathName(suffix='.html')
+        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
+            tgtFile.write(html)
+
+        return htmlPathName
+
+
+class MCDSResultsFilterSortReport(MCDSResultsFullReport):
+
+    """A specialized filtered and sorted full report for MCDS analyses, with actual output formatting
+    and above all auto-filtered and sorted results aiming at showing the few best results to the user
+    among which to (manually) select THE best (for each sample).
+
+    Just like MCDSResultsFullReport, but for filtered and sorted results for 1-only scheme.
+    """
+
+    # Translation table.
+    DTrans = _mergeTransTables(base=MCDSResultsFullReport.DTrans,
+        update=dict(en={'Scheme': 'Scheme', 'Step': 'Step',
+                        'Property': 'Property', 'Value': 'Value',
+                        'AFS': 'AFS', 'AFSM': 'AFSM', 'Steps': 'Steps',
+                        'Filter & Sort steps': 'Filter and sort steps'},
+                    fr={'Scheme': 'Méthode', 'Step': 'Etape',
+                        'Property': 'Propriété', 'Value': 'Valeur',
+                        'AFS': 'FTA', 'AFSM': 'MFTA', 'Steps': 'Etapes',
+                        'Filter & Sort steps': 'Etapes de filtrage et tri'}))
+
+    ResClass = MCDSAnalysisResultsSet
+
+    def __init__(self, resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                 sampleCols, paramCols, resultCols, synthCols=None, sortCols=None, sortAscend=None,
+                 dCustomTrans=None, lang='en',
+                 filSorSchemes=[dict(method=ResClass.filterSortOnExecCode)],
+                 superSynthPlotsHeight=288, plotImgFormat='png', plotImgSize=(640, 400), plotImgQuality=90,
+                 plotLineWidth=1, plotDotWidth=4, plotFontSizes=dict(title=11, axes=10, ticks=9, legend=10),
+                 pySources=[], tgtFolder='.', tgtPrefix='results'):
+
+        """Ctor
+        
+        Parameters:
+        :param resultsSet: source results (an instance of MCDSAnalysisResultsSet, or subclass,
+                                           named ResClass or R below)
+        :param title: main page title (and <title> tag in HTML header) for the HTML report (1 scheme only)
+        :param subTitle: main page sub-title (under the title, lower font size) for the HTML report (1 scheme only) ;
+                         any {fsMeth} placeholder will get replaced by the method name of the used filter sort scheme 
+        :param description: main page description text (under the sub-title, lower font size)
+                            for the HTML report (1 scheme only) ; any {fsMeth} placeholder formatted as in subTitle
+        :param anlysSubTitle: analysis pages title
+        :param keywords: for HTML header <meta name="keywords" ...>
+        :param sampleCols: for main page table, 1st column (top)
+        :param paramCols: for main page table, 1st column (bottom)
+        :param resultCols: for main page table, 2nd and 3rd columns
+        :param synthCols: Subset and order of columns to keep at the end (before translation)
+                          as the synthesis table of each filter-sort sub-report (None = [] = all)
+                          Warning: No need to specify here pre-selection and final selection columns,
+                                   as they'll be added automatically, and relocated at a non-customisable place.
+        :param sortCols: sorting columns for report tables ??? which ones ???
+        :param sortAscend: sorting order for report tables, as a bool or list of bools,
+                           of len(synthCols) ??? which ones ???
+        :param dCustomTrans: custom translations to complete the report standard ones,
+                             as a dict(fr=dict(src: fr-trans), en=dict(src: en-trans))
+        :param lang: Target language for translation
+        :param filSorSchemes: filter and sort schemes to apply in order to generate each sub-report
+                 as a list of dict(method= <results set class>.filterSortOnXXX method to use,
+                                   deduplicate= dict(dupSubset=, dDupRounds=) of deduplication params
+                                       (if not or partially given, see RCLS.filterSortOnXXX defaults)
+                                   filterSort= dict of other <method>-specific params,
+                                   preselCols= target columns for generating auto-preselection ones,
+                                               containing [1, preselNum] ranks ; default: []
+                                   preselAscs= Rank direction to use for each column (list),
+                                               or all (single bool) ; default: True
+                                               (True means that lower values are "better" ones)
+                                   preselThrhs= Eliminating threshold for each column (list),
+                                                or all (single number) ; default: 0.2
+                                                (eliminated above if preselAscs True, below otherwise)
+                                   preselNum= number of (best) pre-selections to keep for each sample) ;
+                                              default: 5
+                 example: [dict(method=R.filterSortOnExecCode,  # let R = MCDSTruncOptanalysisResultsSet
+                                preselCols=[R.CLCmbQuaBal1, R.CLCmbQuaBal2], preselAscs=False,
+                                preselThrhs=0.2, preselNum=5),
+                           dict(method=R.filterSortOnExCAicMulQua,
+                                deduplicate=dict(dupSubset=[R.CLNObs, R.CLEffort, R.CLDeltaAic, R.CLChi2,
+                                                            R.CLKS, R.CLCvMUw, R.CLCvMCw, R.CLDCv]),
+                                                 dDupRounds={R.CLDeltaAic: 1, R.CLChi2: 2, R.CLKS: 2,
+                                                             R.CLCvMUw: 2, R.CLCvMCw: 2, R.CLDCv: 2})
+                                filterSort=dict(sightRate=92.5, nBestAIC=3, nBestQua=1,
+                                                whichBestQua=[R.CLGrpOrdClTrChi2KSDCv, R.CLGrpOrdClTrQuaBal3],
+                                                nFinalRes=12, whichFinalQua=R.CLCmbQuaBal3, ascFinalQua=False),
+                                preselCols=[R.CLCmbQuaBal1, R.CLDCv], preselAscs=[False, True],
+                                preselThrhs=[0.2, 0.5], preselNum=3)]        
+        :param superSynthPlotsHeight: Display height (in pixels) of the super-synthesis table plots
+        :param plotImgFormat: png, svg and jpg all work with Matplotlib 3.2.1+
+        :param plotImgSize: size of the image generated for each plot = (width, height) in pixels
+        :param plotImgQuality: JPEG format quality (%) ; ignored if plotImgFormat not in ('jpg', 'jpeg')
+        :param plotLineWidth: width (unit: pixel) of drawn lines (observation histograms, fitted curves)
+        :param plotDotWidth: width (unit: pixel) of drawn dots / points (observation distances)
+        :param plotFontSizes: font sizes (unit: point) for plots (dict with keys from title, axes, ticks, legend)
+        :param pySources: path-name of source files to copy in report folder and link in report
+        :param tgtFolder: target folder for the report (for _all_ generated files)
+        :param tgtPrefix: default target file name for the report
+        """
+
+        super().__init__(resultsSet, title, subTitle, anlysSubTitle, description, keywords,
+                         sampleCols=sampleCols, paramCols=paramCols, resultCols=resultCols, synthCols=synthCols,
+                         sortCols=sortCols, sortAscend=sortAscend, dCustomTrans=dCustomTrans, lang=lang,
+                         superSynthPlotsHeight=superSynthPlotsHeight,
+                         plotImgFormat=plotImgFormat, plotImgSize=plotImgSize, plotImgQuality=plotImgQuality,
+                         plotLineWidth=plotLineWidth, plotDotWidth=plotDotWidth, plotFontSizes=plotFontSizes,
+                         pySources=pySources, tgtFolder=tgtFolder, tgtPrefix=tgtPrefix)
+        
+        self.filSorSchemes = filSorSchemes
+
+    def asWorkbook(self, subset=None, rebuild=False):
+
+        """Format as a "generic" workbook format, i.e. as a dict(name=(DataFrame, useIndex))
+        where each item is a named worksheet
+
+        Parameters:
+        :param subset: Selected list of data categories to include ; None = [] = all
+                       (categories in {'specs', 'samples', results'})
+        :param rebuild: If True, force rebuild of filtered and sorted sub-report
+                        => prevent use of / reset results set filter & sort cache
+        """
+        
+        logger.debug(f'MCDSResultsFilterSortReport.asWorkbook({subset=}, {rebuild=})')
+
+        ddfWbk = dict()
+
+        logger.info('Formatting FilterSort sub-reports as a workbook ...')
+
+        # Build results worksheets if specified in subset
+        if not subset or 'results' in subset:
+
+            # TODO: Add better formatting (color, ... etc)
+
+            # For each filter and sort scheme:
+            repLog = list()
+            for scheme in self.filSorSchemes:
+
+                logger.info1('* filter & sort "{}" scheme ...'
+                             .format(self.resultsSet.filSorSchemeId(scheme)))
+
+                # Apply it
+                filSorSchId, dfFilSorRes, filSorSteps = \
+                    self.resultsSet.dfFilSorData(scheme=scheme, rebuild=rebuild,
+                                                 columns=self.synthCols, lang=self.lang)
+
+                # Store results in workbook
+                ddfWbk['-'.join([self.tr('AFSM'), filSorSchId])] = (dfFilSorRes, False)
+
+                # Update all-scheme log
+                repLog += filSorSteps
+
+            # Log of opérations, for traceability.
+            logger.info1('* filter & sort steps ...')
+
+            indexCols = [self.tr(col) for col in ['Scheme', 'Step']]
+            dataCols = [self.tr(col) for col in ['Property', 'Value']]
+            dfFilSorHist = pd.DataFrame(repLog, columns=indexCols + dataCols)
+            dfFilSorHist.set_index(indexCols, inplace=True)
+
+            ddfWbk['-'.join([self.tr('AFS'), self.tr('Steps')])] = (dfFilSorHist, True)
+
+        # Append inherited worksheets.
+        ddfWbk.update(super().asWorkbook(subset=subset, rebuild=rebuild))
+
+        # Done
+        logger.info('... done.')
+
+        return ddfWbk
+
+    def getRawTransData(self, filSorScheme=dict(method=ResClass.filterSortOnExecCode),
+                        rebuild=False):
+
+        """Retrieve input translated raw data to be formatted
+
+        :return: 2 dataFrames, for synthesis (synCols) and detailed (all) column sets,
+                 + the id of the applied scheme and the log of its application. 
+        """
+
+        # Generate translated synthesis table.
+        synthCols = self.synthCols
+        if self.resultsSet.analysisClass.RunFolderColumn not in synthCols:
+            synthCols += [self.resultsSet.analysisClass.RunFolderColumn]
+        filSorSchId, dfFilSorSynRes, filSorSteps = \
+            self.resultsSet.dfFilSorData(scheme=filSorScheme, rebuild=rebuild, columns=synthCols, lang=self.lang)
+
+        # Generate translated detailed table.
+        _, dfFilSorDetRes, _ = \
+            self.resultsSet.dfFilSorData(scheme=filSorScheme, rebuild=rebuild, lang=self.lang)
+
+        # Side check as soon as possible : Are all report needed columns available ?
+        self.checkNeededColumns()
+
+        return dfFilSorSynRes, dfFilSorDetRes, (filSorSchId, filSorSteps)
+
+    def toHtmlAllAnalyses(self, filSorScheme=dict(method=ResClass.filterSortOnExecCode),
+                          rebuild=False):
+
+        """Top page for a given scheme.
+        """
+
+        logger.info('Top page ...')
+        
+        # 1. Get source translated raw data to format (post-compute, filter and sort, extract)
+        dfSynRes, dfDetRes, (filSorSchId, filSorSteps) = \
+            self.getRawTransData(filSorScheme=filSorScheme, rebuild=rebuild)
+
+        # 2. List estimator selection criterion and CV interval values used
+        #    (these values MUST be notified in the report, even if not in the selected columns to show)
+        estimSelCrits = dfDetRes[self.trEnColNames('Mod Chc Crit')].unique().tolist()
+        confIntervals = dfDetRes[self.trEnColNames('Conf Interv')].unique().tolist()
+
+        # 3. Super-synthesis: Format filtered and translated data.
+        #    (index + 5 columns : sample + params, results, Qq plot, ProbDens plot, DetProb plot)
+        dfDet = dfDetRes.copy()
+
+        # a. Styling not used for super-synthesis, so don't do it.
+        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=True,
+                                                 convert=True, round_=True, style=False)
+        dfDet = dfsDet.data
+
+        # b. Translate sample, parameter and result columns
+        sampleTrCols = self.resultsSet.transColumns(self.sampleCols, self.lang)
+        paramTrCols = self.resultsSet.transColumns(self.paramCols, self.lang)
+        result1TrCols = self.resultsSet.transColumns(self.resultCols, self.lang)
+
+        midResInd = len(result1TrCols) // 2 + len(result1TrCols) % 2
+        result2TrCols = result1TrCols[midResInd:]
+        result1TrCols = result1TrCols[:midResInd]
+        
+        # c. Fill target table index and columns
+        dfSupSyn = \
+            pd.DataFrame(dict(SampleParams=dfDet[sampleTrCols + paramTrCols].apply(self.series2VertTable,
+                                                                                   axis='columns'),
+                              Results1=dfDet[result1TrCols].apply(self.series2VertTable, axis='columns'),
+                              Results2=dfDet[result2TrCols].apply(self.series2VertTable, axis='columns'),
+                              QqPlot=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                      plotImgPrfx=self.PlotImgPrfxQqPlot,
+                                                                      plotHeight=self.superSynthPlotsHeight),
+                              ProbDens=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                        plotImgPrfx=self.PlotImgPrfxProbDens,
+                                                                        plotHeight=self.superSynthPlotsHeight),
+                              DetProb=dfDet[self.trRunFolderCol].apply(self.plotImageHtmlElement,
+                                                                       plotImgPrfx=self.PlotImgPrfxDetProb,
+                                                                       plotHeight=self.superSynthPlotsHeight)))
+        
+        idxFmt = '{{n:0{}d}}'.format(1+max(int(math.log10(len(dfSupSyn))), 1))
+        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
+
+        def numNavLink(sAnlys):
+            return numNavLinkFmt.format(p=self.relativeRunFolderUrl(sAnlys[self.trRunFolderCol]), n=sAnlys.name)
+        dfSupSyn.index = dfDet.apply(numNavLink, axis='columns')
+        
+        # d. Translate table columns.
+        dfSupSyn.columns = [self.tr(col) for col in dfSupSyn.columns]
+
+        # 4. Synthesis: Format filtered and translated data.
+        # a. Add run folder column if not selected (will serve to generate the link to the analysis detailed report)
+        dfSyn = dfSynRes
+        dfSyn[self.trRunFolderCol] = dfSyn[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+        
+        # b. Links to each analysis detailed report.
+        idxFmt = '{{n:0{}d}}'.format(1 + max(int(math.log10(len(dfSyn))), 1))
+        numNavLinkFmt = '<a href="./{{p}}/index.html">{}</a>'.format(idxFmt)
+
+        def numNavLink(sAnlys):
+            return numNavLinkFmt.format(p=sAnlys[self.trRunFolderCol], n=sAnlys.name)
+       
+        # c. Post-format as specified in actual class.
+        dfsSyn = self.finalFormatAllAnalysesData(dfSyn, sort=True, indexer=numNavLink,
+                                                 convert=True, round_=True, style=True)
+
+        # 5. Details: Format filtered and translated data.
+        dfDet = dfDetRes
+
+        # a. Add run folder column if not there (will serve to generate the link to the analysis detailed report)
+        detTrCols = list(dfDet.columns)
+        if self.trRunFolderCol not in detTrCols:
+            detTrCols += [self.trRunFolderCol]
+        dfDet[self.trRunFolderCol] = dfDet[self.trRunFolderCol].apply(self.relativeRunFolderUrl)
+        dfDet = dfDet.reindex(columns=detTrCols)
+       
+        # b. Links to each analysis detailed report.
+        dfsDet = self.finalFormatAllAnalysesData(dfDet, sort=True, indexer=numNavLink,
+                                                 convert=False, round_=False, style=True)
+
+        # 6. Generate traceability infos section.
+        # a. Log of opérations.
+        indexCols = [self.tr(col) for col in ['Scheme', 'Step']]
+        dataCols = [self.tr(col) for col in ['Property', 'Value']]
+        dfFilSorHist = pd.DataFrame(filSorSteps, columns=indexCols + dataCols)
+        dfFilSorHist.set_index(indexCols, inplace=True)
+        ddfTrc = {self.tr('Filter & Sort steps'): (dfFilSorHist, True)}
+
+        # b. Results specs
+        ddfTrc.update(self.asWorkbook(subset=['specs', 'samples']))
+
+        # 7. Generate top report page.
+        genDateTime = dt.datetime.now().strftime('%d/%m/%Y %H:%M:%S')
+        tmpl = self.getTemplateEnv().get_template('mcds/fulltop.htpl')
+        xlFileUrl = os.path.basename(self.targetFilePathName(suffix='.xlsx')).replace(os.sep, '/')
+        html = tmpl.render(supersynthesis=dfSupSyn.to_html(escape=False),
+                           synthesis=dfsSyn.render(),  # escape=False, index=False),
+                           details=dfsDet.render(),  # escape=False, index=False),
+                           traceability={trcName: dfTrcTable.to_html(escape=False, na_rep='')
+                                         for trcName, (dfTrcTable, _) in ddfTrc.items()},
+                           title=self.title, keywords=self.keywords,
+                           subtitle=self.subTitle.format(fsId=filSorSchId.split('@')[0]),
+                           description=self.description.format(fsId=filSorSchId.split('@')[0]),
+                           xlUrl=xlFileUrl, tr=self.dTrans[self.lang],
+                           pySources=[pl.Path(fpn).name for fpn in self.pySources],
+                           genDateTime=genDateTime, version=__version__, libVersions=self._libVersions(), 
+                           distanceUnit=self.tr(self.resultsSet.distanceUnit),
+                           areaUnit=self.tr(self.resultsSet.areaUnit),
+                           surveyType=self.tr(self.resultsSet.surveyType),
+                           distanceType=self.tr(self.resultsSet.distanceType),
+                           clustering=self.tr('Clustering' if self.resultsSet.clustering else 'No clustering'),
+                           estimSelCrits=estimSelCrits, confIntervals=[str(v) for v in confIntervals])
+        html = re.sub('(?:[ \t]*\n){2,}', '\n'*2, html)  # Cleanup blank lines series to one only
+
+        # 8. Write top HTML to file.
+        filSorSchId = self.resultsSet.filSorSchemeId(filSorScheme)
+        htmlPathName = self.targetFilePathName(suffix=f'.{filSorSchId}.html')
+        with codecs.open(htmlPathName, mode='w', encoding='utf-8-sig') as tgtFile:
+            tgtFile.write(html)
+
+        return htmlPathName
+
+    def toHtml(self, filSorScheme=dict(method=ResClass.filterSortOnExecCode),
+               rebuild=False):
+
+        """HTML report generation for a given scheme.
+
+        Parameters:
+        :param filSorScheme: the 1 (and only) scheme to use for building the report (see ctor)
+        :param rebuild: if True, rebuild from scratch (data extraction + plots) ;
+                        otherwise, use any cached data or existing plot image files
+
+        Note: Parallelism does not work for this class (WTF ?), hence the absence of the 'generators' parameter.
+              Actually, it seems to work only the first time, and only when rebuild == False
+              (and may be no matplotlib drawing actually done) ;
+              but then, Exception: Can't pickle <function sync_do_first ... :-(
+        """
+
+        logger.debug(f'MCDSResultsFilterSortReport.toHtml({rebuild=}, {filSorScheme=})')
+
+        generators = None
+
+        # Install needed attached files.
+        self.installAttFiles(self.AttachedFiles)
+            
+        # Generate full report detailed pages (one for each analysis)
+        # (done first to have plot image files generated for top report page generation right below).
+        filSorSchId = self.resultsSet.filSorSchemeId(filSorScheme)
+        self.toHtmlEachAnalysis(filSorScheme=filSorScheme, rebuild=rebuild, generators=generators,
+                                topSuffix=f'.{filSorSchId}.html')
+        
+        # Generate top = main report page (one for all analyses).
+        topHtmlPathName = self.toHtmlAllAnalyses(filSorScheme=filSorScheme, rebuild=rebuild)
+
+        logger.info('... done.')
+                
+        return topHtmlPathName
+
+
+if __name__ == '__main__':
+
+    sys.exit(0)
```

### Comparing `pyaudisam-0.9.3/setup.py` & `pyaudisam-1.0.1/setup.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,59 +1,59 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# This script is for building the source and binary PyPI packages for pyaudisam:
-# you can use the official python way (after installing the 'build' package):
-# $ python -m build
-# ... or the old way (should also work out of the box):
-# $ python setup.py sdist bdist_wheel
-
-from setuptools import setup
-import pathlib as pl
-import re
-
-# The directory containing this file
-here = pl.Path(__file__).parent
-
-# Retrieve version from __init__.py
-with open(here / 'pyaudisam' / '__init__.py') as file:
-    version = re.search(r'__version__\s*=\s*[\'"]([^\'"]*)[\'"]', file.read()).group(1)
-
-# Retrieve install_requires from requirements.txt
-with open(here / 'README.md') as file:
-    long_desc = file.read()
-
-# Retrieve install_requires from requirements.txt
-with open(here / 'requirements.txt') as file:
-    requirements = file.read().splitlines()
-
-# This call to setup() does all the final work !
-setup(name='pyaudisam', version=version, url='https://github.com/denmedius/pyaudisam',
-      description='Distance Sampling automation through Distance sofware',
-      long_description=long_desc, long_description_content_type='text/markdown',
-      author='denmedius',
-      license='GPLv3+',
-      classifiers=['Topic :: Software Development :: Libraries',
-                   'Topic :: Software Development :: Libraries :: Python Modules',
-                   'Intended Audience :: Science/Research',
-                   'Development Status :: 3 - Alpha',
-                   'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',
-                   'Programming Language :: Python :: 3 :: Only',
-                   'Programming Language :: Python :: 3.8',
-                   'Environment :: Win32 (MS Windows)'],
-      packages=['pyaudisam'],
-      include_package_data=True,
-      python_requires='>=3.8',
-      install_requires=requirements,
-      entry_points={'console_scripts': []})
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# This script is for building the source and binary PyPI packages for pyaudisam:
+# you can use the official python way (after installing the 'build' package):
+# $ python -m build
+# ... or the old way (should also work out of the box):
+# $ python setup.py sdist bdist_wheel
+
+from setuptools import setup
+import pathlib as pl
+import re
+
+# The directory containing this file
+here = pl.Path(__file__).parent
+
+# Retrieve version from __init__.py
+with open(here / 'pyaudisam' / '__init__.py') as file:
+    version = re.search(r'__version__\s*=\s*[\'"]([^\'"]*)[\'"]', file.read()).group(1)
+
+# Retrieve install_requires from requirements.txt
+with open(here / 'README.md') as file:
+    long_desc = file.read()
+
+# Retrieve install_requires from requirements.txt
+with open(here / 'requirements.txt') as file:
+    requirements = file.read().splitlines()
+
+# This call to setup() does all the final work !
+setup(name='pyaudisam', version=version, url='https://github.com/denmedius/pyaudisam',
+      description='Distance Sampling automation through python and Distance sofware',
+      long_description=long_desc, long_description_content_type='text/markdown',
+      author='denmedius', author_email='fefeqe22.vucuqu82@murena.io',
+      license='GPLv3+',
+      classifiers=['Topic :: Software Development :: Libraries',
+                   'Topic :: Software Development :: Libraries :: Python Modules',
+                   'Intended Audience :: Science/Research',
+                   'Development Status :: 3 - Alpha',
+                   'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',
+                   'Programming Language :: Python :: 3 :: Only',
+                   'Programming Language :: Python :: 3.8',
+                   'Environment :: Win32 (MS Windows)'],
+      packages=['pyaudisam'],
+      include_package_data=True,
+      python_requires='>=3.8',
+      install_requires=requirements,
+      entry_points={'console_scripts': []})
```

### Comparing `pyaudisam-0.9.3/tests/conftest.py` & `pyaudisam-1.0.1/tests/conftest.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret, Sylvain Sainnier
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Pytest configuration file for all automated unit and integration tests
-
-import sys
-import pathlib as pl
-
-
-KTestSrcPath = pl.Path(__file__).parent
-
-# Update PYTHONPATH for pyaudisam package to be importable.
-sys.path.insert(0, KTestSrcPath.parent.as_posix())
-
-# Temporary work folder.
-tmpDir = KTestSrcPath / 'tmp'
-tmpDir.mkdir(exist_ok=True)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret, Sylvain Sainnier
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Pytest configuration file for all automated unit and integration tests
+
+import sys
+import pathlib as pl
+
+
+KTestSrcPath = pl.Path(__file__).parent
+
+# Update PYTHONPATH for pyaudisam package to be importable.
+sys.path.insert(0, KTestSrcPath.parent.as_posix())
+
+# Temporary work folder.
+tmpDir = KTestSrcPath / 'tmp'
+tmpDir.mkdir(exist_ok=True)
```

### Comparing `pyaudisam-0.9.3/tests/devarchives1.ipynb` & `pyaudisam-1.0.1/tests/devarchives1.ipynb`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/devarchives2.ipynb` & `pyaudisam-1.0.1/tests/devarchives2.ipynb`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/ipython_notebook_toc.js` & `pyaudisam-1.0.1/tests/ipython_notebook_toc.js`

 * *Files 18% similar despite different names*

#### js-beautify {}

```diff
@@ -1,99 +1,99 @@
-// ipython_notebook_goodies
-// ========================
-// Make a table of contents for your notebook. Uses headings (e.g. H1, H2, etc.) to build TOC, 
-// and provides anchors (added where needed).
-// 
-// Author: see https://github.com/kmahelona/ipython_notebook_goodies
-//
-// Changes (by Jean-Philippe Meuret) :
-// * removed articifial roman numbering
-// * added support for notebook title
-//
-// Known bugs:
-// * as jupyter notebook seems to force <Hi> ids to their innerHTML,
-//   you can't hide the NB title in the generated TOC
-// 
-// **Usage:** 
-// 
-// 1. Add a *markdown* cell at the top of your notebook with the following code inside :
-//    Note: the last div sections is mandatory
-//    Note: but above it, you can add any HTML code, even with <h> elements
-//          (with class="tocIgnore" if you don't want them to appear in the generated TOC)
-// 
-// <!-- Auto table of contents -->
-// <h1 id='tocIgnoreNbTitle'>... title of notebook ...</h1>
-// <p> ... some text as you like </p>
-// <p> ... but may be any HTML you like </p>
-// <div style="overflow-y: auto">
-//   <h2 id='tocTitle'>Table of contents</h2>
-//   <div id="toc"></div>
-// </div>
-// 
-// 2. Add a *code* cell anywhere in the notebook with the following:
-//    (provided that this file is named ipython_notebook_toc.js
-//     and located in the folder of the notebook)
-// 
-// %%javascript
-// $.getScript('./ipython_notebook_toc.js')
-// 
-// 3. Re-run it to update the TOC
-
-// Builds a <ul> Table of Contents from all <h> in DOM (ex: <h3> => level 3 nested <ul>)
-function createTOC() {
-    let toc = "";
-    let level = 0;
-    let levels = {};
-    $('#toc').html('');
-
-    $(":header").each(function(i) {
-
-        //$('#toc').append('<br>' + this.innerHTML + ' : ' + this.className);
-        //return;
-
-        // Ignore any <h> with 'tocIgnore' class
-        if (this.className == 'tocIgnore')
-            return;
-
-        let titleText = this.innerHTML;
-        let openLevel = this.tagName[1];
-
-        if (levels[openLevel]) {
-            levels[openLevel] += 1;
-        } else {
-            levels[openLevel] = 1;
-        }
-
-        if (openLevel > level) {
-            toc += (new Array(openLevel - level + 1)).join('<ul class="toc">');
-        } else if (openLevel < level) {
-            toc += (new Array(level - openLevel + 1)).join("</ul>");
-            for (i = level; i > openLevel; i--)
-                levels[i] = 0;
-        }
-
-        level = parseInt(openLevel);
-
-        if (this.id == '')
-            this.id = this.innerHTML.replace(/ /g, "-");
-        let anchor = this.id;
-
-        toc += '<li><a href="#' + encodeURIComponent(anchor) + '">' +
-            titleText +
-            '</a></li>';
-
-    });
-
-    if (level)
-        toc += (new Array(level + 1)).join("</ul>");
-
-    $('#toc').append(toc);
-
-};
-
-// Executes the createToc function
-setTimeout(function() {
-    createTOC();
-}, 100);
-
-// Rebuild to TOC every minute
+// ipython_notebook_goodies
+// ========================
+// Make a table of contents for your notebook. Uses headings (e.g. H1, H2, etc.) to build TOC, 
+// and provides anchors (added where needed).
+// 
+// Author: see https://github.com/kmahelona/ipython_notebook_goodies
+//
+// Changes (by Jean-Philippe Meuret) :
+// * removed articifial roman numbering
+// * added support for notebook title
+//
+// Known bugs:
+// * as jupyter notebook seems to force <Hi> ids to their innerHTML,
+//   you can't hide the NB title in the generated TOC
+// 
+// **Usage:** 
+// 
+// 1. Add a *markdown* cell at the top of your notebook with the following code inside :
+//    Note: the last div sections is mandatory
+//    Note: but above it, you can add any HTML code, even with <h> elements
+//          (with class="tocIgnore" if you don't want them to appear in the generated TOC)
+// 
+// <!-- Auto table of contents -->
+// <h1 id='tocIgnoreNbTitle'>... title of notebook ...</h1>
+// <p> ... some text as you like </p>
+// <p> ... but may be any HTML you like </p>
+// <div style="overflow-y: auto">
+//   <h2 id='tocTitle'>Table of contents</h2>
+//   <div id="toc"></div>
+// </div>
+// 
+// 2. Add a *code* cell anywhere in the notebook with the following:
+//    (provided that this file is named ipython_notebook_toc.js
+//     and located in the folder of the notebook)
+// 
+// %%javascript
+// $.getScript('./ipython_notebook_toc.js')
+// 
+// 3. Re-run it to update the TOC
+
+// Builds a <ul> Table of Contents from all <h> in DOM (ex: <h3> => level 3 nested <ul>)
+function createTOC() {
+    let toc = "";
+    let level = 0;
+    let levels = {};
+    $('#toc').html('');
+
+    $(":header").each(function(i) {
+
+        //$('#toc').append('<br>' + this.innerHTML + ' : ' + this.className);
+        //return;
+
+        // Ignore any <h> with 'tocIgnore' class
+        if (this.className == 'tocIgnore')
+            return;
+
+        let titleText = this.innerHTML;
+        let openLevel = this.tagName[1];
+
+        if (levels[openLevel]) {
+            levels[openLevel] += 1;
+        } else {
+            levels[openLevel] = 1;
+        }
+
+        if (openLevel > level) {
+            toc += (new Array(openLevel - level + 1)).join('<ul class="toc">');
+        } else if (openLevel < level) {
+            toc += (new Array(level - openLevel + 1)).join("</ul>");
+            for (i = level; i > openLevel; i--)
+                levels[i] = 0;
+        }
+
+        level = parseInt(openLevel);
+
+        if (this.id == '')
+            this.id = this.innerHTML.replace(/ /g, "-");
+        let anchor = this.id;
+
+        toc += '<li><a href="#' + encodeURIComponent(anchor) + '">' +
+            titleText +
+            '</a></li>';
+
+    });
+
+    if (level)
+        toc += (new Array(level + 1)).join("</ul>");
+
+    $('#toc').append(toc);
+
+};
+
+// Executes the createToc function
+setTimeout(function() {
+    createTOC();
+}, 100);
+
+// Rebuild to TOC every minute
 //setInterval(function(){createTOC();},60000);
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,725 +1,725 @@
-Ref	Passage	Observateur	Point	DateHeure	Espèce	nMalAd10	nAutAd10	nMalAd5	nAutAd5	distMem
-4446066	a	A	194	2019-04-07 07:40:00	Periparus ater	1	0	1	0	24,9606330209045
-4446096	a	A	194	2019-04-07 07:41:00	Fringilla coelebs	1	0	1	0	80,7914842071552
-4446095	a	A	194	2019-04-07 07:41:00	Fringilla coelebs	1	0	1	0	102,55447488593398
-4446097	a	A	194	2019-04-07 07:42:00	Cyanistes caeruleus	1	0	1	0	43,725797738755595
-4446098	a	A	194	2019-04-07 07:42:00	Poecile palustris	1	0	1	0	49,2618554158065
-4446062	a	A	194	2019-04-07 07:42:00	Emberiza cirlus	1	0	1	0	69,1728982854307
-4446061	a	A	194	2019-04-07 07:42:00	Phylloscopus collybita	1	0	1	0	102,257397840765
-4446099	a	A	194	2019-04-07 07:43:00	Alauda arvensis	2	0	2	0	73,1088920519477
-4446101	a	A	194	2019-04-07 07:44:00	Garrulus glandarius	0	1	0	1	30,3439103651818
-4446100	a	A	194	2019-04-07 07:44:00	Garrulus glandarius	0	1	0	1	65,96671079042599
-4446102	a	A	194	2019-04-07 07:44:00	Columba palumbus	1	0	1	0	129,169235786498
-4446104	a	A	194	2019-04-07 07:45:00	Certhia brachydactyla	1	0	0	0	81,45592084658149
-4446103	a	A	194	2019-04-07 07:45:00	Columba palumbus	1	0	0	0	86,4808530685552
-4446063	a	A	194	2019-04-07 07:46:00	Troglodytes troglodytes	1	0	0	0	36,0135679204489
-4446064	a	A	194	2019-04-07 07:46:00	Lullula arborea	1	0	0	0	50,6558130162114
-4446105	a	A	194	2019-04-07 07:46:00	Phoenicurus ochruros	1	0	0	0	104,114803024612
-4446106	a	A	194	2019-04-07 07:46:00	Streptopelia decaocto	1	1	0	0	134,50384189803
-4446065	a	A	194	2019-04-07 07:46:00	Parus major	1	0	0	0	187,78234298681
-4446107	a	A	194	2019-04-07 07:47:00	Emberiza citrinella	1	0	0	0	31,1656734957903
-4446110	a	A	194	2019-04-07 07:48:00	Erithacus rubecula	1	0	0	0	31,9605534312686
-4446108	a	A	194	2019-04-07 07:48:00	Sylvia atricapilla	1	0	0	0	53,693028418789204
-4446109	a	A	194	2019-04-07 07:48:00	Cuculus canorus	1	0	0	0	263,792364527315
-4446055	a	A	194	2019-04-07 07:49:00	Picus viridis	1	0	0	0	61,2093625094651
-4446056	a	A	194	2019-04-07 07:49:00	Columba palumbus	1	0	0	0	72,1510898599061
-4446057	a	A	194	2019-04-07 07:50:00	Dendrocopos major	0	1	0	0	83,2634044230843
-4446058	a	A	194	2019-04-07 07:50:00	Corvus corone	0	2	0	0	151,426282530952
-4446060	a	A	194	2019-04-07 07:51:00	Columba palumbus	1	0	0	0	98,0057739520934
-4446059	a	A	194	2019-04-07 07:51:00	Turdus philomelos	1	0	0	0	245,92490706446802
-4446077	a	A	195	2019-04-07 08:23:00	Motacilla alba	1	1	1	1	128,44767445640102
-4446075	a	A	195	2019-04-07 08:23:00	Passer domesticus	0	5	0	5	132,270121126544
-4446076	a	A	195	2019-04-07 08:23:00	Phoenicurus ochruros	1	0	1	0	140,767813325697
-4446078	a	A	195	2019-04-07 08:24:00	Alauda arvensis	1	0	1	0	87,2835106936389
-4446040	a	A	195	2019-04-07 08:24:00	Carduelis carduelis	1	0	1	0	109,307804058053
-4446039	a	A	195	2019-04-07 08:24:00	Lullula arborea	1	0	1	0	131,068527722403
-4446041	a	A	195	2019-04-07 08:25:00	Streptopelia decaocto	1	0	1	0	347,15954885764
-4446042	a	A	195	2019-04-07 08:28:00	Turdus philomelos	1	0	0	0	107,662600137994
-4446043	a	A	195	2019-04-07 08:28:00	Lullula arborea	1	0	0	0	113,157304067095
-4446044	a	A	195	2019-04-07 08:30:00	Pica pica	0	1	0	0	87,75268388246641
-4446046	a	A	195	2019-04-07 08:31:00	Saxicola rubicola	2	1	0	0	79,9003641584516
-4446045	a	A	195	2019-04-07 08:31:00	Erithacus rubecula	1	0	0	0	95,02107241514679
-4446085	a	A	196	2019-04-07 08:52:00	Turdus merula	1	0	1	0	11,6000185699332
-4446084	a	A	196	2019-04-07 08:52:00	Columba palumbus	0	1	0	1	33,5261237560319
-4446087	a	A	196	2019-04-07 08:53:00	Emberiza calandra	1	0	1	0	39,30553807688229
-4446086	a	A	196	2019-04-07 08:53:00	Emberiza citrinella	1	0	1	0	42,025672877656795
-4446088	a	A	196	2019-04-07 08:54:00	Parus major	1	1	1	1	33,7275326232273
-4446089	a	A	196	2019-04-07 08:55:00	Erithacus rubecula	1	0	1	0	55,4298561065516
-4446090	a	A	196	2019-04-07 08:55:00	Erithacus rubecula	1	0	1	0	78,3969567684819
-4446054	a	A	196	2019-04-07 08:56:00	Saxicola rubicola	1	0	0	0	64,3749528877979
-4446053	a	A	196	2019-04-07 08:56:00	Alauda arvensis	1	0	0	0	68,3669717300092
-4446047	a	A	196	2019-04-07 08:58:00	Sylvia atricapilla	1	0	0	0	25,949260661692602
-4446091	a	A	196	2019-04-07 08:58:00	Aegithalos caudatus	0	1	0	0	35,35691737051629
-4446048	a	A	196	2019-04-07 08:59:00	Emberiza cirlus	1	0	0	0	52,3609953632046
-4446049	a	A	196	2019-04-07 09:00:00	Cuculus canorus	1	0	0	0	344,638173682424
-4446050	a	A	196	2019-04-07 09:01:00	Lullula arborea	1	0	0	0	55,5288829275318
-4446052	a	A	196	2019-04-07 09:01:00	Phylloscopus collybita	1	0	0	0	72,2398428860285
-4446051	a	A	196	2019-04-07 09:01:00	Lullula arborea	1	0	0	0	107,908990125091
-4446174	a	A	197	2019-04-07 09:21:00	Lullula arborea	0	2	0	2	60,58664890040129
-4446175	a	A	197	2019-04-07 09:21:00	Phylloscopus collybita	1	0	1	0	65,695133697924
-4446176	a	A	197	2019-04-07 09:22:00	Emberiza calandra	1	0	1	0	70,1324894641736
-4446177	a	A	197	2019-04-07 09:22:00	Alauda arvensis	1	0	1	0	79,2302802984055
-4446178	a	A	197	2019-04-07 09:23:00	Alauda arvensis	3	0	3	0	46,277858838021295
-4446179	a	A	197	2019-04-07 09:23:00	Phylloscopus collybita	1	0	1	0	105,860918360315
-4446180	a	A	197	2019-04-07 09:23:00	Turdus philomelos	1	0	1	0	107,002225518955
-4446181	a	A	197	2019-04-07 09:24:00	Fringilla coelebs	0	2	0	2	54,266579847239896
-4446182	a	A	197	2019-04-07 09:24:00	Sylvia atricapilla	1	0	1	0	77,66497742168029
-4446183	a	A	197	2019-04-07 09:25:00	Sylvia atricapilla	1	0	0	0	67,4278673885609
-4446184	a	A	197	2019-04-07 09:25:00	Pica pica	0	1	0	0	103,648311893362
-4446185	a	A	197	2019-04-07 09:26:00	Corvus corone	0	2	0	0	115,580021773725
-4446186	a	A	197	2019-04-07 09:27:00	Corvus corone	0	1	0	0	137,210075285205
-4446187	a	A	197	2019-04-07 09:28:00	Cuculus canorus	1	0	0	0	149,219595792382
-4446189	a	A	197	2019-04-07 09:29:00	Saxicola rubicola	1	0	0	0	68,37424928057979
-4446188	a	A	197	2019-04-07 09:29:00	Erithacus rubecula	1	0	0	0	133,388676125305
-4446173	a	A	197	2019-04-07 09:30:00	Turdus philomelos	1	0	0	0	160,269126355142
-4446067	a	A	198	2019-04-07 09:44:00	Alauda arvensis	1	0	1	0	15,116436986297
-4446068	a	A	198	2019-04-07 09:46:00	Saxicola rubicola	1	0	1	0	47,338896465211896
-4446069	a	A	198	2019-04-07 09:47:00	Saxicola rubicola	1	0	1	0	39,2168117525201
-4446071	a	A	198	2019-04-07 09:48:00	Alauda arvensis	1	0	0	0	33,878946190236896
-4446070	a	A	198	2019-04-07 09:48:00	Turdus merula	1	0	0	0	61,4633434395941
-4446072	a	A	198	2019-04-07 09:49:00	Emberiza calandra	1	0	0	0	62,73881747798921
-4446073	a	A	198	2019-04-07 09:50:00	Sturnus vulgaris	0	1	0	0	79,246455279104
-4446074	a	A	198	2019-04-07 09:51:00	Emberiza cirlus	1	0	0	0	67,6226564133751
-4446079	a	A	198	2019-04-07 09:51:00	Carduelis chloris	1	0	0	0	102,67582342048
-4446080	a	A	198	2019-04-07 09:52:00	Carduelis chloris	1	0	0	0	90,34141984390921
-4446081	a	A	198	2019-04-07 09:52:00	Emberiza calandra	1	0	0	0	119,489863843806
-4446082	a	A	198	2019-04-07 09:52:00	Emberiza cirlus	1	0	0	0	132,91720742651302
-4446083	a	A	198	2019-04-07 09:53:00	Carduelis chloris	1	0	0	0	150,085865126107
-4514486	a	A	199	2019-05-01 06:52:00	Alauda arvensis	1	0	1	0	74,3157753135658
-4514484	a	A	199	2019-05-01 06:52:00	Luscinia megarhynchos	1	0	1	0	169,487564638674
-4514485	a	A	199	2019-05-01 06:52:00	Cuculus canorus	1	0	1	0	769,496826687577
-4514487	a	A	199	2019-05-01 06:53:00	Sylvia atricapilla	1	0	1	0	152,836412669501
-4514489	a	A	199	2019-05-01 06:54:00	Sylvia communis	1	0	1	0	99,6234086834815
-4514488	a	A	199	2019-05-01 06:54:00	Fringilla coelebs	1	0	1	0	124,163441470223
-4514490	a	A	199	2019-05-01 06:55:00	Sylvia communis	1	0	1	0	22,9870261266066
-4514491	a	A	199	2019-05-01 06:55:00	Alauda arvensis	1	0	1	0	75,2870376758794
-4514492	a	A	199	2019-05-01 06:56:00	Buteo buteo	0	1	0	0	152,257480194774
-4514493	a	A	199	2019-05-01 06:57:00	Turdus merula	1	0	0	0	91,00652140997542
-4514495	a	A	199	2019-05-01 06:57:00	Phylloscopus collybita	1	0	0	0	123,24618168061001
-4514494	a	A	199	2019-05-01 06:57:00	Alauda arvensis	1	0	0	0	159,409604787853
-4514498	a	A	199	2019-05-01 06:59:00	Carduelis cannabina	1	1	0	0	51,451630711758206
-4514496	a	A	199	2019-05-01 06:59:00	Cuculus canorus	1	0	0	0	757,374219138708
-4514497	a	A	199	2019-05-01 07:00:00	Luscinia megarhynchos	1	0	0	0	53,7637684635736
-4514514	a	A	200	2019-05-01 07:28:00	Erithacus rubecula	1	0	1	0	120,117173099438
-4514515	a	A	200	2019-05-01 07:28:00	Cyanistes caeruleus	1	0	1	0	137,314271970749
-4514516	a	A	200	2019-05-01 07:28:00	Parus major	1	0	1	0	161,24068407153
-4514518	a	A	200	2019-05-01 07:29:00	Sylvia atricapilla	1	0	1	0	80,4150968603295
-4514519	a	A	200	2019-05-01 07:29:00	Alauda arvensis	1	0	1	0	112,967091973585
-4514517	a	A	200	2019-05-01 07:29:00	Parus major	1	0	1	0	121,63165456781401
-4514521	a	A	200	2019-05-01 07:30:00	Alauda arvensis	1	0	1	0	148,52629193069998
-4514520	a	A	200	2019-05-01 07:30:00	Oriolus oriolus	1	0	1	0	195,44440102166598
-4514522	a	A	200	2019-05-01 07:30:00	Lullula arborea	1	0	1	0	210,887904297665
-4514525	a	A	200	2019-05-01 07:31:00	Certhia brachydactyla	1	0	0	0	92,82932552381139
-4514523	a	A	200	2019-05-01 07:31:00	Sturnus vulgaris	0	3	0	0	126,046446255278
-4514526	a	A	200	2019-05-01 07:31:00	Garrulus glandarius	0	1	0	0	172,86427479661498
-4514527	a	A	200	2019-05-01 07:32:00	Turdus merula	1	0	0	0	101,468495992675
-4514528	a	A	200	2019-05-01 07:33:00	Erithacus rubecula	1	0	0	0	159,097695018055
-4514529	a	A	200	2019-05-01 07:34:00	Turdus philomelos	1	0	0	0	77,7252935307379
-4514530	a	A	200	2019-05-01 07:34:00	Phylloscopus collybita	1	0	0	0	91,0279289340391
-4514543	a	A	200	2019-05-01 07:35:00	Alauda arvensis	1	0	0	0	19,979875104238
-4514542	a	A	200	2019-05-01 07:35:00	Fringilla coelebs	1	0	0	0	75,5976993842147
-4514544	a	A	200	2019-05-01 07:36:00	Turdus merula	1	0	0	0	85,9675501862196
-4514546	a	A	200	2019-05-01 07:38:00	Emberiza cirlus	1	1	0	0	75,4000290723015
-4514545	a	A	200	2019-05-01 07:38:00	Cyanistes caeruleus	1	0	0	0	79,8221962158932
-4514385	a	A	201	2019-05-01 07:52:00	Alauda arvensis	1	0	1	0	148,539384031746
-4514386	a	A	201	2019-05-01 07:52:00	Emberiza cirlus	1	1	1	1	185,79005490703602
-4514387	a	A	201	2019-05-01 07:53:00	Alauda arvensis	1	0	1	0	150,666152180301
-4514389	a	A	201	2019-05-01 07:54:00	Sylvia atricapilla	1	0	1	0	200,164523947043
-4514388	a	A	201	2019-05-01 07:54:00	Anthus trivialis	1	0	1	0	200,39370807446602
-4514391	a	A	201	2019-05-01 07:55:00	Phylloscopus sibilatrix	0	1	0	1	196,353594952595
-4514390	a	A	201	2019-05-01 07:55:00	Turdus philomelos	1	0	1	0	215,61012629442
-4514392	a	A	201	2019-05-01 07:58:00	Phylloscopus collybita	1	0	0	0	204,37271843415797
-4514393	a	A	201	2019-05-01 07:58:00	Cuculus canorus	1	0	0	0	731,262694174689
-4514395	a	A	201	2019-05-01 08:01:00	Coturnix coturnix	1	0	0	0	174,506692626328
-4514396	a	A	201	2019-05-01 08:01:00	Luscinia megarhynchos	1	0	0	0	203,563053586152
-4514394	a	A	201	2019-05-01 08:01:00	Columba palumbus	1	0	0	0	268,303495025816
-4514377	a	A	202	2019-05-01 08:42:00	Alauda arvensis	1	0	1	0	33,393940881049
-4514378	a	A	202	2019-05-01 08:43:00	Luscinia megarhynchos	1	0	1	0	91,7845195130493
-4514380	a	A	202	2019-05-01 08:43:00	Pica pica	0	2	0	2	92,3376256725424
-4514379	a	A	202	2019-05-01 08:43:00	Alauda arvensis	1	0	1	0	119,29738308631501
-4514413	a	A	202	2019-05-01 08:43:00	Streptopelia decaocto	2	0	2	0	368,307680170071
-4514414	a	A	202	2019-05-01 08:44:00	Columba palumbus	1	0	1	0	465,434437815679
-4514416	a	A	202	2019-05-01 08:45:00	Luscinia megarhynchos	1	0	1	0	296,000903938687
-4514415	a	A	202	2019-05-01 08:45:00	Turdus merula	1	0	1	0	336,74456220876
-4514417	a	A	202	2019-05-01 08:46:00	Luscinia megarhynchos	1	0	1	0	419,805407182374
-4514419	a	A	202	2019-05-01 08:47:00	Dendrocopos major	1	0	0	0	351,71766385440804
-4514418	a	A	202	2019-05-01 08:47:00	Turdus merula	1	0	0	0	401,93447087362
-4514421	a	A	202	2019-05-01 08:48:00	Hirundo rustica	0	5	0	0	43,3141181704658
-4514420	a	A	202	2019-05-01 08:48:00	Alauda arvensis	1	0	0	0	155,894839090712
-4514422	a	A	202	2019-05-01 08:50:00	Turdus merula	1	0	0	0	282,296034802461
-4514424	a	A	202	2019-05-01 08:51:00	Sylvia atricapilla	1	0	0	0	113,951974938833
-4514423	a	A	202	2019-05-01 08:51:00	Columba palumbus	1	0	0	0	335,241283422957
-4514381	a	A	218	2019-05-01 09:32:00	Sylvia communis	1	0	1	0	56,1725478382866
-4514382	a	A	218	2019-05-01 09:32:00	Luscinia megarhynchos	1	0	1	0	93,2659659718385
-4514397	a	A	218	2019-05-01 09:33:00	Columba palumbus	0	1	0	1	41,7314277896404
-4514383	a	A	218	2019-05-01 09:33:00	Cuculus canorus	1	0	1	0	108,805345226555
-4514384	a	A	218	2019-05-01 09:33:00	Cuculus canorus	1	0	1	0	320,330065357926
-4514399	a	A	218	2019-05-01 09:34:00	Columba palumbus	1	0	1	0	34,801294492999396
-4514398	a	A	218	2019-05-01 09:34:00	Columba palumbus	1	0	1	0	134,901396650027
-4514400	a	A	218	2019-05-01 09:35:00	Sylvia communis	1	0	1	0	36,5272361495435
-4514401	a	A	218	2019-05-01 09:35:00	Jynx torquilla	1	0	1	0	70,1810106709728
-4514403	a	A	218	2019-05-01 09:36:00	Sylvia communis	1	0	0	0	22,609055312411197
-4514402	a	A	218	2019-05-01 09:36:00	Oriolus oriolus	1	0	0	0	53,878476842424895
-4514406	a	A	218	2019-05-01 09:37:00	Cyanistes caeruleus	1	0	0	0	36,4486821753154
-4514405	a	A	218	2019-05-01 09:37:00	Turdus merula	1	0	0	0	46,91301547225039
-4514404	a	A	218	2019-05-01 09:37:00	Fringilla coelebs	1	0	0	0	74,6666192051567
-4514407	a	A	218	2019-05-01 09:38:00	Turdus philomelos	1	0	0	0	93,4671905151107
-4514408	a	A	218	2019-05-01 09:39:00	Sylvia atricapilla	1	0	0	0	95,1608221688229
-4514410	a	A	218	2019-05-01 09:40:00	Sylvia communis	1	0	0	0	74,3365662319095
-4514409	a	A	218	2019-05-01 09:40:00	Erithacus rubecula	1	0	0	0	95,4659997101174
-4514425	a	A	218	2019-05-01 09:41:00	Turdus merula	1	0	0	0	51,88480084910461
-4514411	a	A	218	2019-05-01 09:41:00	Sylvia atricapilla	1	0	0	0	98,1861206524536
-4514412	a	A	218	2019-05-01 09:41:00	Sylvia atricapilla	1	0	0	0	124,444321588718
-4514532	a	A	219	2019-05-01 09:06:00	Alauda arvensis	0	3	0	3	48,358392882597705
-4514531	a	A	219	2019-05-01 09:06:00	Emberiza calandra	1	0	1	0	98,22226641701509
-4514533	a	A	219	2019-05-01 09:07:00	Sylvia atricapilla	1	0	1	0	37,6226780736719
-4514534	a	A	219	2019-05-01 09:07:00	Anthus trivialis	1	0	1	0	84,76202876188641
-4514535	a	A	219	2019-05-01 09:08:00	Emberiza calandra	1	0	1	0	26,941478084676
-4514537	a	A	219	2019-05-01 09:08:00	Phylloscopus collybita	1	0	1	0	72,570088910878
-4514538	a	A	219	2019-05-01 09:08:00	Luscinia megarhynchos	1	0	1	0	77,15951913163009
-4514536	a	A	219	2019-05-01 09:08:00	Cuculus canorus	1	0	1	0	306,029656205173
-4514512	a	A	219	2019-05-01 09:09:00	Alauda arvensis	1	0	1	0	58,7040800747826
-4514539	a	A	219	2019-05-01 09:09:00	Turdus merula	1	0	1	0	91,4777620829747
-4514499	a	A	219	2019-05-01 09:10:00	Columba palumbus	1	0	1	0	69,2646881845744
-4514541	a	A	219	2019-05-01 09:10:00	Luscinia megarhynchos	1	0	1	0	136,006694284895
-4514540	a	A	219	2019-05-01 09:10:00	Picus viridis	0	1	0	1	192,85659255621997
-4514510	a	A	219	2019-05-01 09:11:00	Corvus corone	0	1	0	0	82,1697913312275
-4514511	a	A	219	2019-05-01 09:11:00	Milvus migrans	0	1	0	0	87,0635069747453
-4514513	a	A	219	2019-05-01 09:11:00	Sturnus vulgaris	0	2	0	0	172,67787234981097
-4514502	a	A	219	2019-05-01 09:12:00	Lullula arborea	1	0	0	0	51,4456504599331
-4514500	a	A	219	2019-05-01 09:12:00	Fringilla coelebs	1	0	0	0	57,8464320524095
-4514501	a	A	219	2019-05-01 09:12:00	Parus major	1	0	0	0	66,1792654942266
-4514503	a	A	219	2019-05-01 09:13:00	Parus major	1	0	0	0	111,229684113638
-4514504	a	A	219	2019-05-01 09:14:00	Sylvia atricapilla	1	0	0	0	59,461593886792706
-4514505	a	A	219	2019-05-01 09:15:00	Picus viridis	0	1	0	0	132,13920515473998
-4514507	a	A	219	2019-05-01 09:16:00	Phylloscopus collybita	1	0	0	0	69,42459950189149
-4514506	a	A	219	2019-05-01 09:16:00	Sylvia atricapilla	1	0	0	0	127,49530481443999
-4514508	a	A	219	2019-05-01 09:16:00	Turdus merula	0	3	0	0	162,216021107344
-4514509	a	A	219	2019-05-01 09:16:00	Buteo buteo	0	1	0	0	243,433216686733
-4599015	b	A	194	2019-06-01 06:38:00	Pica pica	0	1	0	1	195,03554977508702
-4599014	b	A	194	2019-06-01 06:38:00	Cuculus canorus	1	0	1	0	262,048646937523
-4599017	b	A	194	2019-06-01 06:39:00	Columba palumbus	1	1	1	1	64,45449705061971
-4599018	b	A	194	2019-06-01 06:39:00	Columba palumbus	1	0	1	0	185,947096324075
-4599016	b	A	194	2019-06-01 06:39:00	Cuculus canorus	1	1	1	1	231,67074715989102
-4599022	b	A	194	2019-06-01 06:40:00	Erithacus rubecula	1	0	1	0	41,3889272611209
-4599020	b	A	194	2019-06-01 06:40:00	Phylloscopus collybita	1	0	1	0	65,57852549835229
-4599021	b	A	194	2019-06-01 06:40:00	Oriolus oriolus	1	0	1	0	87,8604356621269
-4599019	b	A	194	2019-06-01 06:40:00	Columba palumbus	1	0	1	0	271,588122782757
-4599023	b	A	194	2019-06-01 06:41:00	Corvus corone	1	0	1	0	391,739550806667
-4599024	b	A	194	2019-06-01 06:42:00	Troglodytes troglodytes	1	0	0	0	41,588165188660604
-4599026	b	A	194	2019-06-01 06:42:00	Fringilla coelebs	1	0	0	0	50,0026030154457
-4599025	b	A	194	2019-06-01 06:42:00	Streptopelia decaocto	1	0	0	0	88,50232368525579
-4599034	b	A	194	2019-06-01 06:42:00	Coturnix coturnix	1	0	0	0	99,52442596112779
-4599027	b	A	194	2019-06-01 06:44:00	Alauda arvensis	1	0	0	0	112,03750433378599
-4599028	b	A	194	2019-06-01 06:44:00	Alauda arvensis	1	0	0	0	112,03750433378599
-4599029	b	A	194	2019-06-01 06:45:00	Turdus merula	1	0	0	0	115,49613257012501
-4599030	b	A	194	2019-06-01 06:46:00	Passer domesticus	0	2	0	0	140,744809906205
-4599033	b	A	194	2019-06-01 06:47:00	Sylvia communis	1	0	0	0	21,155884224475102
-4599032	b	A	194	2019-06-01 06:47:00	Sylvia atricapilla	1	0	0	0	184,669138393749
-4599031	b	A	194	2019-06-01 06:47:00	Alauda arvensis	1	0	0	0	187,86006032244302
-4599035	b	A	195	2019-06-01 06:59:00	Sylvia communis	1	0	1	0	73,184089980867
-4599037	b	A	195	2019-06-01 07:00:00	Sylvia atricapilla	1	0	1	0	90,5071157462212
-4599038	b	A	195	2019-06-01 07:00:00	Alauda arvensis	1	0	1	0	91,8258747436827
-4599036	b	A	195	2019-06-01 07:00:00	Passer domesticus	6	0	6	0	142,707019200623
-4599040	b	A	195	2019-06-01 07:01:00	Pica pica	1	0	1	0	134,68617471680201
-4599039	b	A	195	2019-06-01 07:01:00	Corvus corone	0	1	0	1	204,507604952764
-4599041	b	A	195	2019-06-01 07:02:00	Corvus corone	0	1	0	1	346,57114860449303
-4599042	b	A	195	2019-06-01 07:03:00	Turdus merula	1	0	1	0	129,485041552095
-4599043	b	A	195	2019-06-01 07:03:00	Passer domesticus	0	12	0	12	144,513263885894
-4599044	b	A	195	2019-06-01 07:03:00	Alauda arvensis	1	0	1	0	172,662084752474
-4599054	b	A	195	2019-06-01 07:04:00	Sylvia communis	1	0	0	0	88,3720227329472
-4599045	b	A	195	2019-06-01 07:04:00	Fringilla coelebs	1	0	0	0	105,980737570432
-4599046	b	A	195	2019-06-01 07:04:00	Luscinia megarhynchos	1	0	0	0	226,477781197181
-4599055	b	A	195	2019-06-01 07:04:00	Streptopelia turtur	1	0	0	0	652,547581256
-4599056	b	A	195	2019-06-01 07:04:00	Cuculus canorus	1	0	0	0	945,749223038415
-4599047	b	A	195	2019-06-01 07:05:00	Passer domesticus	0	1	0	0	80,9891516724061
-4599048	b	A	195	2019-06-01 07:06:00	Parus major	1	0	0	0	100,512240553381
-4599049	b	A	195	2019-06-01 07:06:00	Luscinia megarhynchos	1	0	0	0	148,950271074042
-4599050	b	A	195	2019-06-01 07:07:00	Phylloscopus collybita	1	0	0	0	146,18910931463898
-4599051	b	A	195	2019-06-01 07:07:00	Cuculus canorus	1	0	0	0	860,5520878983681
-4599053	b	A	195	2019-06-01 07:08:00	Sturnus vulgaris	0	2	0	0	139,94864196021499
-4599052	b	A	195	2019-06-01 07:08:00	Turdus merula	1	0	0	0	179,609558218571
-4599277	b	A	196	2019-06-01 07:37:00	Lanius collurio	1	0	1	0	44,679305688868105
-4599279	b	A	196	2019-06-01 07:37:00	Sylvia communis	1	0	1	0	53,5680565082016
-4599278	b	A	196	2019-06-01 07:37:00	Emberiza calandra	1	0	1	0	116,581034592216
-4599281	b	A	196	2019-06-01 07:38:00	Lullula arborea	1	0	1	0	39,5262550821716
-4599280	b	A	196	2019-06-01 07:38:00	Sylvia communis	1	0	1	0	52,28250204876571
-4599282	b	A	196	2019-06-01 07:38:00	Cuculus canorus	1	0	1	0	153,447525368306
-4599284	b	A	196	2019-06-01 07:39:00	Luscinia megarhynchos	1	0	1	0	51,772981048319
-4599283	b	A	196	2019-06-01 07:39:00	Sylvia communis	1	0	1	0	61,001576447121
-4599285	b	A	196	2019-06-01 07:40:00	Cyanistes caeruleus	1	0	1	0	63,116786432809896
-4599286	b	A	196	2019-06-01 07:40:00	Lanius collurio	1	0	1	0	112,851376391869
-4599295	b	A	196	2019-06-01 07:41:00	Lanius collurio	1	0	0	0	70,3001592302993
-4599298	b	A	196	2019-06-01 07:41:00	Lanius collurio	1	1	0	0	79,4656740850447
-4599287	b	A	196	2019-06-01 07:41:00	Turdus merula	1	0	0	0	102,517120064404
-4599297	b	A	196	2019-06-01 07:41:00	Luscinia megarhynchos	1	0	0	0	118,34118297231599
-4599296	b	A	196	2019-06-01 07:41:00	Oriolus oriolus	1	0	0	0	262,608563238583
-4599288	b	A	196	2019-06-01 07:42:00	Sturnus vulgaris	0	1	0	0	59,1241193360105
-4599289	b	A	196	2019-06-01 07:42:00	Garrulus glandarius	0	1	0	0	134,93578890732002
-4599290	b	A	196	2019-06-01 07:43:00	Sylvia communis	1	0	0	0	68,0414750151381
-4599292	b	A	196	2019-06-01 07:44:00	Sylvia communis	1	0	0	0	79,54864868985959
-4599291	b	A	196	2019-06-01 07:44:00	Streptopelia turtur	1	0	0	0	230,704931241834
-4599293	b	A	196	2019-06-01 07:45:00	Cuculus canorus	1	0	0	0	541,6424309332119
-4599294	b	A	196	2019-06-01 07:46:00	Sylvia atricapilla	1	0	0	0	93,95496331103192
-4599300	b	A	197	2019-06-01 08:07:00	Sylvia atricapilla	1	0	1	0	90,746217255329
-4599301	b	A	197	2019-06-01 08:08:00	Phylloscopus collybita	1	0	1	0	67,58907365302589
-4599304	b	A	197	2019-06-01 08:08:00	Phylloscopus collybita	1	0	1	0	89,67757773853951
-4599302	b	A	197	2019-06-01 08:08:00	Sylvia communis	1	0	1	0	104,019738473281
-4599303	b	A	197	2019-06-01 08:08:00	Sylvia communis	1	0	1	0	143,07515172735398
-4599305	b	A	197	2019-06-01 08:09:00	Alauda arvensis	1	0	1	0	71,7660138101057
-4599306	b	A	197	2019-06-01 08:09:00	Sylvia communis	1	0	1	0	95,1300190957579
-4599307	b	A	197	2019-06-01 08:10:00	Columba palumbus	1	0	1	0	141,016764228186
-4599308	b	A	197	2019-06-01 08:10:00	Corvus corone	0	2	0	2	146,419985357581
-4599310	b	A	197	2019-06-01 08:11:00	Garrulus glandarius	0	1	0	1	123,22732111357901
-4599309	b	A	197	2019-06-01 08:11:00	Streptopelia turtur	1	0	1	0	132,791434168627
-4599311	b	A	197	2019-06-01 08:12:00	Fringilla coelebs	1	0	0	0	92,88501524082399
-4599312	b	A	197	2019-06-01 08:12:00	Lullula arborea	1	0	0	0	121,32975402027701
-4599315	b	A	197	2019-06-01 08:14:00	Parus major	1	0	0	0	54,579763679471206
-4599316	b	A	197	2019-06-01 08:15:00	Luscinia megarhynchos	1	0	0	0	74,9956046038237
-4599317	b	A	197	2019-06-01 08:15:00	Prunella modularis	1	0	0	0	98,68584476935129
-4599319	b	A	197	2019-06-01 08:16:00	Sylvia communis	1	0	0	0	77,5114424424689
-4599318	b	A	197	2019-06-01 08:16:00	Alauda arvensis	1	0	0	0	110,41218885765
-4599320	b	A	197	2019-06-01 08:17:00	Lanius collurio	1	0	0	0	105,961253871616
-4599321	b	A	197	2019-06-01 08:17:00	Sylvia communis	1	0	0	0	108,726478476536
-4599323	b	A	197	2019-06-01 08:18:00	Lanius collurio	1	0	0	0	57,8956910668741
-4599322	b	A	197	2019-06-01 08:18:00	Carduelis chloris	1	0	0	0	119,256173541882
-4599328	b	A	198	2019-06-01 08:27:00	Saxicola rubicola	1	0	1	0	30,6314209001073
-4599329	b	A	198	2019-06-01 08:28:00	Emberiza calandra	1	0	1	0	118,65850200101002
-4599331	b	A	198	2019-06-01 08:30:00	Lullula arborea	1	0	1	0	69,1895341258998
-4599330	b	A	198	2019-06-01 08:30:00	Saxicola rubicola	1	0	1	0	112,18652977784402
-4599333	b	A	198	2019-06-01 08:31:00	Sylvia communis	1	0	1	0	178,22407765541
-4599334	b	A	198	2019-06-01 08:32:00	Sylvia atricapilla	1	0	0	0	138,803462350616
-4599335	b	A	198	2019-06-01 08:33:00	Cuculus canorus	1	0	0	0	486,23291826253
-4599337	b	A	198	2019-06-01 08:34:00	Sylvia communis	1	0	0	0	53,66665811101849
-4599336	b	A	198	2019-06-01 08:34:00	Carduelis cannabina	1	0	0	0	55,989643098330795
-4599338	b	A	198	2019-06-01 08:35:00	Sylvia atricapilla	1	0	0	0	82,7403148152994
-4599340	b	A	198	2019-06-01 08:36:00	Emberiza cirlus	1	0	0	0	107,179210632745
-4599339	b	A	198	2019-06-01 08:36:00	Turdus merula	1	0	0	0	108,323656467374
-4599341	b	A	198	2019-06-01 08:37:00	Alauda arvensis	1	0	0	0	55,1090853810248
-4599342	b	A	198	2019-06-01 08:38:00	Turdus merula	1	1	0	0	76,248264190042
-4582077	b	A	199	2019-05-26 06:33:00	Alauda arvensis	1	0	1	0	50,3322170654364
-4582076	b	A	199	2019-05-26 06:33:00	Alauda arvensis	1	0	1	0	64,8030090924468
-4582078	b	A	199	2019-05-26 06:34:00	Lanius collurio	1	0	1	0	40,260971046357795
-4582080	b	A	199	2019-05-26 06:34:00	Picus viridis	0	1	0	1	104,02563827511999
-4582079	b	A	199	2019-05-26 06:34:00	Oriolus oriolus	1	0	1	0	176,45623290265098
-4582095	b	A	199	2019-05-26 06:35:00	Turdus merula	1	0	1	0	31,4282183706542
-4582081	b	A	199	2019-05-26 06:35:00	Turdus philomelos	0	0	0	0	120,235808735937
-4582082	b	A	199	2019-05-26 06:35:00	Turdus philomelos	0	1	0	1	120,235808735937
-4582083	b	A	199	2019-05-26 06:36:00	Emberiza cirlus	1	0	1	0	16,1219474869418
-4582084	b	A	199	2019-05-26 06:36:00	Cuculus canorus	1	0	1	0	199,19121268275399
-4582086	b	A	199	2019-05-26 06:37:00	Alauda arvensis	1	0	0	0	60,7814872186907
-4582087	b	A	199	2019-05-26 06:37:00	Streptopelia turtur	1	0	0	0	111,200976099349
-4582097	b	A	199	2019-05-26 06:37:00	Emberiza calandra	1	0	0	0	123,38767419317699
-4582085	b	A	199	2019-05-26 06:37:00	Cuculus canorus	1	0	0	0	534,403154175238
-4582088	b	A	199	2019-05-26 06:38:00	Streptopelia turtur	1	0	0	0	293,10034684372
-4582089	b	A	199	2019-05-26 06:39:00	Lanius collurio	2	0	0	0	33,4329373686829
-4582099	b	A	199	2019-05-26 06:39:00	Alauda arvensis	1	0	0	0	120,709852718638
-4582091	b	A	199	2019-05-26 06:40:00	Turdus merula	2	0	0	0	44,961899599769794
-4582090	b	A	199	2019-05-26 06:40:00	Lanius collurio	1	0	0	0	78,92241299533849
-4582092	b	A	199	2019-05-26 06:41:00	Sitta europaea	1	0	0	0	115,88437719527799
-4582093	b	A	199	2019-05-26 06:41:00	Sturnus vulgaris	0	4	0	0	124,400886393132
-4582096	b	A	199	2019-05-26 06:41:00	Lanius collurio	1	0	0	0	146,317763208383
-4582094	b	A	199	2019-05-26 06:42:00	Turdus merula	1	0	0	0	128,803580746817
-4582098	b	A	199	2019-05-26 06:42:00	Columba palumbus	1	0	0	0	159,24267734091998
-4582124	b	A	200	2019-05-26 07:01:00	Milvus migrans	0	1	0	1	71,2250594050485
-4582123	b	A	200	2019-05-26 07:01:00	Corvus corone	1	0	1	0	153,15796561968
-4582126	b	A	200	2019-05-26 07:02:00	Sylvia communis	1	0	1	0	88,1944999343612
-4582125	b	A	200	2019-05-26 07:02:00	Sylvia atricapilla	1	0	1	0	127,72292478656
-4582127	b	A	200	2019-05-26 07:03:00	Columba palumbus	1	0	1	0	63,363485881828204
-4582129	b	A	200	2019-05-26 07:03:00	Alauda arvensis	1	0	1	0	87,0309007315132
-4582128	b	A	200	2019-05-26 07:03:00	Streptopelia turtur	1	0	1	0	142,33708757253598
-4582132	b	A	200	2019-05-26 07:04:00	Alauda arvensis	1	0	1	0	24,4566543095746
-4582131	b	A	200	2019-05-26 07:04:00	Streptopelia turtur	1	0	1	0	149,739836984526
-4582130	b	A	200	2019-05-26 07:04:00	Erithacus rubecula	1	0	1	0	159,887983333389
-4582146	b	A	200	2019-05-26 07:05:00	Fringilla coelebs	1	0	0	0	77,8600202838272
-4582133	b	A	200	2019-05-26 07:05:00	Lanius collurio	1	0	0	0	89,1190938450563
-4582144	b	A	200	2019-05-26 07:05:00	Parus major	1	0	0	0	104,434916198451
-4582134	b	A	200	2019-05-26 07:05:00	Alauda arvensis	1	0	0	0	122,109451088076
-4582135	b	A	200	2019-05-26 07:05:00	Dendrocopos major	1	0	0	0	144,747470739939
-4582145	b	A	200	2019-05-26 07:05:00	Cuculus canorus	1	0	0	0	286,223705080483
-4582136	b	A	200	2019-05-26 07:06:00	Columba palumbus	1	0	0	0	161,806479244382
-4582137	b	A	200	2019-05-26 07:07:00	Turdus merula	1	0	0	0	172,81663779251
-4582138	b	A	200	2019-05-26 07:07:00	Fringilla coelebs	1	0	0	0	190,15385009913
-4582139	b	A	200	2019-05-26 07:08:00	Regulus ignicapilla	1	0	0	0	63,010485234576905
-4582140	b	A	200	2019-05-26 07:08:00	Lullula arborea	1	0	0	0	132,53771385632498
-4582142	b	A	200	2019-05-26 07:10:00	Turdus merula	1	0	0	0	68,6868390701636
-4582143	b	A	200	2019-05-26 07:10:00	Sylvia atricapilla	1	0	0	0	71,74632157668941
-4582141	b	A	200	2019-05-26 07:10:00	Turdus merula	1	0	0	0	126,041710608328
-4582100	b	A	201	2019-05-26 07:24:00	Alauda arvensis	1	0	1	0	158,343994260968
-4582102	b	A	201	2019-05-26 07:25:00	Alauda arvensis	1	0	1	0	161,91452906748898
-4582101	b	A	201	2019-05-26 07:25:00	Coturnix coturnix	1	0	1	0	219,048603129183
-4582103	b	A	201	2019-05-26 07:26:00	Turdus merula	1	0	1	0	219,327027519667
-4582105	b	A	201	2019-05-26 07:28:00	Alauda arvensis	1	0	1	0	199,912431732117
-4582104	b	A	201	2019-05-26 07:28:00	Sylvia atricapilla	1	0	1	0	261,55303328123904
-4582106	b	A	201	2019-05-26 07:29:00	Phylloscopus collybita	1	0	0	0	254,63756104157503
-4582107	b	A	201	2019-05-26 07:30:00	Corvus corone	1	0	0	0	197,765126007713
-4582108	b	A	201	2019-05-26 07:30:00	Sylvia atricapilla	1	0	0	0	259,353948088643
-4582109	b	A	201	2019-05-26 07:33:00	Streptopelia turtur	1	0	0	0	618,469077774008
-4582110	b	A	201	2019-05-26 07:34:00	Corvus corone	1	0	0	0	218,219879410551
-4582111	b	A	202	2019-05-26 07:49:00	Turdus merula	1	0	1	0	87,82443526810249
-4582112	b	A	202	2019-05-26 07:50:00	Luscinia megarhynchos	1	0	1	0	373,39188986772103
-4582113	b	A	202	2019-05-26 07:50:00	Columba palumbus	1	0	1	0	397,993747293461
-4582115	b	A	202	2019-05-26 07:51:00	Fringilla coelebs	1	0	1	0	271,94553977054903
-4582114	b	A	202	2019-05-26 07:51:00	Streptopelia decaocto	2	0	2	0	361,790128553409
-4582116	b	A	202	2019-05-26 07:52:00	Sylvia atricapilla	1	0	1	0	366,984383223507
-4582117	b	A	202	2019-05-26 07:53:00	Alauda arvensis	1	0	1	0	153,455693232528
-4582119	b	A	202	2019-05-26 07:54:00	Alauda arvensis	1	0	0	0	132,616284024027
-4582118	b	A	202	2019-05-26 07:54:00	Pica pica	0	1	0	0	155,292175243529
-4582120	b	A	202	2019-05-26 07:55:00	Streptopelia turtur	1	0	0	0	373,633780003376
-4582121	b	A	202	2019-05-26 07:57:00	Turdus merula	0	1	0	0	145,694160622427
-4582122	b	A	202	2019-05-26 07:58:00	Picus viridis	1	0	0	0	426,047256159629
-4582278	b	A	218	2019-05-26 08:30:00	Cyanistes caeruleus	1	0	1	0	15,4059774268207
-4582279	b	A	218	2019-05-26 08:30:00	Streptopelia turtur	1	0	1	0	138,816487301226
-4582280	b	A	218	2019-05-26 08:31:00	Phylloscopus collybita	1	0	1	0	110,403485176878
-4582281	b	A	218	2019-05-26 08:31:00	Corvus corone	0	2	0	2	116,247257852521
-4582282	b	A	218	2019-05-26 08:32:00	Garrulus glandarius	0	1	0	1	151,5170974232
-4582283	b	A	218	2019-05-26 08:32:00	Corvus corone	1	0	1	0	167,125955926159
-4582285	b	A	218	2019-05-26 08:33:00	Columba palumbus	1	0	1	0	104,397798219357
-4582284	b	A	218	2019-05-26 08:33:00	Cuculus canorus	1	0	1	0	165,893452120929
-4582288	b	A	218	2019-05-26 08:34:00	Turdus merula	1	0	1	0	88,8073900347208
-4582287	b	A	218	2019-05-26 08:34:00	Cuculus canorus	1	0	1	0	127,316692522426
-4582290	b	A	218	2019-05-26 08:35:00	Sylvia communis	1	0	0	0	56,6378430777139
-4582291	b	A	218	2019-05-26 08:35:00	Turdus merula	1	0	0	0	67,75575963450011
-4582289	b	A	218	2019-05-26 08:35:00	Phylloscopus collybita	1	0	0	0	106,671151222402
-4582292	b	A	218	2019-05-26 08:36:00	Picus viridis	0	1	0	0	16,2237084887793
-4582293	b	A	218	2019-05-26 08:36:00	Turdus philomelos	1	0	0	0	109,80584922338001
-4582294	b	A	218	2019-05-26 08:37:00	Erithacus rubecula	1	0	0	0	101,548454378169
-4582295	b	A	218	2019-05-26 08:38:00	Oriolus oriolus	1	0	0	0	80,3374005123329
-4582296	b	A	218	2019-05-26 08:39:00	Alauda arvensis	1	0	0	0	40,0200629590777
-4582297	b	A	218	2019-05-26 08:39:00	Picus viridis	0	1	0	0	127,791227731709
-4582298	b	A	218	2019-05-26 08:40:00	Turdus merula	1	0	0	0	86,0061402592512
-4582299	b	A	218	2019-05-26 08:41:00	Turdus merula	1	0	0	0	36,086666094823606
-4582300	b	A	218	2019-05-26 08:41:00	Dendrocopos major	0	1	0	0	137,492517570941
-4582201	b	A	219	2019-05-26 08:07:00	Sylvia communis	1	0	1	0	111,30230646422301
-4582205	b	A	219	2019-05-26 08:08:00	Fringilla coelebs	0	1	0	1	96,55342147389611
-4582204	b	A	219	2019-05-26 08:08:00	Cyanistes caeruleus	0	1	0	1	109,096201827636
-4582203	b	A	219	2019-05-26 08:08:00	Turdus merula	1	0	1	0	113,545259564074
-4582202	b	A	219	2019-05-26 08:08:00	Sylvia communis	1	0	1	0	181,360496946992
-4582206	b	A	219	2019-05-26 08:09:00	Sturnus vulgaris	0	2	0	2	132,543645243619
-4582207	b	A	219	2019-05-26 08:10:00	Alauda arvensis	1	0	1	0	97,52349802977501
-4582208	b	A	219	2019-05-26 08:10:00	Luscinia megarhynchos	1	0	1	0	146,04066560546602
-4582209	b	A	219	2019-05-26 08:11:00	Luscinia megarhynchos	1	0	1	0	420,233219797956
-4582210	b	A	219	2019-05-26 08:13:00	Coturnix coturnix	1	0	0	0	91,1708520735268
-4582211	b	A	219	2019-05-26 08:14:00	Garrulus glandarius	0	1	0	0	115,29384986123699
-4582213	b	A	219	2019-05-26 08:15:00	Coccothraustes coccothraustes	0	1	0	0	71,20667356191609
-4582212	b	A	219	2019-05-26 08:15:00	Cyanistes caeruleus	0	4	0	0	132,735651740161
-4582215	b	A	219	2019-05-26 08:16:00	Emberiza cirlus	1	0	0	0	37,8027025203896
-4582216	b	A	219	2019-05-26 08:16:00	Corvus corone	0	1	0	0	53,704658098953
-4582214	b	A	219	2019-05-26 08:16:00	Saxicola rubicola	1	0	0	0	113,791915108591
-4582219	b	A	219	2019-05-26 08:17:00	Picus viridis	1	0	0	0	73,22073670376221
-4582217	b	A	219	2019-05-26 08:17:00	Sylvia atricapilla	1	0	0	0	85,0910895736893
-4582218	b	A	219	2019-05-26 08:17:00	Alauda arvensis	1	0	0	0	123,176710729404
-4470071	a	B	23	2019-04-13 08:55:00	Carduelis cannabina	1	0	1	0	37,713087798935106
-4470070	a	B	23	2019-04-13 08:55:00	Anthus pratensis	0	5	0	5	74,02432681432079
-4470073	a	B	23	2019-04-13 08:56:00	Alauda arvensis	1	0	1	0	73,2259407031987
-4470072	a	B	23	2019-04-13 08:56:00	Corvus corone	0	1	0	1	146,520673217486
-4470074	a	B	23	2019-04-13 08:57:00	Carduelis chloris	1	0	1	0	67,3265453099303
-4470075	a	B	23	2019-04-13 08:58:00	Emberiza citrinella	1	0	1	0	74,12076992537509
-4470076	a	B	23	2019-04-13 08:58:00	Corvus corone	0	1	0	1	303,093908801071
-4470078	a	B	23	2019-04-13 08:59:00	Sylvia atricapilla	1	0	0	0	25,6429359235936
-4470077	a	B	23	2019-04-13 08:59:00	Turdus merula	1	0	0	0	193,337316696177
-4470218	a	B	23	2019-04-13 08:59:00	Emberiza citrinella	1	0	0	0	204,318185075284
-4470213	a	B	23	2019-04-13 09:00:00	Turdus merula	1	0	0	0	40,632223456078
-4470214	a	B	23	2019-04-13 09:01:00	Prunella modularis	1	0	0	0	36,9652612957816
-4470215	a	B	23	2019-04-13 09:02:00	Turdus merula	1	0	0	0	66,6693172571411
-4470216	a	B	23	2019-04-13 09:03:00	Alauda arvensis	1	0	0	0	43,6591564301282
-4470217	a	B	23	2019-04-13 09:04:00	Lullula arborea	1	0	0	0	74,75760645399441
-4470195	a	B	39	2019-04-13 08:09:00	Turdus merula	1	0	1	0	31,7970918796458
-4470196	a	B	39	2019-04-13 08:10:00	Emberiza citrinella	1	0	1	0	52,0276958907015
-4470198	a	B	39	2019-04-13 08:10:00	Turdus viscivorus	1	0	1	0	235,368579504489
-4470197	a	B	39	2019-04-13 08:10:00	Turdus merula	1	0	1	0	235,845786074981
-4470200	a	B	39	2019-04-13 08:11:00	Prunella modularis	1	0	1	0	42,3602029293287
-4470199	a	B	39	2019-04-13 08:11:00	Alauda arvensis	1	0	1	0	92,88097329353641
-4470202	a	B	39	2019-04-13 08:11:00	Alauda arvensis	1	0	1	0	167,385594552369
-4470201	a	B	39	2019-04-13 08:11:00	Alauda arvensis	1	0	1	0	203,955045847492
-4470067	a	B	39	2019-04-13 08:12:00	Emberiza citrinella	0	1	0	1	67,0925250972644
-4470203	a	B	39	2019-04-13 08:12:00	Dendrocopos major	0	1	0	1	143,663608264769
-4470204	a	B	39	2019-04-13 08:12:00	Turdus merula	1	0	1	0	207,15756198094502
-4470068	a	B	39	2019-04-13 08:14:00	Turdus merula	1	1	0	0	62,0536534029842
-4470175	a	B	39	2019-04-13 08:14:00	Prunella modularis	1	0	0	0	100,178824205671
-4470176	a	B	39	2019-04-13 08:14:00	Alauda arvensis	1	0	0	0	178,97940224334
-4470069	a	B	39	2019-04-13 08:14:00	Jynx torquilla	1	0	0	0	243,934662307415
-4470178	a	B	39	2019-04-13 08:15:00	Turdus merula	1	0	0	0	88,86526318513529
-4470177	a	B	39	2019-04-13 08:15:00	Sylvia atricapilla	1	0	0	0	140,6272226198
-4470179	a	B	39	2019-04-13 08:16:00	Sturnus vulgaris	0	1	0	0	50,40991396650129
-4470180	a	B	39	2019-04-13 08:17:00	Sylvia atricapilla	1	0	0	0	141,42539171289
-4470181	a	B	39	2019-04-13 08:18:00	Saxicola rubicola	1	0	0	0	71,3696943921705
-4470167	a	B	40	2019-04-13 08:29:00	Erithacus rubecula	0	1	0	1	107,446160522627
-4470166	a	B	40	2019-04-13 08:29:00	Emberiza citrinella	1	1	1	1	113,42320854723499
-4470168	a	B	40	2019-04-13 08:30:00	Sylvia atricapilla	1	0	1	0	58,77628852831779
-4470169	a	B	40	2019-04-13 08:30:00	Alauda arvensis	2	0	2	0	239,38413006372502
-4470170	a	B	40	2019-04-13 08:31:00	Turdus merula	1	0	1	0	54,044983922635005
-4470171	a	B	40	2019-04-13 08:31:00	Dendrocopos minor	0	1	0	1	164,31640465133802
-4470172	a	B	40	2019-04-13 08:31:00	Alauda arvensis	1	0	1	0	189,07049339285
-4470173	a	B	40	2019-04-13 08:32:00	Turdus merula	1	0	1	0	66,9869661373368
-4470063	a	B	40	2019-04-13 08:32:00	Sylvia atricapilla	1	0	1	0	123,568714021798
-4470065	a	B	40	2019-04-13 08:36:00	Carduelis cannabina	1	0	0	0	37,0633950232869
-4470064	a	B	40	2019-04-13 08:36:00	Lullula arborea	1	0	0	0	73,9773823110031
-4470066	a	B	40	2019-04-13 08:36:00	Alauda arvensis	1	0	0	0	158,936702074386
-4470051	a	B	41	2019-04-12 10:29:00	Sylvia atricapilla	1	0	1	0	37,170045394943706
-4470052	a	B	41	2019-04-12 10:29:00	Sylvia atricapilla	1	0	1	0	85,6855381803027
-4470055	a	B	41	2019-04-12 10:30:00	Turdus merula	1	0	1	0	49,62186778152989
-4470054	a	B	41	2019-04-12 10:30:00	Sylvia atricapilla	1	0	1	0	126,475258222298
-4470053	a	B	41	2019-04-12 10:30:00	Garrulus glandarius	0	1	0	1	139,642532267887
-4470056	a	B	41	2019-04-12 10:31:00	Fringilla coelebs	1	0	1	0	89,5150351669275
-4470057	a	B	41	2019-04-12 10:31:00	Turdus merula	1	0	1	0	253,133119431361
-4470059	a	B	41	2019-04-12 10:32:00	Fringilla coelebs	1	0	1	0	49,240166060749395
-4470058	a	B	41	2019-04-12 10:32:00	Turdus viscivorus	1	0	1	0	97,12516682884541
-4470060	a	B	41	2019-04-12 10:33:00	Parus major	1	0	1	0	35,1097351961337
-4470061	a	B	41	2019-04-12 10:33:00	Turdus viscivorus	1	0	1	0	50,4371827256985
-4470205	a	B	41	2019-04-12 10:34:00	Certhia brachydactyla	1	0	0	0	36,063811125105204
-4470062	a	B	41	2019-04-12 10:34:00	Parus major	2	0	0	0	140,578604164386
-4470206	a	B	41	2019-04-12 10:36:00	Erithacus rubecula	0	1	0	0	29,608937021832105
-4470207	a	B	41	2019-04-12 10:37:00	Sylvia atricapilla	1	0	0	0	227,973729384829
-4470208	a	B	41	2019-04-12 10:38:00	Erithacus rubecula	1	0	0	0	29,1527533461869
-4470209	a	B	41	2019-04-12 10:38:00	Prunella modularis	1	0	0	0	51,062247611435204
-4470210	a	B	41	2019-04-12 10:38:00	Dendrocopos minor	1	0	0	0	66,9165108241414
-4470219	a	B	42	2019-04-13 09:32:00	Prunella modularis	1	0	1	0	36,238608618858
-4470220	a	B	42	2019-04-13 09:32:00	Turdus merula	1	0	1	0	63,516157343783
-4470222	a	B	42	2019-04-13 09:33:00	Turdus merula	1	0	1	0	127,94830644277599
-4470221	a	B	42	2019-04-13 09:33:00	Turdus merula	1	0	1	0	141,363652327633
-4470223	a	B	42	2019-04-13 09:35:00	Sylvia borin	1	0	1	0	56,5756846563167
-4470224	a	B	42	2019-04-13 09:35:00	Emberiza citrinella	1	0	1	0	69,208854407049
-4470225	a	B	42	2019-04-13 09:35:00	Sylvia atricapilla	1	0	1	0	101,48144219111401
-4470226	a	B	42	2019-04-13 09:36:00	Phylloscopus collybita	1	0	0	0	93,5993809022432
-4470184	a	B	42	2019-04-13 09:36:00	Parus major	1	0	0	0	93,8739945274134
-4470185	a	B	42	2019-04-13 09:36:00	Corvus corone	0	1	0	0	160,37663745847098
-4470186	a	B	42	2019-04-13 09:37:00	Corvus corone	0	1	0	0	182,12420405084703
-4470187	a	B	42	2019-04-13 09:38:00	Lullula arborea	2	0	0	0	134,099791210918
-4470188	a	B	42	2019-04-13 09:39:00	Prunella modularis	1	0	0	0	54,1271259164129
-4470189	a	B	42	2019-04-13 09:41:00	Cyanistes caeruleus	1	0	0	0	56,9758618774403
-4469992	a	B	56	2019-04-12 08:10:00	Saxicola rubicola	1	0	1	0	65,6094643554589
-4469991	a	B	56	2019-04-12 08:10:00	Jynx torquilla	1	0	1	0	225,460201607257
-4469993	a	B	56	2019-04-12 08:11:00	Emberiza citrinella	1	0	1	0	26,932172904777698
-4469994	a	B	56	2019-04-12 08:11:00	Emberiza cirlus	1	0	1	0	58,12240127807161
-4469995	a	B	56	2019-04-12 08:11:00	Emberiza cirlus	1	0	1	0	77,1525169567946
-4469996	a	B	56	2019-04-12 08:12:00	Alauda arvensis	2	0	2	0	51,8531035735715
-4469989	a	B	56	2019-04-12 08:13:00	Turdus merula	1	0	1	0	90,51349451779079
-4469997	a	B	56	2019-04-12 08:13:00	Sylvia atricapilla	1	0	1	0	151,176630948801
-4469990	a	B	56	2019-04-12 08:13:00	Picus viridis	0	1	0	1	259,863798229258
-4469999	a	B	56	2019-04-12 08:14:00	Carduelis cannabina	0	1	0	1	83,3347454287691
-4469998	a	B	56	2019-04-12 08:14:00	Emberiza citrinella	1	0	1	0	104,559844106239
-4470000	a	B	56	2019-04-12 08:15:00	Emberiza citrinella	1	0	0	0	66,87643654478751
-4470001	a	B	56	2019-04-12 08:15:00	Phylloscopus collybita	1	0	0	0	132,467672190077
-4470002	a	B	56	2019-04-12 08:16:00	Turdus merula	1	0	0	0	113,486527568089
-4470003	a	B	56	2019-04-12 08:17:00	Sylvia atricapilla	1	0	0	0	156,510812161839
-4470004	a	B	56	2019-04-12 08:18:00	Passer domesticus	1	0	0	0	63,8942773612831
-4470007	a	B	57	2019-04-12 08:34:00	Parus major	1	0	1	0	21,9548978825842
-4470005	a	B	57	2019-04-12 08:34:00	Sylvia atricapilla	3	0	3	0	27,8878381233664
-4470006	a	B	57	2019-04-12 08:34:00	Lullula arborea	1	0	1	0	59,214996607707704
-4470009	a	B	57	2019-04-12 08:35:00	Jynx torquilla	1	0	1	0	65,7651243452607
-4470008	a	B	57	2019-04-12 08:35:00	Emberiza citrinella	1	0	1	0	68,975418029866
-4470010	a	B	57	2019-04-12 08:36:00	Saxicola rubicola	1	0	1	0	42,0553084825253
-4470012	a	B	57	2019-04-12 08:37:00	Turdus merula	1	0	1	0	29,708385376770202
-4470011	a	B	57	2019-04-12 08:37:00	Cuculus canorus	1	0	1	0	238,547468347089
-4470014	a	B	57	2019-04-12 08:38:00	Dendrocopos major	0	1	0	0	88,0388895376607
-4470013	a	B	57	2019-04-12 08:38:00	Turdus merula	1	0	0	0	91,63020763051821
-4470015	a	B	57	2019-04-12 08:39:00	Emberiza citrinella	1	0	0	0	93,79395206199449
-4470016	a	B	57	2019-04-12 08:39:00	Emberiza cirlus	1	0	0	0	119,166865501643
-4470017	a	B	57	2019-04-12 08:41:00	Cyanistes caeruleus	1	0	0	0	54,227985410203196
-4470022	a	B	57	2019-04-12 08:41:00	Troglodytes troglodytes	1	0	0	0	168,395928151481
-4470018	a	B	57	2019-04-12 08:42:00	Turdus merula	1	0	0	0	78,6909686796878
-4470019	a	B	57	2019-04-12 08:42:00	Certhia brachydactyla	1	0	0	0	84,7755371405548
-4470021	a	B	57	2019-04-12 08:43:00	Erithacus rubecula	1	0	0	0	15,0060369230621
-4470020	a	B	57	2019-04-12 08:43:00	Phylloscopus collybita	1	0	0	0	78,31132639048441
-4470024	a	B	58	2019-04-12 09:00:00	Lullula arborea	1	0	1	0	16,872527499303402
-4470023	a	B	58	2019-04-12 09:00:00	Jynx torquilla	1	0	1	0	70,0137160393338
-4470025	a	B	58	2019-04-12 09:00:00	Sylvia atricapilla	1	0	1	0	85,9254405235922
-4470027	a	B	58	2019-04-12 09:01:00	Phylloscopus collybita	1	0	1	0	111,788079362601
-4470026	a	B	58	2019-04-12 09:01:00	Parus major	1	0	1	0	114,443757000382
-4470029	a	B	58	2019-04-12 09:02:00	Turdus merula	1	0	1	0	77,19875504710609
-4470028	a	B	58	2019-04-12 09:02:00	Carduelis cannabina	1	1	1	1	88,6037048278322
-4470030	a	B	58	2019-04-12 09:03:00	Prunella modularis	1	0	1	0	59,2612049073653
-4470031	a	B	58	2019-04-12 09:03:00	Sylvia atricapilla	1	0	1	0	176,38298254171298
-4470032	a	B	58	2019-04-12 09:03:00	Turdus viscivorus	1	0	1	0	222,771056574005
-4470033	a	B	58	2019-04-12 09:04:00	Sylvia atricapilla	1	0	0	0	241,30114019919102
-4470047	a	B	58	2019-04-12 09:05:00	Phylloscopus collybita	1	0	0	0	56,7318244101805
-4470048	a	B	58	2019-04-12 09:05:00	Prunella modularis	1	0	0	0	107,569087628766
-4470034	a	B	58	2019-04-12 09:05:00	Turdus merula	1	0	0	0	131,28518467455402
-4470050	a	B	58	2019-04-12 09:08:00	Saxicola rubicola	1	0	0	0	96,20490716469851
-4470049	a	B	58	2019-04-12 09:09:00	Jynx torquilla	1	0	0	0	138,05277763203802
-4470035	a	B	59	2019-04-12 09:28:00	Anthus trivialis	1	0	1	0	41,9246757308707
-4470036	a	B	59	2019-04-12 09:28:00	Phylloscopus collybita	1	0	1	0	63,46597767123961
-4470039	a	B	59	2019-04-12 09:29:00	Cyanistes caeruleus	1	0	1	0	79,137988909675
-4470037	a	B	59	2019-04-12 09:29:00	Emberiza cirlus	1	0	1	0	115,80745550140301
-4470040	a	B	59	2019-04-12 09:30:00	Sylvia atricapilla	1	0	1	0	97,3743761982674
-4470042	a	B	59	2019-04-12 09:30:00	Upupa epops	1	0	1	0	100,34733249485201
-4470043	a	B	59	2019-04-12 09:30:00	Sylvia atricapilla	1	0	1	0	108,619280569753
-4470041	a	B	59	2019-04-12 09:30:00	Turdus merula	1	0	1	0	146,66320872028098
-4470045	a	B	59	2019-04-12 09:31:00	Sylvia atricapilla	1	0	1	0	46,6504766672206
-4470044	a	B	59	2019-04-12 09:31:00	Sylvia atricapilla	1	0	1	0	183,912806375259
-4470046	a	B	59	2019-04-12 09:32:00	Turdus merula	1	0	1	0	46,6504766672206
-4470151	a	B	59	2019-04-12 09:32:00	Turdus merula	1	0	1	0	141,929473524032
-4470150	a	B	59	2019-04-12 09:32:00	Phylloscopus collybita	1	0	1	0	171,66127366585
-4470152	a	B	59	2019-04-12 09:33:00	Parus major	1	0	0	0	69,0974864528756
-4470153	a	B	59	2019-04-12 09:33:00	Anthus trivialis	1	0	0	0	113,770490818528
-4470154	a	B	59	2019-04-12 09:35:00	Turdus merula	1	0	0	0	244,896223929482
-4470157	a	B	60	2019-04-12 09:46:00	Phylloscopus collybita	1	0	1	0	65,8545696151003
-4470156	a	B	60	2019-04-12 09:46:00	Lullula arborea	1	0	1	0	164,847250758435
-4470158	a	B	60	2019-04-12 09:47:00	Phylloscopus collybita	1	0	1	0	94,94697563367609
-4470161	a	B	60	2019-04-12 09:48:00	Turdus merula	1	0	1	0	178,021987660643
-4470159	a	B	60	2019-04-12 09:48:00	Turdus merula	1	0	1	0	179,580782712359
-4470160	a	B	60	2019-04-12 09:48:00	Turdus viscivorus	1	0	1	0	186,88489977801498
-4470162	a	B	60	2019-04-12 09:49:00	Alauda arvensis	2	0	2	0	144,13084037678001
-4470163	a	B	60	2019-04-12 09:50:00	Carduelis cannabina	1	0	1	0	55,262572238203106
-4470165	a	B	60	2019-04-12 09:50:00	Prunella modularis	1	0	1	0	63,4113639549621
-4470164	a	B	60	2019-04-12 09:50:00	Emberiza cirlus	1	0	1	0	68,5188315310729
-4470086	a	B	60	2019-04-12 09:53:00	Corvus corone	0	1	0	0	446,218248380213
-4470087	a	B	60	2019-04-12 09:54:00	Lullula arborea	1	0	0	0	184,448796685159
-4470088	a	B	60	2019-04-12 09:55:00	Turdus merula	1	0	0	0	69,29320011546051
-4600361	b	B	23	2019-06-01 07:18:00	Lanius collurio	1	0	1	0	116,18418247667
-4600362	b	B	23	2019-06-01 07:18:00	Lanius collurio	1	0	1	0	150,274423947316
-4600366	b	B	23	2019-06-01 07:19:00	Emberiza cirlus	1	0	1	0	43,75045512002129
-4600363	b	B	23	2019-06-01 07:19:00	Lanius collurio	1	0	1	0	73,83201298076979
-4600365	b	B	23	2019-06-01 07:19:00	Emberiza citrinella	1	0	1	0	79,9210212183632
-4600364	b	B	23	2019-06-01 07:19:00	Alauda arvensis	0	1	0	1	136,97559366056402
-4600367	b	B	23	2019-06-01 07:20:00	Emberiza calandra	1	0	1	0	184,67551454645303
-4600368	b	B	23	2019-06-01 07:20:00	Turdus merula	1	0	1	0	229,53873838390498
-4600369	b	B	23	2019-06-01 07:21:00	Carduelis cannabina	1	0	1	0	22,256373973116304
-4600371	b	B	23	2019-06-01 07:21:00	Turdus merula	1	0	1	0	135,516867264187
-4600370	b	B	23	2019-06-01 07:21:00	Lullula arborea	1	0	1	0	144,418807691414
-4600262	b	B	23	2019-06-01 07:22:00	Lanius collurio	1	0	0	0	204,74557254632703
-4600261	b	B	23	2019-06-01 07:22:00	Alauda arvensis	0	1	0	0	248,009867532714
-4600263	b	B	23	2019-06-01 07:23:00	Luscinia megarhynchos	1	0	0	0	128,696128785875
-4600265	b	B	23	2019-06-01 07:25:00	Sylvia atricapilla	1	0	0	0	213,70962987379698
-4600264	b	B	23	2019-06-01 07:25:00	Alauda arvensis	0	1	0	0	295,291249876831
-4600266	b	B	23	2019-06-01 07:25:00	Sylvia atricapilla	0	1	0	0	321,320577163559
-4600268	b	B	23	2019-06-01 07:26:00	Alauda arvensis	0	1	0	0	87,1859796390958
-4600267	b	B	23	2019-06-01 07:26:00	Fringilla coelebs	1	0	0	0	235,99451017539
-4600395	b	B	39	2019-06-01 06:40:00	Alauda arvensis	0	1	0	1	29,576983279106198
-4600394	b	B	39	2019-06-01 06:40:00	Alauda arvensis	0	1	0	1	50,177434765809295
-4600396	b	B	39	2019-06-01 06:40:00	Lanius collurio	1	0	1	0	85,219652905564
-4600397	b	B	39	2019-06-01 06:41:00	Emberiza calandra	1	0	1	0	82,8356372993052
-4600398	b	B	39	2019-06-01 06:42:00	Emberiza citrinella	1	0	1	0	68,6807987696493
-4600399	b	B	39	2019-06-01 06:43:00	Emberiza citrinella	0	1	0	1	80,3743092740585
-4600400	b	B	39	2019-06-01 06:43:00	Picus viridis	0	1	0	1	244,528483880956
-4600401	b	B	39	2019-06-01 06:44:00	Parus major	1	0	0	0	86,12790589459551
-4600402	b	B	39	2019-06-01 06:45:00	Alauda arvensis	0	1	0	0	125,388197944701
-4600403	b	B	39	2019-06-01 06:46:00	Lanius collurio	1	0	0	0	59,26029382764529
-4600404	b	B	39	2019-06-01 06:46:00	Sylvia atricapilla	1	0	0	0	112,12038489673799
-4600345	b	B	39	2019-06-01 06:48:00	Turdus merula	1	0	0	0	57,984250900842
-4600346	b	B	39	2019-06-01 06:49:00	Sylvia atricapilla	0	1	0	0	147,653629625247
-4600347	b	B	40	2019-06-01 06:56:00	Alauda arvensis	0	1	0	1	134,29452484743402
-4600349	b	B	40	2019-06-01 06:56:00	Buteo buteo	0	1	0	1	208,085578009967
-4600348	b	B	40	2019-06-01 06:56:00	Alauda arvensis	0	1	0	1	221,610767592908
-4600352	b	B	40	2019-06-01 06:57:00	Emberiza cirlus	1	0	1	0	125,046949592083
-4600351	b	B	40	2019-06-01 06:57:00	Lanius collurio	1	0	1	0	179,834104746149
-4600350	b	B	40	2019-06-01 06:57:00	Luscinia megarhynchos	1	0	1	0	210,968471250013
-4600353	b	B	40	2019-06-01 06:58:00	Lanius collurio	1	0	1	0	85,4163435602311
-4600354	b	B	40	2019-06-01 06:58:00	Sylvia atricapilla	0	1	0	1	240,326753089452
-4600356	b	B	40	2019-06-01 06:59:00	Sylvia communis	1	0	1	0	54,9949002593266
-4600357	b	B	40	2019-06-01 06:59:00	Phasianus colchicus	1	0	1	0	188,00175816795
-4600355	b	B	40	2019-06-01 06:59:00	Sylvia atricapilla	0	1	0	1	201,354105083892
-4600358	b	B	40	2019-06-01 07:00:00	Alauda arvensis	0	1	0	0	150,78654274359502
-4600359	b	B	40	2019-06-01 07:01:00	Turdus merula	1	0	0	0	42,5203681379496
-4600360	b	B	40	2019-06-01 07:01:00	Parus major	1	0	0	0	171,959165220573
-4600388	b	B	40	2019-06-01 07:02:00	Lanius collurio	1	0	0	0	130,37220727598
-4600389	b	B	40	2019-06-01 07:03:00	Turdus merula	0	1	0	0	284,637606767073
-4600390	b	B	40	2019-06-01 07:04:00	Alauda arvensis	0	1	0	0	298,260348268765
-4600391	b	B	40	2019-06-01 07:05:00	Saxicola rubicola	0	1	0	0	103,76870776715599
-4600373	b	B	41	2019-05-31 08:48:00	Saxicola rubicola	1	0	1	0	16,0267277457206
-4600372	b	B	41	2019-05-31 08:48:00	Sylvia communis	1	1	1	1	21,096162268601198
-4600374	b	B	41	2019-05-31 08:49:00	Garrulus glandarius	0	1	0	1	28,6056394192067
-4600375	b	B	41	2019-05-31 08:49:00	Sylvia atricapilla	1	0	1	0	29,221708468482397
-4600376	b	B	41	2019-05-31 08:49:00	Parus major	0	1	0	1	43,79522816809521
-4600377	b	B	41	2019-05-31 08:50:00	Parus major	0	1	0	1	52,5650391975061
-4600378	b	B	41	2019-05-31 08:50:00	Fringilla coelebs	1	0	1	0	70,2559272608659
-4600379	b	B	41	2019-05-31 08:51:00	Sylvia atricapilla	0	1	0	1	144,549764258517
-4600380	b	B	41	2019-05-31 08:51:00	Turdus merula	1	0	1	0	144,549764258517
-4600382	b	B	41	2019-05-31 08:52:00	Phylloscopus collybita	1	0	1	0	28,2219056118726
-4600381	b	B	41	2019-05-31 08:52:00	Emberiza citrinella	1	0	1	0	90,7863051460318
-4600384	b	B	41	2019-05-31 08:53:00	Turdus merula	1	0	0	0	59,3193807214437
-4600383	b	B	41	2019-05-31 08:53:00	Sylvia atricapilla	0	1	0	0	235,15175356233897
-4600385	b	B	41	2019-05-31 08:54:00	Phylloscopus collybita	0	1	0	0	53,701306451374705
-4600386	b	B	41	2019-05-31 08:54:00	Streptopelia turtur	1	0	0	0	101,553193594416
-4600407	b	B	41	2019-05-31 08:55:00	Luscinia megarhynchos	0	1	0	0	57,2670918426589
-4600387	b	B	41	2019-05-31 08:55:00	Anthus trivialis	1	0	0	0	100,863387881563
-4600406	b	B	41	2019-05-31 08:55:00	Luscinia megarhynchos	1	0	0	0	228,649991955502
-4600408	b	B	41	2019-05-31 08:56:00	Sylvia atricapilla	0	1	0	0	192,041881643847
-4600164	b	B	56	2019-05-31 06:41:00	Emberiza calandra	1	0	1	0	66,9070683682569
-4600166	b	B	56	2019-05-31 06:42:00	Turdus merula	1	0	1	0	2,9285796227793903
-4600167	b	B	56	2019-05-31 06:42:00	Sylvia communis	1	0	1	0	16,1821797196369
-4600165	b	B	56	2019-05-31 06:42:00	Saxicola rubicola	1	0	1	0	24,314755045111298
-4600168	b	B	56	2019-05-31 06:43:00	Luscinia megarhynchos	1	0	1	0	37,3684763373287
-4600169	b	B	56	2019-05-31 06:43:00	Phylloscopus collybita	0	1	0	1	51,2669980510037
-4600170	b	B	56	2019-05-31 06:43:00	Sylvia atricapilla	0	1	0	1	70,41288500327941
-4600172	b	B	56	2019-05-31 06:44:00	Parus major	1	1	1	1	24,1171877143126
-4600171	b	B	56	2019-05-31 06:44:00	Sylvia atricapilla	0	1	0	1	77,005862761178
-4600174	b	B	56	2019-05-31 06:45:00	Emberiza citrinella	1	0	1	0	62,642984674904696
-4600173	b	B	56	2019-05-31 06:45:00	Emberiza cirlus	1	0	1	0	76,5335247612333
-4600175	b	B	56	2019-05-31 06:45:00	Phasianus colchicus	1	0	1	0	81,9137635503295
-4600176	b	B	56	2019-05-31 06:46:00	Troglodytes troglodytes	1	0	0	0	60,1982439174652
-4600177	b	B	56	2019-05-31 06:47:00	Turdus merula	0	1	0	0	74,597591041633
-4600178	b	B	56	2019-05-31 06:48:00	Buteo buteo	0	1	0	0	85,0767469556345
-4600181	b	B	56	2019-05-31 06:49:00	Prunella modularis	1	0	0	0	60,709476563208
-4600182	b	B	56	2019-05-31 06:50:00	Lanius collurio	1	0	0	0	25,1758876626448
-4600183	b	B	56	2019-05-31 06:51:00	Alauda arvensis	1	1	0	0	42,8915370195404
-4600191	b	B	57	2019-05-31 07:00:00	Sylvia communis	1	0	1	0	46,9425876475909
-4600193	b	B	57	2019-05-31 07:00:00	Turdus merula	0	1	0	1	70,02250631715279
-4600192	b	B	57	2019-05-31 07:00:00	Turdus merula	0	1	0	1	98,37566814038799
-4600196	b	B	57	2019-05-31 07:01:00	Luscinia megarhynchos	1	0	1	0	44,998967100341794
-4600195	b	B	57	2019-05-31 07:01:00	Phasianus colchicus	1	0	1	0	63,775024207828
-4600194	b	B	57	2019-05-31 07:01:00	Turdus merula	0	1	0	1	170,943656486213
-4600199	b	B	57	2019-05-31 07:02:00	Lullula arborea	1	0	1	0	85,1119802375789
-4600197	b	B	57	2019-05-31 07:02:00	Phylloscopus collybita	1	0	1	0	95,20245128685092
-4600198	b	B	57	2019-05-31 07:02:00	Sylvia atricapilla	1	0	1	0	95,20245128685092
-4600201	b	B	57	2019-05-31 07:03:00	Turdus merula	1	0	1	0	45,999375014356495
-4600200	b	B	57	2019-05-31 07:03:00	Phylloscopus collybita	0	1	0	1	66,3618069128881
-4600202	b	B	57	2019-05-31 07:04:00	Parus major	1	0	0	0	61,3039348155877
-4600234	b	B	57	2019-05-31 07:04:00	Lanius collurio	1	0	0	0	105,74874710328501
-4600228	b	B	57	2019-05-31 07:05:00	Picus viridis	0	1	0	0	102,256030733692
-4600229	b	B	57	2019-05-31 07:06:00	Emberiza citrinella	1	0	0	0	93,3855719664583
-4600230	b	B	57	2019-05-31 07:07:00	Dendrocopos minor	1	0	0	0	52,685055045062
-4600232	b	B	57	2019-05-31 07:08:00	Emberiza citrinella	1	0	0	0	76,6244664977523
-4600231	b	B	57	2019-05-31 07:08:00	Sylvia communis	1	0	0	0	90,010129148678
-4600233	b	B	57	2019-05-31 07:09:00	Serinus serinus	1	0	0	0	68,86324666339021
-4600237	b	B	58	2019-05-31 07:21:00	Emberiza cirlus	1	0	1	0	58,0978841805301
-4600238	b	B	58	2019-05-31 07:21:00	Luscinia megarhynchos	1	0	1	0	65,94187231674229
-4600236	b	B	58	2019-05-31 07:21:00	Emberiza citrinella	1	0	1	0	77,459321847895
-4600239	b	B	58	2019-05-31 07:22:00	Phylloscopus collybita	1	0	1	0	58,1361778556275
-4600240	b	B	58	2019-05-31 07:22:00	Turdus philomelos	1	0	1	0	93,13534360271271
-4600241	b	B	58	2019-05-31 07:22:00	Fringilla coelebs	0	1	0	1	178,17266384976898
-4600243	b	B	58	2019-05-31 07:23:00	Luscinia megarhynchos	0	1	0	1	108,142271749205
-4600242	b	B	58	2019-05-31 07:23:00	Fringilla coelebs	0	1	0	1	158,734922338122
-4600203	b	B	58	2019-05-31 07:24:00	Sylvia communis	1	0	1	0	24,4632555353894
-4600204	b	B	58	2019-05-31 07:24:00	Turdus merula	1	0	1	0	24,4632555353894
-4600244	b	B	58	2019-05-31 07:24:00	Sylvia atricapilla	1	0	1	0	79,1165313136128
-4600205	b	B	58	2019-05-31 07:25:00	Sylvia atricapilla	0	1	0	1	184,958212576336
-4600206	b	B	58	2019-05-31 07:26:00	Emberiza citrinella	1	0	0	0	139,68218742012698
-4600207	b	B	58	2019-05-31 07:26:00	Alauda arvensis	1	0	0	0	205,547170171753
-4600208	b	B	58	2019-05-31 07:29:00	Sylvia atricapilla	0	1	0	0	183,70154829254102
-4600210	b	B	58	2019-05-31 07:30:00	Lanius collurio	0	1	0	0	131,78760200065202
-4600209	b	B	58	2019-05-31 07:30:00	Lanius collurio	1	0	0	0	153,152302327091
-4600211	b	B	58	2019-05-31 07:31:00	Alauda arvensis	1	0	0	0	161,3833829564
-4600185	b	B	59	2019-05-31 07:41:00	Phylloscopus collybita	1	0	1	0	80,2596391937535
-4600188	b	B	59	2019-05-31 07:42:00	Emberiza cirlus	1	0	1	0	72,5303969934081
-4600187	b	B	59	2019-05-31 07:42:00	Sylvia atricapilla	0	1	0	1	91,70486583143959
-4600186	b	B	59	2019-05-31 07:42:00	Anthus trivialis	1	0	1	0	94,7737057965374
-4600189	b	B	59	2019-05-31 07:42:00	Sylvia atricapilla	0	1	0	1	121,985043144531
-4600190	b	B	59	2019-05-31 07:43:00	Phylloscopus bonelli	1	0	1	0	69,92635010564
-4600212	b	B	59	2019-05-31 07:44:00	Emberiza citrinella	1	0	1	0	209,855907460444
-4600213	b	B	59	2019-05-31 07:45:00	Sylvia atricapilla	0	1	0	1	62,426695391824104
-4600214	b	B	59	2019-05-31 07:46:00	Turdus merula	1	0	0	0	53,052821435206795
-4600215	b	B	59	2019-05-31 07:46:00	Turdus merula	1	0	0	0	65,85268299766321
-4600220	b	B	59	2019-05-31 07:46:00	Streptopelia turtur	1	0	0	0	77,6945495648783
-4600216	b	B	59	2019-05-31 07:47:00	Luscinia megarhynchos	1	0	0	0	214,29367876171003
-4600217	b	B	59	2019-05-31 07:48:00	Sylvia communis	1	0	0	0	34,355178590605995
-4600218	b	B	59	2019-05-31 07:49:00	Sylvia communis	1	0	0	0	54,4959952095224
-4600219	b	B	59	2019-05-31 07:52:00	Luscinia megarhynchos	0	1	0	0	19,8867802623959
-4600223	b	B	60	2019-05-31 08:06:00	Turdus merula	0	0	0	0	37,129345769779995
-4600222	b	B	60	2019-05-31 08:06:00	Lanius collurio	1	0	1	0	65,9525186102756
-4600221	b	B	60	2019-05-31 08:06:00	Anthus trivialis	1	0	1	0	170,703210457237
-4600224	b	B	60	2019-05-31 08:07:00	Turdus merula	1	0	1	0	166,26329449105802
-4600225	b	B	60	2019-05-31 08:08:00	Lanius collurio	0	1	0	1	147,28978846684402
-4600226	b	B	60	2019-05-31 08:08:00	Fringilla coelebs	0	1	0	1	223,517321893797
-4600227	b	B	60	2019-05-31 08:08:00	Fringilla coelebs	0	1	0	1	275,097674720966
-4600251	b	B	60	2019-05-31 08:09:00	Alauda arvensis	1	0	1	0	89,10780015907429
-4600250	b	B	60	2019-05-31 08:09:00	Phylloscopus collybita	1	0	1	0	191,24563263989398
-4600253	b	B	60	2019-05-31 08:10:00	Alauda arvensis	0	1	0	0	132,144858521759
-4600254	b	B	60	2019-05-31 08:10:00	Sylvia atricapilla	1	0	0	0	246,975567486761
-4600255	b	B	60	2019-05-31 08:11:00	Sylvia communis	1	0	0	0	117,70352673206699
-4600256	b	B	60	2019-05-31 08:12:00	Alauda arvensis	0	1	0	0	36,85914200548179
-4600257	b	B	60	2019-05-31 08:12:00	Sylvia atricapilla	0	1	0	0	137,84732809636
-4600259	b	B	60	2019-05-31 08:13:00	Lanius collurio	0	1	0	0	59,016758746689604
-4600258	b	B	60	2019-05-31 08:13:00	Luscinia megarhynchos	1	0	0	0	176,941396256958
-4600260	b	B	60	2019-05-31 08:16:00	Emberiza citrinella	1	0	0	0	143,104906861358
+Ref	Passage	Observateur	Point	DateHeure	Espèce	nMalAd10	nAutAd10	nMalAd5	nAutAd5	distMem
+4446066	a	A	194	2019-04-07 07:40:00	Periparus ater	1	0	1	0	24,9606330209045
+4446096	a	A	194	2019-04-07 07:41:00	Fringilla coelebs	1	0	1	0	80,7914842071552
+4446095	a	A	194	2019-04-07 07:41:00	Fringilla coelebs	1	0	1	0	102,55447488593398
+4446097	a	A	194	2019-04-07 07:42:00	Cyanistes caeruleus	1	0	1	0	43,725797738755595
+4446098	a	A	194	2019-04-07 07:42:00	Poecile palustris	1	0	1	0	49,2618554158065
+4446062	a	A	194	2019-04-07 07:42:00	Emberiza cirlus	1	0	1	0	69,1728982854307
+4446061	a	A	194	2019-04-07 07:42:00	Phylloscopus collybita	1	0	1	0	102,257397840765
+4446099	a	A	194	2019-04-07 07:43:00	Alauda arvensis	2	0	2	0	73,1088920519477
+4446101	a	A	194	2019-04-07 07:44:00	Garrulus glandarius	0	1	0	1	30,3439103651818
+4446100	a	A	194	2019-04-07 07:44:00	Garrulus glandarius	0	1	0	1	65,96671079042599
+4446102	a	A	194	2019-04-07 07:44:00	Columba palumbus	1	0	1	0	129,169235786498
+4446104	a	A	194	2019-04-07 07:45:00	Certhia brachydactyla	1	0	0	0	81,45592084658149
+4446103	a	A	194	2019-04-07 07:45:00	Columba palumbus	1	0	0	0	86,4808530685552
+4446063	a	A	194	2019-04-07 07:46:00	Troglodytes troglodytes	1	0	0	0	36,0135679204489
+4446064	a	A	194	2019-04-07 07:46:00	Lullula arborea	1	0	0	0	50,6558130162114
+4446105	a	A	194	2019-04-07 07:46:00	Phoenicurus ochruros	1	0	0	0	104,114803024612
+4446106	a	A	194	2019-04-07 07:46:00	Streptopelia decaocto	1	1	0	0	134,50384189803
+4446065	a	A	194	2019-04-07 07:46:00	Parus major	1	0	0	0	187,78234298681
+4446107	a	A	194	2019-04-07 07:47:00	Emberiza citrinella	1	0	0	0	31,1656734957903
+4446110	a	A	194	2019-04-07 07:48:00	Erithacus rubecula	1	0	0	0	31,9605534312686
+4446108	a	A	194	2019-04-07 07:48:00	Sylvia atricapilla	1	0	0	0	53,693028418789204
+4446109	a	A	194	2019-04-07 07:48:00	Cuculus canorus	1	0	0	0	263,792364527315
+4446055	a	A	194	2019-04-07 07:49:00	Picus viridis	1	0	0	0	61,2093625094651
+4446056	a	A	194	2019-04-07 07:49:00	Columba palumbus	1	0	0	0	72,1510898599061
+4446057	a	A	194	2019-04-07 07:50:00	Dendrocopos major	0	1	0	0	83,2634044230843
+4446058	a	A	194	2019-04-07 07:50:00	Corvus corone	0	2	0	0	151,426282530952
+4446060	a	A	194	2019-04-07 07:51:00	Columba palumbus	1	0	0	0	98,0057739520934
+4446059	a	A	194	2019-04-07 07:51:00	Turdus philomelos	1	0	0	0	245,92490706446802
+4446077	a	A	195	2019-04-07 08:23:00	Motacilla alba	1	1	1	1	128,44767445640102
+4446075	a	A	195	2019-04-07 08:23:00	Passer domesticus	0	5	0	5	132,270121126544
+4446076	a	A	195	2019-04-07 08:23:00	Phoenicurus ochruros	1	0	1	0	140,767813325697
+4446078	a	A	195	2019-04-07 08:24:00	Alauda arvensis	1	0	1	0	87,2835106936389
+4446040	a	A	195	2019-04-07 08:24:00	Carduelis carduelis	1	0	1	0	109,307804058053
+4446039	a	A	195	2019-04-07 08:24:00	Lullula arborea	1	0	1	0	131,068527722403
+4446041	a	A	195	2019-04-07 08:25:00	Streptopelia decaocto	1	0	1	0	347,15954885764
+4446042	a	A	195	2019-04-07 08:28:00	Turdus philomelos	1	0	0	0	107,662600137994
+4446043	a	A	195	2019-04-07 08:28:00	Lullula arborea	1	0	0	0	113,157304067095
+4446044	a	A	195	2019-04-07 08:30:00	Pica pica	0	1	0	0	87,75268388246641
+4446046	a	A	195	2019-04-07 08:31:00	Saxicola rubicola	2	1	0	0	79,9003641584516
+4446045	a	A	195	2019-04-07 08:31:00	Erithacus rubecula	1	0	0	0	95,02107241514679
+4446085	a	A	196	2019-04-07 08:52:00	Turdus merula	1	0	1	0	11,6000185699332
+4446084	a	A	196	2019-04-07 08:52:00	Columba palumbus	0	1	0	1	33,5261237560319
+4446087	a	A	196	2019-04-07 08:53:00	Emberiza calandra	1	0	1	0	39,30553807688229
+4446086	a	A	196	2019-04-07 08:53:00	Emberiza citrinella	1	0	1	0	42,025672877656795
+4446088	a	A	196	2019-04-07 08:54:00	Parus major	1	1	1	1	33,7275326232273
+4446089	a	A	196	2019-04-07 08:55:00	Erithacus rubecula	1	0	1	0	55,4298561065516
+4446090	a	A	196	2019-04-07 08:55:00	Erithacus rubecula	1	0	1	0	78,3969567684819
+4446054	a	A	196	2019-04-07 08:56:00	Saxicola rubicola	1	0	0	0	64,3749528877979
+4446053	a	A	196	2019-04-07 08:56:00	Alauda arvensis	1	0	0	0	68,3669717300092
+4446047	a	A	196	2019-04-07 08:58:00	Sylvia atricapilla	1	0	0	0	25,949260661692602
+4446091	a	A	196	2019-04-07 08:58:00	Aegithalos caudatus	0	1	0	0	35,35691737051629
+4446048	a	A	196	2019-04-07 08:59:00	Emberiza cirlus	1	0	0	0	52,3609953632046
+4446049	a	A	196	2019-04-07 09:00:00	Cuculus canorus	1	0	0	0	344,638173682424
+4446050	a	A	196	2019-04-07 09:01:00	Lullula arborea	1	0	0	0	55,5288829275318
+4446052	a	A	196	2019-04-07 09:01:00	Phylloscopus collybita	1	0	0	0	72,2398428860285
+4446051	a	A	196	2019-04-07 09:01:00	Lullula arborea	1	0	0	0	107,908990125091
+4446174	a	A	197	2019-04-07 09:21:00	Lullula arborea	0	2	0	2	60,58664890040129
+4446175	a	A	197	2019-04-07 09:21:00	Phylloscopus collybita	1	0	1	0	65,695133697924
+4446176	a	A	197	2019-04-07 09:22:00	Emberiza calandra	1	0	1	0	70,1324894641736
+4446177	a	A	197	2019-04-07 09:22:00	Alauda arvensis	1	0	1	0	79,2302802984055
+4446178	a	A	197	2019-04-07 09:23:00	Alauda arvensis	3	0	3	0	46,277858838021295
+4446179	a	A	197	2019-04-07 09:23:00	Phylloscopus collybita	1	0	1	0	105,860918360315
+4446180	a	A	197	2019-04-07 09:23:00	Turdus philomelos	1	0	1	0	107,002225518955
+4446181	a	A	197	2019-04-07 09:24:00	Fringilla coelebs	0	2	0	2	54,266579847239896
+4446182	a	A	197	2019-04-07 09:24:00	Sylvia atricapilla	1	0	1	0	77,66497742168029
+4446183	a	A	197	2019-04-07 09:25:00	Sylvia atricapilla	1	0	0	0	67,4278673885609
+4446184	a	A	197	2019-04-07 09:25:00	Pica pica	0	1	0	0	103,648311893362
+4446185	a	A	197	2019-04-07 09:26:00	Corvus corone	0	2	0	0	115,580021773725
+4446186	a	A	197	2019-04-07 09:27:00	Corvus corone	0	1	0	0	137,210075285205
+4446187	a	A	197	2019-04-07 09:28:00	Cuculus canorus	1	0	0	0	149,219595792382
+4446189	a	A	197	2019-04-07 09:29:00	Saxicola rubicola	1	0	0	0	68,37424928057979
+4446188	a	A	197	2019-04-07 09:29:00	Erithacus rubecula	1	0	0	0	133,388676125305
+4446173	a	A	197	2019-04-07 09:30:00	Turdus philomelos	1	0	0	0	160,269126355142
+4446067	a	A	198	2019-04-07 09:44:00	Alauda arvensis	1	0	1	0	15,116436986297
+4446068	a	A	198	2019-04-07 09:46:00	Saxicola rubicola	1	0	1	0	47,338896465211896
+4446069	a	A	198	2019-04-07 09:47:00	Saxicola rubicola	1	0	1	0	39,2168117525201
+4446071	a	A	198	2019-04-07 09:48:00	Alauda arvensis	1	0	0	0	33,878946190236896
+4446070	a	A	198	2019-04-07 09:48:00	Turdus merula	1	0	0	0	61,4633434395941
+4446072	a	A	198	2019-04-07 09:49:00	Emberiza calandra	1	0	0	0	62,73881747798921
+4446073	a	A	198	2019-04-07 09:50:00	Sturnus vulgaris	0	1	0	0	79,246455279104
+4446074	a	A	198	2019-04-07 09:51:00	Emberiza cirlus	1	0	0	0	67,6226564133751
+4446079	a	A	198	2019-04-07 09:51:00	Carduelis chloris	1	0	0	0	102,67582342048
+4446080	a	A	198	2019-04-07 09:52:00	Carduelis chloris	1	0	0	0	90,34141984390921
+4446081	a	A	198	2019-04-07 09:52:00	Emberiza calandra	1	0	0	0	119,489863843806
+4446082	a	A	198	2019-04-07 09:52:00	Emberiza cirlus	1	0	0	0	132,91720742651302
+4446083	a	A	198	2019-04-07 09:53:00	Carduelis chloris	1	0	0	0	150,085865126107
+4514486	a	A	199	2019-05-01 06:52:00	Alauda arvensis	1	0	1	0	74,3157753135658
+4514484	a	A	199	2019-05-01 06:52:00	Luscinia megarhynchos	1	0	1	0	169,487564638674
+4514485	a	A	199	2019-05-01 06:52:00	Cuculus canorus	1	0	1	0	769,496826687577
+4514487	a	A	199	2019-05-01 06:53:00	Sylvia atricapilla	1	0	1	0	152,836412669501
+4514489	a	A	199	2019-05-01 06:54:00	Sylvia communis	1	0	1	0	99,6234086834815
+4514488	a	A	199	2019-05-01 06:54:00	Fringilla coelebs	1	0	1	0	124,163441470223
+4514490	a	A	199	2019-05-01 06:55:00	Sylvia communis	1	0	1	0	22,9870261266066
+4514491	a	A	199	2019-05-01 06:55:00	Alauda arvensis	1	0	1	0	75,2870376758794
+4514492	a	A	199	2019-05-01 06:56:00	Buteo buteo	0	1	0	0	152,257480194774
+4514493	a	A	199	2019-05-01 06:57:00	Turdus merula	1	0	0	0	91,00652140997542
+4514495	a	A	199	2019-05-01 06:57:00	Phylloscopus collybita	1	0	0	0	123,24618168061001
+4514494	a	A	199	2019-05-01 06:57:00	Alauda arvensis	1	0	0	0	159,409604787853
+4514498	a	A	199	2019-05-01 06:59:00	Carduelis cannabina	1	1	0	0	51,451630711758206
+4514496	a	A	199	2019-05-01 06:59:00	Cuculus canorus	1	0	0	0	757,374219138708
+4514497	a	A	199	2019-05-01 07:00:00	Luscinia megarhynchos	1	0	0	0	53,7637684635736
+4514514	a	A	200	2019-05-01 07:28:00	Erithacus rubecula	1	0	1	0	120,117173099438
+4514515	a	A	200	2019-05-01 07:28:00	Cyanistes caeruleus	1	0	1	0	137,314271970749
+4514516	a	A	200	2019-05-01 07:28:00	Parus major	1	0	1	0	161,24068407153
+4514518	a	A	200	2019-05-01 07:29:00	Sylvia atricapilla	1	0	1	0	80,4150968603295
+4514519	a	A	200	2019-05-01 07:29:00	Alauda arvensis	1	0	1	0	112,967091973585
+4514517	a	A	200	2019-05-01 07:29:00	Parus major	1	0	1	0	121,63165456781401
+4514521	a	A	200	2019-05-01 07:30:00	Alauda arvensis	1	0	1	0	148,52629193069998
+4514520	a	A	200	2019-05-01 07:30:00	Oriolus oriolus	1	0	1	0	195,44440102166598
+4514522	a	A	200	2019-05-01 07:30:00	Lullula arborea	1	0	1	0	210,887904297665
+4514525	a	A	200	2019-05-01 07:31:00	Certhia brachydactyla	1	0	0	0	92,82932552381139
+4514523	a	A	200	2019-05-01 07:31:00	Sturnus vulgaris	0	3	0	0	126,046446255278
+4514526	a	A	200	2019-05-01 07:31:00	Garrulus glandarius	0	1	0	0	172,86427479661498
+4514527	a	A	200	2019-05-01 07:32:00	Turdus merula	1	0	0	0	101,468495992675
+4514528	a	A	200	2019-05-01 07:33:00	Erithacus rubecula	1	0	0	0	159,097695018055
+4514529	a	A	200	2019-05-01 07:34:00	Turdus philomelos	1	0	0	0	77,7252935307379
+4514530	a	A	200	2019-05-01 07:34:00	Phylloscopus collybita	1	0	0	0	91,0279289340391
+4514543	a	A	200	2019-05-01 07:35:00	Alauda arvensis	1	0	0	0	19,979875104238
+4514542	a	A	200	2019-05-01 07:35:00	Fringilla coelebs	1	0	0	0	75,5976993842147
+4514544	a	A	200	2019-05-01 07:36:00	Turdus merula	1	0	0	0	85,9675501862196
+4514546	a	A	200	2019-05-01 07:38:00	Emberiza cirlus	1	1	0	0	75,4000290723015
+4514545	a	A	200	2019-05-01 07:38:00	Cyanistes caeruleus	1	0	0	0	79,8221962158932
+4514385	a	A	201	2019-05-01 07:52:00	Alauda arvensis	1	0	1	0	148,539384031746
+4514386	a	A	201	2019-05-01 07:52:00	Emberiza cirlus	1	1	1	1	185,79005490703602
+4514387	a	A	201	2019-05-01 07:53:00	Alauda arvensis	1	0	1	0	150,666152180301
+4514389	a	A	201	2019-05-01 07:54:00	Sylvia atricapilla	1	0	1	0	200,164523947043
+4514388	a	A	201	2019-05-01 07:54:00	Anthus trivialis	1	0	1	0	200,39370807446602
+4514391	a	A	201	2019-05-01 07:55:00	Phylloscopus sibilatrix	0	1	0	1	196,353594952595
+4514390	a	A	201	2019-05-01 07:55:00	Turdus philomelos	1	0	1	0	215,61012629442
+4514392	a	A	201	2019-05-01 07:58:00	Phylloscopus collybita	1	0	0	0	204,37271843415797
+4514393	a	A	201	2019-05-01 07:58:00	Cuculus canorus	1	0	0	0	731,262694174689
+4514395	a	A	201	2019-05-01 08:01:00	Coturnix coturnix	1	0	0	0	174,506692626328
+4514396	a	A	201	2019-05-01 08:01:00	Luscinia megarhynchos	1	0	0	0	203,563053586152
+4514394	a	A	201	2019-05-01 08:01:00	Columba palumbus	1	0	0	0	268,303495025816
+4514377	a	A	202	2019-05-01 08:42:00	Alauda arvensis	1	0	1	0	33,393940881049
+4514378	a	A	202	2019-05-01 08:43:00	Luscinia megarhynchos	1	0	1	0	91,7845195130493
+4514380	a	A	202	2019-05-01 08:43:00	Pica pica	0	2	0	2	92,3376256725424
+4514379	a	A	202	2019-05-01 08:43:00	Alauda arvensis	1	0	1	0	119,29738308631501
+4514413	a	A	202	2019-05-01 08:43:00	Streptopelia decaocto	2	0	2	0	368,307680170071
+4514414	a	A	202	2019-05-01 08:44:00	Columba palumbus	1	0	1	0	465,434437815679
+4514416	a	A	202	2019-05-01 08:45:00	Luscinia megarhynchos	1	0	1	0	296,000903938687
+4514415	a	A	202	2019-05-01 08:45:00	Turdus merula	1	0	1	0	336,74456220876
+4514417	a	A	202	2019-05-01 08:46:00	Luscinia megarhynchos	1	0	1	0	419,805407182374
+4514419	a	A	202	2019-05-01 08:47:00	Dendrocopos major	1	0	0	0	351,71766385440804
+4514418	a	A	202	2019-05-01 08:47:00	Turdus merula	1	0	0	0	401,93447087362
+4514421	a	A	202	2019-05-01 08:48:00	Hirundo rustica	0	5	0	0	43,3141181704658
+4514420	a	A	202	2019-05-01 08:48:00	Alauda arvensis	1	0	0	0	155,894839090712
+4514422	a	A	202	2019-05-01 08:50:00	Turdus merula	1	0	0	0	282,296034802461
+4514424	a	A	202	2019-05-01 08:51:00	Sylvia atricapilla	1	0	0	0	113,951974938833
+4514423	a	A	202	2019-05-01 08:51:00	Columba palumbus	1	0	0	0	335,241283422957
+4514381	a	A	218	2019-05-01 09:32:00	Sylvia communis	1	0	1	0	56,1725478382866
+4514382	a	A	218	2019-05-01 09:32:00	Luscinia megarhynchos	1	0	1	0	93,2659659718385
+4514397	a	A	218	2019-05-01 09:33:00	Columba palumbus	0	1	0	1	41,7314277896404
+4514383	a	A	218	2019-05-01 09:33:00	Cuculus canorus	1	0	1	0	108,805345226555
+4514384	a	A	218	2019-05-01 09:33:00	Cuculus canorus	1	0	1	0	320,330065357926
+4514399	a	A	218	2019-05-01 09:34:00	Columba palumbus	1	0	1	0	34,801294492999396
+4514398	a	A	218	2019-05-01 09:34:00	Columba palumbus	1	0	1	0	134,901396650027
+4514400	a	A	218	2019-05-01 09:35:00	Sylvia communis	1	0	1	0	36,5272361495435
+4514401	a	A	218	2019-05-01 09:35:00	Jynx torquilla	1	0	1	0	70,1810106709728
+4514403	a	A	218	2019-05-01 09:36:00	Sylvia communis	1	0	0	0	22,609055312411197
+4514402	a	A	218	2019-05-01 09:36:00	Oriolus oriolus	1	0	0	0	53,878476842424895
+4514406	a	A	218	2019-05-01 09:37:00	Cyanistes caeruleus	1	0	0	0	36,4486821753154
+4514405	a	A	218	2019-05-01 09:37:00	Turdus merula	1	0	0	0	46,91301547225039
+4514404	a	A	218	2019-05-01 09:37:00	Fringilla coelebs	1	0	0	0	74,6666192051567
+4514407	a	A	218	2019-05-01 09:38:00	Turdus philomelos	1	0	0	0	93,4671905151107
+4514408	a	A	218	2019-05-01 09:39:00	Sylvia atricapilla	1	0	0	0	95,1608221688229
+4514410	a	A	218	2019-05-01 09:40:00	Sylvia communis	1	0	0	0	74,3365662319095
+4514409	a	A	218	2019-05-01 09:40:00	Erithacus rubecula	1	0	0	0	95,4659997101174
+4514425	a	A	218	2019-05-01 09:41:00	Turdus merula	1	0	0	0	51,88480084910461
+4514411	a	A	218	2019-05-01 09:41:00	Sylvia atricapilla	1	0	0	0	98,1861206524536
+4514412	a	A	218	2019-05-01 09:41:00	Sylvia atricapilla	1	0	0	0	124,444321588718
+4514532	a	A	219	2019-05-01 09:06:00	Alauda arvensis	0	3	0	3	48,358392882597705
+4514531	a	A	219	2019-05-01 09:06:00	Emberiza calandra	1	0	1	0	98,22226641701509
+4514533	a	A	219	2019-05-01 09:07:00	Sylvia atricapilla	1	0	1	0	37,6226780736719
+4514534	a	A	219	2019-05-01 09:07:00	Anthus trivialis	1	0	1	0	84,76202876188641
+4514535	a	A	219	2019-05-01 09:08:00	Emberiza calandra	1	0	1	0	26,941478084676
+4514537	a	A	219	2019-05-01 09:08:00	Phylloscopus collybita	1	0	1	0	72,570088910878
+4514538	a	A	219	2019-05-01 09:08:00	Luscinia megarhynchos	1	0	1	0	77,15951913163009
+4514536	a	A	219	2019-05-01 09:08:00	Cuculus canorus	1	0	1	0	306,029656205173
+4514512	a	A	219	2019-05-01 09:09:00	Alauda arvensis	1	0	1	0	58,7040800747826
+4514539	a	A	219	2019-05-01 09:09:00	Turdus merula	1	0	1	0	91,4777620829747
+4514499	a	A	219	2019-05-01 09:10:00	Columba palumbus	1	0	1	0	69,2646881845744
+4514541	a	A	219	2019-05-01 09:10:00	Luscinia megarhynchos	1	0	1	0	136,006694284895
+4514540	a	A	219	2019-05-01 09:10:00	Picus viridis	0	1	0	1	192,85659255621997
+4514510	a	A	219	2019-05-01 09:11:00	Corvus corone	0	1	0	0	82,1697913312275
+4514511	a	A	219	2019-05-01 09:11:00	Milvus migrans	0	1	0	0	87,0635069747453
+4514513	a	A	219	2019-05-01 09:11:00	Sturnus vulgaris	0	2	0	0	172,67787234981097
+4514502	a	A	219	2019-05-01 09:12:00	Lullula arborea	1	0	0	0	51,4456504599331
+4514500	a	A	219	2019-05-01 09:12:00	Fringilla coelebs	1	0	0	0	57,8464320524095
+4514501	a	A	219	2019-05-01 09:12:00	Parus major	1	0	0	0	66,1792654942266
+4514503	a	A	219	2019-05-01 09:13:00	Parus major	1	0	0	0	111,229684113638
+4514504	a	A	219	2019-05-01 09:14:00	Sylvia atricapilla	1	0	0	0	59,461593886792706
+4514505	a	A	219	2019-05-01 09:15:00	Picus viridis	0	1	0	0	132,13920515473998
+4514507	a	A	219	2019-05-01 09:16:00	Phylloscopus collybita	1	0	0	0	69,42459950189149
+4514506	a	A	219	2019-05-01 09:16:00	Sylvia atricapilla	1	0	0	0	127,49530481443999
+4514508	a	A	219	2019-05-01 09:16:00	Turdus merula	0	3	0	0	162,216021107344
+4514509	a	A	219	2019-05-01 09:16:00	Buteo buteo	0	1	0	0	243,433216686733
+4599015	b	A	194	2019-06-01 06:38:00	Pica pica	0	1	0	1	195,03554977508702
+4599014	b	A	194	2019-06-01 06:38:00	Cuculus canorus	1	0	1	0	262,048646937523
+4599017	b	A	194	2019-06-01 06:39:00	Columba palumbus	1	1	1	1	64,45449705061971
+4599018	b	A	194	2019-06-01 06:39:00	Columba palumbus	1	0	1	0	185,947096324075
+4599016	b	A	194	2019-06-01 06:39:00	Cuculus canorus	1	1	1	1	231,67074715989102
+4599022	b	A	194	2019-06-01 06:40:00	Erithacus rubecula	1	0	1	0	41,3889272611209
+4599020	b	A	194	2019-06-01 06:40:00	Phylloscopus collybita	1	0	1	0	65,57852549835229
+4599021	b	A	194	2019-06-01 06:40:00	Oriolus oriolus	1	0	1	0	87,8604356621269
+4599019	b	A	194	2019-06-01 06:40:00	Columba palumbus	1	0	1	0	271,588122782757
+4599023	b	A	194	2019-06-01 06:41:00	Corvus corone	1	0	1	0	391,739550806667
+4599024	b	A	194	2019-06-01 06:42:00	Troglodytes troglodytes	1	0	0	0	41,588165188660604
+4599026	b	A	194	2019-06-01 06:42:00	Fringilla coelebs	1	0	0	0	50,0026030154457
+4599025	b	A	194	2019-06-01 06:42:00	Streptopelia decaocto	1	0	0	0	88,50232368525579
+4599034	b	A	194	2019-06-01 06:42:00	Coturnix coturnix	1	0	0	0	99,52442596112779
+4599027	b	A	194	2019-06-01 06:44:00	Alauda arvensis	1	0	0	0	112,03750433378599
+4599028	b	A	194	2019-06-01 06:44:00	Alauda arvensis	1	0	0	0	112,03750433378599
+4599029	b	A	194	2019-06-01 06:45:00	Turdus merula	1	0	0	0	115,49613257012501
+4599030	b	A	194	2019-06-01 06:46:00	Passer domesticus	0	2	0	0	140,744809906205
+4599033	b	A	194	2019-06-01 06:47:00	Sylvia communis	1	0	0	0	21,155884224475102
+4599032	b	A	194	2019-06-01 06:47:00	Sylvia atricapilla	1	0	0	0	184,669138393749
+4599031	b	A	194	2019-06-01 06:47:00	Alauda arvensis	1	0	0	0	187,86006032244302
+4599035	b	A	195	2019-06-01 06:59:00	Sylvia communis	1	0	1	0	73,184089980867
+4599037	b	A	195	2019-06-01 07:00:00	Sylvia atricapilla	1	0	1	0	90,5071157462212
+4599038	b	A	195	2019-06-01 07:00:00	Alauda arvensis	1	0	1	0	91,8258747436827
+4599036	b	A	195	2019-06-01 07:00:00	Passer domesticus	6	0	6	0	142,707019200623
+4599040	b	A	195	2019-06-01 07:01:00	Pica pica	1	0	1	0	134,68617471680201
+4599039	b	A	195	2019-06-01 07:01:00	Corvus corone	0	1	0	1	204,507604952764
+4599041	b	A	195	2019-06-01 07:02:00	Corvus corone	0	1	0	1	346,57114860449303
+4599042	b	A	195	2019-06-01 07:03:00	Turdus merula	1	0	1	0	129,485041552095
+4599043	b	A	195	2019-06-01 07:03:00	Passer domesticus	0	12	0	12	144,513263885894
+4599044	b	A	195	2019-06-01 07:03:00	Alauda arvensis	1	0	1	0	172,662084752474
+4599054	b	A	195	2019-06-01 07:04:00	Sylvia communis	1	0	0	0	88,3720227329472
+4599045	b	A	195	2019-06-01 07:04:00	Fringilla coelebs	1	0	0	0	105,980737570432
+4599046	b	A	195	2019-06-01 07:04:00	Luscinia megarhynchos	1	0	0	0	226,477781197181
+4599055	b	A	195	2019-06-01 07:04:00	Streptopelia turtur	1	0	0	0	652,547581256
+4599056	b	A	195	2019-06-01 07:04:00	Cuculus canorus	1	0	0	0	945,749223038415
+4599047	b	A	195	2019-06-01 07:05:00	Passer domesticus	0	1	0	0	80,9891516724061
+4599048	b	A	195	2019-06-01 07:06:00	Parus major	1	0	0	0	100,512240553381
+4599049	b	A	195	2019-06-01 07:06:00	Luscinia megarhynchos	1	0	0	0	148,950271074042
+4599050	b	A	195	2019-06-01 07:07:00	Phylloscopus collybita	1	0	0	0	146,18910931463898
+4599051	b	A	195	2019-06-01 07:07:00	Cuculus canorus	1	0	0	0	860,5520878983681
+4599053	b	A	195	2019-06-01 07:08:00	Sturnus vulgaris	0	2	0	0	139,94864196021499
+4599052	b	A	195	2019-06-01 07:08:00	Turdus merula	1	0	0	0	179,609558218571
+4599277	b	A	196	2019-06-01 07:37:00	Lanius collurio	1	0	1	0	44,679305688868105
+4599279	b	A	196	2019-06-01 07:37:00	Sylvia communis	1	0	1	0	53,5680565082016
+4599278	b	A	196	2019-06-01 07:37:00	Emberiza calandra	1	0	1	0	116,581034592216
+4599281	b	A	196	2019-06-01 07:38:00	Lullula arborea	1	0	1	0	39,5262550821716
+4599280	b	A	196	2019-06-01 07:38:00	Sylvia communis	1	0	1	0	52,28250204876571
+4599282	b	A	196	2019-06-01 07:38:00	Cuculus canorus	1	0	1	0	153,447525368306
+4599284	b	A	196	2019-06-01 07:39:00	Luscinia megarhynchos	1	0	1	0	51,772981048319
+4599283	b	A	196	2019-06-01 07:39:00	Sylvia communis	1	0	1	0	61,001576447121
+4599285	b	A	196	2019-06-01 07:40:00	Cyanistes caeruleus	1	0	1	0	63,116786432809896
+4599286	b	A	196	2019-06-01 07:40:00	Lanius collurio	1	0	1	0	112,851376391869
+4599295	b	A	196	2019-06-01 07:41:00	Lanius collurio	1	0	0	0	70,3001592302993
+4599298	b	A	196	2019-06-01 07:41:00	Lanius collurio	1	1	0	0	79,4656740850447
+4599287	b	A	196	2019-06-01 07:41:00	Turdus merula	1	0	0	0	102,517120064404
+4599297	b	A	196	2019-06-01 07:41:00	Luscinia megarhynchos	1	0	0	0	118,34118297231599
+4599296	b	A	196	2019-06-01 07:41:00	Oriolus oriolus	1	0	0	0	262,608563238583
+4599288	b	A	196	2019-06-01 07:42:00	Sturnus vulgaris	0	1	0	0	59,1241193360105
+4599289	b	A	196	2019-06-01 07:42:00	Garrulus glandarius	0	1	0	0	134,93578890732002
+4599290	b	A	196	2019-06-01 07:43:00	Sylvia communis	1	0	0	0	68,0414750151381
+4599292	b	A	196	2019-06-01 07:44:00	Sylvia communis	1	0	0	0	79,54864868985959
+4599291	b	A	196	2019-06-01 07:44:00	Streptopelia turtur	1	0	0	0	230,704931241834
+4599293	b	A	196	2019-06-01 07:45:00	Cuculus canorus	1	0	0	0	541,6424309332119
+4599294	b	A	196	2019-06-01 07:46:00	Sylvia atricapilla	1	0	0	0	93,95496331103192
+4599300	b	A	197	2019-06-01 08:07:00	Sylvia atricapilla	1	0	1	0	90,746217255329
+4599301	b	A	197	2019-06-01 08:08:00	Phylloscopus collybita	1	0	1	0	67,58907365302589
+4599304	b	A	197	2019-06-01 08:08:00	Phylloscopus collybita	1	0	1	0	89,67757773853951
+4599302	b	A	197	2019-06-01 08:08:00	Sylvia communis	1	0	1	0	104,019738473281
+4599303	b	A	197	2019-06-01 08:08:00	Sylvia communis	1	0	1	0	143,07515172735398
+4599305	b	A	197	2019-06-01 08:09:00	Alauda arvensis	1	0	1	0	71,7660138101057
+4599306	b	A	197	2019-06-01 08:09:00	Sylvia communis	1	0	1	0	95,1300190957579
+4599307	b	A	197	2019-06-01 08:10:00	Columba palumbus	1	0	1	0	141,016764228186
+4599308	b	A	197	2019-06-01 08:10:00	Corvus corone	0	2	0	2	146,419985357581
+4599310	b	A	197	2019-06-01 08:11:00	Garrulus glandarius	0	1	0	1	123,22732111357901
+4599309	b	A	197	2019-06-01 08:11:00	Streptopelia turtur	1	0	1	0	132,791434168627
+4599311	b	A	197	2019-06-01 08:12:00	Fringilla coelebs	1	0	0	0	92,88501524082399
+4599312	b	A	197	2019-06-01 08:12:00	Lullula arborea	1	0	0	0	121,32975402027701
+4599315	b	A	197	2019-06-01 08:14:00	Parus major	1	0	0	0	54,579763679471206
+4599316	b	A	197	2019-06-01 08:15:00	Luscinia megarhynchos	1	0	0	0	74,9956046038237
+4599317	b	A	197	2019-06-01 08:15:00	Prunella modularis	1	0	0	0	98,68584476935129
+4599319	b	A	197	2019-06-01 08:16:00	Sylvia communis	1	0	0	0	77,5114424424689
+4599318	b	A	197	2019-06-01 08:16:00	Alauda arvensis	1	0	0	0	110,41218885765
+4599320	b	A	197	2019-06-01 08:17:00	Lanius collurio	1	0	0	0	105,961253871616
+4599321	b	A	197	2019-06-01 08:17:00	Sylvia communis	1	0	0	0	108,726478476536
+4599323	b	A	197	2019-06-01 08:18:00	Lanius collurio	1	0	0	0	57,8956910668741
+4599322	b	A	197	2019-06-01 08:18:00	Carduelis chloris	1	0	0	0	119,256173541882
+4599328	b	A	198	2019-06-01 08:27:00	Saxicola rubicola	1	0	1	0	30,6314209001073
+4599329	b	A	198	2019-06-01 08:28:00	Emberiza calandra	1	0	1	0	118,65850200101002
+4599331	b	A	198	2019-06-01 08:30:00	Lullula arborea	1	0	1	0	69,1895341258998
+4599330	b	A	198	2019-06-01 08:30:00	Saxicola rubicola	1	0	1	0	112,18652977784402
+4599333	b	A	198	2019-06-01 08:31:00	Sylvia communis	1	0	1	0	178,22407765541
+4599334	b	A	198	2019-06-01 08:32:00	Sylvia atricapilla	1	0	0	0	138,803462350616
+4599335	b	A	198	2019-06-01 08:33:00	Cuculus canorus	1	0	0	0	486,23291826253
+4599337	b	A	198	2019-06-01 08:34:00	Sylvia communis	1	0	0	0	53,66665811101849
+4599336	b	A	198	2019-06-01 08:34:00	Carduelis cannabina	1	0	0	0	55,989643098330795
+4599338	b	A	198	2019-06-01 08:35:00	Sylvia atricapilla	1	0	0	0	82,7403148152994
+4599340	b	A	198	2019-06-01 08:36:00	Emberiza cirlus	1	0	0	0	107,179210632745
+4599339	b	A	198	2019-06-01 08:36:00	Turdus merula	1	0	0	0	108,323656467374
+4599341	b	A	198	2019-06-01 08:37:00	Alauda arvensis	1	0	0	0	55,1090853810248
+4599342	b	A	198	2019-06-01 08:38:00	Turdus merula	1	1	0	0	76,248264190042
+4582077	b	A	199	2019-05-26 06:33:00	Alauda arvensis	1	0	1	0	50,3322170654364
+4582076	b	A	199	2019-05-26 06:33:00	Alauda arvensis	1	0	1	0	64,8030090924468
+4582078	b	A	199	2019-05-26 06:34:00	Lanius collurio	1	0	1	0	40,260971046357795
+4582080	b	A	199	2019-05-26 06:34:00	Picus viridis	0	1	0	1	104,02563827511999
+4582079	b	A	199	2019-05-26 06:34:00	Oriolus oriolus	1	0	1	0	176,45623290265098
+4582095	b	A	199	2019-05-26 06:35:00	Turdus merula	1	0	1	0	31,4282183706542
+4582081	b	A	199	2019-05-26 06:35:00	Turdus philomelos	0	0	0	0	120,235808735937
+4582082	b	A	199	2019-05-26 06:35:00	Turdus philomelos	0	1	0	1	120,235808735937
+4582083	b	A	199	2019-05-26 06:36:00	Emberiza cirlus	1	0	1	0	16,1219474869418
+4582084	b	A	199	2019-05-26 06:36:00	Cuculus canorus	1	0	1	0	199,19121268275399
+4582086	b	A	199	2019-05-26 06:37:00	Alauda arvensis	1	0	0	0	60,7814872186907
+4582087	b	A	199	2019-05-26 06:37:00	Streptopelia turtur	1	0	0	0	111,200976099349
+4582097	b	A	199	2019-05-26 06:37:00	Emberiza calandra	1	0	0	0	123,38767419317699
+4582085	b	A	199	2019-05-26 06:37:00	Cuculus canorus	1	0	0	0	534,403154175238
+4582088	b	A	199	2019-05-26 06:38:00	Streptopelia turtur	1	0	0	0	293,10034684372
+4582089	b	A	199	2019-05-26 06:39:00	Lanius collurio	2	0	0	0	33,4329373686829
+4582099	b	A	199	2019-05-26 06:39:00	Alauda arvensis	1	0	0	0	120,709852718638
+4582091	b	A	199	2019-05-26 06:40:00	Turdus merula	2	0	0	0	44,961899599769794
+4582090	b	A	199	2019-05-26 06:40:00	Lanius collurio	1	0	0	0	78,92241299533849
+4582092	b	A	199	2019-05-26 06:41:00	Sitta europaea	1	0	0	0	115,88437719527799
+4582093	b	A	199	2019-05-26 06:41:00	Sturnus vulgaris	0	4	0	0	124,400886393132
+4582096	b	A	199	2019-05-26 06:41:00	Lanius collurio	1	0	0	0	146,317763208383
+4582094	b	A	199	2019-05-26 06:42:00	Turdus merula	1	0	0	0	128,803580746817
+4582098	b	A	199	2019-05-26 06:42:00	Columba palumbus	1	0	0	0	159,24267734091998
+4582124	b	A	200	2019-05-26 07:01:00	Milvus migrans	0	1	0	1	71,2250594050485
+4582123	b	A	200	2019-05-26 07:01:00	Corvus corone	1	0	1	0	153,15796561968
+4582126	b	A	200	2019-05-26 07:02:00	Sylvia communis	1	0	1	0	88,1944999343612
+4582125	b	A	200	2019-05-26 07:02:00	Sylvia atricapilla	1	0	1	0	127,72292478656
+4582127	b	A	200	2019-05-26 07:03:00	Columba palumbus	1	0	1	0	63,363485881828204
+4582129	b	A	200	2019-05-26 07:03:00	Alauda arvensis	1	0	1	0	87,0309007315132
+4582128	b	A	200	2019-05-26 07:03:00	Streptopelia turtur	1	0	1	0	142,33708757253598
+4582132	b	A	200	2019-05-26 07:04:00	Alauda arvensis	1	0	1	0	24,4566543095746
+4582131	b	A	200	2019-05-26 07:04:00	Streptopelia turtur	1	0	1	0	149,739836984526
+4582130	b	A	200	2019-05-26 07:04:00	Erithacus rubecula	1	0	1	0	159,887983333389
+4582146	b	A	200	2019-05-26 07:05:00	Fringilla coelebs	1	0	0	0	77,8600202838272
+4582133	b	A	200	2019-05-26 07:05:00	Lanius collurio	1	0	0	0	89,1190938450563
+4582144	b	A	200	2019-05-26 07:05:00	Parus major	1	0	0	0	104,434916198451
+4582134	b	A	200	2019-05-26 07:05:00	Alauda arvensis	1	0	0	0	122,109451088076
+4582135	b	A	200	2019-05-26 07:05:00	Dendrocopos major	1	0	0	0	144,747470739939
+4582145	b	A	200	2019-05-26 07:05:00	Cuculus canorus	1	0	0	0	286,223705080483
+4582136	b	A	200	2019-05-26 07:06:00	Columba palumbus	1	0	0	0	161,806479244382
+4582137	b	A	200	2019-05-26 07:07:00	Turdus merula	1	0	0	0	172,81663779251
+4582138	b	A	200	2019-05-26 07:07:00	Fringilla coelebs	1	0	0	0	190,15385009913
+4582139	b	A	200	2019-05-26 07:08:00	Regulus ignicapilla	1	0	0	0	63,010485234576905
+4582140	b	A	200	2019-05-26 07:08:00	Lullula arborea	1	0	0	0	132,53771385632498
+4582142	b	A	200	2019-05-26 07:10:00	Turdus merula	1	0	0	0	68,6868390701636
+4582143	b	A	200	2019-05-26 07:10:00	Sylvia atricapilla	1	0	0	0	71,74632157668941
+4582141	b	A	200	2019-05-26 07:10:00	Turdus merula	1	0	0	0	126,041710608328
+4582100	b	A	201	2019-05-26 07:24:00	Alauda arvensis	1	0	1	0	158,343994260968
+4582102	b	A	201	2019-05-26 07:25:00	Alauda arvensis	1	0	1	0	161,91452906748898
+4582101	b	A	201	2019-05-26 07:25:00	Coturnix coturnix	1	0	1	0	219,048603129183
+4582103	b	A	201	2019-05-26 07:26:00	Turdus merula	1	0	1	0	219,327027519667
+4582105	b	A	201	2019-05-26 07:28:00	Alauda arvensis	1	0	1	0	199,912431732117
+4582104	b	A	201	2019-05-26 07:28:00	Sylvia atricapilla	1	0	1	0	261,55303328123904
+4582106	b	A	201	2019-05-26 07:29:00	Phylloscopus collybita	1	0	0	0	254,63756104157503
+4582107	b	A	201	2019-05-26 07:30:00	Corvus corone	1	0	0	0	197,765126007713
+4582108	b	A	201	2019-05-26 07:30:00	Sylvia atricapilla	1	0	0	0	259,353948088643
+4582109	b	A	201	2019-05-26 07:33:00	Streptopelia turtur	1	0	0	0	618,469077774008
+4582110	b	A	201	2019-05-26 07:34:00	Corvus corone	1	0	0	0	218,219879410551
+4582111	b	A	202	2019-05-26 07:49:00	Turdus merula	1	0	1	0	87,82443526810249
+4582112	b	A	202	2019-05-26 07:50:00	Luscinia megarhynchos	1	0	1	0	373,39188986772103
+4582113	b	A	202	2019-05-26 07:50:00	Columba palumbus	1	0	1	0	397,993747293461
+4582115	b	A	202	2019-05-26 07:51:00	Fringilla coelebs	1	0	1	0	271,94553977054903
+4582114	b	A	202	2019-05-26 07:51:00	Streptopelia decaocto	2	0	2	0	361,790128553409
+4582116	b	A	202	2019-05-26 07:52:00	Sylvia atricapilla	1	0	1	0	366,984383223507
+4582117	b	A	202	2019-05-26 07:53:00	Alauda arvensis	1	0	1	0	153,455693232528
+4582119	b	A	202	2019-05-26 07:54:00	Alauda arvensis	1	0	0	0	132,616284024027
+4582118	b	A	202	2019-05-26 07:54:00	Pica pica	0	1	0	0	155,292175243529
+4582120	b	A	202	2019-05-26 07:55:00	Streptopelia turtur	1	0	0	0	373,633780003376
+4582121	b	A	202	2019-05-26 07:57:00	Turdus merula	0	1	0	0	145,694160622427
+4582122	b	A	202	2019-05-26 07:58:00	Picus viridis	1	0	0	0	426,047256159629
+4582278	b	A	218	2019-05-26 08:30:00	Cyanistes caeruleus	1	0	1	0	15,4059774268207
+4582279	b	A	218	2019-05-26 08:30:00	Streptopelia turtur	1	0	1	0	138,816487301226
+4582280	b	A	218	2019-05-26 08:31:00	Phylloscopus collybita	1	0	1	0	110,403485176878
+4582281	b	A	218	2019-05-26 08:31:00	Corvus corone	0	2	0	2	116,247257852521
+4582282	b	A	218	2019-05-26 08:32:00	Garrulus glandarius	0	1	0	1	151,5170974232
+4582283	b	A	218	2019-05-26 08:32:00	Corvus corone	1	0	1	0	167,125955926159
+4582285	b	A	218	2019-05-26 08:33:00	Columba palumbus	1	0	1	0	104,397798219357
+4582284	b	A	218	2019-05-26 08:33:00	Cuculus canorus	1	0	1	0	165,893452120929
+4582288	b	A	218	2019-05-26 08:34:00	Turdus merula	1	0	1	0	88,8073900347208
+4582287	b	A	218	2019-05-26 08:34:00	Cuculus canorus	1	0	1	0	127,316692522426
+4582290	b	A	218	2019-05-26 08:35:00	Sylvia communis	1	0	0	0	56,6378430777139
+4582291	b	A	218	2019-05-26 08:35:00	Turdus merula	1	0	0	0	67,75575963450011
+4582289	b	A	218	2019-05-26 08:35:00	Phylloscopus collybita	1	0	0	0	106,671151222402
+4582292	b	A	218	2019-05-26 08:36:00	Picus viridis	0	1	0	0	16,2237084887793
+4582293	b	A	218	2019-05-26 08:36:00	Turdus philomelos	1	0	0	0	109,80584922338001
+4582294	b	A	218	2019-05-26 08:37:00	Erithacus rubecula	1	0	0	0	101,548454378169
+4582295	b	A	218	2019-05-26 08:38:00	Oriolus oriolus	1	0	0	0	80,3374005123329
+4582296	b	A	218	2019-05-26 08:39:00	Alauda arvensis	1	0	0	0	40,0200629590777
+4582297	b	A	218	2019-05-26 08:39:00	Picus viridis	0	1	0	0	127,791227731709
+4582298	b	A	218	2019-05-26 08:40:00	Turdus merula	1	0	0	0	86,0061402592512
+4582299	b	A	218	2019-05-26 08:41:00	Turdus merula	1	0	0	0	36,086666094823606
+4582300	b	A	218	2019-05-26 08:41:00	Dendrocopos major	0	1	0	0	137,492517570941
+4582201	b	A	219	2019-05-26 08:07:00	Sylvia communis	1	0	1	0	111,30230646422301
+4582205	b	A	219	2019-05-26 08:08:00	Fringilla coelebs	0	1	0	1	96,55342147389611
+4582204	b	A	219	2019-05-26 08:08:00	Cyanistes caeruleus	0	1	0	1	109,096201827636
+4582203	b	A	219	2019-05-26 08:08:00	Turdus merula	1	0	1	0	113,545259564074
+4582202	b	A	219	2019-05-26 08:08:00	Sylvia communis	1	0	1	0	181,360496946992
+4582206	b	A	219	2019-05-26 08:09:00	Sturnus vulgaris	0	2	0	2	132,543645243619
+4582207	b	A	219	2019-05-26 08:10:00	Alauda arvensis	1	0	1	0	97,52349802977501
+4582208	b	A	219	2019-05-26 08:10:00	Luscinia megarhynchos	1	0	1	0	146,04066560546602
+4582209	b	A	219	2019-05-26 08:11:00	Luscinia megarhynchos	1	0	1	0	420,233219797956
+4582210	b	A	219	2019-05-26 08:13:00	Coturnix coturnix	1	0	0	0	91,1708520735268
+4582211	b	A	219	2019-05-26 08:14:00	Garrulus glandarius	0	1	0	0	115,29384986123699
+4582213	b	A	219	2019-05-26 08:15:00	Coccothraustes coccothraustes	0	1	0	0	71,20667356191609
+4582212	b	A	219	2019-05-26 08:15:00	Cyanistes caeruleus	0	4	0	0	132,735651740161
+4582215	b	A	219	2019-05-26 08:16:00	Emberiza cirlus	1	0	0	0	37,8027025203896
+4582216	b	A	219	2019-05-26 08:16:00	Corvus corone	0	1	0	0	53,704658098953
+4582214	b	A	219	2019-05-26 08:16:00	Saxicola rubicola	1	0	0	0	113,791915108591
+4582219	b	A	219	2019-05-26 08:17:00	Picus viridis	1	0	0	0	73,22073670376221
+4582217	b	A	219	2019-05-26 08:17:00	Sylvia atricapilla	1	0	0	0	85,0910895736893
+4582218	b	A	219	2019-05-26 08:17:00	Alauda arvensis	1	0	0	0	123,176710729404
+4470071	a	B	23	2019-04-13 08:55:00	Carduelis cannabina	1	0	1	0	37,713087798935106
+4470070	a	B	23	2019-04-13 08:55:00	Anthus pratensis	0	5	0	5	74,02432681432079
+4470073	a	B	23	2019-04-13 08:56:00	Alauda arvensis	1	0	1	0	73,2259407031987
+4470072	a	B	23	2019-04-13 08:56:00	Corvus corone	0	1	0	1	146,520673217486
+4470074	a	B	23	2019-04-13 08:57:00	Carduelis chloris	1	0	1	0	67,3265453099303
+4470075	a	B	23	2019-04-13 08:58:00	Emberiza citrinella	1	0	1	0	74,12076992537509
+4470076	a	B	23	2019-04-13 08:58:00	Corvus corone	0	1	0	1	303,093908801071
+4470078	a	B	23	2019-04-13 08:59:00	Sylvia atricapilla	1	0	0	0	25,6429359235936
+4470077	a	B	23	2019-04-13 08:59:00	Turdus merula	1	0	0	0	193,337316696177
+4470218	a	B	23	2019-04-13 08:59:00	Emberiza citrinella	1	0	0	0	204,318185075284
+4470213	a	B	23	2019-04-13 09:00:00	Turdus merula	1	0	0	0	40,632223456078
+4470214	a	B	23	2019-04-13 09:01:00	Prunella modularis	1	0	0	0	36,9652612957816
+4470215	a	B	23	2019-04-13 09:02:00	Turdus merula	1	0	0	0	66,6693172571411
+4470216	a	B	23	2019-04-13 09:03:00	Alauda arvensis	1	0	0	0	43,6591564301282
+4470217	a	B	23	2019-04-13 09:04:00	Lullula arborea	1	0	0	0	74,75760645399441
+4470195	a	B	39	2019-04-13 08:09:00	Turdus merula	1	0	1	0	31,7970918796458
+4470196	a	B	39	2019-04-13 08:10:00	Emberiza citrinella	1	0	1	0	52,0276958907015
+4470198	a	B	39	2019-04-13 08:10:00	Turdus viscivorus	1	0	1	0	235,368579504489
+4470197	a	B	39	2019-04-13 08:10:00	Turdus merula	1	0	1	0	235,845786074981
+4470200	a	B	39	2019-04-13 08:11:00	Prunella modularis	1	0	1	0	42,3602029293287
+4470199	a	B	39	2019-04-13 08:11:00	Alauda arvensis	1	0	1	0	92,88097329353641
+4470202	a	B	39	2019-04-13 08:11:00	Alauda arvensis	1	0	1	0	167,385594552369
+4470201	a	B	39	2019-04-13 08:11:00	Alauda arvensis	1	0	1	0	203,955045847492
+4470067	a	B	39	2019-04-13 08:12:00	Emberiza citrinella	0	1	0	1	67,0925250972644
+4470203	a	B	39	2019-04-13 08:12:00	Dendrocopos major	0	1	0	1	143,663608264769
+4470204	a	B	39	2019-04-13 08:12:00	Turdus merula	1	0	1	0	207,15756198094502
+4470068	a	B	39	2019-04-13 08:14:00	Turdus merula	1	1	0	0	62,0536534029842
+4470175	a	B	39	2019-04-13 08:14:00	Prunella modularis	1	0	0	0	100,178824205671
+4470176	a	B	39	2019-04-13 08:14:00	Alauda arvensis	1	0	0	0	178,97940224334
+4470069	a	B	39	2019-04-13 08:14:00	Jynx torquilla	1	0	0	0	243,934662307415
+4470178	a	B	39	2019-04-13 08:15:00	Turdus merula	1	0	0	0	88,86526318513529
+4470177	a	B	39	2019-04-13 08:15:00	Sylvia atricapilla	1	0	0	0	140,6272226198
+4470179	a	B	39	2019-04-13 08:16:00	Sturnus vulgaris	0	1	0	0	50,40991396650129
+4470180	a	B	39	2019-04-13 08:17:00	Sylvia atricapilla	1	0	0	0	141,42539171289
+4470181	a	B	39	2019-04-13 08:18:00	Saxicola rubicola	1	0	0	0	71,3696943921705
+4470167	a	B	40	2019-04-13 08:29:00	Erithacus rubecula	0	1	0	1	107,446160522627
+4470166	a	B	40	2019-04-13 08:29:00	Emberiza citrinella	1	1	1	1	113,42320854723499
+4470168	a	B	40	2019-04-13 08:30:00	Sylvia atricapilla	1	0	1	0	58,77628852831779
+4470169	a	B	40	2019-04-13 08:30:00	Alauda arvensis	2	0	2	0	239,38413006372502
+4470170	a	B	40	2019-04-13 08:31:00	Turdus merula	1	0	1	0	54,044983922635005
+4470171	a	B	40	2019-04-13 08:31:00	Dendrocopos minor	0	1	0	1	164,31640465133802
+4470172	a	B	40	2019-04-13 08:31:00	Alauda arvensis	1	0	1	0	189,07049339285
+4470173	a	B	40	2019-04-13 08:32:00	Turdus merula	1	0	1	0	66,9869661373368
+4470063	a	B	40	2019-04-13 08:32:00	Sylvia atricapilla	1	0	1	0	123,568714021798
+4470065	a	B	40	2019-04-13 08:36:00	Carduelis cannabina	1	0	0	0	37,0633950232869
+4470064	a	B	40	2019-04-13 08:36:00	Lullula arborea	1	0	0	0	73,9773823110031
+4470066	a	B	40	2019-04-13 08:36:00	Alauda arvensis	1	0	0	0	158,936702074386
+4470051	a	B	41	2019-04-12 10:29:00	Sylvia atricapilla	1	0	1	0	37,170045394943706
+4470052	a	B	41	2019-04-12 10:29:00	Sylvia atricapilla	1	0	1	0	85,6855381803027
+4470055	a	B	41	2019-04-12 10:30:00	Turdus merula	1	0	1	0	49,62186778152989
+4470054	a	B	41	2019-04-12 10:30:00	Sylvia atricapilla	1	0	1	0	126,475258222298
+4470053	a	B	41	2019-04-12 10:30:00	Garrulus glandarius	0	1	0	1	139,642532267887
+4470056	a	B	41	2019-04-12 10:31:00	Fringilla coelebs	1	0	1	0	89,5150351669275
+4470057	a	B	41	2019-04-12 10:31:00	Turdus merula	1	0	1	0	253,133119431361
+4470059	a	B	41	2019-04-12 10:32:00	Fringilla coelebs	1	0	1	0	49,240166060749395
+4470058	a	B	41	2019-04-12 10:32:00	Turdus viscivorus	1	0	1	0	97,12516682884541
+4470060	a	B	41	2019-04-12 10:33:00	Parus major	1	0	1	0	35,1097351961337
+4470061	a	B	41	2019-04-12 10:33:00	Turdus viscivorus	1	0	1	0	50,4371827256985
+4470205	a	B	41	2019-04-12 10:34:00	Certhia brachydactyla	1	0	0	0	36,063811125105204
+4470062	a	B	41	2019-04-12 10:34:00	Parus major	2	0	0	0	140,578604164386
+4470206	a	B	41	2019-04-12 10:36:00	Erithacus rubecula	0	1	0	0	29,608937021832105
+4470207	a	B	41	2019-04-12 10:37:00	Sylvia atricapilla	1	0	0	0	227,973729384829
+4470208	a	B	41	2019-04-12 10:38:00	Erithacus rubecula	1	0	0	0	29,1527533461869
+4470209	a	B	41	2019-04-12 10:38:00	Prunella modularis	1	0	0	0	51,062247611435204
+4470210	a	B	41	2019-04-12 10:38:00	Dendrocopos minor	1	0	0	0	66,9165108241414
+4470219	a	B	42	2019-04-13 09:32:00	Prunella modularis	1	0	1	0	36,238608618858
+4470220	a	B	42	2019-04-13 09:32:00	Turdus merula	1	0	1	0	63,516157343783
+4470222	a	B	42	2019-04-13 09:33:00	Turdus merula	1	0	1	0	127,94830644277599
+4470221	a	B	42	2019-04-13 09:33:00	Turdus merula	1	0	1	0	141,363652327633
+4470223	a	B	42	2019-04-13 09:35:00	Sylvia borin	1	0	1	0	56,5756846563167
+4470224	a	B	42	2019-04-13 09:35:00	Emberiza citrinella	1	0	1	0	69,208854407049
+4470225	a	B	42	2019-04-13 09:35:00	Sylvia atricapilla	1	0	1	0	101,48144219111401
+4470226	a	B	42	2019-04-13 09:36:00	Phylloscopus collybita	1	0	0	0	93,5993809022432
+4470184	a	B	42	2019-04-13 09:36:00	Parus major	1	0	0	0	93,8739945274134
+4470185	a	B	42	2019-04-13 09:36:00	Corvus corone	0	1	0	0	160,37663745847098
+4470186	a	B	42	2019-04-13 09:37:00	Corvus corone	0	1	0	0	182,12420405084703
+4470187	a	B	42	2019-04-13 09:38:00	Lullula arborea	2	0	0	0	134,099791210918
+4470188	a	B	42	2019-04-13 09:39:00	Prunella modularis	1	0	0	0	54,1271259164129
+4470189	a	B	42	2019-04-13 09:41:00	Cyanistes caeruleus	1	0	0	0	56,9758618774403
+4469992	a	B	56	2019-04-12 08:10:00	Saxicola rubicola	1	0	1	0	65,6094643554589
+4469991	a	B	56	2019-04-12 08:10:00	Jynx torquilla	1	0	1	0	225,460201607257
+4469993	a	B	56	2019-04-12 08:11:00	Emberiza citrinella	1	0	1	0	26,932172904777698
+4469994	a	B	56	2019-04-12 08:11:00	Emberiza cirlus	1	0	1	0	58,12240127807161
+4469995	a	B	56	2019-04-12 08:11:00	Emberiza cirlus	1	0	1	0	77,1525169567946
+4469996	a	B	56	2019-04-12 08:12:00	Alauda arvensis	2	0	2	0	51,8531035735715
+4469989	a	B	56	2019-04-12 08:13:00	Turdus merula	1	0	1	0	90,51349451779079
+4469997	a	B	56	2019-04-12 08:13:00	Sylvia atricapilla	1	0	1	0	151,176630948801
+4469990	a	B	56	2019-04-12 08:13:00	Picus viridis	0	1	0	1	259,863798229258
+4469999	a	B	56	2019-04-12 08:14:00	Carduelis cannabina	0	1	0	1	83,3347454287691
+4469998	a	B	56	2019-04-12 08:14:00	Emberiza citrinella	1	0	1	0	104,559844106239
+4470000	a	B	56	2019-04-12 08:15:00	Emberiza citrinella	1	0	0	0	66,87643654478751
+4470001	a	B	56	2019-04-12 08:15:00	Phylloscopus collybita	1	0	0	0	132,467672190077
+4470002	a	B	56	2019-04-12 08:16:00	Turdus merula	1	0	0	0	113,486527568089
+4470003	a	B	56	2019-04-12 08:17:00	Sylvia atricapilla	1	0	0	0	156,510812161839
+4470004	a	B	56	2019-04-12 08:18:00	Passer domesticus	1	0	0	0	63,8942773612831
+4470007	a	B	57	2019-04-12 08:34:00	Parus major	1	0	1	0	21,9548978825842
+4470005	a	B	57	2019-04-12 08:34:00	Sylvia atricapilla	3	0	3	0	27,8878381233664
+4470006	a	B	57	2019-04-12 08:34:00	Lullula arborea	1	0	1	0	59,214996607707704
+4470009	a	B	57	2019-04-12 08:35:00	Jynx torquilla	1	0	1	0	65,7651243452607
+4470008	a	B	57	2019-04-12 08:35:00	Emberiza citrinella	1	0	1	0	68,975418029866
+4470010	a	B	57	2019-04-12 08:36:00	Saxicola rubicola	1	0	1	0	42,0553084825253
+4470012	a	B	57	2019-04-12 08:37:00	Turdus merula	1	0	1	0	29,708385376770202
+4470011	a	B	57	2019-04-12 08:37:00	Cuculus canorus	1	0	1	0	238,547468347089
+4470014	a	B	57	2019-04-12 08:38:00	Dendrocopos major	0	1	0	0	88,0388895376607
+4470013	a	B	57	2019-04-12 08:38:00	Turdus merula	1	0	0	0	91,63020763051821
+4470015	a	B	57	2019-04-12 08:39:00	Emberiza citrinella	1	0	0	0	93,79395206199449
+4470016	a	B	57	2019-04-12 08:39:00	Emberiza cirlus	1	0	0	0	119,166865501643
+4470017	a	B	57	2019-04-12 08:41:00	Cyanistes caeruleus	1	0	0	0	54,227985410203196
+4470022	a	B	57	2019-04-12 08:41:00	Troglodytes troglodytes	1	0	0	0	168,395928151481
+4470018	a	B	57	2019-04-12 08:42:00	Turdus merula	1	0	0	0	78,6909686796878
+4470019	a	B	57	2019-04-12 08:42:00	Certhia brachydactyla	1	0	0	0	84,7755371405548
+4470021	a	B	57	2019-04-12 08:43:00	Erithacus rubecula	1	0	0	0	15,0060369230621
+4470020	a	B	57	2019-04-12 08:43:00	Phylloscopus collybita	1	0	0	0	78,31132639048441
+4470024	a	B	58	2019-04-12 09:00:00	Lullula arborea	1	0	1	0	16,872527499303402
+4470023	a	B	58	2019-04-12 09:00:00	Jynx torquilla	1	0	1	0	70,0137160393338
+4470025	a	B	58	2019-04-12 09:00:00	Sylvia atricapilla	1	0	1	0	85,9254405235922
+4470027	a	B	58	2019-04-12 09:01:00	Phylloscopus collybita	1	0	1	0	111,788079362601
+4470026	a	B	58	2019-04-12 09:01:00	Parus major	1	0	1	0	114,443757000382
+4470029	a	B	58	2019-04-12 09:02:00	Turdus merula	1	0	1	0	77,19875504710609
+4470028	a	B	58	2019-04-12 09:02:00	Carduelis cannabina	1	1	1	1	88,6037048278322
+4470030	a	B	58	2019-04-12 09:03:00	Prunella modularis	1	0	1	0	59,2612049073653
+4470031	a	B	58	2019-04-12 09:03:00	Sylvia atricapilla	1	0	1	0	176,38298254171298
+4470032	a	B	58	2019-04-12 09:03:00	Turdus viscivorus	1	0	1	0	222,771056574005
+4470033	a	B	58	2019-04-12 09:04:00	Sylvia atricapilla	1	0	0	0	241,30114019919102
+4470047	a	B	58	2019-04-12 09:05:00	Phylloscopus collybita	1	0	0	0	56,7318244101805
+4470048	a	B	58	2019-04-12 09:05:00	Prunella modularis	1	0	0	0	107,569087628766
+4470034	a	B	58	2019-04-12 09:05:00	Turdus merula	1	0	0	0	131,28518467455402
+4470050	a	B	58	2019-04-12 09:08:00	Saxicola rubicola	1	0	0	0	96,20490716469851
+4470049	a	B	58	2019-04-12 09:09:00	Jynx torquilla	1	0	0	0	138,05277763203802
+4470035	a	B	59	2019-04-12 09:28:00	Anthus trivialis	1	0	1	0	41,9246757308707
+4470036	a	B	59	2019-04-12 09:28:00	Phylloscopus collybita	1	0	1	0	63,46597767123961
+4470039	a	B	59	2019-04-12 09:29:00	Cyanistes caeruleus	1	0	1	0	79,137988909675
+4470037	a	B	59	2019-04-12 09:29:00	Emberiza cirlus	1	0	1	0	115,80745550140301
+4470040	a	B	59	2019-04-12 09:30:00	Sylvia atricapilla	1	0	1	0	97,3743761982674
+4470042	a	B	59	2019-04-12 09:30:00	Upupa epops	1	0	1	0	100,34733249485201
+4470043	a	B	59	2019-04-12 09:30:00	Sylvia atricapilla	1	0	1	0	108,619280569753
+4470041	a	B	59	2019-04-12 09:30:00	Turdus merula	1	0	1	0	146,66320872028098
+4470045	a	B	59	2019-04-12 09:31:00	Sylvia atricapilla	1	0	1	0	46,6504766672206
+4470044	a	B	59	2019-04-12 09:31:00	Sylvia atricapilla	1	0	1	0	183,912806375259
+4470046	a	B	59	2019-04-12 09:32:00	Turdus merula	1	0	1	0	46,6504766672206
+4470151	a	B	59	2019-04-12 09:32:00	Turdus merula	1	0	1	0	141,929473524032
+4470150	a	B	59	2019-04-12 09:32:00	Phylloscopus collybita	1	0	1	0	171,66127366585
+4470152	a	B	59	2019-04-12 09:33:00	Parus major	1	0	0	0	69,0974864528756
+4470153	a	B	59	2019-04-12 09:33:00	Anthus trivialis	1	0	0	0	113,770490818528
+4470154	a	B	59	2019-04-12 09:35:00	Turdus merula	1	0	0	0	244,896223929482
+4470157	a	B	60	2019-04-12 09:46:00	Phylloscopus collybita	1	0	1	0	65,8545696151003
+4470156	a	B	60	2019-04-12 09:46:00	Lullula arborea	1	0	1	0	164,847250758435
+4470158	a	B	60	2019-04-12 09:47:00	Phylloscopus collybita	1	0	1	0	94,94697563367609
+4470161	a	B	60	2019-04-12 09:48:00	Turdus merula	1	0	1	0	178,021987660643
+4470159	a	B	60	2019-04-12 09:48:00	Turdus merula	1	0	1	0	179,580782712359
+4470160	a	B	60	2019-04-12 09:48:00	Turdus viscivorus	1	0	1	0	186,88489977801498
+4470162	a	B	60	2019-04-12 09:49:00	Alauda arvensis	2	0	2	0	144,13084037678001
+4470163	a	B	60	2019-04-12 09:50:00	Carduelis cannabina	1	0	1	0	55,262572238203106
+4470165	a	B	60	2019-04-12 09:50:00	Prunella modularis	1	0	1	0	63,4113639549621
+4470164	a	B	60	2019-04-12 09:50:00	Emberiza cirlus	1	0	1	0	68,5188315310729
+4470086	a	B	60	2019-04-12 09:53:00	Corvus corone	0	1	0	0	446,218248380213
+4470087	a	B	60	2019-04-12 09:54:00	Lullula arborea	1	0	0	0	184,448796685159
+4470088	a	B	60	2019-04-12 09:55:00	Turdus merula	1	0	0	0	69,29320011546051
+4600361	b	B	23	2019-06-01 07:18:00	Lanius collurio	1	0	1	0	116,18418247667
+4600362	b	B	23	2019-06-01 07:18:00	Lanius collurio	1	0	1	0	150,274423947316
+4600366	b	B	23	2019-06-01 07:19:00	Emberiza cirlus	1	0	1	0	43,75045512002129
+4600363	b	B	23	2019-06-01 07:19:00	Lanius collurio	1	0	1	0	73,83201298076979
+4600365	b	B	23	2019-06-01 07:19:00	Emberiza citrinella	1	0	1	0	79,9210212183632
+4600364	b	B	23	2019-06-01 07:19:00	Alauda arvensis	0	1	0	1	136,97559366056402
+4600367	b	B	23	2019-06-01 07:20:00	Emberiza calandra	1	0	1	0	184,67551454645303
+4600368	b	B	23	2019-06-01 07:20:00	Turdus merula	1	0	1	0	229,53873838390498
+4600369	b	B	23	2019-06-01 07:21:00	Carduelis cannabina	1	0	1	0	22,256373973116304
+4600371	b	B	23	2019-06-01 07:21:00	Turdus merula	1	0	1	0	135,516867264187
+4600370	b	B	23	2019-06-01 07:21:00	Lullula arborea	1	0	1	0	144,418807691414
+4600262	b	B	23	2019-06-01 07:22:00	Lanius collurio	1	0	0	0	204,74557254632703
+4600261	b	B	23	2019-06-01 07:22:00	Alauda arvensis	0	1	0	0	248,009867532714
+4600263	b	B	23	2019-06-01 07:23:00	Luscinia megarhynchos	1	0	0	0	128,696128785875
+4600265	b	B	23	2019-06-01 07:25:00	Sylvia atricapilla	1	0	0	0	213,70962987379698
+4600264	b	B	23	2019-06-01 07:25:00	Alauda arvensis	0	1	0	0	295,291249876831
+4600266	b	B	23	2019-06-01 07:25:00	Sylvia atricapilla	0	1	0	0	321,320577163559
+4600268	b	B	23	2019-06-01 07:26:00	Alauda arvensis	0	1	0	0	87,1859796390958
+4600267	b	B	23	2019-06-01 07:26:00	Fringilla coelebs	1	0	0	0	235,99451017539
+4600395	b	B	39	2019-06-01 06:40:00	Alauda arvensis	0	1	0	1	29,576983279106198
+4600394	b	B	39	2019-06-01 06:40:00	Alauda arvensis	0	1	0	1	50,177434765809295
+4600396	b	B	39	2019-06-01 06:40:00	Lanius collurio	1	0	1	0	85,219652905564
+4600397	b	B	39	2019-06-01 06:41:00	Emberiza calandra	1	0	1	0	82,8356372993052
+4600398	b	B	39	2019-06-01 06:42:00	Emberiza citrinella	1	0	1	0	68,6807987696493
+4600399	b	B	39	2019-06-01 06:43:00	Emberiza citrinella	0	1	0	1	80,3743092740585
+4600400	b	B	39	2019-06-01 06:43:00	Picus viridis	0	1	0	1	244,528483880956
+4600401	b	B	39	2019-06-01 06:44:00	Parus major	1	0	0	0	86,12790589459551
+4600402	b	B	39	2019-06-01 06:45:00	Alauda arvensis	0	1	0	0	125,388197944701
+4600403	b	B	39	2019-06-01 06:46:00	Lanius collurio	1	0	0	0	59,26029382764529
+4600404	b	B	39	2019-06-01 06:46:00	Sylvia atricapilla	1	0	0	0	112,12038489673799
+4600345	b	B	39	2019-06-01 06:48:00	Turdus merula	1	0	0	0	57,984250900842
+4600346	b	B	39	2019-06-01 06:49:00	Sylvia atricapilla	0	1	0	0	147,653629625247
+4600347	b	B	40	2019-06-01 06:56:00	Alauda arvensis	0	1	0	1	134,29452484743402
+4600349	b	B	40	2019-06-01 06:56:00	Buteo buteo	0	1	0	1	208,085578009967
+4600348	b	B	40	2019-06-01 06:56:00	Alauda arvensis	0	1	0	1	221,610767592908
+4600352	b	B	40	2019-06-01 06:57:00	Emberiza cirlus	1	0	1	0	125,046949592083
+4600351	b	B	40	2019-06-01 06:57:00	Lanius collurio	1	0	1	0	179,834104746149
+4600350	b	B	40	2019-06-01 06:57:00	Luscinia megarhynchos	1	0	1	0	210,968471250013
+4600353	b	B	40	2019-06-01 06:58:00	Lanius collurio	1	0	1	0	85,4163435602311
+4600354	b	B	40	2019-06-01 06:58:00	Sylvia atricapilla	0	1	0	1	240,326753089452
+4600356	b	B	40	2019-06-01 06:59:00	Sylvia communis	1	0	1	0	54,9949002593266
+4600357	b	B	40	2019-06-01 06:59:00	Phasianus colchicus	1	0	1	0	188,00175816795
+4600355	b	B	40	2019-06-01 06:59:00	Sylvia atricapilla	0	1	0	1	201,354105083892
+4600358	b	B	40	2019-06-01 07:00:00	Alauda arvensis	0	1	0	0	150,78654274359502
+4600359	b	B	40	2019-06-01 07:01:00	Turdus merula	1	0	0	0	42,5203681379496
+4600360	b	B	40	2019-06-01 07:01:00	Parus major	1	0	0	0	171,959165220573
+4600388	b	B	40	2019-06-01 07:02:00	Lanius collurio	1	0	0	0	130,37220727598
+4600389	b	B	40	2019-06-01 07:03:00	Turdus merula	0	1	0	0	284,637606767073
+4600390	b	B	40	2019-06-01 07:04:00	Alauda arvensis	0	1	0	0	298,260348268765
+4600391	b	B	40	2019-06-01 07:05:00	Saxicola rubicola	0	1	0	0	103,76870776715599
+4600373	b	B	41	2019-05-31 08:48:00	Saxicola rubicola	1	0	1	0	16,0267277457206
+4600372	b	B	41	2019-05-31 08:48:00	Sylvia communis	1	1	1	1	21,096162268601198
+4600374	b	B	41	2019-05-31 08:49:00	Garrulus glandarius	0	1	0	1	28,6056394192067
+4600375	b	B	41	2019-05-31 08:49:00	Sylvia atricapilla	1	0	1	0	29,221708468482397
+4600376	b	B	41	2019-05-31 08:49:00	Parus major	0	1	0	1	43,79522816809521
+4600377	b	B	41	2019-05-31 08:50:00	Parus major	0	1	0	1	52,5650391975061
+4600378	b	B	41	2019-05-31 08:50:00	Fringilla coelebs	1	0	1	0	70,2559272608659
+4600379	b	B	41	2019-05-31 08:51:00	Sylvia atricapilla	0	1	0	1	144,549764258517
+4600380	b	B	41	2019-05-31 08:51:00	Turdus merula	1	0	1	0	144,549764258517
+4600382	b	B	41	2019-05-31 08:52:00	Phylloscopus collybita	1	0	1	0	28,2219056118726
+4600381	b	B	41	2019-05-31 08:52:00	Emberiza citrinella	1	0	1	0	90,7863051460318
+4600384	b	B	41	2019-05-31 08:53:00	Turdus merula	1	0	0	0	59,3193807214437
+4600383	b	B	41	2019-05-31 08:53:00	Sylvia atricapilla	0	1	0	0	235,15175356233897
+4600385	b	B	41	2019-05-31 08:54:00	Phylloscopus collybita	0	1	0	0	53,701306451374705
+4600386	b	B	41	2019-05-31 08:54:00	Streptopelia turtur	1	0	0	0	101,553193594416
+4600407	b	B	41	2019-05-31 08:55:00	Luscinia megarhynchos	0	1	0	0	57,2670918426589
+4600387	b	B	41	2019-05-31 08:55:00	Anthus trivialis	1	0	0	0	100,863387881563
+4600406	b	B	41	2019-05-31 08:55:00	Luscinia megarhynchos	1	0	0	0	228,649991955502
+4600408	b	B	41	2019-05-31 08:56:00	Sylvia atricapilla	0	1	0	0	192,041881643847
+4600164	b	B	56	2019-05-31 06:41:00	Emberiza calandra	1	0	1	0	66,9070683682569
+4600166	b	B	56	2019-05-31 06:42:00	Turdus merula	1	0	1	0	2,9285796227793903
+4600167	b	B	56	2019-05-31 06:42:00	Sylvia communis	1	0	1	0	16,1821797196369
+4600165	b	B	56	2019-05-31 06:42:00	Saxicola rubicola	1	0	1	0	24,314755045111298
+4600168	b	B	56	2019-05-31 06:43:00	Luscinia megarhynchos	1	0	1	0	37,3684763373287
+4600169	b	B	56	2019-05-31 06:43:00	Phylloscopus collybita	0	1	0	1	51,2669980510037
+4600170	b	B	56	2019-05-31 06:43:00	Sylvia atricapilla	0	1	0	1	70,41288500327941
+4600172	b	B	56	2019-05-31 06:44:00	Parus major	1	1	1	1	24,1171877143126
+4600171	b	B	56	2019-05-31 06:44:00	Sylvia atricapilla	0	1	0	1	77,005862761178
+4600174	b	B	56	2019-05-31 06:45:00	Emberiza citrinella	1	0	1	0	62,642984674904696
+4600173	b	B	56	2019-05-31 06:45:00	Emberiza cirlus	1	0	1	0	76,5335247612333
+4600175	b	B	56	2019-05-31 06:45:00	Phasianus colchicus	1	0	1	0	81,9137635503295
+4600176	b	B	56	2019-05-31 06:46:00	Troglodytes troglodytes	1	0	0	0	60,1982439174652
+4600177	b	B	56	2019-05-31 06:47:00	Turdus merula	0	1	0	0	74,597591041633
+4600178	b	B	56	2019-05-31 06:48:00	Buteo buteo	0	1	0	0	85,0767469556345
+4600181	b	B	56	2019-05-31 06:49:00	Prunella modularis	1	0	0	0	60,709476563208
+4600182	b	B	56	2019-05-31 06:50:00	Lanius collurio	1	0	0	0	25,1758876626448
+4600183	b	B	56	2019-05-31 06:51:00	Alauda arvensis	1	1	0	0	42,8915370195404
+4600191	b	B	57	2019-05-31 07:00:00	Sylvia communis	1	0	1	0	46,9425876475909
+4600193	b	B	57	2019-05-31 07:00:00	Turdus merula	0	1	0	1	70,02250631715279
+4600192	b	B	57	2019-05-31 07:00:00	Turdus merula	0	1	0	1	98,37566814038799
+4600196	b	B	57	2019-05-31 07:01:00	Luscinia megarhynchos	1	0	1	0	44,998967100341794
+4600195	b	B	57	2019-05-31 07:01:00	Phasianus colchicus	1	0	1	0	63,775024207828
+4600194	b	B	57	2019-05-31 07:01:00	Turdus merula	0	1	0	1	170,943656486213
+4600199	b	B	57	2019-05-31 07:02:00	Lullula arborea	1	0	1	0	85,1119802375789
+4600197	b	B	57	2019-05-31 07:02:00	Phylloscopus collybita	1	0	1	0	95,20245128685092
+4600198	b	B	57	2019-05-31 07:02:00	Sylvia atricapilla	1	0	1	0	95,20245128685092
+4600201	b	B	57	2019-05-31 07:03:00	Turdus merula	1	0	1	0	45,999375014356495
+4600200	b	B	57	2019-05-31 07:03:00	Phylloscopus collybita	0	1	0	1	66,3618069128881
+4600202	b	B	57	2019-05-31 07:04:00	Parus major	1	0	0	0	61,3039348155877
+4600234	b	B	57	2019-05-31 07:04:00	Lanius collurio	1	0	0	0	105,74874710328501
+4600228	b	B	57	2019-05-31 07:05:00	Picus viridis	0	1	0	0	102,256030733692
+4600229	b	B	57	2019-05-31 07:06:00	Emberiza citrinella	1	0	0	0	93,3855719664583
+4600230	b	B	57	2019-05-31 07:07:00	Dendrocopos minor	1	0	0	0	52,685055045062
+4600232	b	B	57	2019-05-31 07:08:00	Emberiza citrinella	1	0	0	0	76,6244664977523
+4600231	b	B	57	2019-05-31 07:08:00	Sylvia communis	1	0	0	0	90,010129148678
+4600233	b	B	57	2019-05-31 07:09:00	Serinus serinus	1	0	0	0	68,86324666339021
+4600237	b	B	58	2019-05-31 07:21:00	Emberiza cirlus	1	0	1	0	58,0978841805301
+4600238	b	B	58	2019-05-31 07:21:00	Luscinia megarhynchos	1	0	1	0	65,94187231674229
+4600236	b	B	58	2019-05-31 07:21:00	Emberiza citrinella	1	0	1	0	77,459321847895
+4600239	b	B	58	2019-05-31 07:22:00	Phylloscopus collybita	1	0	1	0	58,1361778556275
+4600240	b	B	58	2019-05-31 07:22:00	Turdus philomelos	1	0	1	0	93,13534360271271
+4600241	b	B	58	2019-05-31 07:22:00	Fringilla coelebs	0	1	0	1	178,17266384976898
+4600243	b	B	58	2019-05-31 07:23:00	Luscinia megarhynchos	0	1	0	1	108,142271749205
+4600242	b	B	58	2019-05-31 07:23:00	Fringilla coelebs	0	1	0	1	158,734922338122
+4600203	b	B	58	2019-05-31 07:24:00	Sylvia communis	1	0	1	0	24,4632555353894
+4600204	b	B	58	2019-05-31 07:24:00	Turdus merula	1	0	1	0	24,4632555353894
+4600244	b	B	58	2019-05-31 07:24:00	Sylvia atricapilla	1	0	1	0	79,1165313136128
+4600205	b	B	58	2019-05-31 07:25:00	Sylvia atricapilla	0	1	0	1	184,958212576336
+4600206	b	B	58	2019-05-31 07:26:00	Emberiza citrinella	1	0	0	0	139,68218742012698
+4600207	b	B	58	2019-05-31 07:26:00	Alauda arvensis	1	0	0	0	205,547170171753
+4600208	b	B	58	2019-05-31 07:29:00	Sylvia atricapilla	0	1	0	0	183,70154829254102
+4600210	b	B	58	2019-05-31 07:30:00	Lanius collurio	0	1	0	0	131,78760200065202
+4600209	b	B	58	2019-05-31 07:30:00	Lanius collurio	1	0	0	0	153,152302327091
+4600211	b	B	58	2019-05-31 07:31:00	Alauda arvensis	1	0	0	0	161,3833829564
+4600185	b	B	59	2019-05-31 07:41:00	Phylloscopus collybita	1	0	1	0	80,2596391937535
+4600188	b	B	59	2019-05-31 07:42:00	Emberiza cirlus	1	0	1	0	72,5303969934081
+4600187	b	B	59	2019-05-31 07:42:00	Sylvia atricapilla	0	1	0	1	91,70486583143959
+4600186	b	B	59	2019-05-31 07:42:00	Anthus trivialis	1	0	1	0	94,7737057965374
+4600189	b	B	59	2019-05-31 07:42:00	Sylvia atricapilla	0	1	0	1	121,985043144531
+4600190	b	B	59	2019-05-31 07:43:00	Phylloscopus bonelli	1	0	1	0	69,92635010564
+4600212	b	B	59	2019-05-31 07:44:00	Emberiza citrinella	1	0	1	0	209,855907460444
+4600213	b	B	59	2019-05-31 07:45:00	Sylvia atricapilla	0	1	0	1	62,426695391824104
+4600214	b	B	59	2019-05-31 07:46:00	Turdus merula	1	0	0	0	53,052821435206795
+4600215	b	B	59	2019-05-31 07:46:00	Turdus merula	1	0	0	0	65,85268299766321
+4600220	b	B	59	2019-05-31 07:46:00	Streptopelia turtur	1	0	0	0	77,6945495648783
+4600216	b	B	59	2019-05-31 07:47:00	Luscinia megarhynchos	1	0	0	0	214,29367876171003
+4600217	b	B	59	2019-05-31 07:48:00	Sylvia communis	1	0	0	0	34,355178590605995
+4600218	b	B	59	2019-05-31 07:49:00	Sylvia communis	1	0	0	0	54,4959952095224
+4600219	b	B	59	2019-05-31 07:52:00	Luscinia megarhynchos	0	1	0	0	19,8867802623959
+4600223	b	B	60	2019-05-31 08:06:00	Turdus merula	0	0	0	0	37,129345769779995
+4600222	b	B	60	2019-05-31 08:06:00	Lanius collurio	1	0	1	0	65,9525186102756
+4600221	b	B	60	2019-05-31 08:06:00	Anthus trivialis	1	0	1	0	170,703210457237
+4600224	b	B	60	2019-05-31 08:07:00	Turdus merula	1	0	1	0	166,26329449105802
+4600225	b	B	60	2019-05-31 08:08:00	Lanius collurio	0	1	0	1	147,28978846684402
+4600226	b	B	60	2019-05-31 08:08:00	Fringilla coelebs	0	1	0	1	223,517321893797
+4600227	b	B	60	2019-05-31 08:08:00	Fringilla coelebs	0	1	0	1	275,097674720966
+4600251	b	B	60	2019-05-31 08:09:00	Alauda arvensis	1	0	1	0	89,10780015907429
+4600250	b	B	60	2019-05-31 08:09:00	Phylloscopus collybita	1	0	1	0	191,24563263989398
+4600253	b	B	60	2019-05-31 08:10:00	Alauda arvensis	0	1	0	0	132,144858521759
+4600254	b	B	60	2019-05-31 08:10:00	Sylvia atricapilla	1	0	0	0	246,975567486761
+4600255	b	B	60	2019-05-31 08:11:00	Sylvia communis	1	0	0	0	117,70352673206699
+4600256	b	B	60	2019-05-31 08:12:00	Alauda arvensis	0	1	0	0	36,85914200548179
+4600257	b	B	60	2019-05-31 08:12:00	Sylvia atricapilla	0	1	0	0	137,84732809636
+4600259	b	B	60	2019-05-31 08:13:00	Lanius collurio	0	1	0	0	59,016758746689604
+4600258	b	B	60	2019-05-31 08:13:00	Luscinia megarhynchos	1	0	0	0	176,941396256958
+4600260	b	B	60	2019-05-31 08:16:00	Emberiza citrinella	1	0	0	0	143,104906861358
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitSpecsEchants.ods` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitSpecsEchants.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Naturalist-UnitestOptResultats.ods` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Naturalist-UnitestOptResultats.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dec-dist.txt`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,257 +1,257 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	151,2
-ACDC	2400	23	2	184,2
-ACDC	2400	23	2	115,5
-ACDC	2400	39	2	63,5
-ACDC	2400	39	2	133,9
-ACDC	2400	39	2	47,0
-ACDC	2400	39	2	77,5
-ACDC	2400	40	2	88,4
-ACDC	2400	40	2	130,0
-ACDC	2400	40	2	187,2
-ACDC	2400	40	2	249,6
-ACDC	2400	41	2	
-ACDC	2400	42	2	
-ACDC	2400	55	2	
-ACDC	2400	56	2	55,6
-ACDC	2400	56	2	211,3
-ACDC	2400	57	2	
-ACDC	2400	58	2	119,5
-ACDC	2400	58	2	114,0
-ACDC	2400	58	2	202,9
-ACDC	2400	59	2	166,8
-ACDC	2400	60	2	55,6
-ACDC	2400	60	2	91,7
-ACDC	2400	72	2	
-ACDC	2400	73	2	93,0
-ACDC	2400	73	2	155,0
-ACDC	2400	73	2	217,0
-ACDC	2400	73	2	145,7
-ACDC	2400	74	2	112,0
-ACDC	2400	74	2	98,0
-ACDC	2400	75	2	201,6
-ACDC	2400	76	2	
-ACDC	2400	88	2	
-ACDC	2400	89	2	179,2
-ACDC	2400	90	2	77,2
-ACDC	2400	90	2	125,8
-ACDC	2400	90	2	246,4
-ACDC	2400	91	2	126,0
-ACDC	2400	91	2	114,0
-ACDC	2400	105	2	80,0
-ACDC	2400	106	2	
-ACDC	2400	109	2	189,7
-ACDC	2400	109	2	127,3
-ACDC	2400	110	2	
-ACDC	2400	112	2	
-ACDC	2400	113	2	
-ACDC	2400	122	2	
-ACDC	2400	123	2	
-ACDC	2400	125	2	
-ACDC	2400	126	2	189,7
-ACDC	2400	126	2	142,4
-ACDC	2400	126	2	60,6
-ACDC	2400	126	2	146,9
-ACDC	2400	127	2	61,2
-ACDC	2400	127	2	181,8
-ACDC	2400	127	2	127,3
-ACDC	2400	128	2	232,6
-ACDC	2400	129	2	
-ACDC	2400	130	2	
-ACDC	2400	141	2	
-ACDC	2400	142	2	
-ACDC	2400	143	2	174,4
-ACDC	2400	143	2	58,1
-ACDC	2400	143	2	101,0
-ACDC	2400	143	2	181,8
-ACDC	2400	143	2	78,8
-ACDC	2400	143	2	72,7
-ACDC	2400	144	2	45,9
-ACDC	2400	144	2	48,5
-ACDC	2400	145	2	224,0
-ACDC	2400	145	2	144,6
-ACDC	2400	146	2	140,0
-ACDC	2400	146	2	155,7
-ACDC	2400	146	2	50,4
-ACDC	2400	146	2	72,8
-ACDC	2400	147	2	
-ACDC	2400	148	2	167,3
-ACDC	2400	148	2	118,2
-ACDC	2400	148	2	96,4
-ACDC	2400	148	2	74,5
-ACDC	2400	148	2	236,0
-ACDC	2400	148	2	156,4
-ACDC	2400	157	2	116,1
-ACDC	2400	157	2	123,8
-ACDC	2400	158	2	
-ACDC	2400	159	2	
-ACDC	2400	160	2	148,4
-ACDC	2400	161	2	112,0
-ACDC	2400	161	2	97,3
-ACDC	2400	162	2	154,0
-ACDC	2400	162	2	136,2
-ACDC	2400	162	2	144,6
-ACDC	2400	162	2	196,0
-ACDC	2400	162	2	106,4
-ACDC	2400	162	2	140,0
-ACDC	2400	163	2	55,4
-ACDC	2400	163	2	180,1
-ACDC	2400	163	2	188,4
-ACDC	2400	164	2	80,3
-ACDC	2400	164	2	155,1
-ACDC	2400	164	2	138,5
-ACDC	2400	164	2	127,4
-ACDC	2400	164	2	74,8
-ACDC	2400	165	2	124,7
-ACDC	2400	165	2	166,2
-ACDC	2400	165	2	138,5
-ACDC	2400	165	2	110,8
-ACDC	2400	166	2	132,4
-ACDC	2400	166	2	217,0
-ACDC	2400	166	2	167,4
-ACDC	2400	174	2	
-ACDC	2400	175	2	82,9
-ACDC	2400	176	2	
-ACDC	2400	177	2	
-ACDC	2400	178	2	144,0
-ACDC	2400	178	2	196,7
-ACDC	2400	178	2	163,4
-ACDC	2400	178	2	108,0
-ACDC	2400	178	2	144,0
-ACDC	2400	178	2	138,5
-ACDC	2400	179	2	113,6
-ACDC	2400	179	2	166,2
-ACDC	2400	179	2	113,6
-ACDC	2400	179	2	49,9
-ACDC	2400	179	2	83,1
-ACDC	2400	179	2	94,2
-ACDC	2400	179	2	182,8
-ACDC	2400	179	2	41,5
-ACDC	2400	179	2	119,1
-ACDC	2400	179	2	166,2
-ACDC	2400	179	2	169,0
-ACDC	2400	180	2	169,4
-ACDC	2400	181	2	104,7
-ACDC	2400	182	2	120,5
-ACDC	2400	182	2	156,9
-ACDC	2400	182	2	70,3
-ACDC	2400	183	2	183,9
-ACDC	2400	183	2	75,7
-ACDC	2400	183	2	243,8
-ACDC	2400	183	2	243,8
-ACDC	2400	183	2	95,6
-ACDC	2400	183	2	106,4
-ACDC	2400	183	2	196,0
-ACDC	2400	183	2	120,8
-ACDC	2400	183	2	39,7
-ACDC	2400	184	2	228,0
-ACDC	2400	184	2	147,9
-ACDC	2400	184	2	184,0
-ACDC	2400	184	2	188,2
-ACDC	2400	184	2	158,9
-ACDC	2400	184	2	129,6
-ACDC	2400	184	2	154,4
-ACDC	2400	185	2	175,0
-ACDC	2400	185	2	87,7
-ACDC	2400	185	2	94,7
-ACDC	2400	185	2	114,0
-ACDC	2400	185	2	191,1
-ACDC	2400	192	2	
-ACDC	2400	193	2	
-ACDC	2400	194	2	127,3
-ACDC	2400	194	2	168,3
-ACDC	2400	194	2	236,3
-ACDC	2400	194	2	97,9
-ACDC	2400	195	2	153,0
-ACDC	2400	195	2	159,1
-ACDC	2400	195	2	72,7
-ACDC	2400	195	2	107,1
-ACDC	2400	196	2	122,4
-ACDC	2400	196	2	163,6
-ACDC	2400	197	2	85,7
-ACDC	2400	197	2	149,9
-ACDC	2400	198	2	45,5
-ACDC	2400	198	2	300,0
-ACDC	2400	198	2	110,2
-ACDC	2400	198	2	168,3
-ACDC	2400	199	2	121,2
-ACDC	2400	199	2	115,1
-ACDC	2400	199	2	145,4
-ACDC	2400	199	2	186,7
-ACDC	2400	199	2	73,4
-ACDC	2400	199	2	76,5
-ACDC	2400	200	2	76,5
-ACDC	2400	200	2	122,4
-ACDC	2400	200	2	121,2
-ACDC	2400	200	2	184,8
-ACDC	2400	201	2	102,0
-ACDC	2400	201	2	175,0
-ACDC	2400	201	2	180,0
-ACDC	2400	202	2	135,7
-ACDC	2400	202	2	105,8
-ACDC	2400	202	2	75,9
-ACDC	2400	202	2	138,0
-ACDC	2400	210	2	74,5
-ACDC	2400	210	2	89,4
-ACDC	2400	210	2	138,3
-ACDC	2400	210	2	87,2
-ACDC	2400	211	2	98,2
-ACDC	2400	211	2	27,8
-ACDC	2400	211	2	53,7
-ACDC	2400	211	2	100,0
-ACDC	2400	211	2	90,7
-ACDC	2400	211	2	70,4
-ACDC	2400	211	2	64,8
-ACDC	2400	212	2	105,5
-ACDC	2400	213	2	
-ACDC	2400	214	2	
-ACDC	2400	215	2	120,0
-ACDC	2400	215	2	96,0
-ACDC	2400	215	2	100,0
-ACDC	2400	215	2	96,0
-ACDC	2400	216	2	116,3
-ACDC	2400	216	2	107,8
-ACDC	2400	216	2	73,9
-ACDC	2400	218	2	
-ACDC	2400	219	2	84,6
-ACDC	2400	219	2	123,8
-ACDC	2400	228	2	118,4
-ACDC	2400	228	2	88,8
-ACDC	2400	228	2	61,1
-ACDC	2400	228	2	74,0
-ACDC	2400	228	2	51,8
-ACDC	2400	229	2	125,5
-ACDC	2400	229	2	123,2
-ACDC	2400	232	2	160,2
-ACDC	2400	232	2	61,2
-ACDC	2400	232	2	140,8
-ACDC	2400	232	2	137,7
-ACDC	2400	232	2	226,4
-ACDC	2400	232	2	61,6
-ACDC	2400	232	2	138,6
-ACDC	2400	233	2	52,0
-ACDC	2400	233	2	195,8
-ACDC	2400	233	2	138,6
-ACDC	2400	233	2	169,4
-ACDC	2400	233	2	83,2
-ACDC	2400	233	2	61,6
-ACDC	2400	245	2	96,3
-ACDC	2400	246	2	90,0
-ACDC	2400	247	2	
-ACDC	2400	250	2	84,0
-ACDC	2400	262	2	
-ACDC	2400	263	2	83,9
-ACDC	2400	263	2	117,5
-ACDC	2400	265	2	
-ACDC	2400	266	2	
-ACDC	2400	280	2	
-ACDC	2400	281	2	104,9
-ACDC	2400	281	2	100,7
-ACDC	2400	282	2	85,3
-ACDC	2400	282	2	193,1
-ACDC	2400	282	2	130,2
-ACDC	2400	283	2	
-ACDC	2400	284	2	
-ACDC	2400	299	2	
-ACDC	2400	300	2	
-ACDC	2400	301	2	
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	151,2
+ACDC	2400	23	2	184,2
+ACDC	2400	23	2	115,5
+ACDC	2400	39	2	63,5
+ACDC	2400	39	2	133,9
+ACDC	2400	39	2	47,0
+ACDC	2400	39	2	77,5
+ACDC	2400	40	2	88,4
+ACDC	2400	40	2	130,0
+ACDC	2400	40	2	187,2
+ACDC	2400	40	2	249,6
+ACDC	2400	41	2	
+ACDC	2400	42	2	
+ACDC	2400	55	2	
+ACDC	2400	56	2	55,6
+ACDC	2400	56	2	211,3
+ACDC	2400	57	2	
+ACDC	2400	58	2	119,5
+ACDC	2400	58	2	114,0
+ACDC	2400	58	2	202,9
+ACDC	2400	59	2	166,8
+ACDC	2400	60	2	55,6
+ACDC	2400	60	2	91,7
+ACDC	2400	72	2	
+ACDC	2400	73	2	93,0
+ACDC	2400	73	2	155,0
+ACDC	2400	73	2	217,0
+ACDC	2400	73	2	145,7
+ACDC	2400	74	2	112,0
+ACDC	2400	74	2	98,0
+ACDC	2400	75	2	201,6
+ACDC	2400	76	2	
+ACDC	2400	88	2	
+ACDC	2400	89	2	179,2
+ACDC	2400	90	2	77,2
+ACDC	2400	90	2	125,8
+ACDC	2400	90	2	246,4
+ACDC	2400	91	2	126,0
+ACDC	2400	91	2	114,0
+ACDC	2400	105	2	80,0
+ACDC	2400	106	2	
+ACDC	2400	109	2	189,7
+ACDC	2400	109	2	127,3
+ACDC	2400	110	2	
+ACDC	2400	112	2	
+ACDC	2400	113	2	
+ACDC	2400	122	2	
+ACDC	2400	123	2	
+ACDC	2400	125	2	
+ACDC	2400	126	2	189,7
+ACDC	2400	126	2	142,4
+ACDC	2400	126	2	60,6
+ACDC	2400	126	2	146,9
+ACDC	2400	127	2	61,2
+ACDC	2400	127	2	181,8
+ACDC	2400	127	2	127,3
+ACDC	2400	128	2	232,6
+ACDC	2400	129	2	
+ACDC	2400	130	2	
+ACDC	2400	141	2	
+ACDC	2400	142	2	
+ACDC	2400	143	2	174,4
+ACDC	2400	143	2	58,1
+ACDC	2400	143	2	101,0
+ACDC	2400	143	2	181,8
+ACDC	2400	143	2	78,8
+ACDC	2400	143	2	72,7
+ACDC	2400	144	2	45,9
+ACDC	2400	144	2	48,5
+ACDC	2400	145	2	224,0
+ACDC	2400	145	2	144,6
+ACDC	2400	146	2	140,0
+ACDC	2400	146	2	155,7
+ACDC	2400	146	2	50,4
+ACDC	2400	146	2	72,8
+ACDC	2400	147	2	
+ACDC	2400	148	2	167,3
+ACDC	2400	148	2	118,2
+ACDC	2400	148	2	96,4
+ACDC	2400	148	2	74,5
+ACDC	2400	148	2	236,0
+ACDC	2400	148	2	156,4
+ACDC	2400	157	2	116,1
+ACDC	2400	157	2	123,8
+ACDC	2400	158	2	
+ACDC	2400	159	2	
+ACDC	2400	160	2	148,4
+ACDC	2400	161	2	112,0
+ACDC	2400	161	2	97,3
+ACDC	2400	162	2	154,0
+ACDC	2400	162	2	136,2
+ACDC	2400	162	2	144,6
+ACDC	2400	162	2	196,0
+ACDC	2400	162	2	106,4
+ACDC	2400	162	2	140,0
+ACDC	2400	163	2	55,4
+ACDC	2400	163	2	180,1
+ACDC	2400	163	2	188,4
+ACDC	2400	164	2	80,3
+ACDC	2400	164	2	155,1
+ACDC	2400	164	2	138,5
+ACDC	2400	164	2	127,4
+ACDC	2400	164	2	74,8
+ACDC	2400	165	2	124,7
+ACDC	2400	165	2	166,2
+ACDC	2400	165	2	138,5
+ACDC	2400	165	2	110,8
+ACDC	2400	166	2	132,4
+ACDC	2400	166	2	217,0
+ACDC	2400	166	2	167,4
+ACDC	2400	174	2	
+ACDC	2400	175	2	82,9
+ACDC	2400	176	2	
+ACDC	2400	177	2	
+ACDC	2400	178	2	144,0
+ACDC	2400	178	2	196,7
+ACDC	2400	178	2	163,4
+ACDC	2400	178	2	108,0
+ACDC	2400	178	2	144,0
+ACDC	2400	178	2	138,5
+ACDC	2400	179	2	113,6
+ACDC	2400	179	2	166,2
+ACDC	2400	179	2	113,6
+ACDC	2400	179	2	49,9
+ACDC	2400	179	2	83,1
+ACDC	2400	179	2	94,2
+ACDC	2400	179	2	182,8
+ACDC	2400	179	2	41,5
+ACDC	2400	179	2	119,1
+ACDC	2400	179	2	166,2
+ACDC	2400	179	2	169,0
+ACDC	2400	180	2	169,4
+ACDC	2400	181	2	104,7
+ACDC	2400	182	2	120,5
+ACDC	2400	182	2	156,9
+ACDC	2400	182	2	70,3
+ACDC	2400	183	2	183,9
+ACDC	2400	183	2	75,7
+ACDC	2400	183	2	243,8
+ACDC	2400	183	2	243,8
+ACDC	2400	183	2	95,6
+ACDC	2400	183	2	106,4
+ACDC	2400	183	2	196,0
+ACDC	2400	183	2	120,8
+ACDC	2400	183	2	39,7
+ACDC	2400	184	2	228,0
+ACDC	2400	184	2	147,9
+ACDC	2400	184	2	184,0
+ACDC	2400	184	2	188,2
+ACDC	2400	184	2	158,9
+ACDC	2400	184	2	129,6
+ACDC	2400	184	2	154,4
+ACDC	2400	185	2	175,0
+ACDC	2400	185	2	87,7
+ACDC	2400	185	2	94,7
+ACDC	2400	185	2	114,0
+ACDC	2400	185	2	191,1
+ACDC	2400	192	2	
+ACDC	2400	193	2	
+ACDC	2400	194	2	127,3
+ACDC	2400	194	2	168,3
+ACDC	2400	194	2	236,3
+ACDC	2400	194	2	97,9
+ACDC	2400	195	2	153,0
+ACDC	2400	195	2	159,1
+ACDC	2400	195	2	72,7
+ACDC	2400	195	2	107,1
+ACDC	2400	196	2	122,4
+ACDC	2400	196	2	163,6
+ACDC	2400	197	2	85,7
+ACDC	2400	197	2	149,9
+ACDC	2400	198	2	45,5
+ACDC	2400	198	2	300,0
+ACDC	2400	198	2	110,2
+ACDC	2400	198	2	168,3
+ACDC	2400	199	2	121,2
+ACDC	2400	199	2	115,1
+ACDC	2400	199	2	145,4
+ACDC	2400	199	2	186,7
+ACDC	2400	199	2	73,4
+ACDC	2400	199	2	76,5
+ACDC	2400	200	2	76,5
+ACDC	2400	200	2	122,4
+ACDC	2400	200	2	121,2
+ACDC	2400	200	2	184,8
+ACDC	2400	201	2	102,0
+ACDC	2400	201	2	175,0
+ACDC	2400	201	2	180,0
+ACDC	2400	202	2	135,7
+ACDC	2400	202	2	105,8
+ACDC	2400	202	2	75,9
+ACDC	2400	202	2	138,0
+ACDC	2400	210	2	74,5
+ACDC	2400	210	2	89,4
+ACDC	2400	210	2	138,3
+ACDC	2400	210	2	87,2
+ACDC	2400	211	2	98,2
+ACDC	2400	211	2	27,8
+ACDC	2400	211	2	53,7
+ACDC	2400	211	2	100,0
+ACDC	2400	211	2	90,7
+ACDC	2400	211	2	70,4
+ACDC	2400	211	2	64,8
+ACDC	2400	212	2	105,5
+ACDC	2400	213	2	
+ACDC	2400	214	2	
+ACDC	2400	215	2	120,0
+ACDC	2400	215	2	96,0
+ACDC	2400	215	2	100,0
+ACDC	2400	215	2	96,0
+ACDC	2400	216	2	116,3
+ACDC	2400	216	2	107,8
+ACDC	2400	216	2	73,9
+ACDC	2400	218	2	
+ACDC	2400	219	2	84,6
+ACDC	2400	219	2	123,8
+ACDC	2400	228	2	118,4
+ACDC	2400	228	2	88,8
+ACDC	2400	228	2	61,1
+ACDC	2400	228	2	74,0
+ACDC	2400	228	2	51,8
+ACDC	2400	229	2	125,5
+ACDC	2400	229	2	123,2
+ACDC	2400	232	2	160,2
+ACDC	2400	232	2	61,2
+ACDC	2400	232	2	140,8
+ACDC	2400	232	2	137,7
+ACDC	2400	232	2	226,4
+ACDC	2400	232	2	61,6
+ACDC	2400	232	2	138,6
+ACDC	2400	233	2	52,0
+ACDC	2400	233	2	195,8
+ACDC	2400	233	2	138,6
+ACDC	2400	233	2	169,4
+ACDC	2400	233	2	83,2
+ACDC	2400	233	2	61,6
+ACDC	2400	245	2	96,3
+ACDC	2400	246	2	90,0
+ACDC	2400	247	2	
+ACDC	2400	250	2	84,0
+ACDC	2400	262	2	
+ACDC	2400	263	2	83,9
+ACDC	2400	263	2	117,5
+ACDC	2400	265	2	
+ACDC	2400	266	2	
+ACDC	2400	280	2	
+ACDC	2400	281	2	104,9
+ACDC	2400	281	2	100,7
+ACDC	2400	282	2	85,3
+ACDC	2400	282	2	193,1
+ACDC	2400	282	2	130,2
+ACDC	2400	283	2	
+ACDC	2400	284	2	
+ACDC	2400	299	2	
+ACDC	2400	300	2	
+ACDC	2400	301	2
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,257 +1,257 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	151.2
-ACDC	2400	23	2	184.2
-ACDC	2400	23	2	115.5
-ACDC	2400	39	2	63.5
-ACDC	2400	39	2	133.9
-ACDC	2400	39	2	47.0
-ACDC	2400	39	2	77.5
-ACDC	2400	40	2	88.4
-ACDC	2400	40	2	130.0
-ACDC	2400	40	2	187.2
-ACDC	2400	40	2	249.6
-ACDC	2400	41	2	
-ACDC	2400	42	2	
-ACDC	2400	55	2	
-ACDC	2400	56	2	55.6
-ACDC	2400	56	2	211.3
-ACDC	2400	57	2	
-ACDC	2400	58	2	119.5
-ACDC	2400	58	2	114.0
-ACDC	2400	58	2	202.9
-ACDC	2400	59	2	166.8
-ACDC	2400	60	2	55.6
-ACDC	2400	60	2	91.7
-ACDC	2400	72	2	
-ACDC	2400	73	2	93.0
-ACDC	2400	73	2	155.0
-ACDC	2400	73	2	217.0
-ACDC	2400	73	2	145.7
-ACDC	2400	74	2	112.0
-ACDC	2400	74	2	98.0
-ACDC	2400	75	2	201.6
-ACDC	2400	76	2	
-ACDC	2400	88	2	
-ACDC	2400	89	2	179.2
-ACDC	2400	90	2	77.2
-ACDC	2400	90	2	125.8
-ACDC	2400	90	2	246.4
-ACDC	2400	91	2	126.0
-ACDC	2400	91	2	114.0
-ACDC	2400	105	2	80.0
-ACDC	2400	106	2	
-ACDC	2400	109	2	189.7
-ACDC	2400	109	2	127.3
-ACDC	2400	110	2	
-ACDC	2400	112	2	
-ACDC	2400	113	2	
-ACDC	2400	122	2	
-ACDC	2400	123	2	
-ACDC	2400	125	2	
-ACDC	2400	126	2	189.7
-ACDC	2400	126	2	142.4
-ACDC	2400	126	2	60.6
-ACDC	2400	126	2	146.9
-ACDC	2400	127	2	61.2
-ACDC	2400	127	2	181.8
-ACDC	2400	127	2	127.3
-ACDC	2400	128	2	232.6
-ACDC	2400	129	2	
-ACDC	2400	130	2	
-ACDC	2400	141	2	
-ACDC	2400	142	2	
-ACDC	2400	143	2	174.4
-ACDC	2400	143	2	58.1
-ACDC	2400	143	2	101.0
-ACDC	2400	143	2	181.8
-ACDC	2400	143	2	78.8
-ACDC	2400	143	2	72.7
-ACDC	2400	144	2	45.9
-ACDC	2400	144	2	48.5
-ACDC	2400	145	2	224.0
-ACDC	2400	145	2	144.6
-ACDC	2400	146	2	140.0
-ACDC	2400	146	2	155.7
-ACDC	2400	146	2	50.4
-ACDC	2400	146	2	72.8
-ACDC	2400	147	2	
-ACDC	2400	148	2	167.3
-ACDC	2400	148	2	118.2
-ACDC	2400	148	2	96.4
-ACDC	2400	148	2	74.5
-ACDC	2400	148	2	236.0
-ACDC	2400	148	2	156.4
-ACDC	2400	157	2	116.1
-ACDC	2400	157	2	123.8
-ACDC	2400	158	2	
-ACDC	2400	159	2	
-ACDC	2400	160	2	148.4
-ACDC	2400	161	2	112.0
-ACDC	2400	161	2	97.3
-ACDC	2400	162	2	154.0
-ACDC	2400	162	2	136.2
-ACDC	2400	162	2	144.6
-ACDC	2400	162	2	196.0
-ACDC	2400	162	2	106.4
-ACDC	2400	162	2	140.0
-ACDC	2400	163	2	55.4
-ACDC	2400	163	2	180.1
-ACDC	2400	163	2	188.4
-ACDC	2400	164	2	80.3
-ACDC	2400	164	2	155.1
-ACDC	2400	164	2	138.5
-ACDC	2400	164	2	127.4
-ACDC	2400	164	2	74.8
-ACDC	2400	165	2	124.7
-ACDC	2400	165	2	166.2
-ACDC	2400	165	2	138.5
-ACDC	2400	165	2	110.8
-ACDC	2400	166	2	132.4
-ACDC	2400	166	2	217.0
-ACDC	2400	166	2	167.4
-ACDC	2400	174	2	
-ACDC	2400	175	2	82.9
-ACDC	2400	176	2	
-ACDC	2400	177	2	
-ACDC	2400	178	2	144.0
-ACDC	2400	178	2	196.7
-ACDC	2400	178	2	163.4
-ACDC	2400	178	2	108.0
-ACDC	2400	178	2	144.0
-ACDC	2400	178	2	138.5
-ACDC	2400	179	2	113.6
-ACDC	2400	179	2	166.2
-ACDC	2400	179	2	113.6
-ACDC	2400	179	2	49.9
-ACDC	2400	179	2	83.1
-ACDC	2400	179	2	94.2
-ACDC	2400	179	2	182.8
-ACDC	2400	179	2	41.5
-ACDC	2400	179	2	119.1
-ACDC	2400	179	2	166.2
-ACDC	2400	179	2	169.0
-ACDC	2400	180	2	169.4
-ACDC	2400	181	2	104.7
-ACDC	2400	182	2	120.5
-ACDC	2400	182	2	156.9
-ACDC	2400	182	2	70.3
-ACDC	2400	183	2	183.9
-ACDC	2400	183	2	75.7
-ACDC	2400	183	2	243.8
-ACDC	2400	183	2	243.8
-ACDC	2400	183	2	95.6
-ACDC	2400	183	2	106.4
-ACDC	2400	183	2	196.0
-ACDC	2400	183	2	120.8
-ACDC	2400	183	2	39.7
-ACDC	2400	184	2	228.0
-ACDC	2400	184	2	147.9
-ACDC	2400	184	2	184.0
-ACDC	2400	184	2	188.2
-ACDC	2400	184	2	158.9
-ACDC	2400	184	2	129.6
-ACDC	2400	184	2	154.4
-ACDC	2400	185	2	175.0
-ACDC	2400	185	2	87.7
-ACDC	2400	185	2	94.7
-ACDC	2400	185	2	114.0
-ACDC	2400	185	2	191.1
-ACDC	2400	192	2	
-ACDC	2400	193	2	
-ACDC	2400	194	2	127.3
-ACDC	2400	194	2	168.3
-ACDC	2400	194	2	236.3
-ACDC	2400	194	2	97.9
-ACDC	2400	195	2	153.0
-ACDC	2400	195	2	159.1
-ACDC	2400	195	2	72.7
-ACDC	2400	195	2	107.1
-ACDC	2400	196	2	122.4
-ACDC	2400	196	2	163.6
-ACDC	2400	197	2	85.7
-ACDC	2400	197	2	149.9
-ACDC	2400	198	2	45.5
-ACDC	2400	198	2	300.0
-ACDC	2400	198	2	110.2
-ACDC	2400	198	2	168.3
-ACDC	2400	199	2	121.2
-ACDC	2400	199	2	115.1
-ACDC	2400	199	2	145.4
-ACDC	2400	199	2	186.7
-ACDC	2400	199	2	73.4
-ACDC	2400	199	2	76.5
-ACDC	2400	200	2	76.5
-ACDC	2400	200	2	122.4
-ACDC	2400	200	2	121.2
-ACDC	2400	200	2	184.8
-ACDC	2400	201	2	102.0
-ACDC	2400	201	2	175.0
-ACDC	2400	201	2	180.0
-ACDC	2400	202	2	135.7
-ACDC	2400	202	2	105.8
-ACDC	2400	202	2	75.9
-ACDC	2400	202	2	138.0
-ACDC	2400	210	2	74.5
-ACDC	2400	210	2	89.4
-ACDC	2400	210	2	138.3
-ACDC	2400	210	2	87.2
-ACDC	2400	211	2	98.2
-ACDC	2400	211	2	27.8
-ACDC	2400	211	2	53.7
-ACDC	2400	211	2	100.0
-ACDC	2400	211	2	90.7
-ACDC	2400	211	2	70.4
-ACDC	2400	211	2	64.8
-ACDC	2400	212	2	105.5
-ACDC	2400	213	2	
-ACDC	2400	214	2	
-ACDC	2400	215	2	120.0
-ACDC	2400	215	2	96.0
-ACDC	2400	215	2	100.0
-ACDC	2400	215	2	96.0
-ACDC	2400	216	2	116.3
-ACDC	2400	216	2	107.8
-ACDC	2400	216	2	73.9
-ACDC	2400	218	2	
-ACDC	2400	219	2	84.6
-ACDC	2400	219	2	123.8
-ACDC	2400	228	2	118.4
-ACDC	2400	228	2	88.8
-ACDC	2400	228	2	61.1
-ACDC	2400	228	2	74.0
-ACDC	2400	228	2	51.8
-ACDC	2400	229	2	125.5
-ACDC	2400	229	2	123.2
-ACDC	2400	232	2	160.2
-ACDC	2400	232	2	61.2
-ACDC	2400	232	2	140.8
-ACDC	2400	232	2	137.7
-ACDC	2400	232	2	226.4
-ACDC	2400	232	2	61.6
-ACDC	2400	232	2	138.6
-ACDC	2400	233	2	52.0
-ACDC	2400	233	2	195.8
-ACDC	2400	233	2	138.6
-ACDC	2400	233	2	169.4
-ACDC	2400	233	2	83.2
-ACDC	2400	233	2	61.6
-ACDC	2400	245	2	96.3
-ACDC	2400	246	2	90.0
-ACDC	2400	247	2	
-ACDC	2400	250	2	84.0
-ACDC	2400	262	2	
-ACDC	2400	263	2	83.9
-ACDC	2400	263	2	117.5
-ACDC	2400	265	2	
-ACDC	2400	266	2	
-ACDC	2400	280	2	
-ACDC	2400	281	2	104.9
-ACDC	2400	281	2	100.7
-ACDC	2400	282	2	85.3
-ACDC	2400	282	2	193.1
-ACDC	2400	282	2	130.2
-ACDC	2400	283	2	
-ACDC	2400	284	2	
-ACDC	2400	299	2	
-ACDC	2400	300	2	
-ACDC	2400	301	2	
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	151.2
+ACDC	2400	23	2	184.2
+ACDC	2400	23	2	115.5
+ACDC	2400	39	2	63.5
+ACDC	2400	39	2	133.9
+ACDC	2400	39	2	47.0
+ACDC	2400	39	2	77.5
+ACDC	2400	40	2	88.4
+ACDC	2400	40	2	130.0
+ACDC	2400	40	2	187.2
+ACDC	2400	40	2	249.6
+ACDC	2400	41	2	
+ACDC	2400	42	2	
+ACDC	2400	55	2	
+ACDC	2400	56	2	55.6
+ACDC	2400	56	2	211.3
+ACDC	2400	57	2	
+ACDC	2400	58	2	119.5
+ACDC	2400	58	2	114.0
+ACDC	2400	58	2	202.9
+ACDC	2400	59	2	166.8
+ACDC	2400	60	2	55.6
+ACDC	2400	60	2	91.7
+ACDC	2400	72	2	
+ACDC	2400	73	2	93.0
+ACDC	2400	73	2	155.0
+ACDC	2400	73	2	217.0
+ACDC	2400	73	2	145.7
+ACDC	2400	74	2	112.0
+ACDC	2400	74	2	98.0
+ACDC	2400	75	2	201.6
+ACDC	2400	76	2	
+ACDC	2400	88	2	
+ACDC	2400	89	2	179.2
+ACDC	2400	90	2	77.2
+ACDC	2400	90	2	125.8
+ACDC	2400	90	2	246.4
+ACDC	2400	91	2	126.0
+ACDC	2400	91	2	114.0
+ACDC	2400	105	2	80.0
+ACDC	2400	106	2	
+ACDC	2400	109	2	189.7
+ACDC	2400	109	2	127.3
+ACDC	2400	110	2	
+ACDC	2400	112	2	
+ACDC	2400	113	2	
+ACDC	2400	122	2	
+ACDC	2400	123	2	
+ACDC	2400	125	2	
+ACDC	2400	126	2	189.7
+ACDC	2400	126	2	142.4
+ACDC	2400	126	2	60.6
+ACDC	2400	126	2	146.9
+ACDC	2400	127	2	61.2
+ACDC	2400	127	2	181.8
+ACDC	2400	127	2	127.3
+ACDC	2400	128	2	232.6
+ACDC	2400	129	2	
+ACDC	2400	130	2	
+ACDC	2400	141	2	
+ACDC	2400	142	2	
+ACDC	2400	143	2	174.4
+ACDC	2400	143	2	58.1
+ACDC	2400	143	2	101.0
+ACDC	2400	143	2	181.8
+ACDC	2400	143	2	78.8
+ACDC	2400	143	2	72.7
+ACDC	2400	144	2	45.9
+ACDC	2400	144	2	48.5
+ACDC	2400	145	2	224.0
+ACDC	2400	145	2	144.6
+ACDC	2400	146	2	140.0
+ACDC	2400	146	2	155.7
+ACDC	2400	146	2	50.4
+ACDC	2400	146	2	72.8
+ACDC	2400	147	2	
+ACDC	2400	148	2	167.3
+ACDC	2400	148	2	118.2
+ACDC	2400	148	2	96.4
+ACDC	2400	148	2	74.5
+ACDC	2400	148	2	236.0
+ACDC	2400	148	2	156.4
+ACDC	2400	157	2	116.1
+ACDC	2400	157	2	123.8
+ACDC	2400	158	2	
+ACDC	2400	159	2	
+ACDC	2400	160	2	148.4
+ACDC	2400	161	2	112.0
+ACDC	2400	161	2	97.3
+ACDC	2400	162	2	154.0
+ACDC	2400	162	2	136.2
+ACDC	2400	162	2	144.6
+ACDC	2400	162	2	196.0
+ACDC	2400	162	2	106.4
+ACDC	2400	162	2	140.0
+ACDC	2400	163	2	55.4
+ACDC	2400	163	2	180.1
+ACDC	2400	163	2	188.4
+ACDC	2400	164	2	80.3
+ACDC	2400	164	2	155.1
+ACDC	2400	164	2	138.5
+ACDC	2400	164	2	127.4
+ACDC	2400	164	2	74.8
+ACDC	2400	165	2	124.7
+ACDC	2400	165	2	166.2
+ACDC	2400	165	2	138.5
+ACDC	2400	165	2	110.8
+ACDC	2400	166	2	132.4
+ACDC	2400	166	2	217.0
+ACDC	2400	166	2	167.4
+ACDC	2400	174	2	
+ACDC	2400	175	2	82.9
+ACDC	2400	176	2	
+ACDC	2400	177	2	
+ACDC	2400	178	2	144.0
+ACDC	2400	178	2	196.7
+ACDC	2400	178	2	163.4
+ACDC	2400	178	2	108.0
+ACDC	2400	178	2	144.0
+ACDC	2400	178	2	138.5
+ACDC	2400	179	2	113.6
+ACDC	2400	179	2	166.2
+ACDC	2400	179	2	113.6
+ACDC	2400	179	2	49.9
+ACDC	2400	179	2	83.1
+ACDC	2400	179	2	94.2
+ACDC	2400	179	2	182.8
+ACDC	2400	179	2	41.5
+ACDC	2400	179	2	119.1
+ACDC	2400	179	2	166.2
+ACDC	2400	179	2	169.0
+ACDC	2400	180	2	169.4
+ACDC	2400	181	2	104.7
+ACDC	2400	182	2	120.5
+ACDC	2400	182	2	156.9
+ACDC	2400	182	2	70.3
+ACDC	2400	183	2	183.9
+ACDC	2400	183	2	75.7
+ACDC	2400	183	2	243.8
+ACDC	2400	183	2	243.8
+ACDC	2400	183	2	95.6
+ACDC	2400	183	2	106.4
+ACDC	2400	183	2	196.0
+ACDC	2400	183	2	120.8
+ACDC	2400	183	2	39.7
+ACDC	2400	184	2	228.0
+ACDC	2400	184	2	147.9
+ACDC	2400	184	2	184.0
+ACDC	2400	184	2	188.2
+ACDC	2400	184	2	158.9
+ACDC	2400	184	2	129.6
+ACDC	2400	184	2	154.4
+ACDC	2400	185	2	175.0
+ACDC	2400	185	2	87.7
+ACDC	2400	185	2	94.7
+ACDC	2400	185	2	114.0
+ACDC	2400	185	2	191.1
+ACDC	2400	192	2	
+ACDC	2400	193	2	
+ACDC	2400	194	2	127.3
+ACDC	2400	194	2	168.3
+ACDC	2400	194	2	236.3
+ACDC	2400	194	2	97.9
+ACDC	2400	195	2	153.0
+ACDC	2400	195	2	159.1
+ACDC	2400	195	2	72.7
+ACDC	2400	195	2	107.1
+ACDC	2400	196	2	122.4
+ACDC	2400	196	2	163.6
+ACDC	2400	197	2	85.7
+ACDC	2400	197	2	149.9
+ACDC	2400	198	2	45.5
+ACDC	2400	198	2	300.0
+ACDC	2400	198	2	110.2
+ACDC	2400	198	2	168.3
+ACDC	2400	199	2	121.2
+ACDC	2400	199	2	115.1
+ACDC	2400	199	2	145.4
+ACDC	2400	199	2	186.7
+ACDC	2400	199	2	73.4
+ACDC	2400	199	2	76.5
+ACDC	2400	200	2	76.5
+ACDC	2400	200	2	122.4
+ACDC	2400	200	2	121.2
+ACDC	2400	200	2	184.8
+ACDC	2400	201	2	102.0
+ACDC	2400	201	2	175.0
+ACDC	2400	201	2	180.0
+ACDC	2400	202	2	135.7
+ACDC	2400	202	2	105.8
+ACDC	2400	202	2	75.9
+ACDC	2400	202	2	138.0
+ACDC	2400	210	2	74.5
+ACDC	2400	210	2	89.4
+ACDC	2400	210	2	138.3
+ACDC	2400	210	2	87.2
+ACDC	2400	211	2	98.2
+ACDC	2400	211	2	27.8
+ACDC	2400	211	2	53.7
+ACDC	2400	211	2	100.0
+ACDC	2400	211	2	90.7
+ACDC	2400	211	2	70.4
+ACDC	2400	211	2	64.8
+ACDC	2400	212	2	105.5
+ACDC	2400	213	2	
+ACDC	2400	214	2	
+ACDC	2400	215	2	120.0
+ACDC	2400	215	2	96.0
+ACDC	2400	215	2	100.0
+ACDC	2400	215	2	96.0
+ACDC	2400	216	2	116.3
+ACDC	2400	216	2	107.8
+ACDC	2400	216	2	73.9
+ACDC	2400	218	2	
+ACDC	2400	219	2	84.6
+ACDC	2400	219	2	123.8
+ACDC	2400	228	2	118.4
+ACDC	2400	228	2	88.8
+ACDC	2400	228	2	61.1
+ACDC	2400	228	2	74.0
+ACDC	2400	228	2	51.8
+ACDC	2400	229	2	125.5
+ACDC	2400	229	2	123.2
+ACDC	2400	232	2	160.2
+ACDC	2400	232	2	61.2
+ACDC	2400	232	2	140.8
+ACDC	2400	232	2	137.7
+ACDC	2400	232	2	226.4
+ACDC	2400	232	2	61.6
+ACDC	2400	232	2	138.6
+ACDC	2400	233	2	52.0
+ACDC	2400	233	2	195.8
+ACDC	2400	233	2	138.6
+ACDC	2400	233	2	169.4
+ACDC	2400	233	2	83.2
+ACDC	2400	233	2	61.6
+ACDC	2400	245	2	96.3
+ACDC	2400	246	2	90.0
+ACDC	2400	247	2	
+ACDC	2400	250	2	84.0
+ACDC	2400	262	2	
+ACDC	2400	263	2	83.9
+ACDC	2400	263	2	117.5
+ACDC	2400	265	2	
+ACDC	2400	266	2	
+ACDC	2400	280	2	
+ACDC	2400	281	2	104.9
+ACDC	2400	281	2	100.7
+ACDC	2400	282	2	85.3
+ACDC	2400	282	2	193.1
+ACDC	2400	282	2	130.2
+ACDC	2400	283	2	
+ACDC	2400	284	2	
+ACDC	2400	299	2	
+ACDC	2400	300	2	
+ACDC	2400	301	2
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-6dec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-6dec-dist.txt`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,257 +1,257 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	151,250000
-ACDC	2400	23	2	184,250000
-ACDC	2400	23	2	115,500000
-ACDC	2400	39	2	63,450000
-ACDC	2400	39	2	133,950000
-ACDC	2400	39	2	47,000000
-ACDC	2400	39	2	77,550000
-ACDC	2400	40	2	88,400000
-ACDC	2400	40	2	130,000000
-ACDC	2400	40	2	187,200000
-ACDC	2400	40	2	249,600000
-ACDC	2400	41	2	
-ACDC	2400	42	2	
-ACDC	2400	55	2	
-ACDC	2400	56	2	55,600000
-ACDC	2400	56	2	211,280000
-ACDC	2400	57	2	
-ACDC	2400	58	2	119,540000
-ACDC	2400	58	2	113,980000
-ACDC	2400	58	2	202,940000
-ACDC	2400	59	2	166,800000
-ACDC	2400	60	2	55,600000
-ACDC	2400	60	2	91,740000
-ACDC	2400	72	2	
-ACDC	2400	73	2	93,000000
-ACDC	2400	73	2	155,000000
-ACDC	2400	73	2	217,000000
-ACDC	2400	73	2	145,700000
-ACDC	2400	74	2	112,000000
-ACDC	2400	74	2	98,000000
-ACDC	2400	75	2	201,600000
-ACDC	2400	76	2	
-ACDC	2400	88	2	
-ACDC	2400	89	2	179,200000
-ACDC	2400	90	2	77,220000
-ACDC	2400	90	2	125,840000
-ACDC	2400	90	2	246,400000
-ACDC	2400	91	2	126,000000
-ACDC	2400	91	2	114,000000
-ACDC	2400	105	2	80,000000
-ACDC	2400	106	2	
-ACDC	2400	109	2	189,720000
-ACDC	2400	109	2	127,260000
-ACDC	2400	110	2	
-ACDC	2400	112	2	
-ACDC	2400	113	2	
-ACDC	2400	122	2	
-ACDC	2400	123	2	
-ACDC	2400	125	2	
-ACDC	2400	126	2	189,720000
-ACDC	2400	126	2	142,410000
-ACDC	2400	126	2	60,600000
-ACDC	2400	126	2	146,880000
-ACDC	2400	127	2	61,200000
-ACDC	2400	127	2	181,800000
-ACDC	2400	127	2	127,260000
-ACDC	2400	128	2	232,560000
-ACDC	2400	129	2	
-ACDC	2400	130	2	
-ACDC	2400	141	2	
-ACDC	2400	142	2	
-ACDC	2400	143	2	174,420000
-ACDC	2400	143	2	58,140000
-ACDC	2400	143	2	100,980000
-ACDC	2400	143	2	181,800000
-ACDC	2400	143	2	78,780000
-ACDC	2400	143	2	72,720000
-ACDC	2400	144	2	45,900000
-ACDC	2400	144	2	48,480000
-ACDC	2400	145	2	224,000000
-ACDC	2400	145	2	144,560000
-ACDC	2400	146	2	140,000000
-ACDC	2400	146	2	155,680000
-ACDC	2400	146	2	50,400000
-ACDC	2400	146	2	72,800000
-ACDC	2400	147	2	
-ACDC	2400	148	2	167,272727
-ACDC	2400	148	2	118,181818
-ACDC	2400	148	2	96,363636
-ACDC	2400	148	2	74,545455
-ACDC	2400	148	2	236,000000
-ACDC	2400	148	2	156,363636
-ACDC	2400	157	2	116,100000
-ACDC	2400	157	2	123,840000
-ACDC	2400	158	2	
-ACDC	2400	159	2	
-ACDC	2400	160	2	148,400000
-ACDC	2400	161	2	112,000000
-ACDC	2400	161	2	97,300000
-ACDC	2400	162	2	154,000000
-ACDC	2400	162	2	136,220000
-ACDC	2400	162	2	144,560000
-ACDC	2400	162	2	196,000000
-ACDC	2400	162	2	106,400000
-ACDC	2400	162	2	140,000000
-ACDC	2400	163	2	55,400000
-ACDC	2400	163	2	180,050000
-ACDC	2400	163	2	188,360000
-ACDC	2400	164	2	80,330000
-ACDC	2400	164	2	155,120000
-ACDC	2400	164	2	138,500000
-ACDC	2400	164	2	127,420000
-ACDC	2400	164	2	74,790000
-ACDC	2400	165	2	124,650000
-ACDC	2400	165	2	166,200000
-ACDC	2400	165	2	138,500000
-ACDC	2400	165	2	110,800000
-ACDC	2400	166	2	132,428571
-ACDC	2400	166	2	217,035714
-ACDC	2400	166	2	167,375000
-ACDC	2400	174	2	
-ACDC	2400	175	2	82,880000
-ACDC	2400	176	2	
-ACDC	2400	177	2	
-ACDC	2400	178	2	144,040000
-ACDC	2400	178	2	196,670000
-ACDC	2400	178	2	163,430000
-ACDC	2400	178	2	108,030000
-ACDC	2400	178	2	144,040000
-ACDC	2400	178	2	138,500000
-ACDC	2400	179	2	113,570000
-ACDC	2400	179	2	166,200000
-ACDC	2400	179	2	113,570000
-ACDC	2400	179	2	49,860000
-ACDC	2400	179	2	83,100000
-ACDC	2400	179	2	94,180000
-ACDC	2400	179	2	182,820000
-ACDC	2400	179	2	41,550000
-ACDC	2400	179	2	119,110000
-ACDC	2400	179	2	166,200000
-ACDC	2400	179	2	168,970000
-ACDC	2400	180	2	169,400000
-ACDC	2400	181	2	104,720000
-ACDC	2400	182	2	120,491228
-ACDC	2400	182	2	156,910714
-ACDC	2400	182	2	70,339286
-ACDC	2400	183	2	183,906000
-ACDC	2400	183	2	75,726000
-ACDC	2400	183	2	243,750000
-ACDC	2400	183	2	243,750000
-ACDC	2400	183	2	95,589286
-ACDC	2400	183	2	106,377000
-ACDC	2400	183	2	196,000000
-ACDC	2400	183	2	120,801000
-ACDC	2400	183	2	39,678571
-ACDC	2400	184	2	228,000000
-ACDC	2400	184	2	147,857143
-ACDC	2400	184	2	184,000000
-ACDC	2400	184	2	188,181818
-ACDC	2400	184	2	158,909091
-ACDC	2400	184	2	129,636364
-ACDC	2400	184	2	154,428571
-ACDC	2400	185	2	175,000000
-ACDC	2400	185	2	87,719298
-ACDC	2400	185	2	94,736842
-ACDC	2400	185	2	114,035088
-ACDC	2400	185	2	191,071429
-ACDC	2400	192	2	
-ACDC	2400	193	2	
-ACDC	2400	194	2	127,260000
-ACDC	2400	194	2	168,300000
-ACDC	2400	194	2	236,340000
-ACDC	2400	194	2	97,920000
-ACDC	2400	195	2	153,000000
-ACDC	2400	195	2	159,120000
-ACDC	2400	195	2	72,720000
-ACDC	2400	195	2	107,100000
-ACDC	2400	196	2	122,400000
-ACDC	2400	196	2	163,620000
-ACDC	2400	197	2	85,680000
-ACDC	2400	197	2	149,940000
-ACDC	2400	198	2	45,450000
-ACDC	2400	198	2	299,970000
-ACDC	2400	198	2	110,160000
-ACDC	2400	198	2	168,300000
-ACDC	2400	199	2	121,200000
-ACDC	2400	199	2	115,140000
-ACDC	2400	199	2	145,440000
-ACDC	2400	199	2	186,660000
-ACDC	2400	199	2	73,440000
-ACDC	2400	199	2	76,500000
-ACDC	2400	200	2	76,500000
-ACDC	2400	200	2	122,400000
-ACDC	2400	200	2	121,200000
-ACDC	2400	200	2	184,830000
-ACDC	2400	201	2	102,000000
-ACDC	2400	201	2	175,000000
-ACDC	2400	201	2	180,000000
-ACDC	2400	202	2	135,700000
-ACDC	2400	202	2	105,750000
-ACDC	2400	202	2	75,900000
-ACDC	2400	202	2	138,000000
-ACDC	2400	210	2	74,480000
-ACDC	2400	210	2	89,376000
-ACDC	2400	210	2	138,320000
-ACDC	2400	210	2	87,248000
-ACDC	2400	211	2	98,156000
-ACDC	2400	211	2	27,765000
-ACDC	2400	211	2	53,679000
-ACDC	2400	211	2	100,008000
-ACDC	2400	211	2	90,748000
-ACDC	2400	211	2	70,376000
-ACDC	2400	211	2	64,785000
-ACDC	2400	212	2	105,507000
-ACDC	2400	213	2	
-ACDC	2400	214	2	
-ACDC	2400	215	2	120,000000
-ACDC	2400	215	2	96,000000
-ACDC	2400	215	2	100,000000
-ACDC	2400	215	2	96,000000
-ACDC	2400	216	2	116,280000
-ACDC	2400	216	2	107,800000
-ACDC	2400	216	2	73,920000
-ACDC	2400	218	2	
-ACDC	2400	219	2	84,560000
-ACDC	2400	219	2	123,820000
-ACDC	2400	228	2	118,400000
-ACDC	2400	228	2	88,800000
-ACDC	2400	228	2	61,083000
-ACDC	2400	228	2	74,040000
-ACDC	2400	228	2	51,800000
-ACDC	2400	229	2	125,460000
-ACDC	2400	229	2	123,200000
-ACDC	2400	232	2	160,160000
-ACDC	2400	232	2	61,200000
-ACDC	2400	232	2	140,760000
-ACDC	2400	232	2	137,700000
-ACDC	2400	232	2	226,440000
-ACDC	2400	232	2	61,600000
-ACDC	2400	232	2	138,600000
-ACDC	2400	233	2	52,020000
-ACDC	2400	233	2	195,840000
-ACDC	2400	233	2	138,600000
-ACDC	2400	233	2	169,400000
-ACDC	2400	233	2	83,160000
-ACDC	2400	233	2	61,600000
-ACDC	2400	245	2	96,252000
-ACDC	2400	246	2	90,000000
-ACDC	2400	247	2	
-ACDC	2400	250	2	84,000000
-ACDC	2400	262	2	
-ACDC	2400	263	2	83,934426
-ACDC	2400	263	2	117,508197
-ACDC	2400	265	2	
-ACDC	2400	266	2	
-ACDC	2400	280	2	
-ACDC	2400	281	2	104,918033
-ACDC	2400	281	2	100,721311
-ACDC	2400	282	2	85,333333
-ACDC	2400	282	2	193,122807
-ACDC	2400	282	2	130,245614
-ACDC	2400	283	2	
-ACDC	2400	284	2	
-ACDC	2400	299	2	
-ACDC	2400	300	2	
-ACDC	2400	301	2	
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	151,250000
+ACDC	2400	23	2	184,250000
+ACDC	2400	23	2	115,500000
+ACDC	2400	39	2	63,450000
+ACDC	2400	39	2	133,950000
+ACDC	2400	39	2	47,000000
+ACDC	2400	39	2	77,550000
+ACDC	2400	40	2	88,400000
+ACDC	2400	40	2	130,000000
+ACDC	2400	40	2	187,200000
+ACDC	2400	40	2	249,600000
+ACDC	2400	41	2	
+ACDC	2400	42	2	
+ACDC	2400	55	2	
+ACDC	2400	56	2	55,600000
+ACDC	2400	56	2	211,280000
+ACDC	2400	57	2	
+ACDC	2400	58	2	119,540000
+ACDC	2400	58	2	113,980000
+ACDC	2400	58	2	202,940000
+ACDC	2400	59	2	166,800000
+ACDC	2400	60	2	55,600000
+ACDC	2400	60	2	91,740000
+ACDC	2400	72	2	
+ACDC	2400	73	2	93,000000
+ACDC	2400	73	2	155,000000
+ACDC	2400	73	2	217,000000
+ACDC	2400	73	2	145,700000
+ACDC	2400	74	2	112,000000
+ACDC	2400	74	2	98,000000
+ACDC	2400	75	2	201,600000
+ACDC	2400	76	2	
+ACDC	2400	88	2	
+ACDC	2400	89	2	179,200000
+ACDC	2400	90	2	77,220000
+ACDC	2400	90	2	125,840000
+ACDC	2400	90	2	246,400000
+ACDC	2400	91	2	126,000000
+ACDC	2400	91	2	114,000000
+ACDC	2400	105	2	80,000000
+ACDC	2400	106	2	
+ACDC	2400	109	2	189,720000
+ACDC	2400	109	2	127,260000
+ACDC	2400	110	2	
+ACDC	2400	112	2	
+ACDC	2400	113	2	
+ACDC	2400	122	2	
+ACDC	2400	123	2	
+ACDC	2400	125	2	
+ACDC	2400	126	2	189,720000
+ACDC	2400	126	2	142,410000
+ACDC	2400	126	2	60,600000
+ACDC	2400	126	2	146,880000
+ACDC	2400	127	2	61,200000
+ACDC	2400	127	2	181,800000
+ACDC	2400	127	2	127,260000
+ACDC	2400	128	2	232,560000
+ACDC	2400	129	2	
+ACDC	2400	130	2	
+ACDC	2400	141	2	
+ACDC	2400	142	2	
+ACDC	2400	143	2	174,420000
+ACDC	2400	143	2	58,140000
+ACDC	2400	143	2	100,980000
+ACDC	2400	143	2	181,800000
+ACDC	2400	143	2	78,780000
+ACDC	2400	143	2	72,720000
+ACDC	2400	144	2	45,900000
+ACDC	2400	144	2	48,480000
+ACDC	2400	145	2	224,000000
+ACDC	2400	145	2	144,560000
+ACDC	2400	146	2	140,000000
+ACDC	2400	146	2	155,680000
+ACDC	2400	146	2	50,400000
+ACDC	2400	146	2	72,800000
+ACDC	2400	147	2	
+ACDC	2400	148	2	167,272727
+ACDC	2400	148	2	118,181818
+ACDC	2400	148	2	96,363636
+ACDC	2400	148	2	74,545455
+ACDC	2400	148	2	236,000000
+ACDC	2400	148	2	156,363636
+ACDC	2400	157	2	116,100000
+ACDC	2400	157	2	123,840000
+ACDC	2400	158	2	
+ACDC	2400	159	2	
+ACDC	2400	160	2	148,400000
+ACDC	2400	161	2	112,000000
+ACDC	2400	161	2	97,300000
+ACDC	2400	162	2	154,000000
+ACDC	2400	162	2	136,220000
+ACDC	2400	162	2	144,560000
+ACDC	2400	162	2	196,000000
+ACDC	2400	162	2	106,400000
+ACDC	2400	162	2	140,000000
+ACDC	2400	163	2	55,400000
+ACDC	2400	163	2	180,050000
+ACDC	2400	163	2	188,360000
+ACDC	2400	164	2	80,330000
+ACDC	2400	164	2	155,120000
+ACDC	2400	164	2	138,500000
+ACDC	2400	164	2	127,420000
+ACDC	2400	164	2	74,790000
+ACDC	2400	165	2	124,650000
+ACDC	2400	165	2	166,200000
+ACDC	2400	165	2	138,500000
+ACDC	2400	165	2	110,800000
+ACDC	2400	166	2	132,428571
+ACDC	2400	166	2	217,035714
+ACDC	2400	166	2	167,375000
+ACDC	2400	174	2	
+ACDC	2400	175	2	82,880000
+ACDC	2400	176	2	
+ACDC	2400	177	2	
+ACDC	2400	178	2	144,040000
+ACDC	2400	178	2	196,670000
+ACDC	2400	178	2	163,430000
+ACDC	2400	178	2	108,030000
+ACDC	2400	178	2	144,040000
+ACDC	2400	178	2	138,500000
+ACDC	2400	179	2	113,570000
+ACDC	2400	179	2	166,200000
+ACDC	2400	179	2	113,570000
+ACDC	2400	179	2	49,860000
+ACDC	2400	179	2	83,100000
+ACDC	2400	179	2	94,180000
+ACDC	2400	179	2	182,820000
+ACDC	2400	179	2	41,550000
+ACDC	2400	179	2	119,110000
+ACDC	2400	179	2	166,200000
+ACDC	2400	179	2	168,970000
+ACDC	2400	180	2	169,400000
+ACDC	2400	181	2	104,720000
+ACDC	2400	182	2	120,491228
+ACDC	2400	182	2	156,910714
+ACDC	2400	182	2	70,339286
+ACDC	2400	183	2	183,906000
+ACDC	2400	183	2	75,726000
+ACDC	2400	183	2	243,750000
+ACDC	2400	183	2	243,750000
+ACDC	2400	183	2	95,589286
+ACDC	2400	183	2	106,377000
+ACDC	2400	183	2	196,000000
+ACDC	2400	183	2	120,801000
+ACDC	2400	183	2	39,678571
+ACDC	2400	184	2	228,000000
+ACDC	2400	184	2	147,857143
+ACDC	2400	184	2	184,000000
+ACDC	2400	184	2	188,181818
+ACDC	2400	184	2	158,909091
+ACDC	2400	184	2	129,636364
+ACDC	2400	184	2	154,428571
+ACDC	2400	185	2	175,000000
+ACDC	2400	185	2	87,719298
+ACDC	2400	185	2	94,736842
+ACDC	2400	185	2	114,035088
+ACDC	2400	185	2	191,071429
+ACDC	2400	192	2	
+ACDC	2400	193	2	
+ACDC	2400	194	2	127,260000
+ACDC	2400	194	2	168,300000
+ACDC	2400	194	2	236,340000
+ACDC	2400	194	2	97,920000
+ACDC	2400	195	2	153,000000
+ACDC	2400	195	2	159,120000
+ACDC	2400	195	2	72,720000
+ACDC	2400	195	2	107,100000
+ACDC	2400	196	2	122,400000
+ACDC	2400	196	2	163,620000
+ACDC	2400	197	2	85,680000
+ACDC	2400	197	2	149,940000
+ACDC	2400	198	2	45,450000
+ACDC	2400	198	2	299,970000
+ACDC	2400	198	2	110,160000
+ACDC	2400	198	2	168,300000
+ACDC	2400	199	2	121,200000
+ACDC	2400	199	2	115,140000
+ACDC	2400	199	2	145,440000
+ACDC	2400	199	2	186,660000
+ACDC	2400	199	2	73,440000
+ACDC	2400	199	2	76,500000
+ACDC	2400	200	2	76,500000
+ACDC	2400	200	2	122,400000
+ACDC	2400	200	2	121,200000
+ACDC	2400	200	2	184,830000
+ACDC	2400	201	2	102,000000
+ACDC	2400	201	2	175,000000
+ACDC	2400	201	2	180,000000
+ACDC	2400	202	2	135,700000
+ACDC	2400	202	2	105,750000
+ACDC	2400	202	2	75,900000
+ACDC	2400	202	2	138,000000
+ACDC	2400	210	2	74,480000
+ACDC	2400	210	2	89,376000
+ACDC	2400	210	2	138,320000
+ACDC	2400	210	2	87,248000
+ACDC	2400	211	2	98,156000
+ACDC	2400	211	2	27,765000
+ACDC	2400	211	2	53,679000
+ACDC	2400	211	2	100,008000
+ACDC	2400	211	2	90,748000
+ACDC	2400	211	2	70,376000
+ACDC	2400	211	2	64,785000
+ACDC	2400	212	2	105,507000
+ACDC	2400	213	2	
+ACDC	2400	214	2	
+ACDC	2400	215	2	120,000000
+ACDC	2400	215	2	96,000000
+ACDC	2400	215	2	100,000000
+ACDC	2400	215	2	96,000000
+ACDC	2400	216	2	116,280000
+ACDC	2400	216	2	107,800000
+ACDC	2400	216	2	73,920000
+ACDC	2400	218	2	
+ACDC	2400	219	2	84,560000
+ACDC	2400	219	2	123,820000
+ACDC	2400	228	2	118,400000
+ACDC	2400	228	2	88,800000
+ACDC	2400	228	2	61,083000
+ACDC	2400	228	2	74,040000
+ACDC	2400	228	2	51,800000
+ACDC	2400	229	2	125,460000
+ACDC	2400	229	2	123,200000
+ACDC	2400	232	2	160,160000
+ACDC	2400	232	2	61,200000
+ACDC	2400	232	2	140,760000
+ACDC	2400	232	2	137,700000
+ACDC	2400	232	2	226,440000
+ACDC	2400	232	2	61,600000
+ACDC	2400	232	2	138,600000
+ACDC	2400	233	2	52,020000
+ACDC	2400	233	2	195,840000
+ACDC	2400	233	2	138,600000
+ACDC	2400	233	2	169,400000
+ACDC	2400	233	2	83,160000
+ACDC	2400	233	2	61,600000
+ACDC	2400	245	2	96,252000
+ACDC	2400	246	2	90,000000
+ACDC	2400	247	2	
+ACDC	2400	250	2	84,000000
+ACDC	2400	262	2	
+ACDC	2400	263	2	83,934426
+ACDC	2400	263	2	117,508197
+ACDC	2400	265	2	
+ACDC	2400	266	2	
+ACDC	2400	280	2	
+ACDC	2400	281	2	104,918033
+ACDC	2400	281	2	100,721311
+ACDC	2400	282	2	85,333333
+ACDC	2400	282	2	193,122807
+ACDC	2400	282	2	130,245614
+ACDC	2400	283	2	
+ACDC	2400	284	2	
+ACDC	2400	299	2	
+ACDC	2400	300	2	
+ACDC	2400	301	2
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-ttdec-dist.txt` & `pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-saisie-5-cols.txt`

 * *Files 15% similar despite different names*

```diff
@@ -1,257 +1,257 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	151,25
-ACDC	2400	23	2	184,25
-ACDC	2400	23	2	115,5
-ACDC	2400	39	2	63,45
-ACDC	2400	39	2	133,95
-ACDC	2400	39	2	47,0
-ACDC	2400	39	2	77,55
-ACDC	2400	40	2	88,4
-ACDC	2400	40	2	130,0
-ACDC	2400	40	2	187,2
-ACDC	2400	40	2	249,60000000000002
-ACDC	2400	41	2	
-ACDC	2400	42	2	
-ACDC	2400	55	2	
-ACDC	2400	56	2	55,6
-ACDC	2400	56	2	211,27999999999997
-ACDC	2400	57	2	
-ACDC	2400	58	2	119,54
-ACDC	2400	58	2	113,98
-ACDC	2400	58	2	202,94
-ACDC	2400	59	2	166,8
-ACDC	2400	60	2	55,6
-ACDC	2400	60	2	91,74
-ACDC	2400	72	2	
-ACDC	2400	73	2	93,0
-ACDC	2400	73	2	155,0
-ACDC	2400	73	2	217,0
-ACDC	2400	73	2	145,7
-ACDC	2400	74	2	112,0
-ACDC	2400	74	2	98,0
-ACDC	2400	75	2	201,6
-ACDC	2400	76	2	
-ACDC	2400	88	2	
-ACDC	2400	89	2	179,2
-ACDC	2400	90	2	77,22
-ACDC	2400	90	2	125,83999999999999
-ACDC	2400	90	2	246,39999999999998
-ACDC	2400	91	2	126,0
-ACDC	2400	91	2	114,0
-ACDC	2400	105	2	80,0
-ACDC	2400	106	2	
-ACDC	2400	109	2	189,72
-ACDC	2400	109	2	127,26
-ACDC	2400	110	2	
-ACDC	2400	112	2	
-ACDC	2400	113	2	
-ACDC	2400	122	2	
-ACDC	2400	123	2	
-ACDC	2400	125	2	
-ACDC	2400	126	2	189,72
-ACDC	2400	126	2	142,41
-ACDC	2400	126	2	60,6
-ACDC	2400	126	2	146,88
-ACDC	2400	127	2	61,2
-ACDC	2400	127	2	181,8
-ACDC	2400	127	2	127,26
-ACDC	2400	128	2	232,56
-ACDC	2400	129	2	
-ACDC	2400	130	2	
-ACDC	2400	141	2	
-ACDC	2400	142	2	
-ACDC	2400	143	2	174,42
-ACDC	2400	143	2	58,14
-ACDC	2400	143	2	100,98
-ACDC	2400	143	2	181,8
-ACDC	2400	143	2	78,78
-ACDC	2400	143	2	72,72
-ACDC	2400	144	2	45,9
-ACDC	2400	144	2	48,48
-ACDC	2400	145	2	224,0
-ACDC	2400	145	2	144,56
-ACDC	2400	146	2	140,0
-ACDC	2400	146	2	155,68
-ACDC	2400	146	2	50,4
-ACDC	2400	146	2	72,8
-ACDC	2400	147	2	
-ACDC	2400	148	2	167,27272727272728
-ACDC	2400	148	2	118,18181818181817
-ACDC	2400	148	2	96,36363636363636
-ACDC	2400	148	2	74,54545454545455
-ACDC	2400	148	2	236,0
-ACDC	2400	148	2	156,36363636363635
-ACDC	2400	157	2	116,1
-ACDC	2400	157	2	123,84
-ACDC	2400	158	2	
-ACDC	2400	159	2	
-ACDC	2400	160	2	148,39999999999998
-ACDC	2400	161	2	112,0
-ACDC	2400	161	2	97,3
-ACDC	2400	162	2	154,0
-ACDC	2400	162	2	136,22
-ACDC	2400	162	2	144,56
-ACDC	2400	162	2	196,0
-ACDC	2400	162	2	106,4
-ACDC	2400	162	2	140,0
-ACDC	2400	163	2	55,4
-ACDC	2400	163	2	180,05
-ACDC	2400	163	2	188,36
-ACDC	2400	164	2	80,33
-ACDC	2400	164	2	155,12
-ACDC	2400	164	2	138,5
-ACDC	2400	164	2	127,42
-ACDC	2400	164	2	74,79
-ACDC	2400	165	2	124,65
-ACDC	2400	165	2	166,2
-ACDC	2400	165	2	138,5
-ACDC	2400	165	2	110,8
-ACDC	2400	166	2	132,42857142857142
-ACDC	2400	166	2	217,03571428571428
-ACDC	2400	166	2	167,375
-ACDC	2400	174	2	
-ACDC	2400	175	2	82,88
-ACDC	2400	176	2	
-ACDC	2400	177	2	
-ACDC	2400	178	2	144,04
-ACDC	2400	178	2	196,67
-ACDC	2400	178	2	163,43
-ACDC	2400	178	2	108,03
-ACDC	2400	178	2	144,04
-ACDC	2400	178	2	138,5
-ACDC	2400	179	2	113,57
-ACDC	2400	179	2	166,2
-ACDC	2400	179	2	113,57
-ACDC	2400	179	2	49,86
-ACDC	2400	179	2	83,1
-ACDC	2400	179	2	94,18
-ACDC	2400	179	2	182,82
-ACDC	2400	179	2	41,55
-ACDC	2400	179	2	119,11
-ACDC	2400	179	2	166,2
-ACDC	2400	179	2	168,97
-ACDC	2400	180	2	169,4
-ACDC	2400	181	2	104,72
-ACDC	2400	182	2	120,49122807017544
-ACDC	2400	182	2	156,91071428571428
-ACDC	2400	182	2	70,33928571428572
-ACDC	2400	183	2	183,906
-ACDC	2400	183	2	75,726
-ACDC	2400	183	2	243,75
-ACDC	2400	183	2	243,75
-ACDC	2400	183	2	95,58928571428572
-ACDC	2400	183	2	106,377
-ACDC	2400	183	2	196,0
-ACDC	2400	183	2	120,801
-ACDC	2400	183	2	39,67857142857143
-ACDC	2400	184	2	228,0
-ACDC	2400	184	2	147,85714285714286
-ACDC	2400	184	2	184,0
-ACDC	2400	184	2	188,1818181818182
-ACDC	2400	184	2	158,9090909090909
-ACDC	2400	184	2	129,63636363636363
-ACDC	2400	184	2	154,42857142857142
-ACDC	2400	185	2	175,0
-ACDC	2400	185	2	87,71929824561403
-ACDC	2400	185	2	94,73684210526315
-ACDC	2400	185	2	114,03508771929823
-ACDC	2400	185	2	191,07142857142858
-ACDC	2400	192	2	
-ACDC	2400	193	2	
-ACDC	2400	194	2	127,26
-ACDC	2400	194	2	168,3
-ACDC	2400	194	2	236,33999999999997
-ACDC	2400	194	2	97,92
-ACDC	2400	195	2	153,0
-ACDC	2400	195	2	159,12
-ACDC	2400	195	2	72,72
-ACDC	2400	195	2	107,1
-ACDC	2400	196	2	122,4
-ACDC	2400	196	2	163,61999999999998
-ACDC	2400	197	2	85,68
-ACDC	2400	197	2	149,94
-ACDC	2400	198	2	45,45
-ACDC	2400	198	2	299,96999999999997
-ACDC	2400	198	2	110,16
-ACDC	2400	198	2	168,3
-ACDC	2400	199	2	121,19999999999999
-ACDC	2400	199	2	115,13999999999999
-ACDC	2400	199	2	145,44
-ACDC	2400	199	2	186,66
-ACDC	2400	199	2	73,44
-ACDC	2400	199	2	76,5
-ACDC	2400	200	2	76,5
-ACDC	2400	200	2	122,4
-ACDC	2400	200	2	121,19999999999999
-ACDC	2400	200	2	184,83
-ACDC	2400	201	2	102,0
-ACDC	2400	201	2	175,0
-ACDC	2400	201	2	180,0
-ACDC	2400	202	2	135,7
-ACDC	2400	202	2	105,75
-ACDC	2400	202	2	75,89999999999999
-ACDC	2400	202	2	138,0
-ACDC	2400	210	2	74,48
-ACDC	2400	210	2	89,376
-ACDC	2400	210	2	138,32
-ACDC	2400	210	2	87,248
-ACDC	2400	211	2	98,156
-ACDC	2400	211	2	27,765
-ACDC	2400	211	2	53,679
-ACDC	2400	211	2	100,00800000000001
-ACDC	2400	211	2	90,748
-ACDC	2400	211	2	70,376
-ACDC	2400	211	2	64,785
-ACDC	2400	212	2	105,507
-ACDC	2400	213	2	
-ACDC	2400	214	2	
-ACDC	2400	215	2	120,0
-ACDC	2400	215	2	96,0
-ACDC	2400	215	2	100,0
-ACDC	2400	215	2	96,0
-ACDC	2400	216	2	116,28
-ACDC	2400	216	2	107,8
-ACDC	2400	216	2	73,92
-ACDC	2400	218	2	
-ACDC	2400	219	2	84,56
-ACDC	2400	219	2	123,82
-ACDC	2400	228	2	118,4
-ACDC	2400	228	2	88,80000000000001
-ACDC	2400	228	2	61,083
-ACDC	2400	228	2	74,04
-ACDC	2400	228	2	51,8
-ACDC	2400	229	2	125,46
-ACDC	2400	229	2	123,2
-ACDC	2400	232	2	160,16
-ACDC	2400	232	2	61,2
-ACDC	2400	232	2	140,76
-ACDC	2400	232	2	137,7
-ACDC	2400	232	2	226,44
-ACDC	2400	232	2	61,6
-ACDC	2400	232	2	138,6
-ACDC	2400	233	2	52,02
-ACDC	2400	233	2	195,84
-ACDC	2400	233	2	138,6
-ACDC	2400	233	2	169,4
-ACDC	2400	233	2	83,16
-ACDC	2400	233	2	61,6
-ACDC	2400	245	2	96,252
-ACDC	2400	246	2	90,0
-ACDC	2400	247	2	
-ACDC	2400	250	2	84,0
-ACDC	2400	262	2	
-ACDC	2400	263	2	83,9344262295082
-ACDC	2400	263	2	117,50819672131148
-ACDC	2400	265	2	
-ACDC	2400	266	2	
-ACDC	2400	280	2	
-ACDC	2400	281	2	104,91803278688525
-ACDC	2400	281	2	100,72131147540983
-ACDC	2400	282	2	85,33333333333333
-ACDC	2400	282	2	193,12280701754383
-ACDC	2400	282	2	130,24561403508773
-ACDC	2400	283	2	
-ACDC	2400	284	2	
-ACDC	2400	299	2	
-ACDC	2400	300	2	
-ACDC	2400	301	2	
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	151,25
+ACDC	2400	23	2	184,25
+ACDC	2400	23	2	115,5
+ACDC	2400	39	2	63,45
+ACDC	2400	39	2	133,95
+ACDC	2400	39	2	47,0
+ACDC	2400	39	2	77,55
+ACDC	2400	40	2	88,4
+ACDC	2400	40	2	130,0
+ACDC	2400	40	2	187,2
+ACDC	2400	40	2	249,6
+ACDC	2400	56	2	55,6
+ACDC	2400	56	2	211,28
+ACDC	2400	58	2	113,98
+ACDC	2400	58	2	202,94
+ACDC	2400	58	2	119,54
+ACDC	2400	59	2	166,8
+ACDC	2400	60	2	55,6
+ACDC	2400	60	2	91,74
+ACDC	2400	73	2	93,0
+ACDC	2400	73	2	217,0
+ACDC	2400	73	2	145,7
+ACDC	2400	73	2	155,0
+ACDC	2400	74	2	98,0
+ACDC	2400	74	2	112,0
+ACDC	2400	75	2	201,6
+ACDC	2400	89	2	179,2
+ACDC	2400	90	2	246,4
+ACDC	2400	90	2	77,22
+ACDC	2400	90	2	125,84
+ACDC	2400	91	2	114,0
+ACDC	2400	91	2	126,0
+ACDC	2400	105	2	80,0
+ACDC	2400	109	2	127,26
+ACDC	2400	109	2	189,72
+ACDC	2400	126	2	60,6
+ACDC	2400	126	2	142,41
+ACDC	2400	126	2	189,72
+ACDC	2400	126	2	146,88
+ACDC	2400	127	2	127,26
+ACDC	2400	127	2	181,8
+ACDC	2400	127	2	61,2
+ACDC	2400	128	2	232,56
+ACDC	2400	143	2	72,72
+ACDC	2400	143	2	78,78
+ACDC	2400	143	2	181,8
+ACDC	2400	143	2	100,98
+ACDC	2400	143	2	58,14
+ACDC	2400	143	2	174,42
+ACDC	2400	144	2	48,48
+ACDC	2400	144	2	45,9
+ACDC	2400	145	2	144,56
+ACDC	2400	145	2	224,0
+ACDC	2400	146	2	155,68
+ACDC	2400	146	2	50,4
+ACDC	2400	146	2	72,8
+ACDC	2400	146	2	140,0
+ACDC	2400	148	2	167,272727272727
+ACDC	2400	148	2	118,181818181818
+ACDC	2400	148	2	156,363636363636
+ACDC	2400	148	2	96,3636363636364
+ACDC	2400	148	2	74,5454545454546
+ACDC	2400	148	2	236,0
+ACDC	2400	157	2	116,1
+ACDC	2400	157	2	123,84
+ACDC	2400	160	2	148,4
+ACDC	2400	161	2	97,3
+ACDC	2400	161	2	112,0
+ACDC	2400	162	2	136,22
+ACDC	2400	162	2	144,56
+ACDC	2400	162	2	196,0
+ACDC	2400	162	2	106,4
+ACDC	2400	162	2	154,0
+ACDC	2400	162	2	140,0
+ACDC	2400	163	2	55,4
+ACDC	2400	163	2	180,05
+ACDC	2400	163	2	188,36
+ACDC	2400	164	2	127,42
+ACDC	2400	164	2	80,33
+ACDC	2400	164	2	74,79
+ACDC	2400	164	2	138,5
+ACDC	2400	164	2	155,12
+ACDC	2400	165	2	124,65
+ACDC	2400	165	2	166,2
+ACDC	2400	165	2	138,5
+ACDC	2400	165	2	110,8
+ACDC	2400	166	2	132,428571428571
+ACDC	2400	166	2	217,035714285714
+ACDC	2400	166	2	167,375
+ACDC	2400	175	2	82,88
+ACDC	2400	178	2	144,04
+ACDC	2400	178	2	196,67
+ACDC	2400	178	2	163,43
+ACDC	2400	178	2	108,03
+ACDC	2400	178	2	144,04
+ACDC	2400	178	2	138,5
+ACDC	2400	179	2	166,2
+ACDC	2400	179	2	94,18
+ACDC	2400	179	2	41,55
+ACDC	2400	179	2	119,11
+ACDC	2400	179	2	166,2
+ACDC	2400	179	2	113,57
+ACDC	2400	179	2	49,86
+ACDC	2400	179	2	83,1
+ACDC	2400	179	2	113,57
+ACDC	2400	179	2	182,82
+ACDC	2400	179	2	168,97
+ACDC	2400	180	2	169,4
+ACDC	2400	181	2	104,72
+ACDC	2400	182	2	120,491228070175
+ACDC	2400	182	2	156,910714285714
+ACDC	2400	182	2	70,3392857142857
+ACDC	2400	183	2	75,726
+ACDC	2400	183	2	106,377
+ACDC	2400	183	2	120,801
+ACDC	2400	183	2	183,906
+ACDC	2400	183	2	196,0
+ACDC	2400	183	2	39,6785714285714
+ACDC	2400	183	2	95,5892857142857
+ACDC	2400	183	2	243,75
+ACDC	2400	183	2	243,75
+ACDC	2400	184	2	184,0
+ACDC	2400	184	2	188,181818181818
+ACDC	2400	184	2	158,909090909091
+ACDC	2400	184	2	129,636363636364
+ACDC	2400	184	2	154,428571428571
+ACDC	2400	184	2	147,857142857143
+ACDC	2400	184	2	228,0
+ACDC	2400	185	2	191,071428571429
+ACDC	2400	185	2	175,0
+ACDC	2400	185	2	87,719298245614
+ACDC	2400	185	2	114,035087719298
+ACDC	2400	185	2	94,7368421052632
+ACDC	2400	194	2	127,26
+ACDC	2400	194	2	236,34
+ACDC	2400	194	2	97,92
+ACDC	2400	194	2	168,3
+ACDC	2400	195	2	72,72
+ACDC	2400	195	2	159,12
+ACDC	2400	195	2	153,0
+ACDC	2400	195	2	107,1
+ACDC	2400	196	2	163,62
+ACDC	2400	196	2	122,4
+ACDC	2400	197	2	149,94
+ACDC	2400	197	2	85,68
+ACDC	2400	198	2	45,45
+ACDC	2400	198	2	299,97
+ACDC	2400	198	2	110,16
+ACDC	2400	198	2	168,3
+ACDC	2400	199	2	121,2
+ACDC	2400	199	2	115,14
+ACDC	2400	199	2	145,44
+ACDC	2400	199	2	186,66
+ACDC	2400	199	2	73,44
+ACDC	2400	199	2	76,5
+ACDC	2400	200	2	121,2
+ACDC	2400	200	2	184,83
+ACDC	2400	200	2	76,5
+ACDC	2400	200	2	122,4
+ACDC	2400	201	2	102,0
+ACDC	2400	201	2	175,0
+ACDC	2400	201	2	180,0
+ACDC	2400	202	2	105,75
+ACDC	2400	202	2	75,9
+ACDC	2400	202	2	135,7
+ACDC	2400	202	2	138,0
+ACDC	2400	210	2	87,248
+ACDC	2400	210	2	74,48
+ACDC	2400	210	2	138,32
+ACDC	2400	210	2	89,376
+ACDC	2400	211	2	27,765
+ACDC	2400	211	2	53,679
+ACDC	2400	211	2	64,785
+ACDC	2400	211	2	98,156
+ACDC	2400	211	2	100,008
+ACDC	2400	211	2	90,748
+ACDC	2400	211	2	70,376
+ACDC	2400	212	2	105,507
+ACDC	2400	215	2	96,0
+ACDC	2400	215	2	100,0
+ACDC	2400	215	2	96,0
+ACDC	2400	215	2	120,0
+ACDC	2400	216	2	116,28
+ACDC	2400	216	2	73,92
+ACDC	2400	216	2	107,8
+ACDC	2400	219	2	84,56
+ACDC	2400	219	2	123,82
+ACDC	2400	228	2	61,083
+ACDC	2400	228	2	74,04
+ACDC	2400	228	2	51,8
+ACDC	2400	228	2	88,8
+ACDC	2400	228	2	118,4
+ACDC	2400	229	2	125,46
+ACDC	2400	229	2	123,2
+ACDC	2400	232	2	61,2
+ACDC	2400	232	2	140,76
+ACDC	2400	232	2	137,7
+ACDC	2400	232	2	226,44
+ACDC	2400	232	2	61,6
+ACDC	2400	232	2	138,6
+ACDC	2400	232	2	160,16
+ACDC	2400	233	2	52,02
+ACDC	2400	233	2	195,84
+ACDC	2400	233	2	138,6
+ACDC	2400	233	2	169,4
+ACDC	2400	233	2	83,16
+ACDC	2400	233	2	61,6
+ACDC	2400	245	2	96,252
+ACDC	2400	246	2	90,0
+ACDC	2400	250	2	84,0
+ACDC	2400	263	2	83,9344262295082
+ACDC	2400	263	2	117,508196721312
+ACDC	2400	281	2	104,918032786885
+ACDC	2400	281	2	100,72131147541
+ACDC	2400	282	2	193,122807017544
+ACDC	2400	282	2	85,3333333333333
+ACDC	2400	282	2	130,245614035088
+ACDC	2400	41	2	
+ACDC	2400	42	2	
+ACDC	2400	55	2	
+ACDC	2400	57	2	
+ACDC	2400	72	2	
+ACDC	2400	76	2	
+ACDC	2400	88	2	
+ACDC	2400	106	2	
+ACDC	2400	110	2	
+ACDC	2400	112	2	
+ACDC	2400	113	2	
+ACDC	2400	122	2	
+ACDC	2400	123	2	
+ACDC	2400	125	2	
+ACDC	2400	129	2	
+ACDC	2400	130	2	
+ACDC	2400	141	2	
+ACDC	2400	142	2	
+ACDC	2400	147	2	
+ACDC	2400	158	2	
+ACDC	2400	159	2	
+ACDC	2400	174	2	
+ACDC	2400	176	2	
+ACDC	2400	177	2	
+ACDC	2400	192	2	
+ACDC	2400	193	2	
+ACDC	2400	213	2	
+ACDC	2400	214	2	
+ACDC	2400	218	2	
+ACDC	2400	247	2	
+ACDC	2400	262	2	
+ACDC	2400	265	2	
+ACDC	2400	266	2	
+ACDC	2400	280	2	
+ACDC	2400	283	2	
+ACDC	2400	284	2	
+ACDC	2400	299	2	
+ACDC	2400	300	2	
+ACDC	2400	301	2
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-COLPAL-AB-10mn-ttdec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-COLPAL-AB-10mn-ttdec-dist.txt`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,191 +1,191 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	
-ACDC	2400	39	2	115,15
-ACDC	2400	40	2	351,0
-ACDC	2400	40	2	273,0
-ACDC	2400	41	2	117,0
-ACDC	2400	42	2	75,4
-ACDC	2400	55	2	74,4
-ACDC	2400	55	2	131,44
-ACDC	2400	55	2	44,64
-ACDC	2400	55	2	59,04
-ACDC	2400	56	2	152,9
-ACDC	2400	56	2	105,63999999999999
-ACDC	2400	57	2	
-ACDC	2400	58	2	230,73999999999998
-ACDC	2400	58	2	100,08
-ACDC	2400	58	2	164,02
-ACDC	2400	59	2	119,54
-ACDC	2400	60	2	180,7
-ACDC	2400	60	2	208,49999999999997
-ACDC	2400	60	2	258,54
-ACDC	2400	72	2	116,1
-ACDC	2400	73	2	
-ACDC	2400	74	2	
-ACDC	2400	75	2	
-ACDC	2400	76	2	500,0
-ACDC	2400	88	2	
-ACDC	2400	89	2	229,6
-ACDC	2400	89	2	229,6
-ACDC	2400	89	2	266,0
-ACDC	2400	89	2	266,0
-ACDC	2400	90	2	
-ACDC	2400	91	2	102,0
-ACDC	2400	105	2	
-ACDC	2400	106	2	
-ACDC	2400	109	2	212,1
-ACDC	2400	110	2	192,78
-ACDC	2400	112	2	
-ACDC	2400	113	2	233,352
-ACDC	2400	122	2	132,75
-ACDC	2400	122	2	132,75
-ACDC	2400	123	2	174,04
-ACDC	2400	123	2	114,5
-ACDC	2400	123	2	124,74
-ACDC	2400	125	2	111,8
-ACDC	2400	125	2	146,2
-ACDC	2400	126	2	520,2
-ACDC	2400	127	2	
-ACDC	2400	128	2	263,16
-ACDC	2400	129	2	161,124
-ACDC	2400	130	2	
-ACDC	2400	141	2	171,36
-ACDC	2400	142	2	112,05
-ACDC	2400	143	2	
-ACDC	2400	144	2	201,96
-ACDC	2400	145	2	210,0
-ACDC	2400	146	2	210,0
-ACDC	2400	147	2	
-ACDC	2400	148	2	147,27272727272728
-ACDC	2400	148	2	94,54545454545455
-ACDC	2400	148	2	87,27272727272727
-ACDC	2400	148	2	90,9090909090909
-ACDC	2400	157	2	108,36
-ACDC	2400	158	2	102,5
-ACDC	2400	158	2	180,0
-ACDC	2400	158	2	162,5
-ACDC	2400	158	2	137,5
-ACDC	2400	159	2	
-ACDC	2400	160	2	88,96
-ACDC	2400	161	2	
-ACDC	2400	162	2	251,99999999999997
-ACDC	2400	163	2	
-ACDC	2400	164	2	232,68
-ACDC	2400	165	2	105,26
-ACDC	2400	166	2	66,21428571428571
-ACDC	2400	166	2	103,0
-ACDC	2400	166	2	112,19642857142857
-ACDC	2400	166	2	69,89285714285714
-ACDC	2400	174	2	146,32
-ACDC	2400	175	2	
-ACDC	2400	176	2	13,92
-ACDC	2400	176	2	150,79999999999998
-ACDC	2400	177	2	171,74
-ACDC	2400	177	2	166,2
-ACDC	2400	178	2	235,45
-ACDC	2400	178	2	260,38
-ACDC	2400	178	2	207,75
-ACDC	2400	179	2	
-ACDC	2400	180	2	45,9
-ACDC	2400	180	2	215,6
-ACDC	2400	181	2	154,0
-ACDC	2400	182	2	150,6140350877193
-ACDC	2400	182	2	93,78571428571429
-ACDC	2400	182	2	104,60714285714286
-ACDC	2400	182	2	187,57142857142858
-ACDC	2400	183	2	210,951
-ACDC	2400	183	2	205,542
-ACDC	2400	183	2	341,25
-ACDC	2400	183	2	216,42857142857144
-ACDC	2400	183	2	189,375
-ACDC	2400	184	2	330,1518987341772
-ACDC	2400	184	2	185,64285714285714
-ACDC	2400	184	2	95,28571428571428
-ACDC	2400	184	2	269,01265822784814
-ACDC	2400	184	2	246,72727272727272
-ACDC	2400	185	2	237,25
-ACDC	2400	185	2	92,98245614035088
-ACDC	2400	185	2	462,55
-ACDC	2400	185	2	180,35714285714286
-ACDC	2400	185	2	207,14285714285717
-ACDC	2400	192	2	
-ACDC	2400	193	2	126,5
-ACDC	2400	193	2	200,75
-ACDC	2400	193	2	126,5
-ACDC	2400	194	2	134,64000000000001
-ACDC	2400	194	2	121,19999999999999
-ACDC	2400	195	2	153,0
-ACDC	2400	196	2	236,33999999999997
-ACDC	2400	197	2	
-ACDC	2400	198	2	
-ACDC	2400	199	2	321,3
-ACDC	2400	200	2	
-ACDC	2400	201	2	102,0
-ACDC	2400	201	2	500,0
-ACDC	2400	202	2	128,79999999999998
-ACDC	2400	210	2	74,48
-ACDC	2400	210	2	756,0
-ACDC	2400	211	2	
-ACDC	2400	212	2	124,017
-ACDC	2400	213	2	85,0
-ACDC	2400	213	2	160,0
-ACDC	2400	213	2	188,0
-ACDC	2400	213	2	188,0
-ACDC	2400	214	2	
-ACDC	2400	215	2	216,0
-ACDC	2400	216	2	138,6
-ACDC	2400	218	2	159,20999999999998
-ACDC	2400	218	2	208,8
-ACDC	2400	218	2	247,95
-ACDC	2400	218	2	232,29
-ACDC	2400	218	2	232,29
-ACDC	2400	219	2	
-ACDC	2400	228	2	212,865
-ACDC	2400	228	2	840,0
-ACDC	2400	229	2	275,4
-ACDC	2400	229	2	159,12
-ACDC	2400	229	2	101,64
-ACDC	2400	232	2	184,8
-ACDC	2400	233	2	
-ACDC	2400	245	2	203,61
-ACDC	2400	246	2	185,731
-ACDC	2400	246	2	336,0
-ACDC	2400	246	2	378,0
-ACDC	2400	246	2	146,0
-ACDC	2400	246	2	260,0
-ACDC	2400	246	2	146,0
-ACDC	2400	246	2	80,0
-ACDC	2400	247	2	207,0
-ACDC	2400	247	2	184,0
-ACDC	2400	247	2	170,0
-ACDC	2400	247	2	140,0
-ACDC	2400	247	2	90,0
-ACDC	2400	247	2	103,0
-ACDC	2400	250	2	
-ACDC	2400	262	2	218,22950819672133
-ACDC	2400	263	2	
-ACDC	2400	265	2	138,9
-ACDC	2400	265	2	144,378
-ACDC	2400	265	2	210,0
-ACDC	2400	266	2	120,315
-ACDC	2400	266	2	319,2
-ACDC	2400	266	2	252,0
-ACDC	2400	266	2	83,295
-ACDC	2400	266	2	192,504
-ACDC	2400	266	2	129,57
-ACDC	2400	266	2	111,06
-ACDC	2400	280	2	178,36065573770492
-ACDC	2400	280	2	176,2622950819672
-ACDC	2400	281	2	199,344262295082
-ACDC	2400	282	2	
-ACDC	2400	283	2	186,1818181818182
-ACDC	2400	283	2	155,9272727272727
-ACDC	2400	283	2	130,3272727272727
-ACDC	2400	284	2	137,25
-ACDC	2400	284	2	134,73684210526315
-ACDC	2400	299	2	155,27868852459017
-ACDC	2400	299	2	146,88524590163937
-ACDC	2400	299	2	176,2622950819672
-ACDC	2400	300	2	
-ACDC	2400	301	2	182,55737704918033
-ACDC	2400	301	2	178,36065573770492
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	
+ACDC	2400	39	2	115,15
+ACDC	2400	40	2	351,0
+ACDC	2400	40	2	273,0
+ACDC	2400	41	2	117,0
+ACDC	2400	42	2	75,4
+ACDC	2400	55	2	74,4
+ACDC	2400	55	2	131,44
+ACDC	2400	55	2	44,64
+ACDC	2400	55	2	59,04
+ACDC	2400	56	2	152,9
+ACDC	2400	56	2	105,63999999999999
+ACDC	2400	57	2	
+ACDC	2400	58	2	230,73999999999998
+ACDC	2400	58	2	100,08
+ACDC	2400	58	2	164,02
+ACDC	2400	59	2	119,54
+ACDC	2400	60	2	180,7
+ACDC	2400	60	2	208,49999999999997
+ACDC	2400	60	2	258,54
+ACDC	2400	72	2	116,1
+ACDC	2400	73	2	
+ACDC	2400	74	2	
+ACDC	2400	75	2	
+ACDC	2400	76	2	500,0
+ACDC	2400	88	2	
+ACDC	2400	89	2	229,6
+ACDC	2400	89	2	229,6
+ACDC	2400	89	2	266,0
+ACDC	2400	89	2	266,0
+ACDC	2400	90	2	
+ACDC	2400	91	2	102,0
+ACDC	2400	105	2	
+ACDC	2400	106	2	
+ACDC	2400	109	2	212,1
+ACDC	2400	110	2	192,78
+ACDC	2400	112	2	
+ACDC	2400	113	2	233,352
+ACDC	2400	122	2	132,75
+ACDC	2400	122	2	132,75
+ACDC	2400	123	2	174,04
+ACDC	2400	123	2	114,5
+ACDC	2400	123	2	124,74
+ACDC	2400	125	2	111,8
+ACDC	2400	125	2	146,2
+ACDC	2400	126	2	520,2
+ACDC	2400	127	2	
+ACDC	2400	128	2	263,16
+ACDC	2400	129	2	161,124
+ACDC	2400	130	2	
+ACDC	2400	141	2	171,36
+ACDC	2400	142	2	112,05
+ACDC	2400	143	2	
+ACDC	2400	144	2	201,96
+ACDC	2400	145	2	210,0
+ACDC	2400	146	2	210,0
+ACDC	2400	147	2	
+ACDC	2400	148	2	147,27272727272728
+ACDC	2400	148	2	94,54545454545455
+ACDC	2400	148	2	87,27272727272727
+ACDC	2400	148	2	90,9090909090909
+ACDC	2400	157	2	108,36
+ACDC	2400	158	2	102,5
+ACDC	2400	158	2	180,0
+ACDC	2400	158	2	162,5
+ACDC	2400	158	2	137,5
+ACDC	2400	159	2	
+ACDC	2400	160	2	88,96
+ACDC	2400	161	2	
+ACDC	2400	162	2	251,99999999999997
+ACDC	2400	163	2	
+ACDC	2400	164	2	232,68
+ACDC	2400	165	2	105,26
+ACDC	2400	166	2	66,21428571428571
+ACDC	2400	166	2	103,0
+ACDC	2400	166	2	112,19642857142857
+ACDC	2400	166	2	69,89285714285714
+ACDC	2400	174	2	146,32
+ACDC	2400	175	2	
+ACDC	2400	176	2	13,92
+ACDC	2400	176	2	150,79999999999998
+ACDC	2400	177	2	171,74
+ACDC	2400	177	2	166,2
+ACDC	2400	178	2	235,45
+ACDC	2400	178	2	260,38
+ACDC	2400	178	2	207,75
+ACDC	2400	179	2	
+ACDC	2400	180	2	45,9
+ACDC	2400	180	2	215,6
+ACDC	2400	181	2	154,0
+ACDC	2400	182	2	150,6140350877193
+ACDC	2400	182	2	93,78571428571429
+ACDC	2400	182	2	104,60714285714286
+ACDC	2400	182	2	187,57142857142858
+ACDC	2400	183	2	210,951
+ACDC	2400	183	2	205,542
+ACDC	2400	183	2	341,25
+ACDC	2400	183	2	216,42857142857144
+ACDC	2400	183	2	189,375
+ACDC	2400	184	2	330,1518987341772
+ACDC	2400	184	2	185,64285714285714
+ACDC	2400	184	2	95,28571428571428
+ACDC	2400	184	2	269,01265822784814
+ACDC	2400	184	2	246,72727272727272
+ACDC	2400	185	2	237,25
+ACDC	2400	185	2	92,98245614035088
+ACDC	2400	185	2	462,55
+ACDC	2400	185	2	180,35714285714286
+ACDC	2400	185	2	207,14285714285717
+ACDC	2400	192	2	
+ACDC	2400	193	2	126,5
+ACDC	2400	193	2	200,75
+ACDC	2400	193	2	126,5
+ACDC	2400	194	2	134,64000000000001
+ACDC	2400	194	2	121,19999999999999
+ACDC	2400	195	2	153,0
+ACDC	2400	196	2	236,33999999999997
+ACDC	2400	197	2	
+ACDC	2400	198	2	
+ACDC	2400	199	2	321,3
+ACDC	2400	200	2	
+ACDC	2400	201	2	102,0
+ACDC	2400	201	2	500,0
+ACDC	2400	202	2	128,79999999999998
+ACDC	2400	210	2	74,48
+ACDC	2400	210	2	756,0
+ACDC	2400	211	2	
+ACDC	2400	212	2	124,017
+ACDC	2400	213	2	85,0
+ACDC	2400	213	2	160,0
+ACDC	2400	213	2	188,0
+ACDC	2400	213	2	188,0
+ACDC	2400	214	2	
+ACDC	2400	215	2	216,0
+ACDC	2400	216	2	138,6
+ACDC	2400	218	2	159,20999999999998
+ACDC	2400	218	2	208,8
+ACDC	2400	218	2	247,95
+ACDC	2400	218	2	232,29
+ACDC	2400	218	2	232,29
+ACDC	2400	219	2	
+ACDC	2400	228	2	212,865
+ACDC	2400	228	2	840,0
+ACDC	2400	229	2	275,4
+ACDC	2400	229	2	159,12
+ACDC	2400	229	2	101,64
+ACDC	2400	232	2	184,8
+ACDC	2400	233	2	
+ACDC	2400	245	2	203,61
+ACDC	2400	246	2	185,731
+ACDC	2400	246	2	336,0
+ACDC	2400	246	2	378,0
+ACDC	2400	246	2	146,0
+ACDC	2400	246	2	260,0
+ACDC	2400	246	2	146,0
+ACDC	2400	246	2	80,0
+ACDC	2400	247	2	207,0
+ACDC	2400	247	2	184,0
+ACDC	2400	247	2	170,0
+ACDC	2400	247	2	140,0
+ACDC	2400	247	2	90,0
+ACDC	2400	247	2	103,0
+ACDC	2400	250	2	
+ACDC	2400	262	2	218,22950819672133
+ACDC	2400	263	2	
+ACDC	2400	265	2	138,9
+ACDC	2400	265	2	144,378
+ACDC	2400	265	2	210,0
+ACDC	2400	266	2	120,315
+ACDC	2400	266	2	319,2
+ACDC	2400	266	2	252,0
+ACDC	2400	266	2	83,295
+ACDC	2400	266	2	192,504
+ACDC	2400	266	2	129,57
+ACDC	2400	266	2	111,06
+ACDC	2400	280	2	178,36065573770492
+ACDC	2400	280	2	176,2622950819672
+ACDC	2400	281	2	199,344262295082
+ACDC	2400	282	2	
+ACDC	2400	283	2	186,1818181818182
+ACDC	2400	283	2	155,9272727272727
+ACDC	2400	283	2	130,3272727272727
+ACDC	2400	284	2	137,25
+ACDC	2400	284	2	134,73684210526315
+ACDC	2400	299	2	155,27868852459017
+ACDC	2400	299	2	146,88524590163937
+ACDC	2400	299	2	176,2622950819672
+ACDC	2400	300	2	
+ACDC	2400	301	2	182,55737704918033
+ACDC	2400	301	2	178,36065573770492
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-DonneesBrutesPourAutoDS.xlsx` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-DonneesBrutesPourAutoDS.xlsx`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-PHYCOL-AB-10mn-ttdec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-PHYCOL-AB-10mn-ttdec-dist.txt`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,168 +1,168 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	112,75
-ACDC	2400	23	2	151,25
-ACDC	2400	39	2	
-ACDC	2400	40	2	
-ACDC	2400	41	2	36,4
-ACDC	2400	41	2	33,800000000000004
-ACDC	2400	41	2	106,6
-ACDC	2400	41	2	41,6
-ACDC	2400	42	2	52,0
-ACDC	2400	55	2	74,4
-ACDC	2400	55	2	106,64
-ACDC	2400	55	2	44,64
-ACDC	2400	55	2	83,64
-ACDC	2400	56	2	
-ACDC	2400	57	2	125,1
-ACDC	2400	58	2	55,599999999999994
-ACDC	2400	58	2	94,52
-ACDC	2400	58	2	75,06
-ACDC	2400	59	2	91,74
-ACDC	2400	59	2	61,16
-ACDC	2400	59	2	55,6
-ACDC	2400	60	2	125,1
-ACDC	2400	72	2	59,4
-ACDC	2400	72	2	108,0
-ACDC	2400	73	2	
-ACDC	2400	74	2	232,4
-ACDC	2400	75	2	77,75999999999999
-ACDC	2400	76	2	80,0
-ACDC	2400	88	2	17,78
-ACDC	2400	88	2	17,78
-ACDC	2400	88	2	73,66
-ACDC	2400	89	2	
-ACDC	2400	90	2	151,57999999999998
-ACDC	2400	91	2	129,0
-ACDC	2400	105	2	197,5
-ACDC	2400	105	2	197,5
-ACDC	2400	106	2	93,98
-ACDC	2400	106	2	38,1
-ACDC	2400	106	2	38,1
-ACDC	2400	109	2	136,35
-ACDC	2400	109	2	153,0
-ACDC	2400	109	2	131,58
-ACDC	2400	109	2	201,96
-ACDC	2400	110	2	122,4
-ACDC	2400	112	2	29,26
-ACDC	2400	112	2	47,88
-ACDC	2400	112	2	50,540000000000006
-ACDC	2400	113	2	
-ACDC	2400	122	2	73,75
-ACDC	2400	122	2	73,75
-ACDC	2400	123	2	87,02
-ACDC	2400	123	2	92,4
-ACDC	2400	123	2	98,47
-ACDC	2400	125	2	40,85
-ACDC	2400	125	2	30,1
-ACDC	2400	125	2	32,7
-ACDC	2400	126	2	
-ACDC	2400	127	2	212,1
-ACDC	2400	128	2	27,27
-ACDC	2400	128	2	247,86
-ACDC	2400	128	2	149,94
-ACDC	2400	129	2	
-ACDC	2400	130	2	66,636
-ACDC	2400	130	2	62,934
-ACDC	2400	141	2	
-ACDC	2400	142	2	
-ACDC	2400	143	2	121,2
-ACDC	2400	144	2	242,4
-ACDC	2400	145	2	169,58
-ACDC	2400	146	2	47,26
-ACDC	2400	147	2	88,96
-ACDC	2400	147	2	150,12
-ACDC	2400	148	2	58,18181818181818
-ACDC	2400	148	2	60,0
-ACDC	2400	148	2	69,0909090909091
-ACDC	2400	148	2	94,54545454545455
-ACDC	2400	157	2	
-ACDC	2400	158	2	87,5
-ACDC	2400	159	2	
-ACDC	2400	160	2	
-ACDC	2400	161	2	166,8
-ACDC	2400	162	2	
-ACDC	2400	163	2	144,04
-ACDC	2400	163	2	185,59
-ACDC	2400	164	2	77,56
-ACDC	2400	164	2	77,56
-ACDC	2400	165	2	44,32
-ACDC	2400	166	2	
-ACDC	2400	174	2	120,36
-ACDC	2400	174	2	87,32
-ACDC	2400	175	2	32,56
-ACDC	2400	175	2	41,44
-ACDC	2400	176	2	48,72
-ACDC	2400	176	2	32,48
-ACDC	2400	177	2	166,2
-ACDC	2400	178	2	88,64
-ACDC	2400	179	2	238,22
-ACDC	2400	180	2	184,8
-ACDC	2400	180	2	55,08
-ACDC	2400	181	2	122,4
-ACDC	2400	181	2	134,64000000000001
-ACDC	2400	181	2	154,0
-ACDC	2400	182	2	51,3859649122807
-ACDC	2400	182	2	10,631578947368421
-ACDC	2400	182	2	147,0701754385965
-ACDC	2400	182	2	150,6140350877193
-ACDC	2400	183	2	
-ACDC	2400	184	2	182,35714285714286
-ACDC	2400	185	2	
-ACDC	2400	192	2	144,1
-ACDC	2400	192	2	60,260000000000005
-ACDC	2400	193	2	60,5
-ACDC	2400	194	2	
-ACDC	2400	195	2	45,45
-ACDC	2400	195	2	122,4
-ACDC	2400	196	2	127,26
-ACDC	2400	197	2	30,299999999999997
-ACDC	2400	197	2	124,22999999999999
-ACDC	2400	197	2	61,2
-ACDC	2400	198	2	139,38
-ACDC	2400	199	2	181,8
-ACDC	2400	200	2	181,8
-ACDC	2400	200	2	60,599999999999994
-ACDC	2400	201	2	
-ACDC	2400	202	2	
-ACDC	2400	210	2	57,456
-ACDC	2400	210	2	53,2
-ACDC	2400	211	2	
-ACDC	2400	212	2	
-ACDC	2400	213	2	
-ACDC	2400	214	2	
-ACDC	2400	215	2	
-ACDC	2400	216	2	174,42
-ACDC	2400	218	2	99,18
-ACDC	2400	219	2	93,62
-ACDC	2400	219	2	69,46
-ACDC	2400	228	2	
-ACDC	2400	229	2	42,84
-ACDC	2400	229	2	169,4
-ACDC	2400	232	2	
-ACDC	2400	233	2	153,0
-ACDC	2400	245	2	118,464
-ACDC	2400	246	2	10,0
-ACDC	2400	247	2	70,0
-ACDC	2400	250	2	
-ACDC	2400	262	2	
-ACDC	2400	263	2	65,04918032786885
-ACDC	2400	263	2	71,34426229508198
-ACDC	2400	265	2	
-ACDC	2400	266	2	116,613
-ACDC	2400	266	2	111,12
-ACDC	2400	280	2	123,8032786885246
-ACDC	2400	281	2	77,63934426229508
-ACDC	2400	281	2	92,32786885245902
-ACDC	2400	281	2	102,81967213114754
-ACDC	2400	282	2	
-ACDC	2400	283	2	62,836363636363636
-ACDC	2400	283	2	81,45454545454545
-ACDC	2400	284	2	112,28070175438596
-ACDC	2400	284	2	85,5
-ACDC	2400	299	2	86,03278688524591
-ACDC	2400	299	2	4,19672131147541
-ACDC	2400	299	2	144,78688524590166
-ACDC	2400	300	2	88,1311475409836
-ACDC	2400	300	2	172,06557377049182
-ACDC	2400	300	2	121,70491803278689
-ACDC	2400	301	2	
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	112,75
+ACDC	2400	23	2	151,25
+ACDC	2400	39	2	
+ACDC	2400	40	2	
+ACDC	2400	41	2	36,4
+ACDC	2400	41	2	33,800000000000004
+ACDC	2400	41	2	106,6
+ACDC	2400	41	2	41,6
+ACDC	2400	42	2	52,0
+ACDC	2400	55	2	74,4
+ACDC	2400	55	2	106,64
+ACDC	2400	55	2	44,64
+ACDC	2400	55	2	83,64
+ACDC	2400	56	2	
+ACDC	2400	57	2	125,1
+ACDC	2400	58	2	55,599999999999994
+ACDC	2400	58	2	94,52
+ACDC	2400	58	2	75,06
+ACDC	2400	59	2	91,74
+ACDC	2400	59	2	61,16
+ACDC	2400	59	2	55,6
+ACDC	2400	60	2	125,1
+ACDC	2400	72	2	59,4
+ACDC	2400	72	2	108,0
+ACDC	2400	73	2	
+ACDC	2400	74	2	232,4
+ACDC	2400	75	2	77,75999999999999
+ACDC	2400	76	2	80,0
+ACDC	2400	88	2	17,78
+ACDC	2400	88	2	17,78
+ACDC	2400	88	2	73,66
+ACDC	2400	89	2	
+ACDC	2400	90	2	151,57999999999998
+ACDC	2400	91	2	129,0
+ACDC	2400	105	2	197,5
+ACDC	2400	105	2	197,5
+ACDC	2400	106	2	93,98
+ACDC	2400	106	2	38,1
+ACDC	2400	106	2	38,1
+ACDC	2400	109	2	136,35
+ACDC	2400	109	2	153,0
+ACDC	2400	109	2	131,58
+ACDC	2400	109	2	201,96
+ACDC	2400	110	2	122,4
+ACDC	2400	112	2	29,26
+ACDC	2400	112	2	47,88
+ACDC	2400	112	2	50,540000000000006
+ACDC	2400	113	2	
+ACDC	2400	122	2	73,75
+ACDC	2400	122	2	73,75
+ACDC	2400	123	2	87,02
+ACDC	2400	123	2	92,4
+ACDC	2400	123	2	98,47
+ACDC	2400	125	2	40,85
+ACDC	2400	125	2	30,1
+ACDC	2400	125	2	32,7
+ACDC	2400	126	2	
+ACDC	2400	127	2	212,1
+ACDC	2400	128	2	27,27
+ACDC	2400	128	2	247,86
+ACDC	2400	128	2	149,94
+ACDC	2400	129	2	
+ACDC	2400	130	2	66,636
+ACDC	2400	130	2	62,934
+ACDC	2400	141	2	
+ACDC	2400	142	2	
+ACDC	2400	143	2	121,2
+ACDC	2400	144	2	242,4
+ACDC	2400	145	2	169,58
+ACDC	2400	146	2	47,26
+ACDC	2400	147	2	88,96
+ACDC	2400	147	2	150,12
+ACDC	2400	148	2	58,18181818181818
+ACDC	2400	148	2	60,0
+ACDC	2400	148	2	69,0909090909091
+ACDC	2400	148	2	94,54545454545455
+ACDC	2400	157	2	
+ACDC	2400	158	2	87,5
+ACDC	2400	159	2	
+ACDC	2400	160	2	
+ACDC	2400	161	2	166,8
+ACDC	2400	162	2	
+ACDC	2400	163	2	144,04
+ACDC	2400	163	2	185,59
+ACDC	2400	164	2	77,56
+ACDC	2400	164	2	77,56
+ACDC	2400	165	2	44,32
+ACDC	2400	166	2	
+ACDC	2400	174	2	120,36
+ACDC	2400	174	2	87,32
+ACDC	2400	175	2	32,56
+ACDC	2400	175	2	41,44
+ACDC	2400	176	2	48,72
+ACDC	2400	176	2	32,48
+ACDC	2400	177	2	166,2
+ACDC	2400	178	2	88,64
+ACDC	2400	179	2	238,22
+ACDC	2400	180	2	184,8
+ACDC	2400	180	2	55,08
+ACDC	2400	181	2	122,4
+ACDC	2400	181	2	134,64000000000001
+ACDC	2400	181	2	154,0
+ACDC	2400	182	2	51,3859649122807
+ACDC	2400	182	2	10,631578947368421
+ACDC	2400	182	2	147,0701754385965
+ACDC	2400	182	2	150,6140350877193
+ACDC	2400	183	2	
+ACDC	2400	184	2	182,35714285714286
+ACDC	2400	185	2	
+ACDC	2400	192	2	144,1
+ACDC	2400	192	2	60,260000000000005
+ACDC	2400	193	2	60,5
+ACDC	2400	194	2	
+ACDC	2400	195	2	45,45
+ACDC	2400	195	2	122,4
+ACDC	2400	196	2	127,26
+ACDC	2400	197	2	30,299999999999997
+ACDC	2400	197	2	124,22999999999999
+ACDC	2400	197	2	61,2
+ACDC	2400	198	2	139,38
+ACDC	2400	199	2	181,8
+ACDC	2400	200	2	181,8
+ACDC	2400	200	2	60,599999999999994
+ACDC	2400	201	2	
+ACDC	2400	202	2	
+ACDC	2400	210	2	57,456
+ACDC	2400	210	2	53,2
+ACDC	2400	211	2	
+ACDC	2400	212	2	
+ACDC	2400	213	2	
+ACDC	2400	214	2	
+ACDC	2400	215	2	
+ACDC	2400	216	2	174,42
+ACDC	2400	218	2	99,18
+ACDC	2400	219	2	93,62
+ACDC	2400	219	2	69,46
+ACDC	2400	228	2	
+ACDC	2400	229	2	42,84
+ACDC	2400	229	2	169,4
+ACDC	2400	232	2	
+ACDC	2400	233	2	153,0
+ACDC	2400	245	2	118,464
+ACDC	2400	246	2	10,0
+ACDC	2400	247	2	70,0
+ACDC	2400	250	2	
+ACDC	2400	262	2	
+ACDC	2400	263	2	65,04918032786885
+ACDC	2400	263	2	71,34426229508198
+ACDC	2400	265	2	
+ACDC	2400	266	2	116,613
+ACDC	2400	266	2	111,12
+ACDC	2400	280	2	123,8032786885246
+ACDC	2400	281	2	77,63934426229508
+ACDC	2400	281	2	92,32786885245902
+ACDC	2400	281	2	102,81967213114754
+ACDC	2400	282	2	
+ACDC	2400	283	2	62,836363636363636
+ACDC	2400	283	2	81,45454545454545
+ACDC	2400	284	2	112,28070175438596
+ACDC	2400	284	2	85,5
+ACDC	2400	299	2	86,03278688524591
+ACDC	2400	299	2	4,19672131147541
+ACDC	2400	299	2	144,78688524590166
+ACDC	2400	300	2	88,1311475409836
+ACDC	2400	300	2	172,06557377049182
+ACDC	2400	300	2	121,70491803278689
+ACDC	2400	301	2
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-SYLATR-AB-10mn-ttdec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-SYLATR-AB-10mn-ttdec-dist.txt`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,439 +1,439 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	123,75
-ACDC	2400	39	2	51,7
-ACDC	2400	39	2	141,0
-ACDC	2400	39	2	117,5
-ACDC	2400	39	2	138,65
-ACDC	2400	39	2	155,1
-ACDC	2400	40	2	78,0
-ACDC	2400	40	2	145,6
-ACDC	2400	40	2	122,2
-ACDC	2400	40	2	403,0
-ACDC	2400	41	2	91,0
-ACDC	2400	41	2	135,2
-ACDC	2400	41	2	57,2
-ACDC	2400	41	2	44,2
-ACDC	2400	41	2	65,0
-ACDC	2400	42	2	44,2
-ACDC	2400	42	2	88,4
-ACDC	2400	42	2	65,0
-ACDC	2400	42	2	117,0
-ACDC	2400	55	2	74,4
-ACDC	2400	55	2	69,44
-ACDC	2400	55	2	44,28
-ACDC	2400	55	2	132,84
-ACDC	2400	55	2	68,88
-ACDC	2400	55	2	98,4
-ACDC	2400	56	2	166,8
-ACDC	2400	56	2	125,1
-ACDC	2400	56	2	211,28
-ACDC	2400	56	2	119,54
-ACDC	2400	57	2	50,04
-ACDC	2400	57	2	94,52
-ACDC	2400	57	2	88,96
-ACDC	2400	57	2	136,22
-ACDC	2400	57	2	222,39999999999998
-ACDC	2400	58	2	44,48
-ACDC	2400	58	2	69,5
-ACDC	2400	58	2	125,1
-ACDC	2400	58	2	105,63999999999999
-ACDC	2400	58	2	80,61999999999999
-ACDC	2400	59	2	61,16
-ACDC	2400	59	2	55,599999999999994
-ACDC	2400	59	2	127,88
-ACDC	2400	59	2	75,06
-ACDC	2400	59	2	52,82
-ACDC	2400	59	2	111,2
-ACDC	2400	60	2	13,9
-ACDC	2400	60	2	158,45999999999998
-ACDC	2400	72	2	94,5
-ACDC	2400	72	2	229,5
-ACDC	2400	72	2	201,48
-ACDC	2400	72	2	63,48
-ACDC	2400	73	2	77,5
-ACDC	2400	73	2	117,8
-ACDC	2400	73	2	210,8
-ACDC	2400	74	2	137,2
-ACDC	2400	74	2	36,4
-ACDC	2400	75	2	100,8
-ACDC	2400	75	2	97,92
-ACDC	2400	76	2	45,0
-ACDC	2400	76	2	68,0
-ACDC	2400	76	2	81,0
-ACDC	2400	76	2	500,0
-ACDC	2400	88	2	17,78
-ACDC	2400	88	2	127,0
-ACDC	2400	88	2	17,78
-ACDC	2400	88	2	127,0
-ACDC	2400	88	2	66,04
-ACDC	2400	89	2	
-ACDC	2400	90	2	165,88
-ACDC	2400	91	2	102,0
-ACDC	2400	91	2	66,0
-ACDC	2400	91	2	75,0
-ACDC	2400	91	2	90,0
-ACDC	2400	91	2	135,0
-ACDC	2400	105	2	35,0
-ACDC	2400	105	2	35,0
-ACDC	2400	105	2	182,5
-ACDC	2400	106	2	25,4
-ACDC	2400	106	2	25,4
-ACDC	2400	106	2	99,06
-ACDC	2400	109	2	88,74
-ACDC	2400	109	2	137,7
-ACDC	2400	109	2	115,14
-ACDC	2400	109	2	133,32
-ACDC	2400	110	2	136,35
-ACDC	2400	110	2	151,5
-ACDC	2400	110	2	151,5
-ACDC	2400	110	2	124,23
-ACDC	2400	110	2	91,8
-ACDC	2400	110	2	180,54
-ACDC	2400	110	2	162,18
-ACDC	2400	112	2	39,900000000000006
-ACDC	2400	112	2	71,82000000000001
-ACDC	2400	113	2	48,152
-ACDC	2400	113	2	98,156
-ACDC	2400	113	2	62,934
-ACDC	2400	113	2	62,934
-ACDC	2400	113	2	94,452
-ACDC	2400	122	2	26,55
-ACDC	2400	122	2	106,2
-ACDC	2400	122	2	26,55
-ACDC	2400	122	2	106,2
-ACDC	2400	122	2	56,05
-ACDC	2400	122	2	61,95
-ACDC	2400	123	2	87,02
-ACDC	2400	123	2	91,6
-ACDC	2400	125	2	39,24
-ACDC	2400	125	2	43,6
-ACDC	2400	125	2	19,349999999999998
-ACDC	2400	125	2	23,65
-ACDC	2400	126	2	184,83
-ACDC	2400	126	2	136,35
-ACDC	2400	126	2	21,21
-ACDC	2400	127	2	166,65
-ACDC	2400	127	2	107,1
-ACDC	2400	128	2	106,05
-ACDC	2400	128	2	242,4
-ACDC	2400	128	2	146,88
-ACDC	2400	128	2	171,36
-ACDC	2400	129	2	31,467
-ACDC	2400	129	2	33,336
-ACDC	2400	129	2	77,784
-ACDC	2400	130	2	
-ACDC	2400	141	2	188,02
-ACDC	2400	141	2	109,47999999999999
-ACDC	2400	141	2	90,44
-ACDC	2400	141	2	69,02
-ACDC	2400	141	2	61,88
-ACDC	2400	141	2	54,74
-ACDC	2400	141	2	61,88
-ACDC	2400	141	2	64,25999999999999
-ACDC	2400	142	2	64,74
-ACDC	2400	142	2	99,6
-ACDC	2400	142	2	49,8
-ACDC	2400	142	2	136,95
-ACDC	2400	142	2	146,91000000000003
-ACDC	2400	143	2	69,69
-ACDC	2400	143	2	82,62
-ACDC	2400	143	2	214,2
-ACDC	2400	144	2	153,0
-ACDC	2400	144	2	168,3
-ACDC	2400	144	2	115,14
-ACDC	2400	145	2	122,32
-ACDC	2400	145	2	127,88
-ACDC	2400	145	2	154,0
-ACDC	2400	145	2	168,0
-ACDC	2400	146	2	229,6
-ACDC	2400	146	2	156,79999999999998
-ACDC	2400	146	2	175,14
-ACDC	2400	146	2	69,5
-ACDC	2400	146	2	186,26
-ACDC	2400	147	2	56,0
-ACDC	2400	147	2	56,0
-ACDC	2400	148	2	65,45454545454545
-ACDC	2400	148	2	110,9090909090909
-ACDC	2400	148	2	78,18181818181817
-ACDC	2400	148	2	90,9090909090909
-ACDC	2400	148	2	98,18181818181817
-ACDC	2400	148	2	185,45454545454544
-ACDC	2400	148	2	60,0
-ACDC	2400	148	2	81,81818181818181
-ACDC	2400	148	2	103,63636363636363
-ACDC	2400	148	2	94,54545454545455
-ACDC	2400	148	2	63,63636363636363
-ACDC	2400	148	2	69,0909090909091
-ACDC	2400	157	2	51,6
-ACDC	2400	158	2	87,5
-ACDC	2400	158	2	45,0
-ACDC	2400	158	2	47,5
-ACDC	2400	158	2	42,5
-ACDC	2400	159	2	38,339999999999996
-ACDC	2400	159	2	36,21
-ACDC	2400	159	2	31,95
-ACDC	2400	159	2	46,86
-ACDC	2400	160	2	83,4
-ACDC	2400	161	2	189,04
-ACDC	2400	161	2	202,94
-ACDC	2400	161	2	224,0
-ACDC	2400	161	2	196,0
-ACDC	2400	162	2	198,8
-ACDC	2400	162	2	166,8
-ACDC	2400	162	2	140,0
-ACDC	2400	163	2	210,52
-ACDC	2400	163	2	74,79
-ACDC	2400	163	2	138,5
-ACDC	2400	163	2	163,43
-ACDC	2400	164	2	110,8
-ACDC	2400	164	2	49,86
-ACDC	2400	164	2	152,35
-ACDC	2400	164	2	13,85
-ACDC	2400	164	2	102,49
-ACDC	2400	164	2	102,49
-ACDC	2400	165	2	69,25
-ACDC	2400	165	2	60,94
-ACDC	2400	165	2	160,66
-ACDC	2400	165	2	102,49
-ACDC	2400	166	2	123,23214285714285
-ACDC	2400	166	2	58,857142857142854
-ACDC	2400	166	2	86,44642857142857
-ACDC	2400	166	2	22,07142857142857
-ACDC	2400	166	2	73,57142857142857
-ACDC	2400	166	2	202,32142857142856
-ACDC	2400	174	2	40,12
-ACDC	2400	174	2	44,84
-ACDC	2400	174	2	56,64
-ACDC	2400	174	2	92,04
-ACDC	2400	174	2	113,28
-ACDC	2400	175	2	88,8
-ACDC	2400	176	2	64,96
-ACDC	2400	176	2	30,16
-ACDC	2400	176	2	64,96
-ACDC	2400	176	2	32,48
-ACDC	2400	176	2	48,72
-ACDC	2400	177	2	58,17
-ACDC	2400	177	2	180,05
-ACDC	2400	177	2	188,36
-ACDC	2400	177	2	144,04
-ACDC	2400	177	2	174,51
-ACDC	2400	177	2	155,12
-ACDC	2400	178	2	193,9
-ACDC	2400	178	2	221,6
-ACDC	2400	178	2	221,6
-ACDC	2400	178	2	193,9
-ACDC	2400	179	2	
-ACDC	2400	180	2	113,22
-ACDC	2400	180	2	82,62
-ACDC	2400	180	2	45,9
-ACDC	2400	180	2	89,32
-ACDC	2400	180	2	209,44
-ACDC	2400	181	2	30,8
-ACDC	2400	181	2	123,2
-ACDC	2400	181	2	110,88
-ACDC	2400	181	2	83,16
-ACDC	2400	181	2	104,04
-ACDC	2400	181	2	91,8
-ACDC	2400	182	2	151,5
-ACDC	2400	182	2	108,21428571428572
-ACDC	2400	182	2	155,10714285714286
-ACDC	2400	182	2	101,0
-ACDC	2400	182	2	110,01785714285714
-ACDC	2400	182	2	240,98245614035088
-ACDC	2400	182	2	50,5
-ACDC	2400	182	2	146,08928571428572
-ACDC	2400	182	2	256,9298245614035
-ACDC	2400	182	2	92,14035087719299
-ACDC	2400	182	2	79,73684210526315
-ACDC	2400	182	2	17,719298245614034
-ACDC	2400	182	2	54,10714285714286
-ACDC	2400	183	2	178,55357142857144
-ACDC	2400	183	2	316,875
-ACDC	2400	183	2	196,58928571428572
-ACDC	2400	183	2	207,41071428571428
-ACDC	2400	183	2	209,148
-ACDC	2400	183	2	133,46428571428572
-ACDC	2400	183	2	192,0
-ACDC	2400	183	2	207,345
-ACDC	2400	183	2	153,255
-ACDC	2400	183	2	115,392
-ACDC	2400	183	2	265,041
-ACDC	2400	183	2	174,94642857142858
-ACDC	2400	184	2	118,28571428571428
-ACDC	2400	184	2	126,5
-ACDC	2400	184	2	168,0
-ACDC	2400	184	2	346,45569620253167
-ACDC	2400	184	2	217,45454545454544
-ACDC	2400	184	2	223,72727272727272
-ACDC	2400	184	2	232,0909090909091
-ACDC	2400	184	2	58,54545454545455
-ACDC	2400	184	2	49,285714285714285
-ACDC	2400	185	2	66,07142857142857
-ACDC	2400	185	2	62,5
-ACDC	2400	185	2	119,64285714285715
-ACDC	2400	185	2	155,35714285714286
-ACDC	2400	185	2	73,68421052631578
-ACDC	2400	185	2	49,122807017543856
-ACDC	2400	185	2	56,14035087719298
-ACDC	2400	185	2	204,4
-ACDC	2400	185	2	219,0
-ACDC	2400	192	2	91,7
-ACDC	2400	192	2	120,52
-ACDC	2400	192	2	120,52
-ACDC	2400	192	2	70,74
-ACDC	2400	192	2	81,22
-ACDC	2400	192	2	73,36
-ACDC	2400	193	2	68,75
-ACDC	2400	193	2	60,5
-ACDC	2400	194	2	121,19999999999999
-ACDC	2400	194	2	45,45
-ACDC	2400	194	2	122,4
-ACDC	2400	194	2	306,0
-ACDC	2400	194	2	97,92
-ACDC	2400	194	2	145,44
-ACDC	2400	195	2	119,34
-ACDC	2400	195	2	52,02
-ACDC	2400	195	2	151,5
-ACDC	2400	195	2	133,32
-ACDC	2400	195	2	9,18
-ACDC	2400	196	2	24,24
-ACDC	2400	196	2	45,45
-ACDC	2400	196	2	221,19
-ACDC	2400	196	2	183,6
-ACDC	2400	196	2	21,209999999999997
-ACDC	2400	197	2	60,599999999999994
-ACDC	2400	197	2	30,6
-ACDC	2400	197	2	131,58
-ACDC	2400	197	2	165,24
-ACDC	2400	198	2	116,28
-ACDC	2400	198	2	181,8
-ACDC	2400	198	2	163,61999999999998
-ACDC	2400	198	2	223,38
-ACDC	2400	199	2	121,19999999999999
-ACDC	2400	199	2	245,42999999999998
-ACDC	2400	199	2	137,7
-ACDC	2400	199	2	229,5
-ACDC	2400	199	2	168,3
-ACDC	2400	200	2	157,56
-ACDC	2400	200	2	183,6
-ACDC	2400	200	2	177,48
-ACDC	2400	200	2	97,92
-ACDC	2400	200	2	100,98
-ACDC	2400	201	2	90,0
-ACDC	2400	201	2	88,0
-ACDC	2400	201	2	60,0
-ACDC	2400	201	2	73,0
-ACDC	2400	202	2	122,2
-ACDC	2400	202	2	162,15
-ACDC	2400	202	2	121,9
-ACDC	2400	210	2	102,144
-ACDC	2400	210	2	140,448
-ACDC	2400	211	2	
-ACDC	2400	212	2	35,169
-ACDC	2400	212	2	124,017
-ACDC	2400	212	2	55,56
-ACDC	2400	213	2	145,0
-ACDC	2400	213	2	52,0
-ACDC	2400	213	2	130,0
-ACDC	2400	213	2	72,0
-ACDC	2400	213	2	145,0
-ACDC	2400	213	2	100,0
-ACDC	2400	214	2	46,0
-ACDC	2400	214	2	20,0
-ACDC	2400	214	2	160,0
-ACDC	2400	214	2	54,0
-ACDC	2400	215	2	108,0
-ACDC	2400	215	2	24,0
-ACDC	2400	215	2	25,0
-ACDC	2400	215	2	67,0
-ACDC	2400	215	2	168,0
-ACDC	2400	215	2	84,0
-ACDC	2400	216	2	160,16
-ACDC	2400	216	2	166,32
-ACDC	2400	216	2	110,16
-ACDC	2400	218	2	65,25
-ACDC	2400	218	2	91,35
-ACDC	2400	218	2	99,18
-ACDC	2400	218	2	161,82
-ACDC	2400	218	2	174,87
-ACDC	2400	219	2	72,48
-ACDC	2400	219	2	75,5
-ACDC	2400	228	2	
-ACDC	2400	229	2	76,5
-ACDC	2400	229	2	123,2
-ACDC	2400	229	2	107,8
-ACDC	2400	229	2	144,76
-ACDC	2400	229	2	143,82
-ACDC	2400	232	2	214,2
-ACDC	2400	232	2	137,7
-ACDC	2400	233	2	138,6
-ACDC	2400	233	2	211,14
-ACDC	2400	233	2	165,24
-ACDC	2400	245	2	118,464
-ACDC	2400	245	2	157,42000000000002
-ACDC	2400	245	2	107,41600000000001
-ACDC	2400	245	2	157,335
-ACDC	2400	245	2	166,68
-ACDC	2400	246	2	50,0
-ACDC	2400	246	2	130,0
-ACDC	2400	246	2	20,0
-ACDC	2400	246	2	65,312
-ACDC	2400	247	2	70,0
-ACDC	2400	247	2	115,0
-ACDC	2400	250	2	144,0
-ACDC	2400	262	2	10,491803278688526
-ACDC	2400	262	2	33,57377049180328
-ACDC	2400	262	2	73,44262295081968
-ACDC	2400	262	2	96,52459016393443
-ACDC	2400	263	2	86,03278688524591
-ACDC	2400	263	2	109,11475409836066
-ACDC	2400	263	2	33,57377049180328
-ACDC	2400	265	2	83,34
-ACDC	2400	265	2	74,04
-ACDC	2400	265	2	25,92
-ACDC	2400	265	2	27,765
-ACDC	2400	265	2	51,828
-ACDC	2400	265	2	92,55
-ACDC	2400	265	2	44,448
-ACDC	2400	265	2	111,06
-ACDC	2400	266	2	111,12
-ACDC	2400	266	2	120,38000000000001
-ACDC	2400	266	2	77,784
-ACDC	2400	266	2	72,189
-ACDC	2400	266	2	51,828
-ACDC	2400	266	2	27,765
-ACDC	2400	266	2	22,212
-ACDC	2400	266	2	66,636
-ACDC	2400	266	2	118,464
-ACDC	2400	280	2	199,344262295082
-ACDC	2400	280	2	165,7704918032787
-ACDC	2400	281	2	39,868852459016395
-ACDC	2400	281	2	54,55737704918033
-ACDC	2400	281	2	79,73770491803279
-ACDC	2400	282	2	53,89473684210526
-ACDC	2400	282	2	179,64912280701753
-ACDC	2400	282	2	112,28070175438596
-ACDC	2400	282	2	152,7017543859649
-ACDC	2400	282	2	58,3859649122807
-ACDC	2400	283	2	13,963636363636363
-ACDC	2400	284	2	87,75
-ACDC	2400	284	2	76,5
-ACDC	2400	284	2	33,68421052631579
-ACDC	2400	284	2	76,35087719298245
-ACDC	2400	284	2	54,0
-ACDC	2400	284	2	128,25
-ACDC	2400	284	2	152,7017543859649
-ACDC	2400	299	2	69,24590163934427
-ACDC	2400	299	2	58,75409836065574
-ACDC	2400	299	2	125,9016393442623
-ACDC	2400	299	2	94,42622950819673
-ACDC	2400	300	2	165,7704918032787
-ACDC	2400	300	2	75,54098360655738
-ACDC	2400	300	2	100,72131147540983
-ACDC	2400	300	2	117,50819672131148
-ACDC	2400	300	2	148,98360655737704
-ACDC	2400	301	2	39,868852459016395
-ACDC	2400	301	2	52,459016393442624
-ACDC	2400	301	2	18,885245901639344
-ACDC	2400	301	2	86,03278688524591
-ACDC	2400	301	2	104,91803278688525
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	123,75
+ACDC	2400	39	2	51,7
+ACDC	2400	39	2	141,0
+ACDC	2400	39	2	117,5
+ACDC	2400	39	2	138,65
+ACDC	2400	39	2	155,1
+ACDC	2400	40	2	78,0
+ACDC	2400	40	2	145,6
+ACDC	2400	40	2	122,2
+ACDC	2400	40	2	403,0
+ACDC	2400	41	2	91,0
+ACDC	2400	41	2	135,2
+ACDC	2400	41	2	57,2
+ACDC	2400	41	2	44,2
+ACDC	2400	41	2	65,0
+ACDC	2400	42	2	44,2
+ACDC	2400	42	2	88,4
+ACDC	2400	42	2	65,0
+ACDC	2400	42	2	117,0
+ACDC	2400	55	2	74,4
+ACDC	2400	55	2	69,44
+ACDC	2400	55	2	44,28
+ACDC	2400	55	2	132,84
+ACDC	2400	55	2	68,88
+ACDC	2400	55	2	98,4
+ACDC	2400	56	2	166,8
+ACDC	2400	56	2	125,1
+ACDC	2400	56	2	211,28
+ACDC	2400	56	2	119,54
+ACDC	2400	57	2	50,04
+ACDC	2400	57	2	94,52
+ACDC	2400	57	2	88,96
+ACDC	2400	57	2	136,22
+ACDC	2400	57	2	222,39999999999998
+ACDC	2400	58	2	44,48
+ACDC	2400	58	2	69,5
+ACDC	2400	58	2	125,1
+ACDC	2400	58	2	105,63999999999999
+ACDC	2400	58	2	80,61999999999999
+ACDC	2400	59	2	61,16
+ACDC	2400	59	2	55,599999999999994
+ACDC	2400	59	2	127,88
+ACDC	2400	59	2	75,06
+ACDC	2400	59	2	52,82
+ACDC	2400	59	2	111,2
+ACDC	2400	60	2	13,9
+ACDC	2400	60	2	158,45999999999998
+ACDC	2400	72	2	94,5
+ACDC	2400	72	2	229,5
+ACDC	2400	72	2	201,48
+ACDC	2400	72	2	63,48
+ACDC	2400	73	2	77,5
+ACDC	2400	73	2	117,8
+ACDC	2400	73	2	210,8
+ACDC	2400	74	2	137,2
+ACDC	2400	74	2	36,4
+ACDC	2400	75	2	100,8
+ACDC	2400	75	2	97,92
+ACDC	2400	76	2	45,0
+ACDC	2400	76	2	68,0
+ACDC	2400	76	2	81,0
+ACDC	2400	76	2	500,0
+ACDC	2400	88	2	17,78
+ACDC	2400	88	2	127,0
+ACDC	2400	88	2	17,78
+ACDC	2400	88	2	127,0
+ACDC	2400	88	2	66,04
+ACDC	2400	89	2	
+ACDC	2400	90	2	165,88
+ACDC	2400	91	2	102,0
+ACDC	2400	91	2	66,0
+ACDC	2400	91	2	75,0
+ACDC	2400	91	2	90,0
+ACDC	2400	91	2	135,0
+ACDC	2400	105	2	35,0
+ACDC	2400	105	2	35,0
+ACDC	2400	105	2	182,5
+ACDC	2400	106	2	25,4
+ACDC	2400	106	2	25,4
+ACDC	2400	106	2	99,06
+ACDC	2400	109	2	88,74
+ACDC	2400	109	2	137,7
+ACDC	2400	109	2	115,14
+ACDC	2400	109	2	133,32
+ACDC	2400	110	2	136,35
+ACDC	2400	110	2	151,5
+ACDC	2400	110	2	151,5
+ACDC	2400	110	2	124,23
+ACDC	2400	110	2	91,8
+ACDC	2400	110	2	180,54
+ACDC	2400	110	2	162,18
+ACDC	2400	112	2	39,900000000000006
+ACDC	2400	112	2	71,82000000000001
+ACDC	2400	113	2	48,152
+ACDC	2400	113	2	98,156
+ACDC	2400	113	2	62,934
+ACDC	2400	113	2	62,934
+ACDC	2400	113	2	94,452
+ACDC	2400	122	2	26,55
+ACDC	2400	122	2	106,2
+ACDC	2400	122	2	26,55
+ACDC	2400	122	2	106,2
+ACDC	2400	122	2	56,05
+ACDC	2400	122	2	61,95
+ACDC	2400	123	2	87,02
+ACDC	2400	123	2	91,6
+ACDC	2400	125	2	39,24
+ACDC	2400	125	2	43,6
+ACDC	2400	125	2	19,349999999999998
+ACDC	2400	125	2	23,65
+ACDC	2400	126	2	184,83
+ACDC	2400	126	2	136,35
+ACDC	2400	126	2	21,21
+ACDC	2400	127	2	166,65
+ACDC	2400	127	2	107,1
+ACDC	2400	128	2	106,05
+ACDC	2400	128	2	242,4
+ACDC	2400	128	2	146,88
+ACDC	2400	128	2	171,36
+ACDC	2400	129	2	31,467
+ACDC	2400	129	2	33,336
+ACDC	2400	129	2	77,784
+ACDC	2400	130	2	
+ACDC	2400	141	2	188,02
+ACDC	2400	141	2	109,47999999999999
+ACDC	2400	141	2	90,44
+ACDC	2400	141	2	69,02
+ACDC	2400	141	2	61,88
+ACDC	2400	141	2	54,74
+ACDC	2400	141	2	61,88
+ACDC	2400	141	2	64,25999999999999
+ACDC	2400	142	2	64,74
+ACDC	2400	142	2	99,6
+ACDC	2400	142	2	49,8
+ACDC	2400	142	2	136,95
+ACDC	2400	142	2	146,91000000000003
+ACDC	2400	143	2	69,69
+ACDC	2400	143	2	82,62
+ACDC	2400	143	2	214,2
+ACDC	2400	144	2	153,0
+ACDC	2400	144	2	168,3
+ACDC	2400	144	2	115,14
+ACDC	2400	145	2	122,32
+ACDC	2400	145	2	127,88
+ACDC	2400	145	2	154,0
+ACDC	2400	145	2	168,0
+ACDC	2400	146	2	229,6
+ACDC	2400	146	2	156,79999999999998
+ACDC	2400	146	2	175,14
+ACDC	2400	146	2	69,5
+ACDC	2400	146	2	186,26
+ACDC	2400	147	2	56,0
+ACDC	2400	147	2	56,0
+ACDC	2400	148	2	65,45454545454545
+ACDC	2400	148	2	110,9090909090909
+ACDC	2400	148	2	78,18181818181817
+ACDC	2400	148	2	90,9090909090909
+ACDC	2400	148	2	98,18181818181817
+ACDC	2400	148	2	185,45454545454544
+ACDC	2400	148	2	60,0
+ACDC	2400	148	2	81,81818181818181
+ACDC	2400	148	2	103,63636363636363
+ACDC	2400	148	2	94,54545454545455
+ACDC	2400	148	2	63,63636363636363
+ACDC	2400	148	2	69,0909090909091
+ACDC	2400	157	2	51,6
+ACDC	2400	158	2	87,5
+ACDC	2400	158	2	45,0
+ACDC	2400	158	2	47,5
+ACDC	2400	158	2	42,5
+ACDC	2400	159	2	38,339999999999996
+ACDC	2400	159	2	36,21
+ACDC	2400	159	2	31,95
+ACDC	2400	159	2	46,86
+ACDC	2400	160	2	83,4
+ACDC	2400	161	2	189,04
+ACDC	2400	161	2	202,94
+ACDC	2400	161	2	224,0
+ACDC	2400	161	2	196,0
+ACDC	2400	162	2	198,8
+ACDC	2400	162	2	166,8
+ACDC	2400	162	2	140,0
+ACDC	2400	163	2	210,52
+ACDC	2400	163	2	74,79
+ACDC	2400	163	2	138,5
+ACDC	2400	163	2	163,43
+ACDC	2400	164	2	110,8
+ACDC	2400	164	2	49,86
+ACDC	2400	164	2	152,35
+ACDC	2400	164	2	13,85
+ACDC	2400	164	2	102,49
+ACDC	2400	164	2	102,49
+ACDC	2400	165	2	69,25
+ACDC	2400	165	2	60,94
+ACDC	2400	165	2	160,66
+ACDC	2400	165	2	102,49
+ACDC	2400	166	2	123,23214285714285
+ACDC	2400	166	2	58,857142857142854
+ACDC	2400	166	2	86,44642857142857
+ACDC	2400	166	2	22,07142857142857
+ACDC	2400	166	2	73,57142857142857
+ACDC	2400	166	2	202,32142857142856
+ACDC	2400	174	2	40,12
+ACDC	2400	174	2	44,84
+ACDC	2400	174	2	56,64
+ACDC	2400	174	2	92,04
+ACDC	2400	174	2	113,28
+ACDC	2400	175	2	88,8
+ACDC	2400	176	2	64,96
+ACDC	2400	176	2	30,16
+ACDC	2400	176	2	64,96
+ACDC	2400	176	2	32,48
+ACDC	2400	176	2	48,72
+ACDC	2400	177	2	58,17
+ACDC	2400	177	2	180,05
+ACDC	2400	177	2	188,36
+ACDC	2400	177	2	144,04
+ACDC	2400	177	2	174,51
+ACDC	2400	177	2	155,12
+ACDC	2400	178	2	193,9
+ACDC	2400	178	2	221,6
+ACDC	2400	178	2	221,6
+ACDC	2400	178	2	193,9
+ACDC	2400	179	2	
+ACDC	2400	180	2	113,22
+ACDC	2400	180	2	82,62
+ACDC	2400	180	2	45,9
+ACDC	2400	180	2	89,32
+ACDC	2400	180	2	209,44
+ACDC	2400	181	2	30,8
+ACDC	2400	181	2	123,2
+ACDC	2400	181	2	110,88
+ACDC	2400	181	2	83,16
+ACDC	2400	181	2	104,04
+ACDC	2400	181	2	91,8
+ACDC	2400	182	2	151,5
+ACDC	2400	182	2	108,21428571428572
+ACDC	2400	182	2	155,10714285714286
+ACDC	2400	182	2	101,0
+ACDC	2400	182	2	110,01785714285714
+ACDC	2400	182	2	240,98245614035088
+ACDC	2400	182	2	50,5
+ACDC	2400	182	2	146,08928571428572
+ACDC	2400	182	2	256,9298245614035
+ACDC	2400	182	2	92,14035087719299
+ACDC	2400	182	2	79,73684210526315
+ACDC	2400	182	2	17,719298245614034
+ACDC	2400	182	2	54,10714285714286
+ACDC	2400	183	2	178,55357142857144
+ACDC	2400	183	2	316,875
+ACDC	2400	183	2	196,58928571428572
+ACDC	2400	183	2	207,41071428571428
+ACDC	2400	183	2	209,148
+ACDC	2400	183	2	133,46428571428572
+ACDC	2400	183	2	192,0
+ACDC	2400	183	2	207,345
+ACDC	2400	183	2	153,255
+ACDC	2400	183	2	115,392
+ACDC	2400	183	2	265,041
+ACDC	2400	183	2	174,94642857142858
+ACDC	2400	184	2	118,28571428571428
+ACDC	2400	184	2	126,5
+ACDC	2400	184	2	168,0
+ACDC	2400	184	2	346,45569620253167
+ACDC	2400	184	2	217,45454545454544
+ACDC	2400	184	2	223,72727272727272
+ACDC	2400	184	2	232,0909090909091
+ACDC	2400	184	2	58,54545454545455
+ACDC	2400	184	2	49,285714285714285
+ACDC	2400	185	2	66,07142857142857
+ACDC	2400	185	2	62,5
+ACDC	2400	185	2	119,64285714285715
+ACDC	2400	185	2	155,35714285714286
+ACDC	2400	185	2	73,68421052631578
+ACDC	2400	185	2	49,122807017543856
+ACDC	2400	185	2	56,14035087719298
+ACDC	2400	185	2	204,4
+ACDC	2400	185	2	219,0
+ACDC	2400	192	2	91,7
+ACDC	2400	192	2	120,52
+ACDC	2400	192	2	120,52
+ACDC	2400	192	2	70,74
+ACDC	2400	192	2	81,22
+ACDC	2400	192	2	73,36
+ACDC	2400	193	2	68,75
+ACDC	2400	193	2	60,5
+ACDC	2400	194	2	121,19999999999999
+ACDC	2400	194	2	45,45
+ACDC	2400	194	2	122,4
+ACDC	2400	194	2	306,0
+ACDC	2400	194	2	97,92
+ACDC	2400	194	2	145,44
+ACDC	2400	195	2	119,34
+ACDC	2400	195	2	52,02
+ACDC	2400	195	2	151,5
+ACDC	2400	195	2	133,32
+ACDC	2400	195	2	9,18
+ACDC	2400	196	2	24,24
+ACDC	2400	196	2	45,45
+ACDC	2400	196	2	221,19
+ACDC	2400	196	2	183,6
+ACDC	2400	196	2	21,209999999999997
+ACDC	2400	197	2	60,599999999999994
+ACDC	2400	197	2	30,6
+ACDC	2400	197	2	131,58
+ACDC	2400	197	2	165,24
+ACDC	2400	198	2	116,28
+ACDC	2400	198	2	181,8
+ACDC	2400	198	2	163,61999999999998
+ACDC	2400	198	2	223,38
+ACDC	2400	199	2	121,19999999999999
+ACDC	2400	199	2	245,42999999999998
+ACDC	2400	199	2	137,7
+ACDC	2400	199	2	229,5
+ACDC	2400	199	2	168,3
+ACDC	2400	200	2	157,56
+ACDC	2400	200	2	183,6
+ACDC	2400	200	2	177,48
+ACDC	2400	200	2	97,92
+ACDC	2400	200	2	100,98
+ACDC	2400	201	2	90,0
+ACDC	2400	201	2	88,0
+ACDC	2400	201	2	60,0
+ACDC	2400	201	2	73,0
+ACDC	2400	202	2	122,2
+ACDC	2400	202	2	162,15
+ACDC	2400	202	2	121,9
+ACDC	2400	210	2	102,144
+ACDC	2400	210	2	140,448
+ACDC	2400	211	2	
+ACDC	2400	212	2	35,169
+ACDC	2400	212	2	124,017
+ACDC	2400	212	2	55,56
+ACDC	2400	213	2	145,0
+ACDC	2400	213	2	52,0
+ACDC	2400	213	2	130,0
+ACDC	2400	213	2	72,0
+ACDC	2400	213	2	145,0
+ACDC	2400	213	2	100,0
+ACDC	2400	214	2	46,0
+ACDC	2400	214	2	20,0
+ACDC	2400	214	2	160,0
+ACDC	2400	214	2	54,0
+ACDC	2400	215	2	108,0
+ACDC	2400	215	2	24,0
+ACDC	2400	215	2	25,0
+ACDC	2400	215	2	67,0
+ACDC	2400	215	2	168,0
+ACDC	2400	215	2	84,0
+ACDC	2400	216	2	160,16
+ACDC	2400	216	2	166,32
+ACDC	2400	216	2	110,16
+ACDC	2400	218	2	65,25
+ACDC	2400	218	2	91,35
+ACDC	2400	218	2	99,18
+ACDC	2400	218	2	161,82
+ACDC	2400	218	2	174,87
+ACDC	2400	219	2	72,48
+ACDC	2400	219	2	75,5
+ACDC	2400	228	2	
+ACDC	2400	229	2	76,5
+ACDC	2400	229	2	123,2
+ACDC	2400	229	2	107,8
+ACDC	2400	229	2	144,76
+ACDC	2400	229	2	143,82
+ACDC	2400	232	2	214,2
+ACDC	2400	232	2	137,7
+ACDC	2400	233	2	138,6
+ACDC	2400	233	2	211,14
+ACDC	2400	233	2	165,24
+ACDC	2400	245	2	118,464
+ACDC	2400	245	2	157,42000000000002
+ACDC	2400	245	2	107,41600000000001
+ACDC	2400	245	2	157,335
+ACDC	2400	245	2	166,68
+ACDC	2400	246	2	50,0
+ACDC	2400	246	2	130,0
+ACDC	2400	246	2	20,0
+ACDC	2400	246	2	65,312
+ACDC	2400	247	2	70,0
+ACDC	2400	247	2	115,0
+ACDC	2400	250	2	144,0
+ACDC	2400	262	2	10,491803278688526
+ACDC	2400	262	2	33,57377049180328
+ACDC	2400	262	2	73,44262295081968
+ACDC	2400	262	2	96,52459016393443
+ACDC	2400	263	2	86,03278688524591
+ACDC	2400	263	2	109,11475409836066
+ACDC	2400	263	2	33,57377049180328
+ACDC	2400	265	2	83,34
+ACDC	2400	265	2	74,04
+ACDC	2400	265	2	25,92
+ACDC	2400	265	2	27,765
+ACDC	2400	265	2	51,828
+ACDC	2400	265	2	92,55
+ACDC	2400	265	2	44,448
+ACDC	2400	265	2	111,06
+ACDC	2400	266	2	111,12
+ACDC	2400	266	2	120,38000000000001
+ACDC	2400	266	2	77,784
+ACDC	2400	266	2	72,189
+ACDC	2400	266	2	51,828
+ACDC	2400	266	2	27,765
+ACDC	2400	266	2	22,212
+ACDC	2400	266	2	66,636
+ACDC	2400	266	2	118,464
+ACDC	2400	280	2	199,344262295082
+ACDC	2400	280	2	165,7704918032787
+ACDC	2400	281	2	39,868852459016395
+ACDC	2400	281	2	54,55737704918033
+ACDC	2400	281	2	79,73770491803279
+ACDC	2400	282	2	53,89473684210526
+ACDC	2400	282	2	179,64912280701753
+ACDC	2400	282	2	112,28070175438596
+ACDC	2400	282	2	152,7017543859649
+ACDC	2400	282	2	58,3859649122807
+ACDC	2400	283	2	13,963636363636363
+ACDC	2400	284	2	87,75
+ACDC	2400	284	2	76,5
+ACDC	2400	284	2	33,68421052631579
+ACDC	2400	284	2	76,35087719298245
+ACDC	2400	284	2	54,0
+ACDC	2400	284	2	128,25
+ACDC	2400	284	2	152,7017543859649
+ACDC	2400	299	2	69,24590163934427
+ACDC	2400	299	2	58,75409836065574
+ACDC	2400	299	2	125,9016393442623
+ACDC	2400	299	2	94,42622950819673
+ACDC	2400	300	2	165,7704918032787
+ACDC	2400	300	2	75,54098360655738
+ACDC	2400	300	2	100,72131147540983
+ACDC	2400	300	2	117,50819672131148
+ACDC	2400	300	2	148,98360655737704
+ACDC	2400	301	2	39,868852459016395
+ACDC	2400	301	2	52,459016393442624
+ACDC	2400	301	2	18,885245901639344
+ACDC	2400	301	2	86,03278688524591
+ACDC	2400	301	2	104,91803278688525
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-1dec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-ttdec-dist.txt`

 * *Files 15% similar despite different names*

```diff
@@ -1,429 +1,429 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	93,5
-ACDC	2400	23	2	93,5
-ACDC	2400	23	2	134,8
-ACDC	2400	39	2	42,3
-ACDC	2400	39	2	148,1
-ACDC	2400	39	2	164,5
-ACDC	2400	39	2	220,9
-ACDC	2400	40	2	205,4
-ACDC	2400	40	2	299,0
-ACDC	2400	40	2	299,0
-ACDC	2400	40	2	325,0
-ACDC	2400	41	2	78,0
-ACDC	2400	41	2	169,0
-ACDC	2400	41	2	67,6
-ACDC	2400	42	2	122,2
-ACDC	2400	42	2	111,8
-ACDC	2400	55	2	57,0
-ACDC	2400	55	2	57,0
-ACDC	2400	55	2	42,2
-ACDC	2400	55	2	59,5
-ACDC	2400	55	2	37,2
-ACDC	2400	55	2	19,7
-ACDC	2400	55	2	54,1
-ACDC	2400	56	2	111,2
-ACDC	2400	56	2	152,9
-ACDC	2400	56	2	61,2
-ACDC	2400	57	2	161,2
-ACDC	2400	57	2	166,8
-ACDC	2400	57	2	150,1
-ACDC	2400	58	2	222,4
-ACDC	2400	59	2	150,1
-ACDC	2400	59	2	55,6
-ACDC	2400	60	2	114,0
-ACDC	2400	60	2	69,5
-ACDC	2400	60	2	116,8
-ACDC	2400	72	2	234,9
-ACDC	2400	72	2	102,1
-ACDC	2400	72	2	171,1
-ACDC	2400	73	2	108,5
-ACDC	2400	73	2	139,5
-ACDC	2400	73	2	201,5
-ACDC	2400	73	2	71,3
-ACDC	2400	74	2	98,0
-ACDC	2400	74	2	196,0
-ACDC	2400	74	2	168,0
-ACDC	2400	74	2	84,0
-ACDC	2400	74	2	196,0
-ACDC	2400	75	2	43,2
-ACDC	2400	75	2	144,0
-ACDC	2400	76	2	35,0
-ACDC	2400	76	2	102,0
-ACDC	2400	76	2	500,0
-ACDC	2400	88	2	2,5
-ACDC	2400	88	2	2,5
-ACDC	2400	89	2	
-ACDC	2400	90	2	143,0
-ACDC	2400	90	2	183,0
-ACDC	2400	90	2	137,3
-ACDC	2400	91	2	90,0
-ACDC	2400	91	2	45,0
-ACDC	2400	91	2	144,0
-ACDC	2400	91	2	60,0
-ACDC	2400	105	2	
-ACDC	2400	106	2	63,5
-ACDC	2400	109	2	175,7
-ACDC	2400	109	2	12,2
-ACDC	2400	109	2	130,3
-ACDC	2400	110	2	198,9
-ACDC	2400	110	2	107,1
-ACDC	2400	110	2	130,3
-ACDC	2400	110	2	128,5
-ACDC	2400	112	2	90,4
-ACDC	2400	112	2	21,3
-ACDC	2400	112	2	50,5
-ACDC	2400	112	2	53,2
-ACDC	2400	113	2	88,8
-ACDC	2400	113	2	98,1
-ACDC	2400	113	2	31,5
-ACDC	2400	113	2	150,0
-ACDC	2400	113	2	133,3
-ACDC	2400	122	2	
-ACDC	2400	123	2	87,0
-ACDC	2400	123	2	82,4
-ACDC	2400	123	2	104,0
-ACDC	2400	123	2	85,5
-ACDC	2400	123	2	97,0
-ACDC	2400	123	2	82,4
-ACDC	2400	125	2	130,8
-ACDC	2400	125	2	43,0
-ACDC	2400	125	2	45,1
-ACDC	2400	125	2	66,7
-ACDC	2400	126	2	174,4
-ACDC	2400	126	2	146,9
-ACDC	2400	126	2	247,9
-ACDC	2400	126	2	122,4
-ACDC	2400	126	2	87,9
-ACDC	2400	126	2	75,8
-ACDC	2400	126	2	136,3
-ACDC	2400	126	2	70,4
-ACDC	2400	127	2	154,5
-ACDC	2400	127	2	9,2
-ACDC	2400	127	2	88,7
-ACDC	2400	127	2	70,4
-ACDC	2400	127	2	153,0
-ACDC	2400	128	2	51,5
-ACDC	2400	128	2	157,6
-ACDC	2400	128	2	58,1
-ACDC	2400	128	2	226,4
-ACDC	2400	129	2	63,0
-ACDC	2400	129	2	77,7
-ACDC	2400	129	2	101,8
-ACDC	2400	129	2	81,5
-ACDC	2400	129	2	40,7
-ACDC	2400	130	2	77,8
-ACDC	2400	130	2	120,4
-ACDC	2400	130	2	77,7
-ACDC	2400	130	2	129,6
-ACDC	2400	130	2	40,7
-ACDC	2400	130	2	98,1
-ACDC	2400	130	2	107,4
-ACDC	2400	141	2	147,6
-ACDC	2400	141	2	64,3
-ACDC	2400	141	2	61,9
-ACDC	2400	141	2	52,4
-ACDC	2400	141	2	173,7
-ACDC	2400	141	2	88,1
-ACDC	2400	141	2	59,5
-ACDC	2400	141	2	30,9
-ACDC	2400	142	2	122,0
-ACDC	2400	142	2	12,4
-ACDC	2400	142	2	92,1
-ACDC	2400	142	2	99,6
-ACDC	2400	142	2	154,4
-ACDC	2400	143	2	198,9
-ACDC	2400	143	2	39,8
-ACDC	2400	143	2	131,6
-ACDC	2400	143	2	81,8
-ACDC	2400	143	2	24,5
-ACDC	2400	143	2	106,0
-ACDC	2400	143	2	148,5
-ACDC	2400	144	2	54,5
-ACDC	2400	144	2	133,3
-ACDC	2400	144	2	263,6
-ACDC	2400	144	2	131,6
-ACDC	2400	145	2	144,6
-ACDC	2400	145	2	14,0
-ACDC	2400	145	2	159,6
-ACDC	2400	146	2	194,6
-ACDC	2400	146	2	55,6
-ACDC	2400	146	2	50,4
-ACDC	2400	146	2	64,4
-ACDC	2400	146	2	190,4
-ACDC	2400	147	2	130,7
-ACDC	2400	147	2	106,4
-ACDC	2400	147	2	75,1
-ACDC	2400	148	2	41,8
-ACDC	2400	148	2	96,4
-ACDC	2400	148	2	90,9
-ACDC	2400	148	2	87,3
-ACDC	2400	148	2	87,3
-ACDC	2400	148	2	60,0
-ACDC	2400	148	2	90,9
-ACDC	2400	148	2	87,3
-ACDC	2400	148	2	60,0
-ACDC	2400	157	2	131,6
-ACDC	2400	157	2	105,8
-ACDC	2400	157	2	80,0
-ACDC	2400	157	2	54,2
-ACDC	2400	157	2	100,6
-ACDC	2400	157	2	77,4
-ACDC	2400	157	2	54,2
-ACDC	2400	157	2	92,9
-ACDC	2400	158	2	52,5
-ACDC	2400	159	2	46,9
-ACDC	2400	159	2	55,4
-ACDC	2400	159	2	19,2
-ACDC	2400	159	2	31,9
-ACDC	2400	159	2	25,6
-ACDC	2400	159	2	68,2
-ACDC	2400	160	2	38,9
-ACDC	2400	160	2	83,4
-ACDC	2400	160	2	116,8
-ACDC	2400	160	2	128,8
-ACDC	2400	160	2	126,0
-ACDC	2400	160	2	28,0
-ACDC	2400	161	2	202,9
-ACDC	2400	161	2	56,0
-ACDC	2400	162	2	61,2
-ACDC	2400	162	2	47,6
-ACDC	2400	162	2	70,0
-ACDC	2400	163	2	166,2
-ACDC	2400	163	2	97,0
-ACDC	2400	163	2	72,0
-ACDC	2400	163	2	166,2
-ACDC	2400	163	2	249,3
-ACDC	2400	164	2	149,6
-ACDC	2400	164	2	55,4
-ACDC	2400	164	2	130,2
-ACDC	2400	165	2	113,6
-ACDC	2400	165	2	149,6
-ACDC	2400	165	2	119,1
-ACDC	2400	165	2	41,5
-ACDC	2400	166	2	154,5
-ACDC	2400	166	2	112,2
-ACDC	2400	166	2	46,0
-ACDC	2400	166	2	117,7
-ACDC	2400	166	2	62,5
-ACDC	2400	166	2	69,9
-ACDC	2400	166	2	68,1
-ACDC	2400	166	2	71,7
-ACDC	2400	166	2	115,9
-ACDC	2400	174	2	75,5
-ACDC	2400	174	2	82,6
-ACDC	2400	174	2	47,2
-ACDC	2400	174	2	82,6
-ACDC	2400	175	2	59,2
-ACDC	2400	175	2	74,0
-ACDC	2400	175	2	118,4
-ACDC	2400	175	2	74,0
-ACDC	2400	176	2	27,8
-ACDC	2400	176	2	20,9
-ACDC	2400	176	2	27,8
-ACDC	2400	176	2	20,9
-ACDC	2400	176	2	20,9
-ACDC	2400	177	2	102,5
-ACDC	2400	177	2	160,7
-ACDC	2400	177	2	60,9
-ACDC	2400	177	2	97,0
-ACDC	2400	178	2	44,3
-ACDC	2400	178	2	108,0
-ACDC	2400	179	2	271,5
-ACDC	2400	179	2	199,4
-ACDC	2400	179	2	193,9
-ACDC	2400	179	2	235,4
-ACDC	2400	180	2	30,8
-ACDC	2400	180	2	166,3
-ACDC	2400	180	2	85,7
-ACDC	2400	181	2	101,0
-ACDC	2400	181	2	30,8
-ACDC	2400	181	2	92,4
-ACDC	2400	181	2	33,9
-ACDC	2400	181	2	184,8
-ACDC	2400	181	2	119,3
-ACDC	2400	182	2	140,7
-ACDC	2400	182	2	119,0
-ACDC	2400	182	2	164,1
-ACDC	2400	182	2	115,4
-ACDC	2400	182	2	124,4
-ACDC	2400	182	2	138,2
-ACDC	2400	182	2	115,2
-ACDC	2400	182	2	168,3
-ACDC	2400	182	2	37,9
-ACDC	2400	182	2	54,9
-ACDC	2400	182	2	163,0
-ACDC	2400	183	2	203,8
-ACDC	2400	183	2	167,7
-ACDC	2400	183	2	90,2
-ACDC	2400	183	2	203,7
-ACDC	2400	183	2	162,3
-ACDC	2400	183	2	158,7
-ACDC	2400	183	2	90,2
-ACDC	2400	183	2	97,4
-ACDC	2400	184	2	221,8
-ACDC	2400	184	2	180,7
-ACDC	2400	184	2	159,4
-ACDC	2400	184	2	92,0
-ACDC	2400	184	2	230,0
-ACDC	2400	184	2	177,7
-ACDC	2400	184	2	236,3
-ACDC	2400	184	2	108,7
-ACDC	2400	184	2	100,4
-ACDC	2400	184	2	159,0
-ACDC	2400	185	2	54,4
-ACDC	2400	185	2	365,8
-ACDC	2400	185	2	310,0
-ACDC	2400	185	2	110,5
-ACDC	2400	185	2	64,9
-ACDC	2400	185	2	198,2
-ACDC	2400	185	2	446,6
-ACDC	2400	185	2	53,6
-ACDC	2400	185	2	77,2
-ACDC	2400	185	2	5,4
-ACDC	2400	185	2	80,4
-ACDC	2400	185	2	137,5
-ACDC	2400	185	2	76,8
-ACDC	2400	192	2	60,3
-ACDC	2400	192	2	162,4
-ACDC	2400	192	2	65,5
-ACDC	2400	193	2	30,2
-ACDC	2400	193	2	30,2
-ACDC	2400	194	2	97,0
-ACDC	2400	194	2	130,3
-ACDC	2400	194	2	153,0
-ACDC	2400	194	2	128,5
-ACDC	2400	195	2	55,1
-ACDC	2400	195	2	75,8
-ACDC	2400	195	2	106,0
-ACDC	2400	195	2	27,3
-ACDC	2400	195	2	45,5
-ACDC	2400	195	2	115,1
-ACDC	2400	196	2	12,2
-ACDC	2400	196	2	12,2
-ACDC	2400	196	2	12,2
-ACDC	2400	197	2	116,3
-ACDC	2400	197	2	85,7
-ACDC	2400	197	2	133,3
-ACDC	2400	197	2	203,0
-ACDC	2400	197	2	166,7
-ACDC	2400	197	2	131,6
-ACDC	2400	197	2	60,6
-ACDC	2400	198	2	195,8
-ACDC	2400	198	2	306,0
-ACDC	2400	198	2	153,0
-ACDC	2400	198	2	187,9
-ACDC	2400	198	2	85,7
-ACDC	2400	199	2	39,8
-ACDC	2400	199	2	30,6
-ACDC	2400	199	2	137,7
-ACDC	2400	200	2	236,3
-ACDC	2400	200	2	90,9
-ACDC	2400	200	2	90,9
-ACDC	2400	200	2	76,5
-ACDC	2400	201	2	90,0
-ACDC	2400	202	2	66,7
-ACDC	2400	202	2	133,9
-ACDC	2400	202	2	170,2
-ACDC	2400	210	2	
-ACDC	2400	211	2	
-ACDC	2400	212	2	37,0
-ACDC	2400	212	2	336,0
-ACDC	2400	212	2	252,0
-ACDC	2400	213	2	145,0
-ACDC	2400	213	2	116,0
-ACDC	2400	214	2	15,0
-ACDC	2400	214	2	150,0
-ACDC	2400	214	2	72,0
-ACDC	2400	214	2	80,0
-ACDC	2400	215	2	72,0
-ACDC	2400	215	2	72,0
-ACDC	2400	215	2	96,0
-ACDC	2400	216	2	137,7
-ACDC	2400	216	2	85,7
-ACDC	2400	216	2	149,9
-ACDC	2400	216	2	110,9
-ACDC	2400	216	2	160,2
-ACDC	2400	216	2	184,8
-ACDC	2400	218	2	101,8
-ACDC	2400	218	2	20,9
-ACDC	2400	218	2	122,7
-ACDC	2400	218	2	91,3
-ACDC	2400	218	2	54,8
-ACDC	2400	218	2	180,1
-ACDC	2400	218	2	143,6
-ACDC	2400	219	2	105,7
-ACDC	2400	219	2	102,7
-ACDC	2400	219	2	84,6
-ACDC	2400	219	2	48,3
-ACDC	2400	228	2	420,0
-ACDC	2400	228	2	240,5
-ACDC	2400	229	2	9,2
-ACDC	2400	229	2	30,6
-ACDC	2400	229	2	98,6
-ACDC	2400	232	2	166,3
-ACDC	2400	232	2	131,6
-ACDC	2400	232	2	137,7
-ACDC	2400	232	2	55,1
-ACDC	2400	232	2	144,8
-ACDC	2400	233	2	61,6
-ACDC	2400	233	2	67,8
-ACDC	2400	233	2	154,0
-ACDC	2400	233	2	198,9
-ACDC	2400	233	2	107,1
-ACDC	2400	233	2	107,8
-ACDC	2400	245	2	103,7
-ACDC	2400	245	2	129,6
-ACDC	2400	245	2	59,3
-ACDC	2400	245	2	157,3
-ACDC	2400	246	2	96,0
-ACDC	2400	246	2	165,3
-ACDC	2400	246	2	44,9
-ACDC	2400	247	2	65,0
-ACDC	2400	247	2	87,0
-ACDC	2400	247	2	160,0
-ACDC	2400	250	2	110,0
-ACDC	2400	250	2	48,0
-ACDC	2400	262	2	65,0
-ACDC	2400	262	2	90,2
-ACDC	2400	262	2	88,1
-ACDC	2400	262	2	31,5
-ACDC	2400	262	2	130,1
-ACDC	2400	263	2	73,4
-ACDC	2400	263	2	69,2
-ACDC	2400	263	2	27,3
-ACDC	2400	265	2	18,5
-ACDC	2400	265	2	37,0
-ACDC	2400	266	2	35,2
-ACDC	2400	266	2	101,9
-ACDC	2400	266	2	74,1
-ACDC	2400	266	2	151,8
-ACDC	2400	266	2	135,1
-ACDC	2400	266	2	77,7
-ACDC	2400	280	2	193,0
-ACDC	2400	280	2	144,8
-ACDC	2400	281	2	71,3
-ACDC	2400	281	2	77,6
-ACDC	2400	282	2	105,5
-ACDC	2400	282	2	168,4
-ACDC	2400	282	2	112,3
-ACDC	2400	283	2	174,5
-ACDC	2400	283	2	100,1
-ACDC	2400	284	2	92,2
-ACDC	2400	284	2	15,7
-ACDC	2400	284	2	101,1
-ACDC	2400	284	2	47,2
-ACDC	2400	284	2	107,8
-ACDC	2400	299	2	94,4
-ACDC	2400	299	2	67,1
-ACDC	2400	300	2	86,0
-ACDC	2400	300	2	109,1
-ACDC	2400	300	2	159,5
-ACDC	2400	300	2	58,8
-ACDC	2400	300	2	94,4
-ACDC	2400	300	2	111,2
-ACDC	2400	301	2	71,3
-ACDC	2400	301	2	117,5
-ACDC	2400	301	2	98,6
-ACDC	2400	301	2	107,0
-ACDC	2400	301	2	79,7
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	93,5
+ACDC	2400	23	2	93,5
+ACDC	2400	23	2	134,75
+ACDC	2400	39	2	42,3
+ACDC	2400	39	2	148,05
+ACDC	2400	39	2	164,5
+ACDC	2400	39	2	220,9
+ACDC	2400	40	2	205,4
+ACDC	2400	40	2	299,0
+ACDC	2400	40	2	299,0
+ACDC	2400	40	2	325,0
+ACDC	2400	41	2	78,0
+ACDC	2400	41	2	169,0
+ACDC	2400	41	2	67,6
+ACDC	2400	42	2	122,2
+ACDC	2400	42	2	111,8
+ACDC	2400	55	2	57,04
+ACDC	2400	55	2	57,04
+ACDC	2400	55	2	42,16
+ACDC	2400	55	2	59,52
+ACDC	2400	55	2	37,2
+ACDC	2400	55	2	19,68
+ACDC	2400	55	2	54,12
+ACDC	2400	56	2	111,19999999999999
+ACDC	2400	56	2	152,9
+ACDC	2400	56	2	61,16
+ACDC	2400	57	2	161,23999999999998
+ACDC	2400	57	2	166,8
+ACDC	2400	57	2	150,11999999999998
+ACDC	2400	58	2	222,4
+ACDC	2400	59	2	150,12
+ACDC	2400	59	2	55,599999999999994
+ACDC	2400	60	2	113,98
+ACDC	2400	60	2	69,5
+ACDC	2400	60	2	116,76
+ACDC	2400	72	2	234,9
+ACDC	2400	72	2	102,11999999999999
+ACDC	2400	72	2	171,11999999999998
+ACDC	2400	73	2	108,5
+ACDC	2400	73	2	139,5
+ACDC	2400	73	2	201,5
+ACDC	2400	73	2	71,3
+ACDC	2400	74	2	98,0
+ACDC	2400	74	2	196,0
+ACDC	2400	74	2	168,0
+ACDC	2400	74	2	84,0
+ACDC	2400	74	2	196,0
+ACDC	2400	75	2	43,2
+ACDC	2400	75	2	144,0
+ACDC	2400	76	2	35,0
+ACDC	2400	76	2	102,0
+ACDC	2400	76	2	500,0
+ACDC	2400	88	2	2,54
+ACDC	2400	88	2	2,54
+ACDC	2400	89	2	
+ACDC	2400	90	2	143,0
+ACDC	2400	90	2	183,04
+ACDC	2400	90	2	137,28
+ACDC	2400	91	2	90,0
+ACDC	2400	91	2	45,0
+ACDC	2400	91	2	144,0
+ACDC	2400	91	2	60,0
+ACDC	2400	105	2	
+ACDC	2400	106	2	63,5
+ACDC	2400	109	2	175,74
+ACDC	2400	109	2	12,24
+ACDC	2400	109	2	130,29
+ACDC	2400	110	2	198,9
+ACDC	2400	110	2	107,1
+ACDC	2400	110	2	130,29
+ACDC	2400	110	2	128,52
+ACDC	2400	112	2	90,44
+ACDC	2400	112	2	21,28
+ACDC	2400	112	2	50,540000000000006
+ACDC	2400	112	2	53,2
+ACDC	2400	113	2	88,848
+ACDC	2400	113	2	98,103
+ACDC	2400	113	2	31,484
+ACDC	2400	113	2	150,012
+ACDC	2400	113	2	133,344
+ACDC	2400	122	2	
+ACDC	2400	123	2	87,02
+ACDC	2400	123	2	82,44
+ACDC	2400	123	2	103,95
+ACDC	2400	123	2	85,47
+ACDC	2400	123	2	97,02
+ACDC	2400	123	2	82,44
+ACDC	2400	125	2	130,8
+ACDC	2400	125	2	43,0
+ACDC	2400	125	2	45,15
+ACDC	2400	125	2	66,64999999999999
+ACDC	2400	126	2	174,42
+ACDC	2400	126	2	146,88
+ACDC	2400	126	2	247,86
+ACDC	2400	126	2	122,4
+ACDC	2400	126	2	87,87
+ACDC	2400	126	2	75,75
+ACDC	2400	126	2	136,35
+ACDC	2400	126	2	70,38
+ACDC	2400	127	2	154,53
+ACDC	2400	127	2	9,18
+ACDC	2400	127	2	88,74
+ACDC	2400	127	2	70,38
+ACDC	2400	127	2	153,0
+ACDC	2400	128	2	51,51
+ACDC	2400	128	2	157,56
+ACDC	2400	128	2	58,14
+ACDC	2400	128	2	226,44
+ACDC	2400	129	2	62,968
+ACDC	2400	129	2	77,742
+ACDC	2400	129	2	101,805
+ACDC	2400	129	2	81,488
+ACDC	2400	129	2	40,722
+ACDC	2400	130	2	77,784
+ACDC	2400	130	2	120,38000000000001
+ACDC	2400	130	2	77,742
+ACDC	2400	130	2	129,57
+ACDC	2400	130	2	40,722
+ACDC	2400	130	2	98,103
+ACDC	2400	130	2	107,41600000000001
+ACDC	2400	141	2	147,56
+ACDC	2400	141	2	64,25999999999999
+ACDC	2400	141	2	61,88
+ACDC	2400	141	2	52,36
+ACDC	2400	141	2	173,74
+ACDC	2400	141	2	88,06
+ACDC	2400	141	2	59,5
+ACDC	2400	141	2	30,94
+ACDC	2400	142	2	122,01
+ACDC	2400	142	2	12,45
+ACDC	2400	142	2	92,13000000000001
+ACDC	2400	142	2	99,6
+ACDC	2400	142	2	154,38
+ACDC	2400	143	2	198,9
+ACDC	2400	143	2	39,78
+ACDC	2400	143	2	131,58
+ACDC	2400	143	2	81,81
+ACDC	2400	143	2	24,48
+ACDC	2400	143	2	106,05
+ACDC	2400	143	2	148,47
+ACDC	2400	144	2	54,54
+ACDC	2400	144	2	133,32
+ACDC	2400	144	2	263,61
+ACDC	2400	144	2	131,58
+ACDC	2400	145	2	144,56
+ACDC	2400	145	2	14,0
+ACDC	2400	145	2	159,6
+ACDC	2400	146	2	194,6
+ACDC	2400	146	2	55,6
+ACDC	2400	146	2	50,4
+ACDC	2400	146	2	64,39999999999999
+ACDC	2400	146	2	190,39999999999998
+ACDC	2400	147	2	130,66
+ACDC	2400	147	2	106,4
+ACDC	2400	147	2	75,06
+ACDC	2400	148	2	41,81818181818182
+ACDC	2400	148	2	96,36363636363636
+ACDC	2400	148	2	90,9090909090909
+ACDC	2400	148	2	87,27272727272727
+ACDC	2400	148	2	87,27272727272727
+ACDC	2400	148	2	60,0
+ACDC	2400	148	2	90,9090909090909
+ACDC	2400	148	2	87,27272727272727
+ACDC	2400	148	2	60,0
+ACDC	2400	157	2	131,58
+ACDC	2400	157	2	105,78
+ACDC	2400	157	2	79,98
+ACDC	2400	157	2	54,18
+ACDC	2400	157	2	100,62
+ACDC	2400	157	2	77,4
+ACDC	2400	157	2	54,18
+ACDC	2400	157	2	92,88
+ACDC	2400	158	2	52,5
+ACDC	2400	159	2	46,86
+ACDC	2400	159	2	55,38
+ACDC	2400	159	2	19,169999999999998
+ACDC	2400	159	2	31,95
+ACDC	2400	159	2	25,56
+ACDC	2400	159	2	68,16
+ACDC	2400	160	2	38,92
+ACDC	2400	160	2	83,4
+ACDC	2400	160	2	116,76
+ACDC	2400	160	2	128,79999999999998
+ACDC	2400	160	2	125,99999999999999
+ACDC	2400	160	2	28,0
+ACDC	2400	161	2	202,94
+ACDC	2400	161	2	56,0
+ACDC	2400	162	2	61,16
+ACDC	2400	162	2	47,599999999999994
+ACDC	2400	162	2	70,0
+ACDC	2400	163	2	166,2
+ACDC	2400	163	2	96,95
+ACDC	2400	163	2	72,02
+ACDC	2400	163	2	166,2
+ACDC	2400	163	2	249,3
+ACDC	2400	164	2	149,58
+ACDC	2400	164	2	55,4
+ACDC	2400	164	2	130,19
+ACDC	2400	165	2	113,57
+ACDC	2400	165	2	149,58
+ACDC	2400	165	2	119,11
+ACDC	2400	165	2	41,55
+ACDC	2400	166	2	154,5
+ACDC	2400	166	2	112,19642857142857
+ACDC	2400	166	2	45,982142857142854
+ACDC	2400	166	2	117,71428571428571
+ACDC	2400	166	2	62,535714285714285
+ACDC	2400	166	2	69,89285714285714
+ACDC	2400	166	2	68,05357142857143
+ACDC	2400	166	2	71,73214285714285
+ACDC	2400	166	2	115,875
+ACDC	2400	174	2	75,52
+ACDC	2400	174	2	82,6
+ACDC	2400	174	2	47,2
+ACDC	2400	174	2	82,6
+ACDC	2400	175	2	59,2
+ACDC	2400	175	2	74,0
+ACDC	2400	175	2	118,4
+ACDC	2400	175	2	74,0
+ACDC	2400	176	2	27,839999999999996
+ACDC	2400	176	2	20,88
+ACDC	2400	176	2	27,84
+ACDC	2400	176	2	20,88
+ACDC	2400	176	2	20,88
+ACDC	2400	177	2	102,49
+ACDC	2400	177	2	160,66
+ACDC	2400	177	2	60,94
+ACDC	2400	177	2	96,95
+ACDC	2400	178	2	44,32
+ACDC	2400	178	2	108,03
+ACDC	2400	179	2	271,46
+ACDC	2400	179	2	199,44
+ACDC	2400	179	2	193,9
+ACDC	2400	179	2	235,45
+ACDC	2400	180	2	30,8
+ACDC	2400	180	2	166,32
+ACDC	2400	180	2	85,68
+ACDC	2400	181	2	100,98
+ACDC	2400	181	2	30,8
+ACDC	2400	181	2	92,4
+ACDC	2400	181	2	33,88
+ACDC	2400	181	2	184,8
+ACDC	2400	181	2	119,34
+ACDC	2400	182	2	140,67857142857144
+ACDC	2400	182	2	119,03571428571429
+ACDC	2400	182	2	164,125
+ACDC	2400	182	2	115,42857142857143
+ACDC	2400	182	2	124,44642857142857
+ACDC	2400	182	2	138,21052631578948
+ACDC	2400	182	2	115,17543859649122
+ACDC	2400	182	2	168,33333333333334
+ACDC	2400	182	2	37,875
+ACDC	2400	182	2	54,92982456140351
+ACDC	2400	182	2	163,01754385964912
+ACDC	2400	183	2	203,80357142857144
+ACDC	2400	183	2	167,73214285714286
+ACDC	2400	183	2	90,17857142857143
+ACDC	2400	183	2	203,739
+ACDC	2400	183	2	162,27
+ACDC	2400	183	2	158,664
+ACDC	2400	183	2	90,15
+ACDC	2400	183	2	97,39285714285714
+ACDC	2400	184	2	221,78571428571428
+ACDC	2400	184	2	180,7142857142857
+ACDC	2400	184	2	159,35714285714286
+ACDC	2400	184	2	92,0
+ACDC	2400	184	2	230,0
+ACDC	2400	184	2	177,72727272727272
+ACDC	2400	184	2	236,27272727272725
+ACDC	2400	184	2	108,72727272727272
+ACDC	2400	184	2	100,36363636363636
+ACDC	2400	184	2	158,9620253164557
+ACDC	2400	185	2	54,3859649122807
+ACDC	2400	185	2	365,8
+ACDC	2400	185	2	310,0
+ACDC	2400	185	2	110,52631578947368
+ACDC	2400	185	2	64,91228070175438
+ACDC	2400	185	2	198,21428571428572
+ACDC	2400	185	2	446,6
+ACDC	2400	185	2	53,57142857142858
+ACDC	2400	185	2	77,19298245614034
+ACDC	2400	185	2	5,357142857142858
+ACDC	2400	185	2	80,35714285714286
+ACDC	2400	185	2	137,5
+ACDC	2400	185	2	76,78571428571429
+ACDC	2400	192	2	60,26
+ACDC	2400	192	2	162,44
+ACDC	2400	192	2	65,5
+ACDC	2400	193	2	30,25
+ACDC	2400	193	2	30,25
+ACDC	2400	194	2	96,96
+ACDC	2400	194	2	130,29
+ACDC	2400	194	2	153,0
+ACDC	2400	194	2	128,52
+ACDC	2400	195	2	55,08
+ACDC	2400	195	2	75,75
+ACDC	2400	195	2	106,05
+ACDC	2400	195	2	27,27
+ACDC	2400	195	2	45,45
+ACDC	2400	195	2	115,13999999999999
+ACDC	2400	196	2	12,24
+ACDC	2400	196	2	12,24
+ACDC	2400	196	2	12,24
+ACDC	2400	197	2	116,28
+ACDC	2400	197	2	85,68
+ACDC	2400	197	2	133,32
+ACDC	2400	197	2	203,01
+ACDC	2400	197	2	166,64999999999998
+ACDC	2400	197	2	131,58
+ACDC	2400	197	2	60,599999999999994
+ACDC	2400	198	2	195,84
+ACDC	2400	198	2	306,0
+ACDC	2400	198	2	153,0
+ACDC	2400	198	2	187,86
+ACDC	2400	198	2	85,68
+ACDC	2400	199	2	39,78
+ACDC	2400	199	2	30,6
+ACDC	2400	199	2	137,7
+ACDC	2400	200	2	236,33999999999997
+ACDC	2400	200	2	90,9
+ACDC	2400	200	2	90,9
+ACDC	2400	200	2	76,5
+ACDC	2400	201	2	90,0
+ACDC	2400	202	2	66,69999999999999
+ACDC	2400	202	2	133,95000000000002
+ACDC	2400	202	2	170,2
+ACDC	2400	210	2	
+ACDC	2400	211	2	
+ACDC	2400	212	2	37,02
+ACDC	2400	212	2	336,0
+ACDC	2400	212	2	252,0
+ACDC	2400	213	2	145,0
+ACDC	2400	213	2	116,0
+ACDC	2400	214	2	15,0
+ACDC	2400	214	2	150,0
+ACDC	2400	214	2	72,0
+ACDC	2400	214	2	80,0
+ACDC	2400	215	2	72,0
+ACDC	2400	215	2	72,0
+ACDC	2400	215	2	96,0
+ACDC	2400	216	2	137,7
+ACDC	2400	216	2	85,68
+ACDC	2400	216	2	149,94
+ACDC	2400	216	2	110,88
+ACDC	2400	216	2	160,16
+ACDC	2400	216	2	184,8
+ACDC	2400	218	2	101,79
+ACDC	2400	218	2	20,88
+ACDC	2400	218	2	122,66999999999999
+ACDC	2400	218	2	91,35
+ACDC	2400	218	2	54,809999999999995
+ACDC	2400	218	2	180,09
+ACDC	2400	218	2	143,54999999999998
+ACDC	2400	219	2	105,7
+ACDC	2400	219	2	102,68
+ACDC	2400	219	2	84,56
+ACDC	2400	219	2	48,32
+ACDC	2400	228	2	420,0
+ACDC	2400	228	2	240,5
+ACDC	2400	229	2	9,18
+ACDC	2400	229	2	30,6
+ACDC	2400	229	2	98,56
+ACDC	2400	232	2	166,32
+ACDC	2400	232	2	131,58
+ACDC	2400	232	2	137,7
+ACDC	2400	232	2	55,08
+ACDC	2400	232	2	144,76
+ACDC	2400	233	2	61,6
+ACDC	2400	233	2	67,76
+ACDC	2400	233	2	154,0
+ACDC	2400	233	2	198,9
+ACDC	2400	233	2	107,1
+ACDC	2400	233	2	107,8
+ACDC	2400	245	2	103,656
+ACDC	2400	245	2	129,57
+ACDC	2400	245	2	59,264
+ACDC	2400	245	2	157,335
+ACDC	2400	246	2	96,0
+ACDC	2400	246	2	165,321
+ACDC	2400	246	2	44,902
+ACDC	2400	247	2	65,0
+ACDC	2400	247	2	87,0
+ACDC	2400	247	2	160,0
+ACDC	2400	250	2	110,0
+ACDC	2400	250	2	48,0
+ACDC	2400	262	2	65,04918032786885
+ACDC	2400	262	2	90,22950819672131
+ACDC	2400	262	2	88,1311475409836
+ACDC	2400	262	2	31,475409836065577
+ACDC	2400	262	2	130,0983606557377
+ACDC	2400	263	2	73,44262295081968
+ACDC	2400	263	2	69,24590163934427
+ACDC	2400	263	2	27,278688524590166
+ACDC	2400	265	2	18,52
+ACDC	2400	265	2	37,02
+ACDC	2400	266	2	35,188
+ACDC	2400	266	2	101,86
+ACDC	2400	266	2	74,08
+ACDC	2400	266	2	151,782
+ACDC	2400	266	2	135,123
+ACDC	2400	266	2	77,742
+ACDC	2400	280	2	193,04918032786887
+ACDC	2400	280	2	144,78688524590166
+ACDC	2400	281	2	71,34426229508198
+ACDC	2400	281	2	77,63934426229508
+ACDC	2400	282	2	105,5438596491228
+ACDC	2400	282	2	168,42105263157893
+ACDC	2400	282	2	112,28070175438596
+ACDC	2400	283	2	174,54545454545453
+ACDC	2400	283	2	100,07272727272726
+ACDC	2400	284	2	92,25
+ACDC	2400	284	2	15,719298245614034
+ACDC	2400	284	2	101,05263157894737
+ACDC	2400	284	2	47,25
+ACDC	2400	284	2	107,78947368421052
+ACDC	2400	299	2	94,42622950819673
+ACDC	2400	299	2	67,14754098360656
+ACDC	2400	300	2	86,03278688524591
+ACDC	2400	300	2	109,11475409836066
+ACDC	2400	300	2	159,47540983606558
+ACDC	2400	300	2	58,75409836065574
+ACDC	2400	300	2	94,42622950819673
+ACDC	2400	300	2	111,21311475409837
+ACDC	2400	301	2	71,34426229508198
+ACDC	2400	301	2	117,50819672131148
+ACDC	2400	301	2	98,62295081967214
+ACDC	2400	301	2	107,01639344262296
+ACDC	2400	301	2	79,73770491803279
```

### Comparing `pyaudisam-0.9.3/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-6dec-dist.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-TURMER-AB-10mn-6dec-dist.txt`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,429 +1,429 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	93,500000
-ACDC	2400	23	2	93,500000
-ACDC	2400	23	2	134,750000
-ACDC	2400	39	2	42,300000
-ACDC	2400	39	2	148,050000
-ACDC	2400	39	2	164,500000
-ACDC	2400	39	2	220,900000
-ACDC	2400	40	2	205,400000
-ACDC	2400	40	2	299,000000
-ACDC	2400	40	2	299,000000
-ACDC	2400	40	2	325,000000
-ACDC	2400	41	2	78,000000
-ACDC	2400	41	2	169,000000
-ACDC	2400	41	2	67,600000
-ACDC	2400	42	2	122,200000
-ACDC	2400	42	2	111,800000
-ACDC	2400	55	2	57,040000
-ACDC	2400	55	2	57,040000
-ACDC	2400	55	2	42,160000
-ACDC	2400	55	2	59,520000
-ACDC	2400	55	2	37,200000
-ACDC	2400	55	2	19,680000
-ACDC	2400	55	2	54,120000
-ACDC	2400	56	2	111,200000
-ACDC	2400	56	2	152,900000
-ACDC	2400	56	2	61,160000
-ACDC	2400	57	2	161,240000
-ACDC	2400	57	2	166,800000
-ACDC	2400	57	2	150,120000
-ACDC	2400	58	2	222,400000
-ACDC	2400	59	2	150,120000
-ACDC	2400	59	2	55,600000
-ACDC	2400	60	2	113,980000
-ACDC	2400	60	2	69,500000
-ACDC	2400	60	2	116,760000
-ACDC	2400	72	2	234,900000
-ACDC	2400	72	2	102,120000
-ACDC	2400	72	2	171,120000
-ACDC	2400	73	2	108,500000
-ACDC	2400	73	2	139,500000
-ACDC	2400	73	2	201,500000
-ACDC	2400	73	2	71,300000
-ACDC	2400	74	2	98,000000
-ACDC	2400	74	2	196,000000
-ACDC	2400	74	2	168,000000
-ACDC	2400	74	2	84,000000
-ACDC	2400	74	2	196,000000
-ACDC	2400	75	2	43,200000
-ACDC	2400	75	2	144,000000
-ACDC	2400	76	2	35,000000
-ACDC	2400	76	2	102,000000
-ACDC	2400	76	2	500,000000
-ACDC	2400	88	2	2,540000
-ACDC	2400	88	2	2,540000
-ACDC	2400	89	2	
-ACDC	2400	90	2	143,000000
-ACDC	2400	90	2	183,040000
-ACDC	2400	90	2	137,280000
-ACDC	2400	91	2	90,000000
-ACDC	2400	91	2	45,000000
-ACDC	2400	91	2	144,000000
-ACDC	2400	91	2	60,000000
-ACDC	2400	105	2	
-ACDC	2400	106	2	63,500000
-ACDC	2400	109	2	175,740000
-ACDC	2400	109	2	12,240000
-ACDC	2400	109	2	130,290000
-ACDC	2400	110	2	198,900000
-ACDC	2400	110	2	107,100000
-ACDC	2400	110	2	130,290000
-ACDC	2400	110	2	128,520000
-ACDC	2400	112	2	90,440000
-ACDC	2400	112	2	21,280000
-ACDC	2400	112	2	50,540000
-ACDC	2400	112	2	53,200000
-ACDC	2400	113	2	88,848000
-ACDC	2400	113	2	98,103000
-ACDC	2400	113	2	31,484000
-ACDC	2400	113	2	150,012000
-ACDC	2400	113	2	133,344000
-ACDC	2400	122	2	
-ACDC	2400	123	2	87,020000
-ACDC	2400	123	2	82,440000
-ACDC	2400	123	2	103,950000
-ACDC	2400	123	2	85,470000
-ACDC	2400	123	2	97,020000
-ACDC	2400	123	2	82,440000
-ACDC	2400	125	2	130,800000
-ACDC	2400	125	2	43,000000
-ACDC	2400	125	2	45,150000
-ACDC	2400	125	2	66,650000
-ACDC	2400	126	2	174,420000
-ACDC	2400	126	2	146,880000
-ACDC	2400	126	2	247,860000
-ACDC	2400	126	2	122,400000
-ACDC	2400	126	2	87,870000
-ACDC	2400	126	2	75,750000
-ACDC	2400	126	2	136,350000
-ACDC	2400	126	2	70,380000
-ACDC	2400	127	2	154,530000
-ACDC	2400	127	2	9,180000
-ACDC	2400	127	2	88,740000
-ACDC	2400	127	2	70,380000
-ACDC	2400	127	2	153,000000
-ACDC	2400	128	2	51,510000
-ACDC	2400	128	2	157,560000
-ACDC	2400	128	2	58,140000
-ACDC	2400	128	2	226,440000
-ACDC	2400	129	2	62,968000
-ACDC	2400	129	2	77,742000
-ACDC	2400	129	2	101,805000
-ACDC	2400	129	2	81,488000
-ACDC	2400	129	2	40,722000
-ACDC	2400	130	2	77,784000
-ACDC	2400	130	2	120,380000
-ACDC	2400	130	2	77,742000
-ACDC	2400	130	2	129,570000
-ACDC	2400	130	2	40,722000
-ACDC	2400	130	2	98,103000
-ACDC	2400	130	2	107,416000
-ACDC	2400	141	2	147,560000
-ACDC	2400	141	2	64,260000
-ACDC	2400	141	2	61,880000
-ACDC	2400	141	2	52,360000
-ACDC	2400	141	2	173,740000
-ACDC	2400	141	2	88,060000
-ACDC	2400	141	2	59,500000
-ACDC	2400	141	2	30,940000
-ACDC	2400	142	2	122,010000
-ACDC	2400	142	2	12,450000
-ACDC	2400	142	2	92,130000
-ACDC	2400	142	2	99,600000
-ACDC	2400	142	2	154,380000
-ACDC	2400	143	2	198,900000
-ACDC	2400	143	2	39,780000
-ACDC	2400	143	2	131,580000
-ACDC	2400	143	2	81,810000
-ACDC	2400	143	2	24,480000
-ACDC	2400	143	2	106,050000
-ACDC	2400	143	2	148,470000
-ACDC	2400	144	2	54,540000
-ACDC	2400	144	2	133,320000
-ACDC	2400	144	2	263,610000
-ACDC	2400	144	2	131,580000
-ACDC	2400	145	2	144,560000
-ACDC	2400	145	2	14,000000
-ACDC	2400	145	2	159,600000
-ACDC	2400	146	2	194,600000
-ACDC	2400	146	2	55,600000
-ACDC	2400	146	2	50,400000
-ACDC	2400	146	2	64,400000
-ACDC	2400	146	2	190,400000
-ACDC	2400	147	2	130,660000
-ACDC	2400	147	2	106,400000
-ACDC	2400	147	2	75,060000
-ACDC	2400	148	2	41,818182
-ACDC	2400	148	2	96,363636
-ACDC	2400	148	2	90,909091
-ACDC	2400	148	2	87,272727
-ACDC	2400	148	2	87,272727
-ACDC	2400	148	2	60,000000
-ACDC	2400	148	2	90,909091
-ACDC	2400	148	2	87,272727
-ACDC	2400	148	2	60,000000
-ACDC	2400	157	2	131,580000
-ACDC	2400	157	2	105,780000
-ACDC	2400	157	2	79,980000
-ACDC	2400	157	2	54,180000
-ACDC	2400	157	2	100,620000
-ACDC	2400	157	2	77,400000
-ACDC	2400	157	2	54,180000
-ACDC	2400	157	2	92,880000
-ACDC	2400	158	2	52,500000
-ACDC	2400	159	2	46,860000
-ACDC	2400	159	2	55,380000
-ACDC	2400	159	2	19,170000
-ACDC	2400	159	2	31,950000
-ACDC	2400	159	2	25,560000
-ACDC	2400	159	2	68,160000
-ACDC	2400	160	2	38,920000
-ACDC	2400	160	2	83,400000
-ACDC	2400	160	2	116,760000
-ACDC	2400	160	2	128,800000
-ACDC	2400	160	2	126,000000
-ACDC	2400	160	2	28,000000
-ACDC	2400	161	2	202,940000
-ACDC	2400	161	2	56,000000
-ACDC	2400	162	2	61,160000
-ACDC	2400	162	2	47,600000
-ACDC	2400	162	2	70,000000
-ACDC	2400	163	2	166,200000
-ACDC	2400	163	2	96,950000
-ACDC	2400	163	2	72,020000
-ACDC	2400	163	2	166,200000
-ACDC	2400	163	2	249,300000
-ACDC	2400	164	2	149,580000
-ACDC	2400	164	2	55,400000
-ACDC	2400	164	2	130,190000
-ACDC	2400	165	2	113,570000
-ACDC	2400	165	2	149,580000
-ACDC	2400	165	2	119,110000
-ACDC	2400	165	2	41,550000
-ACDC	2400	166	2	154,500000
-ACDC	2400	166	2	112,196429
-ACDC	2400	166	2	45,982143
-ACDC	2400	166	2	117,714286
-ACDC	2400	166	2	62,535714
-ACDC	2400	166	2	69,892857
-ACDC	2400	166	2	68,053571
-ACDC	2400	166	2	71,732143
-ACDC	2400	166	2	115,875000
-ACDC	2400	174	2	75,520000
-ACDC	2400	174	2	82,600000
-ACDC	2400	174	2	47,200000
-ACDC	2400	174	2	82,600000
-ACDC	2400	175	2	59,200000
-ACDC	2400	175	2	74,000000
-ACDC	2400	175	2	118,400000
-ACDC	2400	175	2	74,000000
-ACDC	2400	176	2	27,840000
-ACDC	2400	176	2	20,880000
-ACDC	2400	176	2	27,840000
-ACDC	2400	176	2	20,880000
-ACDC	2400	176	2	20,880000
-ACDC	2400	177	2	102,490000
-ACDC	2400	177	2	160,660000
-ACDC	2400	177	2	60,940000
-ACDC	2400	177	2	96,950000
-ACDC	2400	178	2	44,320000
-ACDC	2400	178	2	108,030000
-ACDC	2400	179	2	271,460000
-ACDC	2400	179	2	199,440000
-ACDC	2400	179	2	193,900000
-ACDC	2400	179	2	235,450000
-ACDC	2400	180	2	30,800000
-ACDC	2400	180	2	166,320000
-ACDC	2400	180	2	85,680000
-ACDC	2400	181	2	100,980000
-ACDC	2400	181	2	30,800000
-ACDC	2400	181	2	92,400000
-ACDC	2400	181	2	33,880000
-ACDC	2400	181	2	184,800000
-ACDC	2400	181	2	119,340000
-ACDC	2400	182	2	140,678571
-ACDC	2400	182	2	119,035714
-ACDC	2400	182	2	164,125000
-ACDC	2400	182	2	115,428571
-ACDC	2400	182	2	124,446429
-ACDC	2400	182	2	138,210526
-ACDC	2400	182	2	115,175439
-ACDC	2400	182	2	168,333333
-ACDC	2400	182	2	37,875000
-ACDC	2400	182	2	54,929825
-ACDC	2400	182	2	163,017544
-ACDC	2400	183	2	203,803571
-ACDC	2400	183	2	167,732143
-ACDC	2400	183	2	90,178571
-ACDC	2400	183	2	203,739000
-ACDC	2400	183	2	162,270000
-ACDC	2400	183	2	158,664000
-ACDC	2400	183	2	90,150000
-ACDC	2400	183	2	97,392857
-ACDC	2400	184	2	221,785714
-ACDC	2400	184	2	180,714286
-ACDC	2400	184	2	159,357143
-ACDC	2400	184	2	92,000000
-ACDC	2400	184	2	230,000000
-ACDC	2400	184	2	177,727273
-ACDC	2400	184	2	236,272727
-ACDC	2400	184	2	108,727273
-ACDC	2400	184	2	100,363636
-ACDC	2400	184	2	158,962025
-ACDC	2400	185	2	54,385965
-ACDC	2400	185	2	365,800000
-ACDC	2400	185	2	310,000000
-ACDC	2400	185	2	110,526316
-ACDC	2400	185	2	64,912281
-ACDC	2400	185	2	198,214286
-ACDC	2400	185	2	446,600000
-ACDC	2400	185	2	53,571429
-ACDC	2400	185	2	77,192982
-ACDC	2400	185	2	5,357143
-ACDC	2400	185	2	80,357143
-ACDC	2400	185	2	137,500000
-ACDC	2400	185	2	76,785714
-ACDC	2400	192	2	60,260000
-ACDC	2400	192	2	162,440000
-ACDC	2400	192	2	65,500000
-ACDC	2400	193	2	30,250000
-ACDC	2400	193	2	30,250000
-ACDC	2400	194	2	96,960000
-ACDC	2400	194	2	130,290000
-ACDC	2400	194	2	153,000000
-ACDC	2400	194	2	128,520000
-ACDC	2400	195	2	55,080000
-ACDC	2400	195	2	75,750000
-ACDC	2400	195	2	106,050000
-ACDC	2400	195	2	27,270000
-ACDC	2400	195	2	45,450000
-ACDC	2400	195	2	115,140000
-ACDC	2400	196	2	12,240000
-ACDC	2400	196	2	12,240000
-ACDC	2400	196	2	12,240000
-ACDC	2400	197	2	116,280000
-ACDC	2400	197	2	85,680000
-ACDC	2400	197	2	133,320000
-ACDC	2400	197	2	203,010000
-ACDC	2400	197	2	166,650000
-ACDC	2400	197	2	131,580000
-ACDC	2400	197	2	60,600000
-ACDC	2400	198	2	195,840000
-ACDC	2400	198	2	306,000000
-ACDC	2400	198	2	153,000000
-ACDC	2400	198	2	187,860000
-ACDC	2400	198	2	85,680000
-ACDC	2400	199	2	39,780000
-ACDC	2400	199	2	30,600000
-ACDC	2400	199	2	137,700000
-ACDC	2400	200	2	236,340000
-ACDC	2400	200	2	90,900000
-ACDC	2400	200	2	90,900000
-ACDC	2400	200	2	76,500000
-ACDC	2400	201	2	90,000000
-ACDC	2400	202	2	66,700000
-ACDC	2400	202	2	133,950000
-ACDC	2400	202	2	170,200000
-ACDC	2400	210	2	
-ACDC	2400	211	2	
-ACDC	2400	212	2	37,020000
-ACDC	2400	212	2	336,000000
-ACDC	2400	212	2	252,000000
-ACDC	2400	213	2	145,000000
-ACDC	2400	213	2	116,000000
-ACDC	2400	214	2	15,000000
-ACDC	2400	214	2	150,000000
-ACDC	2400	214	2	72,000000
-ACDC	2400	214	2	80,000000
-ACDC	2400	215	2	72,000000
-ACDC	2400	215	2	72,000000
-ACDC	2400	215	2	96,000000
-ACDC	2400	216	2	137,700000
-ACDC	2400	216	2	85,680000
-ACDC	2400	216	2	149,940000
-ACDC	2400	216	2	110,880000
-ACDC	2400	216	2	160,160000
-ACDC	2400	216	2	184,800000
-ACDC	2400	218	2	101,790000
-ACDC	2400	218	2	20,880000
-ACDC	2400	218	2	122,670000
-ACDC	2400	218	2	91,350000
-ACDC	2400	218	2	54,810000
-ACDC	2400	218	2	180,090000
-ACDC	2400	218	2	143,550000
-ACDC	2400	219	2	105,700000
-ACDC	2400	219	2	102,680000
-ACDC	2400	219	2	84,560000
-ACDC	2400	219	2	48,320000
-ACDC	2400	228	2	420,000000
-ACDC	2400	228	2	240,500000
-ACDC	2400	229	2	9,180000
-ACDC	2400	229	2	30,600000
-ACDC	2400	229	2	98,560000
-ACDC	2400	232	2	166,320000
-ACDC	2400	232	2	131,580000
-ACDC	2400	232	2	137,700000
-ACDC	2400	232	2	55,080000
-ACDC	2400	232	2	144,760000
-ACDC	2400	233	2	61,600000
-ACDC	2400	233	2	67,760000
-ACDC	2400	233	2	154,000000
-ACDC	2400	233	2	198,900000
-ACDC	2400	233	2	107,100000
-ACDC	2400	233	2	107,800000
-ACDC	2400	245	2	103,656000
-ACDC	2400	245	2	129,570000
-ACDC	2400	245	2	59,264000
-ACDC	2400	245	2	157,335000
-ACDC	2400	246	2	96,000000
-ACDC	2400	246	2	165,321000
-ACDC	2400	246	2	44,902000
-ACDC	2400	247	2	65,000000
-ACDC	2400	247	2	87,000000
-ACDC	2400	247	2	160,000000
-ACDC	2400	250	2	110,000000
-ACDC	2400	250	2	48,000000
-ACDC	2400	262	2	65,049180
-ACDC	2400	262	2	90,229508
-ACDC	2400	262	2	88,131148
-ACDC	2400	262	2	31,475410
-ACDC	2400	262	2	130,098361
-ACDC	2400	263	2	73,442623
-ACDC	2400	263	2	69,245902
-ACDC	2400	263	2	27,278689
-ACDC	2400	265	2	18,520000
-ACDC	2400	265	2	37,020000
-ACDC	2400	266	2	35,188000
-ACDC	2400	266	2	101,860000
-ACDC	2400	266	2	74,080000
-ACDC	2400	266	2	151,782000
-ACDC	2400	266	2	135,123000
-ACDC	2400	266	2	77,742000
-ACDC	2400	280	2	193,049180
-ACDC	2400	280	2	144,786885
-ACDC	2400	281	2	71,344262
-ACDC	2400	281	2	77,639344
-ACDC	2400	282	2	105,543860
-ACDC	2400	282	2	168,421053
-ACDC	2400	282	2	112,280702
-ACDC	2400	283	2	174,545455
-ACDC	2400	283	2	100,072727
-ACDC	2400	284	2	92,250000
-ACDC	2400	284	2	15,719298
-ACDC	2400	284	2	101,052632
-ACDC	2400	284	2	47,250000
-ACDC	2400	284	2	107,789474
-ACDC	2400	299	2	94,426230
-ACDC	2400	299	2	67,147541
-ACDC	2400	300	2	86,032787
-ACDC	2400	300	2	109,114754
-ACDC	2400	300	2	159,475410
-ACDC	2400	300	2	58,754098
-ACDC	2400	300	2	94,426230
-ACDC	2400	300	2	111,213115
-ACDC	2400	301	2	71,344262
-ACDC	2400	301	2	117,508197
-ACDC	2400	301	2	98,622951
-ACDC	2400	301	2	107,016393
-ACDC	2400	301	2	79,737705
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	93,500000
+ACDC	2400	23	2	93,500000
+ACDC	2400	23	2	134,750000
+ACDC	2400	39	2	42,300000
+ACDC	2400	39	2	148,050000
+ACDC	2400	39	2	164,500000
+ACDC	2400	39	2	220,900000
+ACDC	2400	40	2	205,400000
+ACDC	2400	40	2	299,000000
+ACDC	2400	40	2	299,000000
+ACDC	2400	40	2	325,000000
+ACDC	2400	41	2	78,000000
+ACDC	2400	41	2	169,000000
+ACDC	2400	41	2	67,600000
+ACDC	2400	42	2	122,200000
+ACDC	2400	42	2	111,800000
+ACDC	2400	55	2	57,040000
+ACDC	2400	55	2	57,040000
+ACDC	2400	55	2	42,160000
+ACDC	2400	55	2	59,520000
+ACDC	2400	55	2	37,200000
+ACDC	2400	55	2	19,680000
+ACDC	2400	55	2	54,120000
+ACDC	2400	56	2	111,200000
+ACDC	2400	56	2	152,900000
+ACDC	2400	56	2	61,160000
+ACDC	2400	57	2	161,240000
+ACDC	2400	57	2	166,800000
+ACDC	2400	57	2	150,120000
+ACDC	2400	58	2	222,400000
+ACDC	2400	59	2	150,120000
+ACDC	2400	59	2	55,600000
+ACDC	2400	60	2	113,980000
+ACDC	2400	60	2	69,500000
+ACDC	2400	60	2	116,760000
+ACDC	2400	72	2	234,900000
+ACDC	2400	72	2	102,120000
+ACDC	2400	72	2	171,120000
+ACDC	2400	73	2	108,500000
+ACDC	2400	73	2	139,500000
+ACDC	2400	73	2	201,500000
+ACDC	2400	73	2	71,300000
+ACDC	2400	74	2	98,000000
+ACDC	2400	74	2	196,000000
+ACDC	2400	74	2	168,000000
+ACDC	2400	74	2	84,000000
+ACDC	2400	74	2	196,000000
+ACDC	2400	75	2	43,200000
+ACDC	2400	75	2	144,000000
+ACDC	2400	76	2	35,000000
+ACDC	2400	76	2	102,000000
+ACDC	2400	76	2	500,000000
+ACDC	2400	88	2	2,540000
+ACDC	2400	88	2	2,540000
+ACDC	2400	89	2	
+ACDC	2400	90	2	143,000000
+ACDC	2400	90	2	183,040000
+ACDC	2400	90	2	137,280000
+ACDC	2400	91	2	90,000000
+ACDC	2400	91	2	45,000000
+ACDC	2400	91	2	144,000000
+ACDC	2400	91	2	60,000000
+ACDC	2400	105	2	
+ACDC	2400	106	2	63,500000
+ACDC	2400	109	2	175,740000
+ACDC	2400	109	2	12,240000
+ACDC	2400	109	2	130,290000
+ACDC	2400	110	2	198,900000
+ACDC	2400	110	2	107,100000
+ACDC	2400	110	2	130,290000
+ACDC	2400	110	2	128,520000
+ACDC	2400	112	2	90,440000
+ACDC	2400	112	2	21,280000
+ACDC	2400	112	2	50,540000
+ACDC	2400	112	2	53,200000
+ACDC	2400	113	2	88,848000
+ACDC	2400	113	2	98,103000
+ACDC	2400	113	2	31,484000
+ACDC	2400	113	2	150,012000
+ACDC	2400	113	2	133,344000
+ACDC	2400	122	2	
+ACDC	2400	123	2	87,020000
+ACDC	2400	123	2	82,440000
+ACDC	2400	123	2	103,950000
+ACDC	2400	123	2	85,470000
+ACDC	2400	123	2	97,020000
+ACDC	2400	123	2	82,440000
+ACDC	2400	125	2	130,800000
+ACDC	2400	125	2	43,000000
+ACDC	2400	125	2	45,150000
+ACDC	2400	125	2	66,650000
+ACDC	2400	126	2	174,420000
+ACDC	2400	126	2	146,880000
+ACDC	2400	126	2	247,860000
+ACDC	2400	126	2	122,400000
+ACDC	2400	126	2	87,870000
+ACDC	2400	126	2	75,750000
+ACDC	2400	126	2	136,350000
+ACDC	2400	126	2	70,380000
+ACDC	2400	127	2	154,530000
+ACDC	2400	127	2	9,180000
+ACDC	2400	127	2	88,740000
+ACDC	2400	127	2	70,380000
+ACDC	2400	127	2	153,000000
+ACDC	2400	128	2	51,510000
+ACDC	2400	128	2	157,560000
+ACDC	2400	128	2	58,140000
+ACDC	2400	128	2	226,440000
+ACDC	2400	129	2	62,968000
+ACDC	2400	129	2	77,742000
+ACDC	2400	129	2	101,805000
+ACDC	2400	129	2	81,488000
+ACDC	2400	129	2	40,722000
+ACDC	2400	130	2	77,784000
+ACDC	2400	130	2	120,380000
+ACDC	2400	130	2	77,742000
+ACDC	2400	130	2	129,570000
+ACDC	2400	130	2	40,722000
+ACDC	2400	130	2	98,103000
+ACDC	2400	130	2	107,416000
+ACDC	2400	141	2	147,560000
+ACDC	2400	141	2	64,260000
+ACDC	2400	141	2	61,880000
+ACDC	2400	141	2	52,360000
+ACDC	2400	141	2	173,740000
+ACDC	2400	141	2	88,060000
+ACDC	2400	141	2	59,500000
+ACDC	2400	141	2	30,940000
+ACDC	2400	142	2	122,010000
+ACDC	2400	142	2	12,450000
+ACDC	2400	142	2	92,130000
+ACDC	2400	142	2	99,600000
+ACDC	2400	142	2	154,380000
+ACDC	2400	143	2	198,900000
+ACDC	2400	143	2	39,780000
+ACDC	2400	143	2	131,580000
+ACDC	2400	143	2	81,810000
+ACDC	2400	143	2	24,480000
+ACDC	2400	143	2	106,050000
+ACDC	2400	143	2	148,470000
+ACDC	2400	144	2	54,540000
+ACDC	2400	144	2	133,320000
+ACDC	2400	144	2	263,610000
+ACDC	2400	144	2	131,580000
+ACDC	2400	145	2	144,560000
+ACDC	2400	145	2	14,000000
+ACDC	2400	145	2	159,600000
+ACDC	2400	146	2	194,600000
+ACDC	2400	146	2	55,600000
+ACDC	2400	146	2	50,400000
+ACDC	2400	146	2	64,400000
+ACDC	2400	146	2	190,400000
+ACDC	2400	147	2	130,660000
+ACDC	2400	147	2	106,400000
+ACDC	2400	147	2	75,060000
+ACDC	2400	148	2	41,818182
+ACDC	2400	148	2	96,363636
+ACDC	2400	148	2	90,909091
+ACDC	2400	148	2	87,272727
+ACDC	2400	148	2	87,272727
+ACDC	2400	148	2	60,000000
+ACDC	2400	148	2	90,909091
+ACDC	2400	148	2	87,272727
+ACDC	2400	148	2	60,000000
+ACDC	2400	157	2	131,580000
+ACDC	2400	157	2	105,780000
+ACDC	2400	157	2	79,980000
+ACDC	2400	157	2	54,180000
+ACDC	2400	157	2	100,620000
+ACDC	2400	157	2	77,400000
+ACDC	2400	157	2	54,180000
+ACDC	2400	157	2	92,880000
+ACDC	2400	158	2	52,500000
+ACDC	2400	159	2	46,860000
+ACDC	2400	159	2	55,380000
+ACDC	2400	159	2	19,170000
+ACDC	2400	159	2	31,950000
+ACDC	2400	159	2	25,560000
+ACDC	2400	159	2	68,160000
+ACDC	2400	160	2	38,920000
+ACDC	2400	160	2	83,400000
+ACDC	2400	160	2	116,760000
+ACDC	2400	160	2	128,800000
+ACDC	2400	160	2	126,000000
+ACDC	2400	160	2	28,000000
+ACDC	2400	161	2	202,940000
+ACDC	2400	161	2	56,000000
+ACDC	2400	162	2	61,160000
+ACDC	2400	162	2	47,600000
+ACDC	2400	162	2	70,000000
+ACDC	2400	163	2	166,200000
+ACDC	2400	163	2	96,950000
+ACDC	2400	163	2	72,020000
+ACDC	2400	163	2	166,200000
+ACDC	2400	163	2	249,300000
+ACDC	2400	164	2	149,580000
+ACDC	2400	164	2	55,400000
+ACDC	2400	164	2	130,190000
+ACDC	2400	165	2	113,570000
+ACDC	2400	165	2	149,580000
+ACDC	2400	165	2	119,110000
+ACDC	2400	165	2	41,550000
+ACDC	2400	166	2	154,500000
+ACDC	2400	166	2	112,196429
+ACDC	2400	166	2	45,982143
+ACDC	2400	166	2	117,714286
+ACDC	2400	166	2	62,535714
+ACDC	2400	166	2	69,892857
+ACDC	2400	166	2	68,053571
+ACDC	2400	166	2	71,732143
+ACDC	2400	166	2	115,875000
+ACDC	2400	174	2	75,520000
+ACDC	2400	174	2	82,600000
+ACDC	2400	174	2	47,200000
+ACDC	2400	174	2	82,600000
+ACDC	2400	175	2	59,200000
+ACDC	2400	175	2	74,000000
+ACDC	2400	175	2	118,400000
+ACDC	2400	175	2	74,000000
+ACDC	2400	176	2	27,840000
+ACDC	2400	176	2	20,880000
+ACDC	2400	176	2	27,840000
+ACDC	2400	176	2	20,880000
+ACDC	2400	176	2	20,880000
+ACDC	2400	177	2	102,490000
+ACDC	2400	177	2	160,660000
+ACDC	2400	177	2	60,940000
+ACDC	2400	177	2	96,950000
+ACDC	2400	178	2	44,320000
+ACDC	2400	178	2	108,030000
+ACDC	2400	179	2	271,460000
+ACDC	2400	179	2	199,440000
+ACDC	2400	179	2	193,900000
+ACDC	2400	179	2	235,450000
+ACDC	2400	180	2	30,800000
+ACDC	2400	180	2	166,320000
+ACDC	2400	180	2	85,680000
+ACDC	2400	181	2	100,980000
+ACDC	2400	181	2	30,800000
+ACDC	2400	181	2	92,400000
+ACDC	2400	181	2	33,880000
+ACDC	2400	181	2	184,800000
+ACDC	2400	181	2	119,340000
+ACDC	2400	182	2	140,678571
+ACDC	2400	182	2	119,035714
+ACDC	2400	182	2	164,125000
+ACDC	2400	182	2	115,428571
+ACDC	2400	182	2	124,446429
+ACDC	2400	182	2	138,210526
+ACDC	2400	182	2	115,175439
+ACDC	2400	182	2	168,333333
+ACDC	2400	182	2	37,875000
+ACDC	2400	182	2	54,929825
+ACDC	2400	182	2	163,017544
+ACDC	2400	183	2	203,803571
+ACDC	2400	183	2	167,732143
+ACDC	2400	183	2	90,178571
+ACDC	2400	183	2	203,739000
+ACDC	2400	183	2	162,270000
+ACDC	2400	183	2	158,664000
+ACDC	2400	183	2	90,150000
+ACDC	2400	183	2	97,392857
+ACDC	2400	184	2	221,785714
+ACDC	2400	184	2	180,714286
+ACDC	2400	184	2	159,357143
+ACDC	2400	184	2	92,000000
+ACDC	2400	184	2	230,000000
+ACDC	2400	184	2	177,727273
+ACDC	2400	184	2	236,272727
+ACDC	2400	184	2	108,727273
+ACDC	2400	184	2	100,363636
+ACDC	2400	184	2	158,962025
+ACDC	2400	185	2	54,385965
+ACDC	2400	185	2	365,800000
+ACDC	2400	185	2	310,000000
+ACDC	2400	185	2	110,526316
+ACDC	2400	185	2	64,912281
+ACDC	2400	185	2	198,214286
+ACDC	2400	185	2	446,600000
+ACDC	2400	185	2	53,571429
+ACDC	2400	185	2	77,192982
+ACDC	2400	185	2	5,357143
+ACDC	2400	185	2	80,357143
+ACDC	2400	185	2	137,500000
+ACDC	2400	185	2	76,785714
+ACDC	2400	192	2	60,260000
+ACDC	2400	192	2	162,440000
+ACDC	2400	192	2	65,500000
+ACDC	2400	193	2	30,250000
+ACDC	2400	193	2	30,250000
+ACDC	2400	194	2	96,960000
+ACDC	2400	194	2	130,290000
+ACDC	2400	194	2	153,000000
+ACDC	2400	194	2	128,520000
+ACDC	2400	195	2	55,080000
+ACDC	2400	195	2	75,750000
+ACDC	2400	195	2	106,050000
+ACDC	2400	195	2	27,270000
+ACDC	2400	195	2	45,450000
+ACDC	2400	195	2	115,140000
+ACDC	2400	196	2	12,240000
+ACDC	2400	196	2	12,240000
+ACDC	2400	196	2	12,240000
+ACDC	2400	197	2	116,280000
+ACDC	2400	197	2	85,680000
+ACDC	2400	197	2	133,320000
+ACDC	2400	197	2	203,010000
+ACDC	2400	197	2	166,650000
+ACDC	2400	197	2	131,580000
+ACDC	2400	197	2	60,600000
+ACDC	2400	198	2	195,840000
+ACDC	2400	198	2	306,000000
+ACDC	2400	198	2	153,000000
+ACDC	2400	198	2	187,860000
+ACDC	2400	198	2	85,680000
+ACDC	2400	199	2	39,780000
+ACDC	2400	199	2	30,600000
+ACDC	2400	199	2	137,700000
+ACDC	2400	200	2	236,340000
+ACDC	2400	200	2	90,900000
+ACDC	2400	200	2	90,900000
+ACDC	2400	200	2	76,500000
+ACDC	2400	201	2	90,000000
+ACDC	2400	202	2	66,700000
+ACDC	2400	202	2	133,950000
+ACDC	2400	202	2	170,200000
+ACDC	2400	210	2	
+ACDC	2400	211	2	
+ACDC	2400	212	2	37,020000
+ACDC	2400	212	2	336,000000
+ACDC	2400	212	2	252,000000
+ACDC	2400	213	2	145,000000
+ACDC	2400	213	2	116,000000
+ACDC	2400	214	2	15,000000
+ACDC	2400	214	2	150,000000
+ACDC	2400	214	2	72,000000
+ACDC	2400	214	2	80,000000
+ACDC	2400	215	2	72,000000
+ACDC	2400	215	2	72,000000
+ACDC	2400	215	2	96,000000
+ACDC	2400	216	2	137,700000
+ACDC	2400	216	2	85,680000
+ACDC	2400	216	2	149,940000
+ACDC	2400	216	2	110,880000
+ACDC	2400	216	2	160,160000
+ACDC	2400	216	2	184,800000
+ACDC	2400	218	2	101,790000
+ACDC	2400	218	2	20,880000
+ACDC	2400	218	2	122,670000
+ACDC	2400	218	2	91,350000
+ACDC	2400	218	2	54,810000
+ACDC	2400	218	2	180,090000
+ACDC	2400	218	2	143,550000
+ACDC	2400	219	2	105,700000
+ACDC	2400	219	2	102,680000
+ACDC	2400	219	2	84,560000
+ACDC	2400	219	2	48,320000
+ACDC	2400	228	2	420,000000
+ACDC	2400	228	2	240,500000
+ACDC	2400	229	2	9,180000
+ACDC	2400	229	2	30,600000
+ACDC	2400	229	2	98,560000
+ACDC	2400	232	2	166,320000
+ACDC	2400	232	2	131,580000
+ACDC	2400	232	2	137,700000
+ACDC	2400	232	2	55,080000
+ACDC	2400	232	2	144,760000
+ACDC	2400	233	2	61,600000
+ACDC	2400	233	2	67,760000
+ACDC	2400	233	2	154,000000
+ACDC	2400	233	2	198,900000
+ACDC	2400	233	2	107,100000
+ACDC	2400	233	2	107,800000
+ACDC	2400	245	2	103,656000
+ACDC	2400	245	2	129,570000
+ACDC	2400	245	2	59,264000
+ACDC	2400	245	2	157,335000
+ACDC	2400	246	2	96,000000
+ACDC	2400	246	2	165,321000
+ACDC	2400	246	2	44,902000
+ACDC	2400	247	2	65,000000
+ACDC	2400	247	2	87,000000
+ACDC	2400	247	2	160,000000
+ACDC	2400	250	2	110,000000
+ACDC	2400	250	2	48,000000
+ACDC	2400	262	2	65,049180
+ACDC	2400	262	2	90,229508
+ACDC	2400	262	2	88,131148
+ACDC	2400	262	2	31,475410
+ACDC	2400	262	2	130,098361
+ACDC	2400	263	2	73,442623
+ACDC	2400	263	2	69,245902
+ACDC	2400	263	2	27,278689
+ACDC	2400	265	2	18,520000
+ACDC	2400	265	2	37,020000
+ACDC	2400	266	2	35,188000
+ACDC	2400	266	2	101,860000
+ACDC	2400	266	2	74,080000
+ACDC	2400	266	2	151,782000
+ACDC	2400	266	2	135,123000
+ACDC	2400	266	2	77,742000
+ACDC	2400	280	2	193,049180
+ACDC	2400	280	2	144,786885
+ACDC	2400	281	2	71,344262
+ACDC	2400	281	2	77,639344
+ACDC	2400	282	2	105,543860
+ACDC	2400	282	2	168,421053
+ACDC	2400	282	2	112,280702
+ACDC	2400	283	2	174,545455
+ACDC	2400	283	2	100,072727
+ACDC	2400	284	2	92,250000
+ACDC	2400	284	2	15,719298
+ACDC	2400	284	2	101,052632
+ACDC	2400	284	2	47,250000
+ACDC	2400	284	2	107,789474
+ACDC	2400	299	2	94,426230
+ACDC	2400	299	2	67,147541
+ACDC	2400	300	2	86,032787
+ACDC	2400	300	2	109,114754
+ACDC	2400	300	2	159,475410
+ACDC	2400	300	2	58,754098
+ACDC	2400	300	2	94,426230
+ACDC	2400	300	2	111,213115
+ACDC	2400	301	2	71,344262
+ACDC	2400	301	2	117,508197
+ACDC	2400	301	2	98,622951
+ACDC	2400	301	2	107,016393
+ACDC	2400	301	2	79,737705
```

### Comparing `pyaudisam-0.9.3/tests/refin/TURMER-10mn-1dec-hnorm-cos.odt` & `pyaudisam-1.0.1/tests/refin/TURMER-10mn-1dec-hnorm-cos.odt`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refout/ACDC2019-Naturalist-ExtraitOptResultats.ods` & `pyaudisam-1.0.1/tests/refout/ACDC2019-Naturalist-ExtraitOptResultats.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refout/ACDC2019-Naturalist-ExtraitPreResultats.ods` & `pyaudisam-1.0.1/tests/refout/ACDC2019-Naturalist-ExtraitPreResultats.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refout/ACDC2019-Naturalist-ExtraitResultats.ods` & `pyaudisam-1.0.1/tests/refout/ACDC2019-Naturalist-ExtraitResultats.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-distance-73.xlsx` & `pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-distance-73.xlsx`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-postcomp.ods` & `pyaudisam-1.0.1/tests/refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-postcomp.ods`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/refout/ACDC2019-Papyrus-ALAARV-saisie-5-cols.txt` & `pyaudisam-1.0.1/tests/refin/ACDC2019-Papyrus-ALAARV-AB-10mn-ttdec-dist.txt`

 * *Files 24% similar despite different names*

```diff
@@ -1,257 +1,257 @@
-Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
-ACDC	2400	23	2	151,25
-ACDC	2400	23	2	184,25
-ACDC	2400	23	2	115,5
-ACDC	2400	39	2	63,45
-ACDC	2400	39	2	133,95
-ACDC	2400	39	2	47,0
-ACDC	2400	39	2	77,55
-ACDC	2400	40	2	88,4
-ACDC	2400	40	2	130,0
-ACDC	2400	40	2	187,2
-ACDC	2400	40	2	249,6
-ACDC	2400	56	2	55,6
-ACDC	2400	56	2	211,28
-ACDC	2400	58	2	113,98
-ACDC	2400	58	2	202,94
-ACDC	2400	58	2	119,54
-ACDC	2400	59	2	166,8
-ACDC	2400	60	2	55,6
-ACDC	2400	60	2	91,74
-ACDC	2400	73	2	93,0
-ACDC	2400	73	2	217,0
-ACDC	2400	73	2	145,7
-ACDC	2400	73	2	155,0
-ACDC	2400	74	2	98,0
-ACDC	2400	74	2	112,0
-ACDC	2400	75	2	201,6
-ACDC	2400	89	2	179,2
-ACDC	2400	90	2	246,4
-ACDC	2400	90	2	77,22
-ACDC	2400	90	2	125,84
-ACDC	2400	91	2	114,0
-ACDC	2400	91	2	126,0
-ACDC	2400	105	2	80,0
-ACDC	2400	109	2	127,26
-ACDC	2400	109	2	189,72
-ACDC	2400	126	2	60,6
-ACDC	2400	126	2	142,41
-ACDC	2400	126	2	189,72
-ACDC	2400	126	2	146,88
-ACDC	2400	127	2	127,26
-ACDC	2400	127	2	181,8
-ACDC	2400	127	2	61,2
-ACDC	2400	128	2	232,56
-ACDC	2400	143	2	72,72
-ACDC	2400	143	2	78,78
-ACDC	2400	143	2	181,8
-ACDC	2400	143	2	100,98
-ACDC	2400	143	2	58,14
-ACDC	2400	143	2	174,42
-ACDC	2400	144	2	48,48
-ACDC	2400	144	2	45,9
-ACDC	2400	145	2	144,56
-ACDC	2400	145	2	224,0
-ACDC	2400	146	2	155,68
-ACDC	2400	146	2	50,4
-ACDC	2400	146	2	72,8
-ACDC	2400	146	2	140,0
-ACDC	2400	148	2	167,272727272727
-ACDC	2400	148	2	118,181818181818
-ACDC	2400	148	2	156,363636363636
-ACDC	2400	148	2	96,3636363636364
-ACDC	2400	148	2	74,5454545454546
-ACDC	2400	148	2	236,0
-ACDC	2400	157	2	116,1
-ACDC	2400	157	2	123,84
-ACDC	2400	160	2	148,4
-ACDC	2400	161	2	97,3
-ACDC	2400	161	2	112,0
-ACDC	2400	162	2	136,22
-ACDC	2400	162	2	144,56
-ACDC	2400	162	2	196,0
-ACDC	2400	162	2	106,4
-ACDC	2400	162	2	154,0
-ACDC	2400	162	2	140,0
-ACDC	2400	163	2	55,4
-ACDC	2400	163	2	180,05
-ACDC	2400	163	2	188,36
-ACDC	2400	164	2	127,42
-ACDC	2400	164	2	80,33
-ACDC	2400	164	2	74,79
-ACDC	2400	164	2	138,5
-ACDC	2400	164	2	155,12
-ACDC	2400	165	2	124,65
-ACDC	2400	165	2	166,2
-ACDC	2400	165	2	138,5
-ACDC	2400	165	2	110,8
-ACDC	2400	166	2	132,428571428571
-ACDC	2400	166	2	217,035714285714
-ACDC	2400	166	2	167,375
-ACDC	2400	175	2	82,88
-ACDC	2400	178	2	144,04
-ACDC	2400	178	2	196,67
-ACDC	2400	178	2	163,43
-ACDC	2400	178	2	108,03
-ACDC	2400	178	2	144,04
-ACDC	2400	178	2	138,5
-ACDC	2400	179	2	166,2
-ACDC	2400	179	2	94,18
-ACDC	2400	179	2	41,55
-ACDC	2400	179	2	119,11
-ACDC	2400	179	2	166,2
-ACDC	2400	179	2	113,57
-ACDC	2400	179	2	49,86
-ACDC	2400	179	2	83,1
-ACDC	2400	179	2	113,57
-ACDC	2400	179	2	182,82
-ACDC	2400	179	2	168,97
-ACDC	2400	180	2	169,4
-ACDC	2400	181	2	104,72
-ACDC	2400	182	2	120,491228070175
-ACDC	2400	182	2	156,910714285714
-ACDC	2400	182	2	70,3392857142857
-ACDC	2400	183	2	75,726
-ACDC	2400	183	2	106,377
-ACDC	2400	183	2	120,801
-ACDC	2400	183	2	183,906
-ACDC	2400	183	2	196,0
-ACDC	2400	183	2	39,6785714285714
-ACDC	2400	183	2	95,5892857142857
-ACDC	2400	183	2	243,75
-ACDC	2400	183	2	243,75
-ACDC	2400	184	2	184,0
-ACDC	2400	184	2	188,181818181818
-ACDC	2400	184	2	158,909090909091
-ACDC	2400	184	2	129,636363636364
-ACDC	2400	184	2	154,428571428571
-ACDC	2400	184	2	147,857142857143
-ACDC	2400	184	2	228,0
-ACDC	2400	185	2	191,071428571429
-ACDC	2400	185	2	175,0
-ACDC	2400	185	2	87,719298245614
-ACDC	2400	185	2	114,035087719298
-ACDC	2400	185	2	94,7368421052632
-ACDC	2400	194	2	127,26
-ACDC	2400	194	2	236,34
-ACDC	2400	194	2	97,92
-ACDC	2400	194	2	168,3
-ACDC	2400	195	2	72,72
-ACDC	2400	195	2	159,12
-ACDC	2400	195	2	153,0
-ACDC	2400	195	2	107,1
-ACDC	2400	196	2	163,62
-ACDC	2400	196	2	122,4
-ACDC	2400	197	2	149,94
-ACDC	2400	197	2	85,68
-ACDC	2400	198	2	45,45
-ACDC	2400	198	2	299,97
-ACDC	2400	198	2	110,16
-ACDC	2400	198	2	168,3
-ACDC	2400	199	2	121,2
-ACDC	2400	199	2	115,14
-ACDC	2400	199	2	145,44
-ACDC	2400	199	2	186,66
-ACDC	2400	199	2	73,44
-ACDC	2400	199	2	76,5
-ACDC	2400	200	2	121,2
-ACDC	2400	200	2	184,83
-ACDC	2400	200	2	76,5
-ACDC	2400	200	2	122,4
-ACDC	2400	201	2	102,0
-ACDC	2400	201	2	175,0
-ACDC	2400	201	2	180,0
-ACDC	2400	202	2	105,75
-ACDC	2400	202	2	75,9
-ACDC	2400	202	2	135,7
-ACDC	2400	202	2	138,0
-ACDC	2400	210	2	87,248
-ACDC	2400	210	2	74,48
-ACDC	2400	210	2	138,32
-ACDC	2400	210	2	89,376
-ACDC	2400	211	2	27,765
-ACDC	2400	211	2	53,679
-ACDC	2400	211	2	64,785
-ACDC	2400	211	2	98,156
-ACDC	2400	211	2	100,008
-ACDC	2400	211	2	90,748
-ACDC	2400	211	2	70,376
-ACDC	2400	212	2	105,507
-ACDC	2400	215	2	96,0
-ACDC	2400	215	2	100,0
-ACDC	2400	215	2	96,0
-ACDC	2400	215	2	120,0
-ACDC	2400	216	2	116,28
-ACDC	2400	216	2	73,92
-ACDC	2400	216	2	107,8
-ACDC	2400	219	2	84,56
-ACDC	2400	219	2	123,82
-ACDC	2400	228	2	61,083
-ACDC	2400	228	2	74,04
-ACDC	2400	228	2	51,8
-ACDC	2400	228	2	88,8
-ACDC	2400	228	2	118,4
-ACDC	2400	229	2	125,46
-ACDC	2400	229	2	123,2
-ACDC	2400	232	2	61,2
-ACDC	2400	232	2	140,76
-ACDC	2400	232	2	137,7
-ACDC	2400	232	2	226,44
-ACDC	2400	232	2	61,6
-ACDC	2400	232	2	138,6
-ACDC	2400	232	2	160,16
-ACDC	2400	233	2	52,02
-ACDC	2400	233	2	195,84
-ACDC	2400	233	2	138,6
-ACDC	2400	233	2	169,4
-ACDC	2400	233	2	83,16
-ACDC	2400	233	2	61,6
-ACDC	2400	245	2	96,252
-ACDC	2400	246	2	90,0
-ACDC	2400	250	2	84,0
-ACDC	2400	263	2	83,9344262295082
-ACDC	2400	263	2	117,508196721312
-ACDC	2400	281	2	104,918032786885
-ACDC	2400	281	2	100,72131147541
-ACDC	2400	282	2	193,122807017544
-ACDC	2400	282	2	85,3333333333333
-ACDC	2400	282	2	130,245614035088
-ACDC	2400	41	2	
-ACDC	2400	42	2	
-ACDC	2400	55	2	
-ACDC	2400	57	2	
-ACDC	2400	72	2	
-ACDC	2400	76	2	
-ACDC	2400	88	2	
-ACDC	2400	106	2	
-ACDC	2400	110	2	
-ACDC	2400	112	2	
-ACDC	2400	113	2	
-ACDC	2400	122	2	
-ACDC	2400	123	2	
-ACDC	2400	125	2	
-ACDC	2400	129	2	
-ACDC	2400	130	2	
-ACDC	2400	141	2	
-ACDC	2400	142	2	
-ACDC	2400	147	2	
-ACDC	2400	158	2	
-ACDC	2400	159	2	
-ACDC	2400	174	2	
-ACDC	2400	176	2	
-ACDC	2400	177	2	
-ACDC	2400	192	2	
-ACDC	2400	193	2	
-ACDC	2400	213	2	
-ACDC	2400	214	2	
-ACDC	2400	218	2	
-ACDC	2400	247	2	
-ACDC	2400	262	2	
-ACDC	2400	265	2	
-ACDC	2400	266	2	
-ACDC	2400	280	2	
-ACDC	2400	283	2	
-ACDC	2400	284	2	
-ACDC	2400	299	2	
-ACDC	2400	300	2	
-ACDC	2400	301	2	
+Region*Label	Region*Area	Point transect*Label	Point transect*Survey effort	Observation*Radial distance
+ACDC	2400	23	2	151,25
+ACDC	2400	23	2	184,25
+ACDC	2400	23	2	115,5
+ACDC	2400	39	2	63,45
+ACDC	2400	39	2	133,95
+ACDC	2400	39	2	47,0
+ACDC	2400	39	2	77,55
+ACDC	2400	40	2	88,4
+ACDC	2400	40	2	130,0
+ACDC	2400	40	2	187,2
+ACDC	2400	40	2	249,60000000000002
+ACDC	2400	41	2	
+ACDC	2400	42	2	
+ACDC	2400	55	2	
+ACDC	2400	56	2	55,6
+ACDC	2400	56	2	211,27999999999997
+ACDC	2400	57	2	
+ACDC	2400	58	2	119,54
+ACDC	2400	58	2	113,98
+ACDC	2400	58	2	202,94
+ACDC	2400	59	2	166,8
+ACDC	2400	60	2	55,6
+ACDC	2400	60	2	91,74
+ACDC	2400	72	2	
+ACDC	2400	73	2	93,0
+ACDC	2400	73	2	155,0
+ACDC	2400	73	2	217,0
+ACDC	2400	73	2	145,7
+ACDC	2400	74	2	112,0
+ACDC	2400	74	2	98,0
+ACDC	2400	75	2	201,6
+ACDC	2400	76	2	
+ACDC	2400	88	2	
+ACDC	2400	89	2	179,2
+ACDC	2400	90	2	77,22
+ACDC	2400	90	2	125,83999999999999
+ACDC	2400	90	2	246,39999999999998
+ACDC	2400	91	2	126,0
+ACDC	2400	91	2	114,0
+ACDC	2400	105	2	80,0
+ACDC	2400	106	2	
+ACDC	2400	109	2	189,72
+ACDC	2400	109	2	127,26
+ACDC	2400	110	2	
+ACDC	2400	112	2	
+ACDC	2400	113	2	
+ACDC	2400	122	2	
+ACDC	2400	123	2	
+ACDC	2400	125	2	
+ACDC	2400	126	2	189,72
+ACDC	2400	126	2	142,41
+ACDC	2400	126	2	60,6
+ACDC	2400	126	2	146,88
+ACDC	2400	127	2	61,2
+ACDC	2400	127	2	181,8
+ACDC	2400	127	2	127,26
+ACDC	2400	128	2	232,56
+ACDC	2400	129	2	
+ACDC	2400	130	2	
+ACDC	2400	141	2	
+ACDC	2400	142	2	
+ACDC	2400	143	2	174,42
+ACDC	2400	143	2	58,14
+ACDC	2400	143	2	100,98
+ACDC	2400	143	2	181,8
+ACDC	2400	143	2	78,78
+ACDC	2400	143	2	72,72
+ACDC	2400	144	2	45,9
+ACDC	2400	144	2	48,48
+ACDC	2400	145	2	224,0
+ACDC	2400	145	2	144,56
+ACDC	2400	146	2	140,0
+ACDC	2400	146	2	155,68
+ACDC	2400	146	2	50,4
+ACDC	2400	146	2	72,8
+ACDC	2400	147	2	
+ACDC	2400	148	2	167,27272727272728
+ACDC	2400	148	2	118,18181818181817
+ACDC	2400	148	2	96,36363636363636
+ACDC	2400	148	2	74,54545454545455
+ACDC	2400	148	2	236,0
+ACDC	2400	148	2	156,36363636363635
+ACDC	2400	157	2	116,1
+ACDC	2400	157	2	123,84
+ACDC	2400	158	2	
+ACDC	2400	159	2	
+ACDC	2400	160	2	148,39999999999998
+ACDC	2400	161	2	112,0
+ACDC	2400	161	2	97,3
+ACDC	2400	162	2	154,0
+ACDC	2400	162	2	136,22
+ACDC	2400	162	2	144,56
+ACDC	2400	162	2	196,0
+ACDC	2400	162	2	106,4
+ACDC	2400	162	2	140,0
+ACDC	2400	163	2	55,4
+ACDC	2400	163	2	180,05
+ACDC	2400	163	2	188,36
+ACDC	2400	164	2	80,33
+ACDC	2400	164	2	155,12
+ACDC	2400	164	2	138,5
+ACDC	2400	164	2	127,42
+ACDC	2400	164	2	74,79
+ACDC	2400	165	2	124,65
+ACDC	2400	165	2	166,2
+ACDC	2400	165	2	138,5
+ACDC	2400	165	2	110,8
+ACDC	2400	166	2	132,42857142857142
+ACDC	2400	166	2	217,03571428571428
+ACDC	2400	166	2	167,375
+ACDC	2400	174	2	
+ACDC	2400	175	2	82,88
+ACDC	2400	176	2	
+ACDC	2400	177	2	
+ACDC	2400	178	2	144,04
+ACDC	2400	178	2	196,67
+ACDC	2400	178	2	163,43
+ACDC	2400	178	2	108,03
+ACDC	2400	178	2	144,04
+ACDC	2400	178	2	138,5
+ACDC	2400	179	2	113,57
+ACDC	2400	179	2	166,2
+ACDC	2400	179	2	113,57
+ACDC	2400	179	2	49,86
+ACDC	2400	179	2	83,1
+ACDC	2400	179	2	94,18
+ACDC	2400	179	2	182,82
+ACDC	2400	179	2	41,55
+ACDC	2400	179	2	119,11
+ACDC	2400	179	2	166,2
+ACDC	2400	179	2	168,97
+ACDC	2400	180	2	169,4
+ACDC	2400	181	2	104,72
+ACDC	2400	182	2	120,49122807017544
+ACDC	2400	182	2	156,91071428571428
+ACDC	2400	182	2	70,33928571428572
+ACDC	2400	183	2	183,906
+ACDC	2400	183	2	75,726
+ACDC	2400	183	2	243,75
+ACDC	2400	183	2	243,75
+ACDC	2400	183	2	95,58928571428572
+ACDC	2400	183	2	106,377
+ACDC	2400	183	2	196,0
+ACDC	2400	183	2	120,801
+ACDC	2400	183	2	39,67857142857143
+ACDC	2400	184	2	228,0
+ACDC	2400	184	2	147,85714285714286
+ACDC	2400	184	2	184,0
+ACDC	2400	184	2	188,1818181818182
+ACDC	2400	184	2	158,9090909090909
+ACDC	2400	184	2	129,63636363636363
+ACDC	2400	184	2	154,42857142857142
+ACDC	2400	185	2	175,0
+ACDC	2400	185	2	87,71929824561403
+ACDC	2400	185	2	94,73684210526315
+ACDC	2400	185	2	114,03508771929823
+ACDC	2400	185	2	191,07142857142858
+ACDC	2400	192	2	
+ACDC	2400	193	2	
+ACDC	2400	194	2	127,26
+ACDC	2400	194	2	168,3
+ACDC	2400	194	2	236,33999999999997
+ACDC	2400	194	2	97,92
+ACDC	2400	195	2	153,0
+ACDC	2400	195	2	159,12
+ACDC	2400	195	2	72,72
+ACDC	2400	195	2	107,1
+ACDC	2400	196	2	122,4
+ACDC	2400	196	2	163,61999999999998
+ACDC	2400	197	2	85,68
+ACDC	2400	197	2	149,94
+ACDC	2400	198	2	45,45
+ACDC	2400	198	2	299,96999999999997
+ACDC	2400	198	2	110,16
+ACDC	2400	198	2	168,3
+ACDC	2400	199	2	121,19999999999999
+ACDC	2400	199	2	115,13999999999999
+ACDC	2400	199	2	145,44
+ACDC	2400	199	2	186,66
+ACDC	2400	199	2	73,44
+ACDC	2400	199	2	76,5
+ACDC	2400	200	2	76,5
+ACDC	2400	200	2	122,4
+ACDC	2400	200	2	121,19999999999999
+ACDC	2400	200	2	184,83
+ACDC	2400	201	2	102,0
+ACDC	2400	201	2	175,0
+ACDC	2400	201	2	180,0
+ACDC	2400	202	2	135,7
+ACDC	2400	202	2	105,75
+ACDC	2400	202	2	75,89999999999999
+ACDC	2400	202	2	138,0
+ACDC	2400	210	2	74,48
+ACDC	2400	210	2	89,376
+ACDC	2400	210	2	138,32
+ACDC	2400	210	2	87,248
+ACDC	2400	211	2	98,156
+ACDC	2400	211	2	27,765
+ACDC	2400	211	2	53,679
+ACDC	2400	211	2	100,00800000000001
+ACDC	2400	211	2	90,748
+ACDC	2400	211	2	70,376
+ACDC	2400	211	2	64,785
+ACDC	2400	212	2	105,507
+ACDC	2400	213	2	
+ACDC	2400	214	2	
+ACDC	2400	215	2	120,0
+ACDC	2400	215	2	96,0
+ACDC	2400	215	2	100,0
+ACDC	2400	215	2	96,0
+ACDC	2400	216	2	116,28
+ACDC	2400	216	2	107,8
+ACDC	2400	216	2	73,92
+ACDC	2400	218	2	
+ACDC	2400	219	2	84,56
+ACDC	2400	219	2	123,82
+ACDC	2400	228	2	118,4
+ACDC	2400	228	2	88,80000000000001
+ACDC	2400	228	2	61,083
+ACDC	2400	228	2	74,04
+ACDC	2400	228	2	51,8
+ACDC	2400	229	2	125,46
+ACDC	2400	229	2	123,2
+ACDC	2400	232	2	160,16
+ACDC	2400	232	2	61,2
+ACDC	2400	232	2	140,76
+ACDC	2400	232	2	137,7
+ACDC	2400	232	2	226,44
+ACDC	2400	232	2	61,6
+ACDC	2400	232	2	138,6
+ACDC	2400	233	2	52,02
+ACDC	2400	233	2	195,84
+ACDC	2400	233	2	138,6
+ACDC	2400	233	2	169,4
+ACDC	2400	233	2	83,16
+ACDC	2400	233	2	61,6
+ACDC	2400	245	2	96,252
+ACDC	2400	246	2	90,0
+ACDC	2400	247	2	
+ACDC	2400	250	2	84,0
+ACDC	2400	262	2	
+ACDC	2400	263	2	83,9344262295082
+ACDC	2400	263	2	117,50819672131148
+ACDC	2400	265	2	
+ACDC	2400	266	2	
+ACDC	2400	280	2	
+ACDC	2400	281	2	104,91803278688525
+ACDC	2400	281	2	100,72131147540983
+ACDC	2400	282	2	85,33333333333333
+ACDC	2400	282	2	193,12280701754383
+ACDC	2400	282	2	130,24561403508773
+ACDC	2400	283	2	
+ACDC	2400	284	2	
+ACDC	2400	299	2	
+ACDC	2400	300	2	
+ACDC	2400	301	2
```

### Comparing `pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/output.txt` & `pyaudisam-1.0.1/docs/how-it-works/preanlys/SylvAtri-ab-10mn-m-haz-cos-fx8uk14r/output.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,756 +1,698 @@
-	Estimation Options Listing	
-
- Parameter Estimation Specification
- ----------------------------------
- Encounter rate for all data combined
- Detection probability for all data combined
- Density for all data combined
-
- Distances:
- ----------
- Analysis based on exact distances
- Width: use largest measurement/last interval endpoint
-
- Estimators:
- -----------
- Estimator  1
- Key: Uniform
- Adjustments - Function                 : Cosines
-             - Term selection mode      : Sequential
-             - Term selection criterion : Akaike Information Criterion (AIC)
-             - Distances scaled by      : W (right truncation distance)
-
- Estimator selection: Choose estimator with minimum  AIC
- Estimation functions: constrained to be nearly monotone non-increasing
-
- Variances:
- ----------
- Variance of n: Empirical estimate from sample
-                (design-derived estimator R2/P2)
- Variance of f(0): MLE estimate
-
- Goodness of fit:
- ----------------
- Cut points chosen by program
-
-
-
- Glossary of terms
- -----------------
-
- Data items:
- n    - number of observed objects (single or clusters of animals)
- L    - total length of transect line(s) 
- k    - number of samples
- K    - point transect effort, typically K=k
- T    - length of time searched in cue counting
- ER   - encounter rate (n/L or n/K or n/T)
- W    - width of line transect or radius of point transect
- x(i) - distance to i-th observation
- s(i) - cluster size of i-th observation
- r-p  - probability for regression test
- chi-p- probability for chi-square goodness-of-fit test
-
-
- Parameters or functions of parameters:
- m    - number of parameters in the model
- A(I) - i-th parameter in the estimated probability density function(pdf)
- f(0) - 1/u = value of pdf at zero for line transects
- u    - W*p = ESW, effective detection area for line transects
- h(0) - 2*PI/v
- v    - PI*W*W*p, is the effective detection area for point transects
- p    - probability of observing an object in defined area
- ESW  - for line transects, effective strip width = W*p
- EDR  - for point transects, effective detection radius  = W*sqrt(p)
- rho  - for cue counts, the cue rate
- DS   - estimate of density of clusters
- E(S) - estimate of expected value of cluster size
- D    - estimate of density of animals
- N    - estimate of number of animals in specified area
-	Detection Fct/Global/Model Fitting	
-
-
- Effort        :    208.0000    
- # samples     :   104
- Width         :    500.0000    
- # observations:   423
-
-
-
- Model  1
-    Uniform key, k(y) = 1/W
-       Results:
-       Convergence was achieved with    1 function evaluations.
-       Final Ln(likelihood) value =  -3067.2869    
-       Akaike information criterion =   6134.5737    
-       Bayesian information criterion =   6134.5737    
-       AICc =   6134.5737    
-       Final parameter values: 
-
-
- Model  2
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1
-       Results:
-       Convergence was achieved with   47 function evaluations.
-       Final Ln(likelihood) value =  -2635.5634    
-       Akaike information criterion =   5273.1270    
-       Bayesian information criterion =   5277.1743    
-       AICc =   5273.1362    
-       Final parameter values:  0.99768112    
-
-    Likelihood ratio test between models  1 and  2
-       Likelihood ratio test value    =   863.4471
-       Probability of a greater value =   0.000000
- *** Model  2 selected over model  1 based on minimum AIC              
-
-
- Model  3
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2
-       Results:
-       Convergence was achieved with   37 function evaluations.
-       Final Ln(likelihood) value =  -2459.2116    
-       Akaike information criterion =   4922.4233    
-       Bayesian information criterion =   4930.5181    
-       AICc =   4922.4521    
-       Final parameter values:   1.3676947     0.37907805    
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-
-    Likelihood ratio test between models  2 and  3
-       Likelihood ratio test value    =   352.7036
-       Probability of a greater value =   0.000000
- *** Model  3 selected over model  2 based on minimum AIC              
-
-
- Model  4
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3
-       Results:
-       Convergence was achieved with   28 function evaluations.
-       Final Ln(likelihood) value =  -2354.2419    
-       Akaike information criterion =   4714.4839    
-       Bayesian information criterion =   4726.6260    
-       AICc =   4714.5410    
-       Final parameter values:   1.5986401     0.83347200     0.23245685    
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-
-    Likelihood ratio test between models  3 and  4
-       Likelihood ratio test value    =   209.9394
-       Probability of a greater value =   0.000000
- *** Model  4 selected over model  3 based on minimum AIC              
-
-
- Model  5
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3, 4
-       Results:
-       Convergence was achieved with   32 function evaluations.
-       Final Ln(likelihood) value =  -12559.731    
-       Akaike information criterion =   25127.463    
-       Bayesian information criterion =   25143.652    
-       AICc =   25127.559    
-       Final parameter values:  -1000000.0     -917249.06     -458764.69     -395884.09    
-
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-      ** Warning: Parameter  1 is at a lower bound. **
-
-    Likelihood ratio test between models  4 and  5
-       Likelihood ratio test value    = **********
-       Probability of a greater value =   1.000000
- *** Model  4 selected over model  5 based on minimum AIC              
-	Detection Fct/Global/Parameter Estimates	
-
-
- Effort        :    208.0000    
- # samples     :   104
- Width         :    500.0000    
- # observations:   423
-
- Model
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3
-
-
-              Point        Standard    Percent Coef.        95 Percent
-  Parameter   Estimate       Error      of Variation     Confidence Interval
-  ---------  -----------  -----------  --------------  ----------------------
-    A( 1)      1.599       0.7702E-02
-    A( 2)     0.8335       0.2562E-01
-    A( 3)     0.2325       0.2063E-01
-    h(0)     0.85807E-04  0.20716E-05       2.41      0.81830E-04  0.89976E-04
-    p        0.93233E-01  0.22509E-02       2.41      0.88912E-01  0.97763E-01
-    EDR       152.67       1.8429           1.21       149.09       156.34    
-  ---------  -----------  -----------  --------------  ----------------------
-
-
- Sampling Correlation of Estimated Parameters
-
-
-         A( 1)   A( 2)   A( 3)
- A( 1)  1.000   0.742   0.576
- A( 2)  0.742   1.000   0.970
- A( 3)  0.576   0.970   1.000
-	Detection Fct/Global/Plot: Qq-plot	
-	Detection Fct/Global/K-S GOF Test	
-
-
- Kolmogorov-Smirnov test
- -----------------------
-
- D_n                      = 0.1774                 p  = 0.0000
-
-
- Cramer-von Mises family tests
- -----------------------------
-
- W-sq (uniform weighting) = 5.9977          0.000 < p <= 0.001
-   Relevant critical values:
-     W-sq crit(alpha=0.001) = 1.1724
-
- C-sq (cosine weighting)  = 4.1992          0.000 < p <= 0.001
-   Relevant critical values:
-     C-sq crit(alpha=0.001) = 0.8110
-	Detection Fct/Global/Plot: Detection Probability 1	
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-     1.8438 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |******                                                            |
-     1.6190 +*    *                                                            +
-            |*    *                                                            |
-            |*    *                                                            |
-            |*    *                                                            |
-            |*    *                                                            |
- D   1.3941 +*    *                                                            +
- e          |*    *                                                            |
- t          |*    *                                                            |
- e          |*    ******                                                       |
- c          |*    *    *                                                       |
- t   1.1693 +*    *    *                                                       +
- i          |*    *    *                                                       |
- o          |*    *    *                                                       |
- n          |*    *    *                                                       |
-            |*fff *    *                                                       |
- P   0.9444 +*   fff   *                                                       +
- r          |*    * f  *                                                       |
- o          |*    *  f ******                                                  |
- b          |*    *   ff    *                                                  |
- a          |*    *    *f   *                                                  |
- b   0.7195 +*    *    * f  *                                                  +
- i          |*    *    *  f *                                                  |
- l          |*    *    *   f*                                                  |
- i          |*    *    *    f                                                  |
- t          |*    *    *    *f                                                 |
- y   0.4947 +*    *    *    * f                                                +
-            |*    *    *    *  f                                               |
-            |*    *    *    ****f*                                             |
-            |*    *    *    *    ff                                            |
-            |*    *    *    *    * f                                           |
-     0.2698 +*    *    *    *    *  f                                          +
-            |*    *    *    *    *   f                                         |
-            |*    *    *    *    *****ff                                       |
-            |*    *    *    *    *    * ff                                     |
-            |*    *    *    *    *    ****ff                                   |
-     0.0450 +*    *    *    *    *    *    *ffff*                              +
-            |                                   fffffffffffffffffffffffffffffff|
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-          0.000        115.385   192.308   269.231   346.154   423.077   500.000
-              38.462        153.846   230.769   307.692   384.615   461.538     
-
-                                Radial distance in meters          
-	Detection Fct/Global/Plot: Pdf 1	
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-     0.0076 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |          ******                                                  |
-     0.0067 +          *    *                                                  +
-            |          *    *                                                  |
-            |     ******    *                                                  |
-            |     *    *    *                                                  |
-            |     *    *    *                                                  |
-     0.0058 +     *    * fffff                                                 +
-            |     *    *f   * f                                                |
- P          |     *    *    *  f                                               |
- r          |     *    f    *   f                                              |
- o          |     *   f*    *    f                                             |
- b   0.0048 +     *    *    *                                                  +
- a          |     *  f *    ******f                                            |
- b          |     *    *    *    *                                             |
- i          |     *    *    *    * f                                           |
- l          |     * f  *    *    *  f                                          |
- i   0.0039 +     *    *    *    *                                             +
- t          |     *f   *    *    *   f                                         |
- y          |     *    *    *    *                                             |
-            |     *    *    *    *    f                                        |
- D          |     f    *    *    *                                             |
- e   0.0030 +     *    *    *    *     f                                       +
- n          |******    *    *    ******                                        |
- s          |*   f*    *    *    *    * f                                      |
- i          |*    *    *    *    *    *                                        |
- t          |*    *    *    *    *    *  f                                     |
- y   0.0020 +*    *    *    *    *    *                                        +
-            |*  f *    *    *    *    *   f                                    |
-            |*    *    *    *    *    *    f                                   |
-            |*    *    *    *    *    *                                        |
-            |* f  *    *    *    *    ******f                                  |
-     0.0011 +*    *    *    *    *    *    * f                                 +
-            |*    *    *    *    *    *    *  f                                |
-            |*f   *    *    *    *    *    *   f                               |
-            |*    *    *    *    *    *    *****f                              |
-            |*    *    *    *    *    *    *    *f            fffffff          |
-     0.0002 +*    *    *    *    *    *    *    **ffffffffffff       ffffff    +
-            |                                             *****************ffff|
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-          0.000        115.385   192.308   269.231   346.154   423.077   500.000
-              38.462        153.846   230.769   307.692   384.615   461.538     
-
-                                Radial distance in meters          
-	Detection Fct/Global/Chi-sq GOF Test 1	
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        38.5           45       26.11       13.668
-   2      38.5        76.9          102       69.99       14.644
-   3      76.9        115.          113       92.52        4.534
-   4      115.        154.           75       89.87        2.460
-   5      154.        192.           46       68.21        7.232
-   6      192.        231.           22       39.76        7.936
-   7      231.        269.            9       16.36        3.313
-   8      269.        308.            4        4.13        0.004
-   9      308.        346.            3        1.97        0.537
-  10      346.        385.            1        4.21        2.451
-  11      385.        423.            1        5.32        3.512
-  12      423.        462.            1        3.52        1.802
-  13      462.        500.            1        1.02        0.001
- -----------------------------------------------------------------
- Total Chi-square value =    62.0912  Degrees of Freedom =  9.00
-
-Probability of a greater chi-square value, P = 0.00000
-
- The program has limited capability for pooling.  The user should
- judge the necessity for pooling and if necessary, do pooling by hand.
-
- Goodness of Fit Testing with some Pooling
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        38.5           45       26.11       13.668
-   2      38.5        76.9          102       69.99       14.644
-   3      76.9        115.          113       92.52        4.534
-   4      115.        154.           75       89.87        2.460
-   5      154.        192.           46       68.21        7.232
-   6      192.        231.           22       39.76        7.936
-   7      231.        269.            9       16.36        3.313
-   8      269.        308.            4        4.13        0.004
-   9      308.        346.            3        1.97        0.537
-  10      346.        385.            1        4.21        2.451
-  11      385.        423.            1        5.32        3.512
-  12      423.        500.            2        4.54        1.422
- -----------------------------------------------------------------
- Total Chi-square value =    61.7104  Degrees of Freedom =  8.00
-
-Probability of a greater chi-square value, P = 0.00000
-	Detection Fct/Global/Plot: Detection Probability 2	
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-     2.1336 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |****                                                              |
-     1.8734 +*  *                                                              +
-            |*  *                                                              |
-            |*  *                                                              |
-            |*  *                                                              |
-            |*  *                                                              |
- D   1.6132 +*  *                                                              +
- e          |*  *                                                              |
- t          |*  *                                                              |
- e          |*  *                                                              |
- c          |*  *                                                              |
- t   1.3530 +*  *****                                                          +
- i          |*  *   ****                                                       |
- o          |*  *   *  *                                                       |
- n          |*  *   *  *                                                       |
-            |*  *   *  *                                                       |
- P   1.0928 +*  *   *  *                                                       +
- r          |*  *   *  *                                                       |
- o          |*ffff  *  ****                                                    |
- b          |*  * ff*  *  *                                                    |
- a          |*  *   ff *  *                                                    |
- b   0.8326 +*  *   * f*  *                                                    +
- i          |*  *   *  ff *                                                    |
- l          |*  *   *  * f*                                                    |
- i          |*  *   *  *  f                                                    |
- t          |*  *   *  *  *f                                                   |
- y   0.5724 +*  *   *  *  **ff                                                 +
-            |*  *   *  *  *  *f                                                |
-            |*  *   *  *  *  * f                                               |
-            |*  *   *  *  *  ***f*                                             |
-            |*  *   *  *  *  *   ff                                            |
-     0.3122 +*  *   *  *  *  *   **f*                                          +
-            |*  *   *  *  *  *   *  f                                          |
-            |*  *   *  *  *  *   *  *ff                                        |
-            |*  *   *  *  *  *   *  *  ff                                      |
-            |*  *   *  *  *  *   *  **** fff                                   |
-     0.0520 +*  *   *  *  *  *   *  *  *****ffff                               +
-            |                                 **fffffffffffffffffffffffffffffff|
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-          0.000      100.000   175.000   250.000  325.000   400.000   475.000   
-            25.000      125.000   200.000   275.000   350.000  425.000   500.000
-
-                                Radial distance in meters          
-	Detection Fct/Global/Plot: Pdf 2	
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-     0.0080 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |          ****                                                    |
-     0.0070 +          *  *                                                    +
-            |       ****  *                                                    |
-            |       *  *  *                                                    |
-            |       *  *  *                                                    |
-            |       *  *  *                                                    |
-     0.0061 +       *  *  *                                                    +
-            |       *  *  ffff                                                 |
- P          |       *  * f****f                                                |
- r          |       *  *f *  * f                                               |
- o          |       *  f  *  *  f                                              |
- b   0.0051 +       *  *  *  *   f                                             +
- a          |       * f*  *  *                                                 |
- b          |       *  *  *  *****f                                            |
- i          |       *f *  *  *   *                                             |
- l          |   *****  *  *  *   * f                                           |
- i   0.0041 +   *   f  *  *  *   ***f                                          +
- t          |   *   *  *  *  *   *  *                                          |
- y          |   *  f*  *  *  *   *  *f                                         |
-            |   *   *  *  *  *   *  *                                          |
- D          |   *   *  *  *  *   *  * f                                        |
- e   0.0031 +   * f *  *  *  *   *  *                                          +
- n          |   *   *  *  *  *   *  *  f                                       |
- s          |   *   *  *  *  *   *  *                                          |
- i          |   *f  *  *  *  *   *  *   f                                      |
- t          |   *   *  *  *  *   *  *    f                                     |
- y   0.0021 +****   *  *  *  *   *  *                                          +
-            |*  f   *  *  *  *   *  ****  f                                    |
-            |*  *   *  *  *  *   *  *  *                                       |
-            |*  *   *  *  *  *   *  *  *   f                                   |
-            |* f*   *  *  *  *   *  *  *    f                                  |
-     0.0012 +*  *   *  *  *  *   *  *  *     f                                 +
-            |*  *   *  *  *  *   *  *  *                                       |
-            |*  *   *  *  *  *   *  *  *******f                                |
-            |*f *   *  *  *  *   *  *  *  *   *ff                              |
-            |*  *   *  *  *  *   *  *  *  *   *  f            fffffff          |
-     0.0002 +*  *   *  *  *  *   *  *  *  *   ****ffffffffffff       fffff     +
-            |                                          *******************fffff|
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-          0.000      100.000   175.000   250.000  325.000   400.000   475.000   
-            25.000      125.000   200.000   275.000   350.000  425.000   500.000
-
-                                Radial distance in meters          
-	Detection Fct/Global/Chi-sq GOF Test 2	
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        25.0           22       11.21       10.384
-   2      25.0        50.0           46       32.07        6.047
-   3      50.0        75.0           73       48.57       12.294
-   4      75.0        100.           77       58.70        5.702
-   5      100.        125.           60       61.69        0.046
-   6      125.        150.           50       58.00        1.103
-   7      150.        175.           44       49.18        0.546
-   8      175.        200.           20       37.48        8.154
-   9      200.        225.            9       25.31       10.508
-  10      225.        250.            9       14.71        2.215
-  11      250.        275.            3        7.01        2.292
-  12      275.        300.            2        2.63        0.149
-  13      300.        325.            3        1.13        3.066
-  14      325.        350.            1        1.54        0.188
-  15      350.        375.            1        2.64        1.019
-  16      375.        400.            0        3.44        3.438
-  17      400.        425.            1        3.39        1.687
-  18      425.        450.            1        2.53        0.921
-  19      450.        475.            0        1.32        1.320
-  20      475.        500.            1        0.46        0.644
- -----------------------------------------------------------------
- Total Chi-square value =    71.7249  Degrees of Freedom = 16.00
-
-Probability of a greater chi-square value, P = 0.00000
-
- The program has limited capability for pooling.  The user should
- judge the necessity for pooling and if necessary, do pooling by hand.
-
- Goodness of Fit Testing with some Pooling
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        25.0           22       11.21       10.384
-   2      25.0        50.0           46       32.07        6.047
-   3      50.0        75.0           73       48.57       12.294
-   4      75.0        100.           77       58.70        5.702
-   5      100.        125.           60       61.69        0.046
-   6      125.        150.           50       58.00        1.103
-   7      150.        175.           44       49.18        0.546
-   8      175.        200.           20       37.48        8.154
-   9      200.        225.            9       25.31       10.508
-  10      225.        250.            9       14.71        2.215
-  11      250.        275.            3        7.01        2.292
-  12      275.        300.            2        2.63        0.149
-  13      300.        325.            3        1.13        3.066
-  14      325.        350.            1        1.54        0.188
-  15      350.        375.            1        2.64        1.019
-  16      375.        400.            0        3.44        3.438
-  17      400.        425.            1        3.39        1.687
-  18      425.        500.            2        4.30        1.232
- -----------------------------------------------------------------
- Total Chi-square value =    70.0717  Degrees of Freedom = 14.00
-
-Probability of a greater chi-square value, P = 0.00000
-	Detection Fct/Global/Plot: Detection Probability 3	
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-     2.8367 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |***                                                               |
-     2.4907 +* *                                                               +
-            |* *                                                               |
-            |* *                                                               |
-            |* *                                                               |
-            |* *                                                               |
- D   2.1448 +* *                                                               +
- e          |* *                                                               |
- t          |* *                                                               |
- e          |* *                                                               |
- c          |* *                                                               |
- t   1.7989 +* *                                                               +
- i          |* *                                                               |
- o          |* ***                                                             |
- n          |* * *                                                             |
-            |* * *                                                             |
- P   1.4529 +* * *                                                             +
- r          |* * *  ***                                                        |
- o          |* * *  * *                                                        |
- b          |* * *  * *                                                        |
- a          |* * **** *                                                        |
- b   1.1070 +* * *  * ***                                                      +
- i          |* * *  * * *                                                      |
- l          |*fffff * * ***                                                    |
- i          |* * * fff* * *                                                    |
- t          |* * *  * ff* *                                                    |
- y   0.7611 +* * *  * * f *                                                    +
-            |* * *  * * *ff**                                                  |
-            |* * *  * * * *ff                                                  |
-            |* * *  * * * * *f                                                 |
-            |* * *  * * * * **ff                                               |
-     0.4151 +* * *  * * * * * * f                                              +
-            |* * *  * * * * * ***ff*                                           |
-            |* * *  * * * * * *  * ff                                          |
-            |* * *  * * * * * *  * * ff                                        |
-            |* * *  * * * * * *  * ****fff                                     |
-     0.0692 +* * *  * * * * * *  * * * ***fffff                                +
-            |                                **ffffffffffffffffffffffffffffffff|
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-          0.000      100.000  166.667   250.000  316.667  383.333    466.667    
-           16.667   83.333  150.000   233.333  300.000  366.667   450.000       
-
-                                Radial distance in meters          
-	Detection Fct/Global/Plot: Pdf 3	
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-     0.0083 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |           ***                                                    |
-     0.0073 +           * *                                                    +
-            |           * *                                                    |
-            |       ***** *                                                    |
-            |       * * * *                                                    |
-            |       * * * ***                                                  |
-     0.0063 +       * * * * *                                                  +
-            |       * * * * *                                                  |
- P          |       * * * ffff                                                 |
- r          |       * * *f* * f                                                |
- o          |       * * f * *  f                                               |
- b   0.0052 +       * *f* * *   f                                              +
- a          |       * f * * ***  f                                             |
- b          |       * * * * * *                                                |
- i          |       *f* * * * *  *f*                                           |
- l          |       * * * * * *  * f                                           |
- i   0.0042 +    ***f * * * * *  * *                                           +
- t          |    *  * * * * * **** *f                                          |
- y          |    *  * * * * * *  * *                                           |
-            |  *** f* * * * * *  * * f                                         |
- D          |  * *  * * * * * *  * *                                           |
- e   0.0032 +  * *  * * * * * *  * *  f                                        +
- n          |  * *f * * * * * *  * *   f                                       |
- s          |  * *  * * * * * *  * *                                           |
- i          |  * f  * * * * * *  * *    f                                      |
- t          |  * *  * * * * * *  * ***                                         |
- y   0.0022 +  * *  * * * * * *  * * *** f                                     +
-            |  *f*  * * * * * *  * * * *  f                                    |
-            |*** *  * * * * * *  * * * *                                       |
-            |* * *  * * * * * *  * * * *   f                                   |
-            |* * *  * * * * * *  * * * *    f                                  |
-     0.0012 +* f *  * * * * * *  * * * *                                       +
-            |* * *  * * * * * *  * * * *   **f                                 |
-            |* * *  * * * * * *  * * * ***** *f                                |
-            |*f* *  * * * * * *  * * * * * * * ff                              |
-            |* * *  * * * * * *  * * * * * * *   f            ffffff           |
-     0.0002 +* * *  * * * * * *  * * * * * * *****ffff*fffffff     *ffffff  ***+
-            |                                         f      *******   ***fffff|
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-          0.000      100.000  166.667   250.000  316.667  383.333    466.667    
-           16.667   83.333  150.000   233.333  300.000  366.667   450.000       
-
-                                Radial distance in meters          
-	Detection Fct/Global/Chi-sq GOF Test 3	
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        16.7           13        5.02       12.713
-   2      16.7        33.3           25       14.73        7.156
-   3      33.3        50.0           30       23.54        1.775
-   4      50.0        66.7           48       30.91        9.452
-   5      66.7        83.3           49       36.45        4.323
-   6      83.3        100.           53       39.91        4.291
-   7      100.        117.           46       41.23        0.552
-   8      117.        133.           35       40.50        0.746
-   9      133.        150.           29       37.96        2.115
-  10      150.        167.           32       34.00        0.117
-  11      167.        183.           17       29.05        5.000
-  12      183.        200.           15       23.62        3.144
-  13      200.        217.            6       18.16        8.144
-  14      217.        233.            5       13.10        5.009
-  15      233.        250.            7        8.75        0.351
-  16      250.        267.            2        5.32        2.073
-  17      267.        283.            1        2.89        1.236
-  18      283.        300.            2        1.42        0.235
-  19      300.        317.            2        0.78        1.903
-  20      317.        333.            1        0.76        0.074
-  21      333.        350.            1        1.13        0.015
-  22      350.        367.            1        1.64        0.250
-  23      367.        383.            0        2.09        2.094
-  24      383.        400.            0        2.34        2.345
-  25      400.        417.            0        2.32        2.323
-  26      417.        433.            1        2.04        0.527
-  27      433.        450.            1        1.56        0.199
-  28      450.        467.            0        1.01        1.007
-  29      467.        483.            0        0.53        0.526
-  30      483.        500.            1        0.24        2.332
- -----------------------------------------------------------------
- Total Chi-square value =    82.0279  Degrees of Freedom = 26.00
-
-Probability of a greater chi-square value, P = 0.00000
-
- The program has limited capability for pooling.  The user should
- judge the necessity for pooling and if necessary, do pooling by hand.
-
- Goodness of Fit Testing with some Pooling
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        16.7           13        5.02       12.713
-   2      16.7        33.3           25       14.73        7.156
-   3      33.3        50.0           30       23.54        1.775
-   4      50.0        66.7           48       30.91        9.452
-   5      66.7        83.3           49       36.45        4.323
-   6      83.3        100.           53       39.91        4.291
-   7      100.        117.           46       41.23        0.552
-   8      117.        133.           35       40.50        0.746
-   9      133.        150.           29       37.96        2.115
-  10      150.        167.           32       34.00        0.117
-  11      167.        183.           17       29.05        5.000
-  12      183.        200.           15       23.62        3.144
-  13      200.        217.            6       18.16        8.144
-  14      217.        233.            5       13.10        5.009
-  15      233.        250.            7        8.75        0.351
-  16      250.        267.            2        5.32        2.073
-  17      267.        283.            1        2.89        1.236
-  18      283.        300.            2        1.42        0.235
-  19      300.        317.            2        0.78        1.903
-  20      317.        333.            1        0.76        0.074
-  21      333.        350.            1        1.13        0.015
-  22      350.        367.            1        1.64        0.250
-  23      367.        383.            0        2.09        2.094
-  24      383.        400.            0        2.34        2.345
-  25      400.        417.            0        2.32        2.323
-  26      417.        433.            1        2.04        0.527
-  27      433.        500.            2        3.33        0.534
- -----------------------------------------------------------------
- Total Chi-square value =    78.4983  Degrees of Freedom = 23.00
-
-Probability of a greater chi-square value, P = 0.00000
-
- One or more expected values is < 1.
- Try pooling some some cells by hand to obtain a more reliable test.
-	Density Estimates/Global	
-
-
- Effort        :    208.0000    
- # samples     :   104
- Width         :    500.0000    
- # observations:   423
-
- Model  4
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3
-
-
-              Point        Standard    Percent Coef.        95% Percent
-  Parameter   Estimate       Error      of Variation     Confidence Interval
-  ---------  -----------  -----------  --------------  ----------------------
-    D        0.27773      0.17176E-01       6.18      0.24580      0.31381    
-    N         667.00       41.251           6.18       590.00       753.00    
-  ---------  -----------  -----------  --------------  ----------------------
-
- Measurement Units                
- ---------------------------------
- Density: Numbers/hectares       
-     EDR: meters         
-
- Component Percentages of Var(D)
- -------------------------------
- Detection probability   :  15.2
- Encounter rate          :  84.8
-	Estimation Summary - Encounter rates         	
-
-                         Estimate      %CV     df     95% Confidence Interval
-                        ------------------------------------------------------
-                 n       423.00    
-                 k       104.00    
-                 K       208.00    
-                 n/K     2.0337        5.69   103.00  1.8167       2.2766    
-                 Left    0.0000
-                 Width   500.00    
-	Estimation Summary - Detection probability   	
-
-                         Estimate      %CV     df     95% Confidence Interval
-                        ------------------------------------------------------
- Uniform/Cosine         
-                 m       3.0000    
-                 LnL    -2354.2    
-                 AIC     4714.5    
-                 AICc    4714.5    
-                 BIC     4726.6    
-                 Chi-p  0.59605E-07
-                 h(0)   0.85807E-04    2.41   420.00 0.81830E-04  0.89976E-04
-                 p      0.93233E-01    2.41   420.00 0.88912E-01  0.97763E-01
-                 EDR     152.67        1.21   420.00  149.09       156.34    
-	Estimation Summary - Density&Abundance       	
-
-                         Estimate      %CV     df     95% Confidence Interval
-                        ------------------------------------------------------
- Uniform/Cosine         
-                 D      0.27773        6.18   142.24 0.24580      0.31381    
-                 N       667.00        6.18   142.24  590.00       753.00    
+	Estimation Options Listing	
+
+ Parameter Estimation Specification
+ ----------------------------------
+ Encounter rate for all data combined
+ Detection probability for all data combined
+ Density for all data combined
+
+ Distances:
+ ----------
+ Analysis based on exact distances
+ Width: use largest measurement/last interval endpoint
+
+ Estimators:
+ -----------
+ Estimator  1
+ Key: Hazard Rate
+ Adjustments - Function                 : Cosines
+             - Term selection mode      : Sequential
+             - Term selection criterion : Akaike Information Criterion (AIC)
+             - Distances scaled by      : W (right truncation distance)
+
+ Estimator selection: Choose estimator with minimum  AIC
+ Estimation functions: constrained to be nearly monotone non-increasing
+
+ Variances:
+ ----------
+ Variance of n: Empirical estimate from sample
+                (design-derived estimator R2/P2)
+ Variance of f(0): MLE estimate
+
+ Goodness of fit:
+ ----------------
+ Cut points chosen by program
+
+
+
+ Glossary of terms
+ -----------------
+
+ Data items:
+ n    - number of observed objects (single or clusters of animals)
+ L    - total length of transect line(s) 
+ k    - number of samples
+ K    - point transect effort, typically K=k
+ T    - length of time searched in cue counting
+ ER   - encounter rate (n/L or n/K or n/T)
+ W    - width of line transect or radius of point transect
+ x(i) - distance to i-th observation
+ s(i) - cluster size of i-th observation
+ r-p  - probability for regression test
+ chi-p- probability for chi-square goodness-of-fit test
+
+
+ Parameters or functions of parameters:
+ m    - number of parameters in the model
+ A(I) - i-th parameter in the estimated probability density function(pdf)
+ f(0) - 1/u = value of pdf at zero for line transects
+ u    - W*p = ESW, effective detection area for line transects
+ h(0) - 2*PI/v
+ v    - PI*W*W*p, is the effective detection area for point transects
+ p    - probability of observing an object in defined area
+ ESW  - for line transects, effective strip width = W*p
+ EDR  - for point transects, effective detection radius  = W*sqrt(p)
+ rho  - for cue counts, the cue rate
+ DS   - estimate of density of clusters
+ E(S) - estimate of expected value of cluster size
+ D    - estimate of density of animals
+ N    - estimate of number of animals in specified area
+	Detection Fct/Global/Model Fitting	
+
+
+ Effort        :    190.0000    
+ # samples     :    96
+ Width         :    511.4098    
+ # observations:   403
+
+
+
+ Model  1
+    Hazard Rate key, k(y) = 1 - Exp(-(y/A(1))**-A(2))
+       Results:
+       Convergence was achieved with   14 function evaluations.
+       Final Ln(likelihood) value =  -2261.3819    
+       Akaike information criterion =   4526.7637    
+       Bayesian information criterion =   4534.7617    
+       AICc =   4526.7935    
+       Final parameter values:   100.79350      3.8291985    
+
+
+ Model  2
+    Hazard Rate key, k(y) = 1 - Exp(-(y/A(1))**-A(2))
+    Cosine adjustments of order(s) :  2
+       Results:
+       Convergence was achieved with    6 function evaluations.
+       Final Ln(likelihood) value =  -2261.3819    
+       Akaike information criterion =   4528.7637    
+       Bayesian information criterion =   4540.7607    
+       AICc =   4528.8237    
+       Final parameter values:   100.79353      3.8293413     0.24215909E-10
+      ** Warning: Parameters are being constrained to obtain monotonicity. **
+
+    Likelihood ratio test between models  1 and  2
+       Likelihood ratio test value    =     0.0000
+       Probability of a greater value =   0.997724
+ *** Model  1 selected over model  2 based on minimum AIC              
+	Detection Fct/Global/Parameter Estimates	
+
+
+ Effort        :    190.0000    
+ # samples     :    96
+ Width         :    511.4098    
+ # observations:   403
+
+ Model
+    Hazard Rate key, k(y) = 1 - Exp(-(y/A(1))**-A(2))
+
+
+              Point        Standard    Percent Coef.        95 Percent
+  Parameter   Estimate       Error      of Variation     Confidence Interval
+  ---------  -----------  -----------  --------------  ----------------------
+    A( 1)      100.8        5.191    
+    A( 2)      3.829       0.2107    
+    h(0)     0.10947E-03  0.77788E-05       7.11      0.95216E-04  0.12586E-03
+    p        0.69854E-01  0.49637E-02       7.11      0.60758E-01  0.80313E-01
+    EDR       135.17       4.8023           3.55       126.05       144.94    
+  ---------  -----------  -----------  --------------  ----------------------
+
+
+ Sampling Correlation of Estimated Parameters
+
+
+         A( 1)   A( 2)
+ A( 1)  1.000   0.760
+ A( 2)  0.760   1.000
+	Detection Fct/Global/Plot: Qq-plot	
+	Detection Fct/Global/K-S GOF Test	
+
+
+ Kolmogorov-Smirnov test
+ -----------------------
+
+ D_n                      = 0.0407                 p  = 0.5174
+
+
+ Cramer-von Mises family tests
+ -----------------------------
+
+ W-sq (uniform weighting) = 0.1158          0.500 < p <= 0.600
+   Relevant critical values:
+     W-sq crit(alpha=0.600) = 0.0968
+     W-sq crit(alpha=0.500) = 0.1187
+
+ C-sq (cosine weighting)  = 0.0654          0.500 < p <= 0.600
+   Relevant critical values:
+     C-sq crit(alpha=0.600) = 0.0622
+     C-sq crit(alpha=0.500) = 0.0769
+	Detection Fct/Global/Plot: Detection Probability 1	
+            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
+     1.2567 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |******                                                            |
+     1.1034 +*    *                                                            +
+            |*    *                                                            |
+            |*    *                                                            |
+            |*ffffffff                                                         |
+            |*    *   f                                                        |
+ D   0.9502 +*    *                                                            +
+ e          |*    *****f                                                       |
+ t          |*    *    *                                                       |
+ e          |*    *    *                                                       |
+ c          |*    *    *f                                                      |
+ t   0.7969 +*    *    *                                                       +
+ i          |*    *    *                                                       |
+ o          |*    *    * f                                                     |
+ n          |*    *    *                                                       |
+            |*    *    *                                                       |
+ P   0.6437 +*    *    ******                                                  +
+ r          |*    *    *  f *                                                  |
+ o          |*    *    *    *                                                  |
+ b          |*    *    *    *                                                  |
+ a          |*    *    *   f*                                                  |
+ b   0.4904 +*    *    *    *                                                  +
+ i          |*    *    *    *                                                  |
+ l          |*    *    *    f                                                  |
+ i          |*    *    *    *                                                  |
+ t          |*    *    *    *                                                  |
+ y   0.3372 +*    *    *    *f                                                 +
+            |*    *    *    *                                                  |
+            |*    *    *    **f***                                             |
+            |*    *    *    *  f *                                             |
+            |*    *    *    *    *                                             |
+     0.1839 +*    *    *    *   f*                                             +
+            |*    *    *    *    ff                                            |
+            |*    *    *    *    **f***                                        |
+            |*    *    *    *    *  ff*                                        |
+            |*    *    *    *    *    ffff**                                   |
+     0.0307 +*    *    *    *    *    *   ffffffffff                           +
+            |                                   ****fffffffffffffffffffffffffff|
+            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
+          0.000        118.018   196.696   275.374   354.053   432.731   511.410
+              39.339        157.357   236.035   314.714   393.392   472.071     
+
+                                Radial distance in meters          
+	Detection Fct/Global/Plot: Pdf 1	
+            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
+     0.0088 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |          ff                                                      |
+     0.0077 +         f                                                        +
+            |            f                                                     |
+            |                                                                  |
+            |                                                                  |
+            |        f ***f**                                                  |
+     0.0066 +          *    *                                                  +
+            |          *    *                                                  |
+ P          |          *   f*                                                  |
+ r          |       f  *    *                                                  |
+ o          |     ******    *                                                  |
+ b   0.0056 +     *    *    *                                                  +
+ a          |     *    *    f                                                  |
+ b          |     *f   *    *                                                  |
+ i          |     *    *    *                                                  |
+ l          |     *    *    *f                                                 |
+ i   0.0045 +     *    *    *                                                  +
+ t          |     f    *    * f                                                |
+ y          |     *    *    ******                                             |
+            |     *    *    *    *                                             |
+ D          |     *    *    *  f *                                             |
+ e   0.0034 +    f*    *    *    *                                             +
+ n          |     *    *    *   f*                                             |
+ s          |     *    *    *    *                                             |
+ i          |     *    *    *    f                                             |
+ t          |***f**    *    *    *f****                                        |
+ y   0.0023 +*    *    *    *    * f  *                                        +
+            |*    *    *    *    *    *                                        |
+            |*    *    *    *    *  f *                                        |
+            |* f  *    *    *    *   f******                                   |
+            |*    *    *    *    *    ff   *                                   |
+     0.0013 +*    *    *    *    *    * ff *                                   +
+            |*    *    *    *    *    *   ff                                   |
+            |*f   *    *    *    *    *    *fff                                |
+            |*    *    *    *    *    *    ****ffff                            |
+            |*    *    *    *    *    *    *    *  fffffff******               |
+     0.0002 +*    *    *    *    *    *    *    ******    fffffffffffffffffffff+
+            |                                        ******         ******     |
+            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
+          0.000        118.018   196.696   275.374   354.053   432.731   511.410
+              39.339        157.357   236.035   314.714   393.392   472.071     
+
+                                Radial distance in meters          
+	Detection Fct/Global/Chi-sq GOF Test 1	
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        39.3           39       34.14        0.692
+   2      39.3        78.7           93      101.02        0.636
+   3      78.7        118.          109      111.40        0.052
+   4      118.        157.           64       63.59        0.003
+   5      157.        197.           42       34.17        1.794
+   6      197.        236.           27       19.89        2.545
+   7      236.        275.           10       12.51        0.504
+   8      275.        315.            5        8.38        1.361
+   9      315.        354.            1        5.89        4.057
+  10      354.        393.            6        4.30        0.672
+  11      393.        433.            4        3.24        0.178
+  12      433.        472.            1        2.50        0.904
+  13      472.        511.            2        1.98        0.000
+ -----------------------------------------------------------------
+ Total Chi-square value =    13.3971  Degrees of Freedom = 10.00
+
+Probability of a greater chi-square value, P = 0.20231
+
+ The program has limited capability for pooling.  The user should
+ judge the necessity for pooling and if necessary, do pooling by hand.
+
+ Goodness of Fit Testing with some Pooling
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        39.3           39       34.14        0.692
+   2      39.3        78.7           93      101.02        0.636
+   3      78.7        118.          109      111.40        0.052
+   4      118.        157.           64       63.59        0.003
+   5      157.        197.           42       34.17        1.794
+   6      197.        236.           27       19.89        2.545
+   7      236.        275.           10       12.51        0.504
+   8      275.        315.            5        8.38        1.361
+   9      315.        354.            1        5.89        4.057
+  10      354.        393.            6        4.30        0.672
+  11      393.        433.            4        3.24        0.178
+  12      433.        511.            3        4.48        0.491
+ -----------------------------------------------------------------
+ Total Chi-square value =    12.9835  Degrees of Freedom =  9.00
+
+Probability of a greater chi-square value, P = 0.16335
+	Detection Fct/Global/Plot: Detection Probability 2	
+            |+--+---------+--+------+--+------+--+-----+--+------+--+------+--+|
+     1.3728 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |****                                                              |
+     1.2054 +*  *                                                              +
+            |*  *                                                              |
+            |*  *                                                              |
+            |*  *                                                              |
+            |*  *                                                              |
+ D   1.0380 +*  *                                                              +
+ e          |*ffffffff                                                         |
+ t          |*  *     f                                                        |
+ e          |*  *******f                                                       |
+ c          |*  *   *  ****                                                    |
+ t   0.8706 +*  *   *  *  *                                                    +
+ i          |*  *   *  *f *                                                    |
+ o          |*  *   *  *  *                                                    |
+ n          |*  *   *  *  *                                                    |
+            |*  *   *  * f*                                                    |
+ P   0.7032 +*  *   *  *  *                                                    +
+ r          |*  *   *  *  *                                                    |
+ o          |*  *   *  *  *                                                    |
+ b          |*  *   *  *  f                                                    |
+ a          |*  *   *  *  *                                                    |
+ b   0.5357 +*  *   *  *  *                                                    +
+ i          |*  *   *  *  *f                                                   |
+ l          |*  *   *  *  *                                                    |
+ i          |*  *   *  *  * f                                                  |
+ t          |*  *   *  *  *                                                    |
+ y   0.3683 +*  *   *  *  *                                                    +
+            |*  *   *  *  *  f                                                 |
+            |*  *   *  *  ****f                                                |
+            |*  *   *  *  *  *****                                             |
+            |*  *   *  *  *  * f *                                             |
+     0.2009 +*  *   *  *  *  *  f*                                             +
+            |*  *   *  *  *  *   f                                             |
+            |*  *   *  *  *  *   *ff****                                       |
+            |*  *   *  *  *  *   *  ff *                                       |
+            |*  *   *  *  *  *   *  * fff *****                                |
+     0.0335 +*  *   *  *  *  *   *  *  **ffffffffff                            +
+            |                                 *****ffffffffffffffffffffffffffff|
+            |+--+---------+--+------+--+------+--+-----+--+------+--+------+--+|
+          0.000      102.282   178.993   255.705  332.416   409.128   485.839   
+            25.570      127.852   204.564   281.275  357.987   434.698   511.410
+
+                                Radial distance in meters          
+	Detection Fct/Global/Plot: Pdf 2	
+            |+--+---------+--+------+--+------+--+-----+--+------+--+------+--+|
+     0.0097 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |          ****                                                    |
+     0.0085 +          *  *                                                    +
+            |          *  *                                                    |
+            |          f  *                                                    |
+            |          *f *                                                    |
+            |         f* f*                                                    |
+     0.0073 +          *  *                                                    +
+            |          *  *                                                    |
+ P          |        f *  f                                                    |
+ r          |          *  *                                                    |
+ o          |       ****  *                                                    |
+ b   0.0062 +       *  *  *f                                                   +
+ a          |       f  *  *                                                    |
+ b          |       *  *  *                                                    |
+ i          |       *  *  * f                                                  |
+ l          |      f*  *  *                                                    |
+ i   0.0050 +       *  *  *                                                    +
+ t          |       *  *  *  f                                                 |
+ y          |       *  *  *                                                    |
+            |     f *  *  *  *f***                                             |
+ D          |       *  *  ****   *                                             |
+ e   0.0038 +   *****  *  *  * f *                                             +
+ n          |   *f  *  *  *  *   *                                             |
+ s          |   *   *  *  *  *  f*                                             |
+ i          |   *   *  *  *  *   *                                             |
+ t          |   *   *  *  *  *   f                                             |
+ y   0.0026 +   f   *  *  *  *   *f ****                                       +
+            |   *   *  *  *  *   ****  *                                       |
+            |   *   *  *  *  *   * f*  *                                       |
+            |   *   *  *  *  *   *  ff *                                       |
+            |**f*   *  *  *  *   *  * f*                                       |
+     0.0014 +*  *   *  *  *  *   *  *  f  *****                                +
+            |*  *   *  *  *  *   *  *  *fff   *                                |
+            |*f *   *  *  *  *   *  *  *  *ff *                                |
+            |*  *   *  *  *  *   *  *  *  *  ffff                              |
+            |*  *   *  *  *  *   *  *  *  *   ***ffffffff ********             |
+     0.0002 +*  *   *  *  *  *   *  *  *  *   *  *  *    ffffffffffffffffffff**+
+            |                                       *******      *********** ff|
+            |+--+---------+--+------+--+------+--+-----+--+------+--+------+--+|
+          0.000      102.282   178.993   255.705  332.416   409.128   485.839   
+            25.570      127.852   204.564   281.275  357.987   434.698   511.410
+
+                                Radial distance in meters          
+	Detection Fct/Global/Chi-sq GOF Test 2	
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        25.6           18       14.42        0.886
+   2      25.6        51.1           40       43.27        0.248
+   3      51.1        76.7           67       71.16        0.243
+   4      76.7        102.           91       78.74        1.909
+   5      102.        128.           41       58.89        5.434
+   6      128.        153.           43       38.97        0.417
+   7      153.        179.           25       25.89        0.030
+   8      179.        205.           28       17.78        5.869
+   9      205.        230.           12       12.67        0.035
+  10      230.        256.           15        9.32        3.457
+  11      256.        281.            5        7.06        0.600
+  12      281.        307.            4        5.47        0.396
+  13      307.        332.            1        4.33        2.560
+  14      332.        358.            0        3.49        3.486
+  15      358.        384.            6        2.85        3.481
+  16      384.        409.            4        2.36        1.137
+  17      409.        435.            0        1.98        1.979
+  18      435.        460.            0        1.68        1.676
+  19      460.        486.            1        1.43        0.131
+  20      486.        511.            2        1.23        0.475
+ -----------------------------------------------------------------
+ Total Chi-square value =    34.4487  Degrees of Freedom = 17.00
+
+Probability of a greater chi-square value, P = 0.00734
+
+ The program has limited capability for pooling.  The user should
+ judge the necessity for pooling and if necessary, do pooling by hand.
+
+ Goodness of Fit Testing with some Pooling
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        25.6           18       14.42        0.886
+   2      25.6        51.1           40       43.27        0.248
+   3      51.1        76.7           67       71.16        0.243
+   4      76.7        102.           91       78.74        1.909
+   5      102.        128.           41       58.89        5.434
+   6      128.        153.           43       38.97        0.417
+   7      153.        179.           25       25.89        0.030
+   8      179.        205.           28       17.78        5.869
+   9      205.        230.           12       12.67        0.035
+  10      230.        256.           15        9.32        3.457
+  11      256.        281.            5        7.06        0.600
+  12      281.        307.            4        5.47        0.396
+  13      307.        332.            1        4.33        2.560
+  14      332.        358.            0        3.49        3.486
+  15      358.        384.            6        2.85        3.481
+  16      384.        409.            4        2.36        1.137
+  17      409.        435.            0        1.98        1.979
+  18      435.        460.            0        1.68        1.676
+  19      460.        511.            3        2.67        0.042
+ -----------------------------------------------------------------
+ Total Chi-square value =    33.8849  Degrees of Freedom = 16.00
+
+Probability of a greater chi-square value, P = 0.00563
+	Detection Fct/Global/Plot: Detection Probability 3	
+            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
+     1.4300 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |  ***                                                             |
+     1.2556 +  * *                                                             +
+            |  * *                                                             |
+            |  * *                                                             |
+            |  * *                                                             |
+            |  * *                                                             |
+ D   1.0812 +*** *                                                             +
+ e          |* * *                                                             |
+ t          |*ffffffff***                                                      |
+ e          |* * *    f *                                                      |
+ c          |* * *    *f*                                                      |
+ t   0.9068 +* * *  *** *                                                      +
+ i          |* * *  * * *                                                      |
+ o          |* * *  * * f**                                                    |
+ n          |* * **** * * *                                                    |
+            |* * *  * * * *                                                    |
+ P   0.7324 +* * *  * * *f*                                                    +
+ r          |* * *  * * * *                                                    |
+ o          |* * *  * * * *                                                    |
+ b          |* * *  * * * f                                                    |
+ a          |* * *  * * * *                                                    |
+ b   0.5581 +* * *  * * * *                                                    +
+ i          |* * *  * * * *f                                                   |
+ l          |* * *  * * * *                                                    |
+ i          |* * *  * * * *                                                    |
+ t          |* * *  * * * * f                                                  |
+ y   0.3837 +* * *  * * * *                                                    +
+            |* * *  * * * *  f                                                 |
+            |* * *  * * * *** ****                                             |
+            |* * *  * * * * * f  *                                             |
+            |* * *  * * * * ***f *                                             |
+     0.2093 +* * *  * * * * * * f*                                             +
+            |* * *  * * * * * *  f                                             |
+            |* * *  * * * * * *  *f***                                         |
+            |* * *  * * * * * *  * ff***                                       |
+            |* * *  * * * * * *  * * ffff***                                   |
+     0.0349 +* * *  * * * * * *  * * * **fffffffff                             +
+            |                                   **fffffffffffffffffffffffffffff|
+            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
+          0.000      102.282  170.470   255.705  323.893  392.081    477.316    
+           17.047   85.235  153.423   238.658  306.846  375.034   460.269       
+
+                                Radial distance in meters          
+	Detection Fct/Global/Plot: Pdf 3	
+            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
+     0.0094 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |         *****                                                    |
+     0.0083 +         * * *                                                    +
+            |         *f* *                                                    |
+            |         * f *                                                    |
+            |         f * *                                                    |
+            |         * *f*                                                    |
+     0.0071 +         * * *                                                    +
+            |        f* * f                                                    |
+ P          |         * * *                                                    |
+ r          |         * * *                                                    |
+ o          |         * * *f                                                   |
+ b   0.0060 +       f** * *                                                    +
+ a          |       * * * *                                                    |
+ b          |       * * * * f                                                  |
+ i          |       * * * *                                                    |
+ l          |      f* * * *                                                    |
+ i   0.0048 +       * * * *  f****                                             +
+ t          |       * * * *   *  *                                             |
+ y          |     f * * * *   *  *                                             |
+            |       * * * *   f  *                                             |
+ D          |       * * * *   *  *                                             |
+ e   0.0037 +  ****** * * *****f *                                             +
+ n          |  * f  * * * * * *  *                                             |
+ s          |  * *  * * * * * * f*                                             |
+ i          |  * *  * * * * * *  * ***                                         |
+ t          |  * *  * * * * * *  f * *                                         |
+ y   0.0025 +  *f*  * * * * * *  *f* *                                         +
+            |  * *  * * * * * *  **f ***                                       |
+            |  * *  * * * * * *  * *f* * ***                                   |
+            |  * *  * * * * * *  * * f * * *                                   |
+            |  f *  * * * * * *  * * *f* * *                                   |
+     0.0014 +  * *  * * * * * *  * * * ff* *                                   +
+            |  * *  * * * * * *  * * * * ff*                                   |
+            |*f* *  * * * * * *  * * * *** fff                                 |
+            |* * *  * * * * * *  * * * * * * *ffff        **** ***             |
+            |* * *  * * * * * *  * * * * * * *  * fffffff *  * * *             |
+     0.0002 +* * *  * * * * * *  * * * * * * *  *** ***  fffffffffffffffffffff*+
+            |                                         *****      *******      f|
+            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
+          0.000      102.282  170.470   255.705  323.893  392.081    477.316    
+           17.047   85.235  153.423   238.658  306.846  375.034   460.269       
+
+                                Radial distance in meters          
+	Detection Fct/Global/Chi-sq GOF Test 3	
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        17.0            7        6.41        0.054
+   2      17.0        34.1           25       19.23        1.729
+   3      34.1        51.1           26       32.05        1.144
+   4      51.1        68.2           41       44.77        0.318
+   5      68.2        85.2           58       53.76        0.335
+   6      85.2        102.           59       51.37        1.132
+   7      102.        119.           25       41.80        6.751
+   8      119.        136.           25       31.94        1.509
+   9      136.        153.           34       24.12        4.048
+  10      153.        170.           16       18.36        0.303
+  11      170.        188.           21       14.18        3.280
+  12      188.        205.           16       11.14        2.125
+  13      205.        222.            6        8.89        0.937
+  14      222.        239.           15        7.20        8.460
+  15      239.        256.            6        5.91        0.001
+  16      256.        273.            4        4.91        0.168
+  17      273.        290.            2        4.12        1.093
+  18      290.        307.            3        3.50        0.071
+  19      307.        324.            1        2.99        1.326
+  20      324.        341.            0        2.58        2.581
+  21      341.        358.            0        2.24        2.242
+  22      358.        375.            4        1.96        2.121
+  23      375.        392.            2        1.72        0.044
+  24      392.        409.            4        1.53        4.012
+  25      409.        426.            0        1.36        1.357
+  26      426.        443.            0        1.21        1.212
+  27      443.        460.            0        1.09        1.087
+  28      460.        477.            1        0.98        0.000
+  29      477.        494.            1        0.88        0.015
+  30      494.        511.            1        0.80        0.048
+ -----------------------------------------------------------------
+ Total Chi-square value =    49.5032  Degrees of Freedom = 27.00
+
+Probability of a greater chi-square value, P = 0.00519
+
+ The program has limited capability for pooling.  The user should
+ judge the necessity for pooling and if necessary, do pooling by hand.
+
+ Goodness of Fit Testing with some Pooling
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        17.0            7        6.41        0.054
+   2      17.0        34.1           25       19.23        1.729
+   3      34.1        51.1           26       32.05        1.144
+   4      51.1        68.2           41       44.77        0.318
+   5      68.2        85.2           58       53.76        0.335
+   6      85.2        102.           59       51.37        1.132
+   7      102.        119.           25       41.80        6.751
+   8      119.        136.           25       31.94        1.509
+   9      136.        153.           34       24.12        4.048
+  10      153.        170.           16       18.36        0.303
+  11      170.        188.           21       14.18        3.280
+  12      188.        205.           16       11.14        2.125
+  13      205.        222.            6        8.89        0.937
+  14      222.        239.           15        7.20        8.460
+  15      239.        256.            6        5.91        0.001
+  16      256.        273.            4        4.91        0.168
+  17      273.        290.            2        4.12        1.093
+  18      290.        307.            3        3.50        0.071
+  19      307.        324.            1        2.99        1.326
+  20      324.        341.            0        2.58        2.581
+  21      341.        358.            0        2.24        2.242
+  22      358.        375.            4        1.96        2.121
+  23      375.        392.            2        1.72        0.044
+  24      392.        409.            4        1.53        4.012
+  25      409.        426.            0        1.36        1.357
+  26      426.        443.            0        1.21        1.212
+  27      443.        460.            0        1.09        1.087
+  28      460.        511.            3        2.67        0.042
+ -----------------------------------------------------------------
+ Total Chi-square value =    49.4810  Degrees of Freedom = 25.00
+
+Probability of a greater chi-square value, P = 0.00247
+	Density Estimates/Global	
+
+
+ Effort        :    190.0000    
+ # samples     :    96
+ Width         :    511.4098    
+ # observations:   403
+
+ Model  1
+    Hazard Rate key, k(y) = 1 - Exp(-(y/A(1))**-A(2))
+
+
+              Point        Standard    Percent Coef.        95% Percent
+  Parameter   Estimate       Error      of Variation     Confidence Interval
+  ---------  -----------  -----------  --------------  ----------------------
+    D         36.955       3.6444           9.86       30.452       44.846    
+    N         887.00       87.475           9.86       731.00       1076.0    
+  ---------  -----------  -----------  --------------  ----------------------
+
+ Measurement Units                
+ ---------------------------------
+ Density: Numbers/Sq. kilometers 
+     EDR: meters         
+
+ Component Percentages of Var(D)
+ -------------------------------
+ Detection probability   :  51.9
+ Encounter rate          :  48.1
+	Estimation Summary - Encounter rates         	
+
+                         Estimate      %CV     df     95% Confidence Interval
+                        ------------------------------------------------------
+                 n       403.00    
+                 k       96.000    
+                 K       190.00    
+                 n/K     2.1211        6.84    95.00  1.8521       2.4291    
+                 Left    0.0000
+                 Width   511.41    
+	Estimation Summary - Detection probability   	
+
+                         Estimate      %CV     df     95% Confidence Interval
+                        ------------------------------------------------------
+ Hazard/Cosine          
+                 m       2.0000    
+                 LnL    -2261.4    
+                 AIC     4526.8    
+                 AICc    4526.8    
+                 BIC     4534.8    
+                 Chi-p  0.24681E-02
+                 h(0)   0.10947E-03    7.11   401.00 0.95216E-04  0.12586E-03
+                 p      0.69854E-01    7.11   401.00 0.60758E-01  0.80313E-01
+                 EDR     135.17        3.55   401.00  126.05       144.94    
+	Estimation Summary - Density&Abundance       	
+
+                         Estimate      %CV     df     95% Confidence Interval
+                        ------------------------------------------------------
+ Hazard/Cosine          
+                 D       36.955        9.86   321.97  30.452       44.846    
+                 N       887.00        9.86   321.97  731.00       1076.0
```

### Comparing `pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-dist-order/stats.txt` & `pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/stats.txt`

 * *Files 18% similar despite different names*

```diff
@@ -1,32 +1,31 @@
-                                                   
-     0     0 1 1   1   423.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   2   104.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   3   208.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   4   2.033654      0.5693950E-01   1.816659       2.276567       103.0000    
-     0     0 1 1   5      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   6   500.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   1   3.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   4  0.8580667E-04  0.2414248E-01  0.8183037E-04  0.8997619E-04   420.0000    
-     0     0 1 2   5  0.9323284E-01  0.2414248E-01  0.8891241E-01  0.9776321E-01   420.0000    
-     0     0 1 2   6   152.6703      0.1207124E-01   149.0905       156.3360       420.0000    
-     0     0 1 2   2   4714.484          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   7   4714.541          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   8   4726.626          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   9  -2354.242          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  13   1.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  14   3.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  15      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  16   3.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  17      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2 101   1.598640      0.4818109E-02      0.0000000      0.0000000      0.0000000
-     0     0 1 2 102  0.8334720      0.3073854E-01      0.0000000      0.0000000      0.0000000
-     0     0 1 2 103  0.2324568      0.8876383E-01      0.0000000      0.0000000      0.0000000
-     0     0 1 2  10      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  11  0.1000000E-02      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  12  0.1000000E-02      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   3      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   3      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   3  0.5960464E-07      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 4   1  0.2777270      0.6184631E-01  0.2457952      0.3138072       142.2359    
-     0     0 1 4   2  0.2777270      0.6184631E-01  0.2457952      0.3138072       142.2359    
-     0     0 1 4   3   667.0000      0.6184631E-01   590.0000       753.0000       142.2359    
+                                                   
+     0     0 1 1   1   47.00000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   2   96.00000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   3   190.0000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   4  0.2473684      0.1770701      0.1745236      0.3506182       95.00000    
+     0     0 1 1   5      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   6   271.2211          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   1   2.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   4  0.1583892E-03  0.1642317      0.1140314E-03  0.2200020E-03   45.00000    
+     0     0 1 2   5  0.1716554      0.1642317      0.1235823      0.2384287       45.00000    
+     0     0 1 2   6   112.3705      0.8211586E-01   95.26753       132.5438       45.00000    
+     0     0 1 2   2   494.6088          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   7   494.8816          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   8   498.3091          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   9  -245.3044          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  13   1.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  14   3.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  15      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  16   2.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  17      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2 101   1.331354      0.7642985E-01      0.0000000      0.0000000      0.0000000
+     0     0 1 2 102  0.3510245      0.2849297          0.0000000      0.0000000      0.0000000
+     0     0 1 2  10  0.4739598          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  11  0.6000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  12  0.6000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   3  0.3653752          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   3  0.4143333E-01      0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   3  0.2125555          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 4   1   6.235769      0.2415075       3.893096       9.988147       128.3036    
+     0     0 1 4   2   6.235769      0.2415075       3.893096       9.988147       128.3036    
+     0     0 1 4   3   150.0000      0.2415075       93.00000       240.0000       128.3036
```

### Comparing `pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/output.txt` & `pyaudisam-1.0.1/docs/how-it-works/preanlys/PrunModu-ab-10mn-m-uni-cos-f3smf1_a/output.txt`

 * *Files 23% similar despite different names*

```diff
@@ -1,786 +1,635 @@
-	Estimation Options Listing	
-
- Parameter Estimation Specification
- ----------------------------------
- Encounter rate for all data combined
- Detection probability for all data combined
- Density for all data combined
-
- Distances:
- ----------
- Analysis based on exact distances
- Width: use largest measurement/last interval endpoint
-
- Estimators:
- -----------
- Estimator  1
- Key: Uniform
- Adjustments - Function                 : Cosines
-             - Term selection mode      : Sequential
-             - Term selection criterion : Akaike Information Criterion (AIC)
-             - Distances scaled by      : W (right truncation distance)
-
- Estimator selection: Choose estimator with minimum  AIC
- Estimation functions: constrained to be nearly monotone non-increasing
-
- Variances:
- ----------
- Variance of n: Empirical estimate from sample
-                (design-derived estimator R2/P2)
- Variance of f(0): MLE estimate
-
- Goodness of fit:
- ----------------
- Cut points chosen by program
-
-
-
- Glossary of terms
- -----------------
-
- Data items:
- n    - number of observed objects (single or clusters of animals)
- L    - total length of transect line(s) 
- k    - number of samples
- K    - point transect effort, typically K=k
- T    - length of time searched in cue counting
- ER   - encounter rate (n/L or n/K or n/T)
- W    - width of line transect or radius of point transect
- x(i) - distance to i-th observation
- s(i) - cluster size of i-th observation
- r-p  - probability for regression test
- chi-p- probability for chi-square goodness-of-fit test
-
-
- Parameters or functions of parameters:
- m    - number of parameters in the model
- A(I) - i-th parameter in the estimated probability density function(pdf)
- f(0) - 1/u = value of pdf at zero for line transects
- u    - W*p = ESW, effective detection area for line transects
- h(0) - 2*PI/v
- v    - PI*W*W*p, is the effective detection area for point transects
- p    - probability of observing an object in defined area
- ESW  - for line transects, effective strip width = W*p
- EDR  - for point transects, effective detection radius  = W*sqrt(p)
- rho  - for cue counts, the cue rate
- DS   - estimate of density of clusters
- E(S) - estimate of expected value of cluster size
- D    - estimate of density of animals
- N    - estimate of number of animals in specified area
-	Detection Fct/Global/Model Fitting	
-
-
- Effort        :    208.0000    
- # samples     :   104
- Width         :    500.0000    
- # observations:   423
-
-
-
- Model  1
-    Uniform key, k(y) = 1/W
-       Results:
-       Convergence was achieved with    1 function evaluations.
-       Final Ln(likelihood) value =  -3067.2869    
-       Akaike information criterion =   6134.5737    
-       Bayesian information criterion =   6134.5737    
-       AICc =   6134.5737    
-       Final parameter values: 
-
-
- Model  2
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1
-       Results:
-       Convergence was achieved with   47 function evaluations.
-       Final Ln(likelihood) value =  -2635.5634    
-       Akaike information criterion =   5273.1270    
-       Bayesian information criterion =   5277.1743    
-       AICc =   5273.1362    
-       Final parameter values:  0.99768112    
-
-    Likelihood ratio test between models  1 and  2
-       Likelihood ratio test value    =   863.4471
-       Probability of a greater value =   0.000000
- *** Model  2 selected over model  1 based on minimum AIC              
-
-
- Model  3
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2
-       Results:
-       Convergence was achieved with   37 function evaluations.
-       Final Ln(likelihood) value =  -2459.2116    
-       Akaike information criterion =   4922.4233    
-       Bayesian information criterion =   4930.5181    
-       AICc =   4922.4521    
-       Final parameter values:   1.3676947     0.37907805    
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-
-    Likelihood ratio test between models  2 and  3
-       Likelihood ratio test value    =   352.7036
-       Probability of a greater value =   0.000000
- *** Model  3 selected over model  2 based on minimum AIC              
-
-
- Model  4
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3
-       Results:
-       Convergence was achieved with   28 function evaluations.
-       Final Ln(likelihood) value =  -2354.2419    
-       Akaike information criterion =   4714.4839    
-       Bayesian information criterion =   4726.6260    
-       AICc =   4714.5410    
-       Final parameter values:   1.5986401     0.83347200     0.23245685    
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-
-    Likelihood ratio test between models  3 and  4
-       Likelihood ratio test value    =   209.9394
-       Probability of a greater value =   0.000000
- *** Model  4 selected over model  3 based on minimum AIC              
-
-
- Model  5
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3, 4
-       Results:
-       Convergence was achieved with  101 function evaluations.
-       Final Ln(likelihood) value =  -2324.0944    
-       Akaike information criterion =   4656.1890    
-       Bayesian information criterion =   4672.3784    
-       AICc =   4656.2847    
-       Final parameter values:   1.6921291      1.0445838     0.44937013     0.10132487    
-
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-
-    Likelihood ratio test between models  4 and  5
-       Likelihood ratio test value    =    60.2949
-       Probability of a greater value =   0.000000
- *** Model  5 selected over model  4 based on minimum AIC              
-
-
- Model  6
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3, 4, 5
-       Results:
-       Convergence was achieved with   90 function evaluations.
-       Final Ln(likelihood) value =  -2318.6096    
-       Akaike information criterion =   4647.2192    
-       Bayesian information criterion =   4667.4561    
-       AICc =   4647.3633    
-       Final parameter values:   1.7300604      1.1415922     0.57135392     0.20382930    
-                                 0.39028356E-01
-      ** Warning: Parameters are being constrained to obtain monotonicity. **
-
-    Likelihood ratio test between models  5 and  6
-       Likelihood ratio test value    =    10.9697
-       Probability of a greater value =   0.000926
- *** Model  6 selected over model  5 based on minimum AIC              
-	Detection Fct/Global/Parameter Estimates	
-
-
- Effort        :    208.0000    
- # samples     :   104
- Width         :    500.0000    
- # observations:   423
-
- Model
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3, 4, 5
-
-
-              Point        Standard    Percent Coef.        95 Percent
-  Parameter   Estimate       Error      of Variation     Confidence Interval
-  ---------  -----------  -----------  --------------  ----------------------
-    A( 1)      1.730       0.1191E-01
-    A( 2)      1.142       0.3110E-01
-    A( 3)     0.5714       0.3910E-01
-    A( 4)     0.2038       0.3000E-01
-    A( 5)     0.3903E-01   0.1216E-01
-    h(0)     0.13756E-03  0.66619E-05       4.84      0.12508E-03  0.15129E-03
-    p        0.58156E-01  0.28164E-02       4.84      0.52878E-01  0.63960E-01
-    EDR       120.58       2.9197           2.42       114.97       126.45    
-  ---------  -----------  -----------  --------------  ----------------------
-
-
- Sampling Correlation of Estimated Parameters
-
-
-         A( 1)   A( 2)   A( 3)   A( 4)   A( 5)
- A( 1)  1.000   0.951   0.850   0.716   0.570
- A( 2)  0.951   1.000   0.956   0.848   0.696
- A( 3)  0.850   0.956   1.000   0.956   0.827
- A( 4)  0.716   0.848   0.956   1.000   0.932
- A( 5)  0.570   0.696   0.827   0.932   1.000
-	Detection Fct/Global/Plot: Qq-plot	
-	Detection Fct/Global/K-S GOF Test	
-
-
- Kolmogorov-Smirnov test
- -----------------------
-
- D_n                      = 0.0244                 p  = 0.9628
-
-
- Cramer-von Mises family tests
- -----------------------------
-
- W-sq (uniform weighting) = 0.0646          0.700 < p <= 0.800
-   Relevant critical values:
-     W-sq crit(alpha=0.800) = 0.0622
-     W-sq crit(alpha=0.700) = 0.0785
-
- C-sq (cosine weighting)  = 0.0449          0.700 < p <= 0.800
-   Relevant critical values:
-     C-sq crit(alpha=0.800) = 0.0391
-     C-sq crit(alpha=0.700) = 0.0499
-	Detection Fct/Global/Plot: Detection Probability 1	
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-     1.1501 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |******                                                            |
-     1.0099 +*f   *                                                            +
-            |* f  *                                                            |
-            |*  f *                                                            |
-            |*   f*                                                            |
-            |*    f                                                            |
- D   0.8696 +*    *f                                                           +
- e          |*    *                                                            |
- t          |*    * f                                                          |
- e          |*    ***f**                                                       |
- c          |*    *    *                                                       |
- t   0.7293 +*    *   f*                                                       +
- i          |*    *    *                                                       |
- o          |*    *    f                                                       |
- n          |*    *    *                                                       |
-            |*    *    *f                                                      |
- P   0.5891 +*    *    *                                                       +
- r          |*    *    * f                                                     |
- o          |*    *    ******                                                  |
- b          |*    *    *  f *                                                  |
- a          |*    *    *    *                                                  |
- b   0.4488 +*    *    *   f*                                                  +
- i          |*    *    *    *                                                  |
- l          |*    *    *    f                                                  |
- i          |*    *    *    *f                                                 |
- t          |*    *    *    *                                                  |
- y   0.3086 +*    *    *    * f                                                +
-            |*    *    *    *                                                  |
-            |*    *    *    ***f**                                             |
-            |*    *    *    *   f*                                             |
-            |*    *    *    *    f                                             |
-     0.1683 +*    *    *    *    *f                                            +
-            |*    *    *    *    * f                                           |
-            |*    *    *    *    ***f**                                        |
-            |*    *    *    *    *   f*                                        |
-            |*    *    *    *    *    fff***                                   |
-     0.0281 +*    *    *    *    *    *  fffff***                              +
-            |                                 fffffffffffffffffffffffffffffffff|
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-          0.000        115.385   192.308   269.231   346.154   423.077   500.000
-              38.462        153.846   230.769   307.692   384.615   461.538     
-
-                                Radial distance in meters          
-	Detection Fct/Global/Plot: Pdf 1	
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-     0.0079 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |          fff                                                     |
-     0.0069 +         f***f**                                                  +
-            |          *   f*                                                  |
-            |        f *    *                                                  |
-            |     ******    f                                                  |
-            |     * f  *    *                                                  |
-     0.0060 +     *    *    *f                                                 +
-            |     *    *    *                                                  |
- P          |     *f   *    *                                                  |
- r          |     *    *    * f                                                |
- o          |     *    *    *                                                  |
- b   0.0050 +     *    *    *  f                                               +
- a          |     f    *    *                                                  |
- b          |     *    *    ******                                             |
- i          |     *    *    *   f*                                             |
- l          |     *    *    *    *                                             |
- i   0.0040 +    f*    *    *    f                                             +
- t          |     *    *    *    *                                             |
- y          |     *    *    *    *                                             |
-            |     *    *    *    *f                                            |
- D          |     *    *    *    *                                             |
- e   0.0031 +   f *    *    *    *                                             +
- n          |     *    *    *    **f***                                        |
- s          |******    *    *    *    *                                        |
- i          |*    *    *    *    *  f *                                        |
- t          |*    *    *    *    *    *                                        |
- y   0.0021 +* f  *    *    *    *   f*                                        +
-            |*    *    *    *    *    *                                        |
-            |*    *    *    *    *    f                                        |
-            |*    *    *    *    *    *f                                       |
-            |*    *    *    *    *    ******                                   |
-     0.0012 +*    *    *    *    *    * f  *                                   +
-            |*f   *    *    *    *    *  f *                                   |
-            |*    *    *    *    *    *   ff                                   |
-            |*    *    *    *    *    *    *ff***                              |
-            |*    *    *    *    *    *    *  fffff                            |
-     0.0002 +*    *    *    *    *    *    *    ***ffffffff                    +
-            |                                             *ffffffffffffffffffff|
-            |+----+---------+----+----+----+----+----+----+----+----+----+----+|
-          0.000        115.385   192.308   269.231   346.154   423.077   500.000
-              38.462        153.846   230.769   307.692   384.615   461.538     
-
-                                Radial distance in meters          
-	Detection Fct/Global/Chi-sq GOF Test 1	
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        38.5           45       41.01        0.389
-   2      38.5        76.9          102      101.22        0.006
-   3      76.9        115.          113      113.07        0.000
-   4      115.        154.           75       84.68        1.108
-   5      154.        192.           46       45.43        0.007
-   6      192.        231.           22       18.57        0.633
-   7      231.        269.            9        7.73        0.207
-   8      269.        308.            4        4.60        0.079
-   9      308.        346.            3        2.60        0.063
-  10      346.        385.            1        1.04        0.002
-  11      385.        423.            1        0.78        0.063
-  12      423.        462.            1        1.09        0.008
-  13      462.        500.            1        1.18        0.026
- -----------------------------------------------------------------
- Total Chi-square value =     2.5911  Degrees of Freedom =  7.00
-
-Probability of a greater chi-square value, P = 0.92008
-
- The program has limited capability for pooling.  The user should
- judge the necessity for pooling and if necessary, do pooling by hand.
-
- Goodness of Fit Testing with some Pooling
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        38.5           45       41.01        0.389
-   2      38.5        76.9          102      101.22        0.006
-   3      76.9        115.          113      113.07        0.000
-   4      115.        154.           75       84.68        1.108
-   5      154.        192.           46       45.43        0.007
-   6      192.        231.           22       18.57        0.633
-   7      231.        269.            9        7.73        0.207
-   8      269.        308.            4        4.60        0.079
-   9      308.        346.            3        2.60        0.063
-  10      346.        385.            1        1.04        0.002
-  11      385.        423.            1        0.78        0.063
-  12      423.        500.            2        2.27        0.032
- -----------------------------------------------------------------
- Total Chi-square value =     2.5886  Degrees of Freedom =  6.00
-
-Probability of a greater chi-square value, P = 0.85842
-
- One or more expected values is < 1.
- Try pooling some some cells by hand to obtain a more reliable test.
-	Detection Fct/Global/Plot: Detection Probability 2	
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-     1.3308 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |****                                                              |
-     1.1685 +*  *                                                              +
-            |*  *                                                              |
-            |*  *                                                              |
-            |*  *                                                              |
-            |*  *                                                              |
- D   1.0063 +*f *                                                              +
- e          |* ff                                                              |
- t          |*  *f                                                             |
- e          |*  * f                                                            |
- c          |*  *  f                                                           |
- t   0.8440 +*  *****                                                          +
- i          |*  *   f***                                                       |
- o          |*  *   *f *                                                       |
- n          |*  *   *  *                                                       |
-            |*  *   * f*                                                       |
- P   0.6817 +*  *   *  f                                                       +
- r          |*  *   *  *                                                       |
- o          |*  *   *  *f**                                                    |
- b          |*  *   *  *  *                                                    |
- a          |*  *   *  * f*                                                    |
- b   0.5194 +*  *   *  *  f                                                    +
- i          |*  *   *  *  *                                                    |
- l          |*  *   *  *  *f                                                   |
- i          |*  *   *  *  *                                                    |
- t          |*  *   *  *  * f                                                  |
- y   0.3571 +*  *   *  *  ***f                                                 +
-            |*  *   *  *  *  *                                                 |
-            |*  *   *  *  *  *f                                                |
-            |*  *   *  *  *  **f**                                             |
-            |*  *   *  *  *  *  f*                                             |
-     0.1948 +*  *   *  *  *  *   f***                                          +
-            |*  *   *  *  *  *   *f *                                          |
-            |*  *   *  *  *  *   * f*                                          |
-            |*  *   *  *  *  *   *  ff                                         |
-            |*  *   *  *  *  *   *  **ff                                       |
-     0.0325 +*  *   *  *  *  *   *  *  *fffff**                                +
-            |                                ffffffffffffffffffffffffffffffffff|
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-          0.000      100.000   175.000   250.000  325.000   400.000   475.000   
-            25.000      125.000   200.000   275.000   350.000  425.000   500.000
-
-                                Radial distance in meters          
-	Detection Fct/Global/Plot: Pdf 2	
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-     0.0080 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |          *f**                                                    |
-     0.0070 +          f ff                                                    +
-            |       **f*  *                                                    |
-            |       *f *  *f                                                   |
-            |       *  *  * f                                                  |
-            |       *  *  *                                                    |
-     0.0061 +       f  *  *                                                    +
-            |       *  *  *  f                                                 |
- P          |       *  *  ****                                                 |
- r          |      f*  *  *  *f                                                |
- o          |       *  *  *  *                                                 |
- b   0.0051 +       *  *  *  *                                                 +
- a          |     f *  *  *  * f                                               |
- b          |       *  *  *  *****                                             |
- i          |       *  *  *  *  f*                                             |
- l          |   *****  *  *  *   *                                             |
- i   0.0041 +   *   *  *  *  *   ****                                          +
- t          |   *f  *  *  *  *   f  *                                          |
- y          |   *   *  *  *  *   *  *                                          |
-            |   *   *  *  *  *   *f *                                          |
- D          |   *   *  *  *  *   *  *                                          |
- e   0.0031 +   f   *  *  *  *   *  *                                          +
- n          |   *   *  *  *  *   * f*                                          |
- s          |   *   *  *  *  *   *  *                                          |
- i          |   *   *  *  *  *   *  f                                          |
- t          |   *   *  *  *  *   *  *                                          |
- y   0.0021 +**f*   *  *  *  *   *  *f                                         +
-            |*  *   *  *  *  *   *  ****                                       |
-            |*  *   *  *  *  *   *  * f*                                       |
-            |*  *   *  *  *  *   *  *  f                                       |
-            |*  *   *  *  *  *   *  *  *                                       |
-     0.0012 +*  *   *  *  *  *   *  *  *f                                      +
-            |*f *   *  *  *  *   *  *  * f                                     |
-            |*  *   *  *  *  *   *  *  ***f****                                |
-            |*  *   *  *  *  *   *  *  *  *fff*                                |
-            |*  *   *  *  *  *   *  *  *  *   fffff                            |
-     0.0002 +*  *   *  *  *  *   *  *  *  *   *****ffffffff                    +
-            |                                          ****ffffffffffffffffffff|
-            |+--+---------+--+------+--+------+--+-----+---+-----+--+------+--+|
-          0.000      100.000   175.000   250.000  325.000   400.000   475.000   
-            25.000      125.000   200.000   275.000   350.000  425.000   500.000
-
-                                Radial distance in meters          
-	Detection Fct/Global/Chi-sq GOF Test 2	
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        25.0           22       17.82        0.983
-   2      25.0        50.0           46       49.22        0.210
-   3      50.0        75.0           73       69.42        0.184
-   4      75.0        100.           77       75.32        0.038
-   5      100.        125.           60       68.32        1.013
-   6      125.        150.           50       53.24        0.197
-   7      150.        175.           44       36.00        1.776
-   8      175.        200.           20       21.35        0.085
-   9      200.        225.            9       11.51        0.549
-  10      225.        250.            9        6.29        1.168
-  11      250.        275.            3        4.04        0.269
-  12      275.        300.            2        3.02        0.344
-  13      300.        325.            3        2.18        0.307
-  14      325.        350.            1        1.33        0.083
-  15      350.        375.            1        0.70        0.127
-  16      375.        400.            0        0.47        0.468
-  17      400.        425.            1        0.54        0.381
-  18      425.        450.            1        0.69        0.134
-  19      450.        475.            0        0.76        0.761
-  20      475.        500.            1        0.76        0.072
- -----------------------------------------------------------------
- Total Chi-square value =     9.1484  Degrees of Freedom = 14.00
-
-Probability of a greater chi-square value, P = 0.82143
-
- The program has limited capability for pooling.  The user should
- judge the necessity for pooling and if necessary, do pooling by hand.
-
- Goodness of Fit Testing with some Pooling
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        25.0           22       17.82        0.983
-   2      25.0        50.0           46       49.22        0.210
-   3      50.0        75.0           73       69.42        0.184
-   4      75.0        100.           77       75.32        0.038
-   5      100.        125.           60       68.32        1.013
-   6      125.        150.           50       53.24        0.197
-   7      150.        175.           44       36.00        1.776
-   8      175.        200.           20       21.35        0.085
-   9      200.        225.            9       11.51        0.549
-  10      225.        250.            9        6.29        1.168
-  11      250.        275.            3        4.04        0.269
-  12      275.        300.            2        3.02        0.344
-  13      300.        325.            3        2.18        0.307
-  14      325.        350.            1        1.33        0.083
-  15      350.        375.            1        0.70        0.127
-  16      375.        400.            0        0.47        0.468
-  17      400.        425.            1        0.54        0.381
-  18      425.        500.            2        2.22        0.022
- -----------------------------------------------------------------
- Total Chi-square value =     8.2030  Degrees of Freedom = 12.00
-
-Probability of a greater chi-square value, P = 0.76907
-
- One or more expected values is < 1.
- Try pooling some some cells by hand to obtain a more reliable test.
-	Detection Fct/Global/Plot: Detection Probability 3	
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-     1.7694 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |***                                                               |
-     1.5536 +* *                                                               +
-            |* *                                                               |
-            |* *                                                               |
-            |* *                                                               |
-            |* *                                                               |
- D   1.3379 +* *                                                               +
- e          |* *                                                               |
- t          |* *                                                               |
- e          |* *                                                               |
- c          |* *                                                               |
- t   1.1221 +* *                                                               +
- i          |* *                                                               |
- o          |* ***                                                             |
- n          |*ff *                                                             |
-            |* *ff                                                             |
- P   0.9063 +* * *f                                                            +
- r          |* * * f***                                                        |
- o          |* * *  f *                                                        |
- b          |* * *  *f*                                                        |
- a          |* * **** f                                                        |
- b   0.6905 +* * *  * *f*                                                      +
- i          |* * *  * * *                                                      |
- l          |* * *  * * f**                                                    |
- i          |* * *  * * *f*                                                    |
- t          |* * *  * * * f                                                    |
- y   0.4747 +* * *  * * * *                                                    +
-            |* * *  * * * *f*                                                  |
-            |* * *  * * * * f                                                  |
-            |* * *  * * * * *f                                                 |
-            |* * *  * * * * **f                                                |
-     0.2589 +* * *  * * * * * *f                                               +
-            |* * *  * * * * * **f***                                           |
-            |* * *  * * * * * *  ff*                                           |
-            |* * *  * * * * * *  * f                                           |
-            |* * *  * * * * * *  * *fff*                                       |
-     0.0432 +* * *  * * * * * *  * * * ffff***                                 +
-            |                              ffffffffffffffffffffffffffffffffffff|
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-          0.000      100.000  166.667   250.000  316.667  383.333    466.667    
-           16.667   83.333  150.000   233.333  300.000  366.667   450.000       
-
-                                Radial distance in meters          
-	Detection Fct/Global/Plot: Pdf 3	
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-     0.0083 +                                                                  +
-            |                                                                  |
-            |                                                                  |
-            |                                                                  |
-            |           ***                                                    |
-     0.0073 +           f *                                                    +
-            |          f*ff                                                    |
-            |       **f** *                                                    |
-            |       *f* * *f                                                   |
-            |       * * * **f                                                  |
-     0.0063 +       * * * * *                                                  +
-            |       f * * * *                                                  |
- P          |       * * * * *f                                                 |
- r          |       * * * * *                                                  |
- o          |      f* * * * * f                                                |
- b   0.0052 +       * * * * *                                                  +
- a          |       * * * * ***f                                               |
- b          |     f * * * * * *                                                |
- i          |       * * * * * *  ***                                           |
- l          |       * * * * * * f* *                                           |
- i   0.0042 +    **** * * * * *  * *                                           +
- t          |    f  * * * * * ***f *                                           |
- y          |    *  * * * * * *  * *                                           |
-            |  ***  * * * * * *  * *                                           |
- D          |  * *  * * * * * *  *f*                                           |
- e   0.0032 +  * *  * * * * * *  * *                                           +
- n          |  *f*  * * * * * *  * f                                           |
- s          |  * *  * * * * * *  * *                                           |
- i          |  * *  * * * * * *  * *f                                          |
- t          |  * *  * * * * * *  * ***                                         |
- y   0.0022 +  * *  * * * * * *  * * f**                                       +
-            |  f *  * * * * * *  * * * *                                       |
-            |*** *  * * * * * *  * * *f*                                       |
-            |* * *  * * * * * *  * * * *                                       |
-            |* * *  * * * * * *  * * * f                                       |
-     0.0012 +* * *  * * * * * *  * * * *f                                      +
-            |*f* *  * * * * * *  * * * * f ***                                 |
-            |* * *  * * * * * *  * * * ***f* *                                 |
-            |* * *  * * * * * *  * * * * * ff*                                 |
-            |* * *  * * * * * *  * * * * * * fffff                             |
-     0.0002 +* * *  * * * * * *  * * * * * * *****ffffffff****     *****    ***+
-            |                                             fffffffffffffffffffff|
-            |+-+--------+-+------+-+-------+-+------+-+------+-+-------+--+----|
-          0.000      100.000  166.667   250.000  316.667  383.333    466.667    
-           16.667   83.333  150.000   233.333  300.000  366.667   450.000       
-
-                                Radial distance in meters          
-	Detection Fct/Global/Chi-sq GOF Test 3	
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        16.7           13        8.01        3.110
-   2      16.7        33.3           25       23.16        0.146
-   3      33.3        50.0           30       35.86        0.958
-   4      50.0        66.7           48       44.91        0.213
-   5      66.7        83.3           49       49.67        0.009
-   6      83.3        100.           53       50.16        0.161
-   7      100.        117.           46       46.95        0.019
-   8      117.        133.           35       41.02        0.884
-   9      133.        150.           29       33.59        0.628
-  10      150.        167.           32       25.83        1.473
-  11      167.        183.           17       18.70        0.154
-  12      183.        200.           15       12.82        0.370
-  13      200.        217.            6        8.46        0.715
-  14      217.        233.            5        5.55        0.054
-  15      233.        250.            7        3.80        2.696
-  16      250.        267.            2        2.84        0.248
-  17      267.        283.            1        2.30        0.736
-  18      283.        300.            2        1.92        0.003
-  19      300.        317.            2        1.55        0.130
-  20      317.        333.            1        1.16        0.023
-  21      333.        350.            1        0.80        0.051
-  22      350.        367.            1        0.51        0.459
-  23      367.        383.            0        0.35        0.350
-  24      383.        400.            0        0.31        0.305
-  25      400.        417.            0        0.34        0.345
-  26      417.        433.            1        0.42        0.815
-  27      433.        450.            1        0.48        0.573
-  28      450.        467.            0        0.51        0.506
-  29      467.        483.            0        0.51        0.509
-  30      483.        500.            1        0.51        0.470
- -----------------------------------------------------------------
- Total Chi-square value =    17.1130  Degrees of Freedom = 24.00
-
-Probability of a greater chi-square value, P = 0.84380
-
- The program has limited capability for pooling.  The user should
- judge the necessity for pooling and if necessary, do pooling by hand.
-
- Goodness of Fit Testing with some Pooling
-
-  Cell           Cut           Observed     Expected   Chi-square
-   i            Points          Values       Values       Values
- -----------------------------------------------------------------
-   1     0.000        16.7           13        8.01        3.110
-   2      16.7        33.3           25       23.16        0.146
-   3      33.3        50.0           30       35.86        0.958
-   4      50.0        66.7           48       44.91        0.213
-   5      66.7        83.3           49       49.67        0.009
-   6      83.3        100.           53       50.16        0.161
-   7      100.        117.           46       46.95        0.019
-   8      117.        133.           35       41.02        0.884
-   9      133.        150.           29       33.59        0.628
-  10      150.        167.           32       25.83        1.473
-  11      167.        183.           17       18.70        0.154
-  12      183.        200.           15       12.82        0.370
-  13      200.        217.            6        8.46        0.715
-  14      217.        233.            5        5.55        0.054
-  15      233.        250.            7        3.80        2.696
-  16      250.        267.            2        2.84        0.248
-  17      267.        283.            1        2.30        0.736
-  18      283.        300.            2        1.92        0.003
-  19      300.        317.            2        1.55        0.130
-  20      317.        333.            1        1.16        0.023
-  21      333.        350.            1        0.80        0.051
-  22      350.        367.            1        0.51        0.459
-  23      367.        383.            0        0.35        0.350
-  24      383.        400.            0        0.31        0.305
-  25      400.        417.            0        0.34        0.345
-  26      417.        433.            1        0.42        0.815
-  27      433.        500.            2        2.00        0.000
- -----------------------------------------------------------------
- Total Chi-square value =    15.0551  Degrees of Freedom = 21.00
-
-Probability of a greater chi-square value, P = 0.82018
-
- One or more expected values is < 1.
- Try pooling some some cells by hand to obtain a more reliable test.
-	Density Estimates/Global	
-
-
- Effort        :    208.0000    
- # samples     :   104
- Width         :    500.0000    
- # observations:   423
-
- Model  6
-    Uniform key, k(y) = 1/W
-    Cosine adjustments of order(s) :  1, 2, 3, 4, 5
-
-
-              Point        Standard    Percent Coef.        95% Percent
-  Parameter   Estimate       Error      of Variation     Confidence Interval
-  ---------  -----------  -----------  --------------  ----------------------
-    D        0.44524      0.33281E-01       7.47      0.38439      0.51572    
-    N         1069.0       79.907           7.47       923.00       1238.0    
-  ---------  -----------  -----------  --------------  ----------------------
-
- Measurement Units                
- ---------------------------------
- Density: Numbers/hectares       
-     EDR: meters         
-
- Component Percentages of Var(D)
- -------------------------------
- Detection probability   :  42.0
- Encounter rate          :  58.0
-	Estimation Summary - Encounter rates         	
-
-                         Estimate      %CV     df     95% Confidence Interval
-                        ------------------------------------------------------
-                 n       423.00    
-                 k       104.00    
-                 K       208.00    
-                 n/K     2.0337        5.69   103.00  1.8167       2.2766    
-                 Left    0.0000
-                 Width   500.00    
-	Estimation Summary - Detection probability   	
-
-                         Estimate      %CV     df     95% Confidence Interval
-                        ------------------------------------------------------
- Uniform/Cosine         
-                 m       5.0000    
-                 LnL    -2318.6    
-                 AIC     4647.2    
-                 AICc    4647.4    
-                 BIC     4667.5    
-                 Chi-p  0.82018    
-                 h(0)   0.13756E-03    4.84   418.00 0.12508E-03  0.15129E-03
-                 p      0.58156E-01    4.84   418.00 0.52878E-01  0.63960E-01
-                 EDR     120.58        2.42   418.00  114.97       126.45    
-	Estimation Summary - Density&Abundance       	
-
-                         Estimate      %CV     df     95% Confidence Interval
-                        ------------------------------------------------------
- Uniform/Cosine         
-                 D      0.44524        7.47   270.98 0.38439      0.51572    
-                 N       1069.0        7.47   270.98  923.00       1238.0    
+	Estimation Options Listing	
+
+ Parameter Estimation Specification
+ ----------------------------------
+ Encounter rate for all data combined
+ Detection probability for all data combined
+ Density for all data combined
+
+ Distances:
+ ----------
+ Analysis based on exact distances
+ Width: use largest measurement/last interval endpoint
+
+ Estimators:
+ -----------
+ Estimator  1
+ Key: Uniform
+ Adjustments - Function                 : Cosines
+             - Term selection mode      : Sequential
+             - Term selection criterion : Akaike Information Criterion (AIC)
+             - Distances scaled by      : W (right truncation distance)
+
+ Estimator selection: Choose estimator with minimum  AIC
+ Estimation functions: constrained to be nearly monotone non-increasing
+
+ Variances:
+ ----------
+ Variance of n: Empirical estimate from sample
+                (design-derived estimator R2/P2)
+ Variance of f(0): MLE estimate
+
+ Goodness of fit:
+ ----------------
+ Cut points chosen by program
+
+
+
+ Glossary of terms
+ -----------------
+
+ Data items:
+ n    - number of observed objects (single or clusters of animals)
+ L    - total length of transect line(s) 
+ k    - number of samples
+ K    - point transect effort, typically K=k
+ T    - length of time searched in cue counting
+ ER   - encounter rate (n/L or n/K or n/T)
+ W    - width of line transect or radius of point transect
+ x(i) - distance to i-th observation
+ s(i) - cluster size of i-th observation
+ r-p  - probability for regression test
+ chi-p- probability for chi-square goodness-of-fit test
+
+
+ Parameters or functions of parameters:
+ m    - number of parameters in the model
+ A(I) - i-th parameter in the estimated probability density function(pdf)
+ f(0) - 1/u = value of pdf at zero for line transects
+ u    - W*p = ESW, effective detection area for line transects
+ h(0) - 2*PI/v
+ v    - PI*W*W*p, is the effective detection area for point transects
+ p    - probability of observing an object in defined area
+ ESW  - for line transects, effective strip width = W*p
+ EDR  - for point transects, effective detection radius  = W*sqrt(p)
+ rho  - for cue counts, the cue rate
+ DS   - estimate of density of clusters
+ E(S) - estimate of expected value of cluster size
+ D    - estimate of density of animals
+ N    - estimate of number of animals in specified area
+	Detection Fct/Global/Model Fitting	
+
+
+ Effort        :    190.0000    
+ # samples     :    96
+ Width         :    271.2211    
+ # observations:    47
+
+
+
+ Model  1
+    Uniform key, k(y) = 1/W
+       Results:
+       Convergence was achieved with    1 function evaluations.
+       Final Ln(likelihood) value =  -285.35577    
+       Akaike information criterion =   570.71155    
+       Bayesian information criterion =   570.71155    
+       AICc =   570.71155    
+       Final parameter values: 
+
+
+ Model  2
+    Uniform key, k(y) = 1/W
+    Cosine adjustments of order(s) :  1
+       Results:
+       Convergence was achieved with   24 function evaluations.
+       Final Ln(likelihood) value =  -252.28137    
+       Akaike information criterion =   506.56274    
+       Bayesian information criterion =   508.41290    
+       AICc =   506.65164    
+       Final parameter values:  0.97570701    
+
+    Likelihood ratio test between models  1 and  2
+       Likelihood ratio test value    =    66.1488
+       Probability of a greater value =   0.000000
+ *** Model  2 selected over model  1 based on minimum AIC              
+
+
+ Model  3
+    Uniform key, k(y) = 1/W
+    Cosine adjustments of order(s) :  1, 2
+       Results:
+       Convergence was achieved with   33 function evaluations.
+       Final Ln(likelihood) value =  -245.30442    
+       Akaike information criterion =   494.60883    
+       Bayesian information criterion =   498.30914    
+       AICc =   494.88156    
+       Final parameter values:   1.3313540     0.35102447    
+      ** Warning: Parameters are being constrained to obtain monotonicity. **
+
+    Likelihood ratio test between models  2 and  3
+       Likelihood ratio test value    =    13.9539
+       Probability of a greater value =   0.000187
+ *** Model  3 selected over model  2 based on minimum AIC              
+
+
+ Model  4
+    Uniform key, k(y) = 1/W
+    Cosine adjustments of order(s) :  1, 2, 3
+       Results:
+       Convergence was achieved with   16 function evaluations.
+       Final Ln(likelihood) value =  -527.06363    
+       Akaike information criterion =   1060.1273    
+       Bayesian information criterion =   1065.6777    
+       AICc =   1060.6854    
+       Final parameter values:   61575.195     -27718.076     -61572.895    
+      ** Warning: Parameters are being constrained to obtain monotonicity. **
+
+    Likelihood ratio test between models  3 and  4
+       Likelihood ratio test value    =  -563.5184
+       Probability of a greater value =   1.000000
+ *** Model  3 selected over model  4 based on minimum AIC              
+	Detection Fct/Global/Parameter Estimates	
+
+
+ Effort        :    190.0000    
+ # samples     :    96
+ Width         :    271.2211    
+ # observations:    47
+
+ Model
+    Uniform key, k(y) = 1/W
+    Cosine adjustments of order(s) :  1, 2
+
+
+              Point        Standard    Percent Coef.        95 Percent
+  Parameter   Estimate       Error      of Variation     Confidence Interval
+  ---------  -----------  -----------  --------------  ----------------------
+    A( 1)      1.331       0.1018    
+    A( 2)     0.3510       0.1000    
+    h(0)     0.15839E-03  0.26013E-04      16.42      0.11403E-03  0.22000E-03
+    p        0.17166      0.28191E-01      16.42      0.12358      0.23843    
+    EDR       112.37       9.2274           8.21       95.268       132.54    
+  ---------  -----------  -----------  --------------  ----------------------
+
+
+ Sampling Correlation of Estimated Parameters
+
+
+         A( 1)   A( 2)
+ A( 1)  1.000   0.981
+ A( 2)  0.981   1.000
+	Detection Fct/Global/Plot: Qq-plot	
+	Detection Fct/Global/K-S GOF Test	
+
+
+ Kolmogorov-Smirnov test
+ -----------------------
+
+ D_n                      = 0.1232                 p  = 0.4740
+
+
+ Cramer-von Mises family tests
+ -----------------------------
+
+ W-sq (uniform weighting) = 0.1046          0.500 < p <= 0.600
+   Relevant critical values:
+     W-sq crit(alpha=0.600) = 0.0974
+     W-sq crit(alpha=0.500) = 0.1193
+
+ C-sq (cosine weighting)  = 0.0668          0.500 < p <= 0.600
+   Relevant critical values:
+     C-sq crit(alpha=0.600) = 0.0625
+     C-sq crit(alpha=0.500) = 0.0773
+	Detection Fct/Global/Plot: Detection Probability 1	
+            |+---------------+----------------+---------------+---------------+|
+     1.0987 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            | ffff                                                             |
+     0.9647 +     ff                                                           +
+            |*******ff********                                                 |
+            |*        f      *                                                 |
+            |*         f     *                                                 |
+            |*          f    *                                                 |
+ D   0.8307 +*           f   *                                                 +
+ e          |*            f  *                                                 |
+ t          |*             f *                                                 |
+ e          |*              f*                                                 |
+ c          |*               f                                                 |
+ t   0.6967 +*               *f                                                +
+ i          |*               * f                                               |
+ o          |*               *  f                                              |
+ n          |*               *   f                                             |
+            |*               *    f                                            |
+ P   0.5627 +*               *                                                 +
+ r          |*               *     f                                           |
+ o          |*               *      f                                          |
+ b          |*               *       f                                         |
+ a          |*               *********f********                                |
+ b   0.4288 +*               *         f      *                                +
+ i          |*               *          f     *                                |
+ l          |*               *                *                                |
+ i          |*               *           f    *                                |
+ t          |*               *            f   *                                |
+ y   0.2948 +*               *             f  *                                +
+            |*               *              f *                                |
+            |*               *               ff                                |
+            |*               *                *f                               |
+            |*               *                * f                              |
+     0.1608 +*               *                *  f                             +
+            |*               *                *   ff                           |
+            |*               *                *     ff                         |
+            |*               *                ********ff*******                |
+            |*               *                *         fff   *                |
+     0.0268 +*               *                *            ffffff              +
+            |                                                 ***ffffffffffffff|
+            |+---------------+----------------+---------------+---------------+|
+          0.000          67.805          135.611         203.416         271.221
+                                                                                
+
+                                Radial distance in meters          
+	Detection Fct/Global/Plot: Pdf 1	
+            |+---------------+----------------+---------------+---------------+|
+     0.0088 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |                 ffffff                                           |
+     0.0078 +                f      f                                          +
+            |               f********ff********                                |
+            |              f *                *                                |
+            |                *         f      *                                |
+            |             f  *          f     *                                |
+     0.0067 +            f   *           f    *                                +
+            |                *            f   *                                |
+ P          |           f    *                *                                |
+ r          |                *             f  *                                |
+ o          |          f     *              f *                                |
+ b   0.0056 +                *                *                                +
+ a          |         f      *               f*                                |
+ b          |                *                *                                |
+ i          |********f********                f                                |
+ l          |*               *                *f                               |
+ i   0.0045 +*               *                *                                +
+ t          |*      f        *                * f                              |
+ y          |*               *                *                                |
+            |*     f         *                *  f                             |
+ D          |*               *                *   f                            |
+ e   0.0035 +*               *                *                                +
+ n          |*    f          *                *    f                           |
+ s          |*               *                *     f                          |
+ i          |*               *                *                                |
+ t          |*   f           *                *      f                         |
+ y   0.0024 +*               *                *       f                        +
+            |*               *                *                                |
+            |*  f            *                *********f*******                |
+            |*               *                *         f     *                |
+            |*               *                *          f    *                |
+     0.0013 +* f             *                *           f   *                +
+            |*               *                *            f  *                |
+            |*               *                *             ff*                |
+            |*f              *                *               ff               |
+            |*               *                *               * fff            |
+     0.0002 +*               *                *               *****ffffffffffff+
+            |                                                                  |
+            |+---------------+----------------+---------------+---------------+|
+          0.000          67.805          135.611         203.416         271.221
+                                                                                
+
+                                Radial distance in meters          
+	Detection Fct/Global/Chi-sq GOF Test 1	
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        67.8           16       14.65        0.125
+   2      67.8        136.           24       22.97        0.047
+   3      136.        203.            6        8.31        0.642
+   4      203.        271.            1        1.08        0.006
+ -----------------------------------------------------------------
+ Total Chi-square value =     0.8193  Degrees of Freedom =  1.00
+
+Probability of a greater chi-square value, P = 0.36538
+
+ The program has limited capability for pooling.  The user should
+ judge the necessity for pooling and if necessary, do pooling by hand.
+	Detection Fct/Global/Plot: Detection Probability 2	
+            |+----------+----------+----------+---------+----------+----------+|
+     1.0987 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            | ffff                                                             |
+     0.9647 +     ff                                                           +
+            |       ff                                                         |
+            |*********f**                                                      |
+            |*         f*                                                      |
+            |*          f                                                      |
+ D   0.8307 +*          *f                                                     +
+ e          |*          * f                                                    |
+ t          |*          *  f                                                   |
+ e          |*          *   f                                                  |
+ c          |*          *    f                                                 |
+ t   0.6967 +*          *     f                                                +
+ i          |*          *      f                                               |
+ o          |*          *       f                                              |
+ n          |*          *        f                                             |
+            |*          *         f************                                |
+ P   0.5627 +*          *          *          *                                +
+ r          |*          *          f          *                                |
+ o          |*          *          *f         *                                |
+ b          |*          ************ f        *                                |
+ a          |*          *          *  f       *                                |
+ b   0.4288 +*          *          *   f      *                                +
+ i          |*          *          *    f     *                                |
+ l          |*          *          *          *                                |
+ i          |*          *          *     f    *                                |
+ t          |*          *          *      f   *                                |
+ y   0.2948 +*          *          *       f  *                                +
+            |*          *          *        f *                                |
+            |*          *          *         ff                                |
+            |*          *          *          *f                               |
+            |*          *          *          * f                              |
+     0.1608 +*          *          *          *  f                             +
+            |*          *          *          *   ff                           |
+            |*          *          *          ******ff***                      |
+            |*          *          *          *       ff*                      |
+            |*          *          *          *         fff                    |
+     0.0268 +*          *          *          *         *  ffffff              +
+            |                                           *********ffffffffffffff|
+            |+----------+----------+----------+---------+----------+----------+|
+          0.000                90.407    135.611   180.814    226.018    271.221
+                    45.204                                                      
+
+                                Radial distance in meters          
+	Detection Fct/Global/Plot: Pdf 2	
+            |+----------+----------+----------+---------+----------+----------+|
+     0.0114 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |                      ************                                |
+     0.0100 +                      *          *                                +
+            |                      *          *                                |
+            |                      *          *                                |
+            |                      *          *                                |
+            |                      *          *                                |
+     0.0086 +                      *          *                                +
+            |                      *          *                                |
+ P          |                  fffff          *                                |
+ r          |                ff    *ff        *                                |
+ o          |               f      *  f       *                                |
+ b   0.0072 +              f       *   f      *                                +
+ a          |             f        *    f     *                                |
+ b          |            f         *     f    *                                |
+ i          |           f          *      f   *                                |
+ l          |                      *       f  *                                |
+ i   0.0058 +          f           *        f *                                +
+ t          |                      *          *                                |
+ y          |         f ************         f*                                |
+            |        f  *          *          f                                |
+ D          |           *          *          *f                               |
+ e   0.0044 +       f   *          *          *                                +
+ n          |           *          *          * f                              |
+ s          |      f    *          *          *  f                             |
+ i          |           *          *          *   f                            |
+ t          |*****f******          *          *    f                           |
+ y   0.0031 +*          *          *          *     f                          +
+            |*          *          *          ***********                      |
+            |*   f      *          *          *      f  *                      |
+            |*          *          *          *       f *                      |
+            |*  f       *          *          *        f*                      |
+     0.0017 +*          *          *          *         ff                     +
+            |* f        *          *          *         * f                    |
+            |*          *          *          *         *  f                   |
+            |*          *          *          *         *   ff                 |
+            |*f         *          *          *         *     fff  ************|
+     0.0003 +*          *          *          *         *        ffffffffffffff+
+            |                                           ************           |
+            |+----------+----------+----------+---------+----------+----------+|
+          0.000                90.407    135.611   180.814    226.018    271.221
+                    45.204                                                      
+
+                                Radial distance in meters          
+	Detection Fct/Global/Chi-sq GOF Test 2	
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        45.2            7        7.09        0.001
+   2      45.2        90.4           11       16.02        1.575
+   3      90.4        136.           22       14.49        3.890
+   4      136.        181.            6        7.09        0.168
+   5      181.        226.            0        1.71        1.706
+   6      226.        271.            1        0.59        0.284
+ -----------------------------------------------------------------
+ Total Chi-square value =     7.6250  Degrees of Freedom =  3.00
+
+Probability of a greater chi-square value, P = 0.05443
+
+ The program has limited capability for pooling.  The user should
+ judge the necessity for pooling and if necessary, do pooling by hand.
+
+ Goodness of Fit Testing with some Pooling
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        45.2            7        7.09        0.001
+   2      45.2        90.4           11       16.02        1.575
+   3      90.4        136.           22       14.49        3.890
+   4      136.        181.            6        7.09        0.168
+   5      181.        271.            1        2.30        0.732
+ -----------------------------------------------------------------
+ Total Chi-square value =     6.3673  Degrees of Freedom =  2.00
+
+Probability of a greater chi-square value, P = 0.04143
+	Detection Fct/Global/Plot: Detection Probability 3	
+            |+------+-----+------+-----+------+-----+-----+------+------+-----+|
+     1.2052 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |       *******                                                    |
+     1.0583 +       *     *                                                    +
+            |       *     *                                                    |
+            | fff   *     *                                                    |
+            |    fff*     *                                                    |
+            |       ff    *                                                    |
+ D   0.9113 +       * f   *                                                    +
+ e          |       *  f  *                                                    |
+ t          |       *   ff*                                                    |
+ e          |       *     f                                                    |
+ c          |       *     *f                                                   |
+ t   0.7643 +       *     * f                                                  +
+ i          |********     *  f                                                 |
+ o          |*      *     *   f                                                |
+ n          |*      *     *    f                                               |
+            |*      *     *     f                                              |
+ P   0.6173 +*      *     *      f                                             +
+ r          |*      *     *       f                                            |
+ o          |*      *     *        f                                           |
+ b          |*      *     *      **************                                |
+ a          |*      *     *      *  f  *      *                                |
+ b   0.4703 +*      *     *      *   f *      *                                +
+ i          |*      *     ********    f*      *                                |
+ l          |*      *     *      *     f      *                                |
+ i          |*      *     *      *     *f     *                                |
+ t          |*      *     *      *     * f    *                                |
+ y   0.3234 +*      *     *      *     *  f   *                                +
+            |*      *     *      *     *   ff *                                |
+            |*      *     *      *     *     f*                                |
+            |*      *     *      *     *      f                                |
+            |*      *     *      *     *      *f                               |
+     0.1764 +*      *     *      *     *      * ff                             +
+            |*      *     *      *     *      ****f**                          |
+            |*      *     *      *     *      *    ff                          |
+            |*      *     *      *     *      *     *ff                        |
+            |*      *     *      *     *      *     ***ffff                    |
+     0.0294 +*      *     *      *     *      *     *     *fffff        *******+
+            |                                             ******fffffffffffffff|
+            |+------+-----+------+-----+------+-----+-----+------+------+-----+|
+          0.000       54.244      108.488      162.733      216.977      271.221
+                27.122       81.366      135.611     189.855       244.099      
+
+                                Radial distance in meters          
+	Detection Fct/Global/Plot: Pdf 3	
+            |+------+-----+------+-----+------+-----+-----+------+------+-----+|
+     0.0112 +                                                                  +
+            |                                                                  |
+            |                                                                  |
+            |                                                                  |
+            |                          ********                                |
+     0.0098 +                          *      *                                +
+            |                          *      *                                |
+            |                          *      *                                |
+            |                          *      *                                |
+            |                          *      *                                |
+     0.0085 +                          *      *                                +
+            |                          *      *                                |
+ P          |                 fffffff***      *                                |
+ r          |               ff   *   f *      *                                |
+ o          |              f     *    f*      *                                |
+ b   0.0071 +       ******f      *     f      *                                +
+ a          |       *     *      *     *f     *                                |
+ b          |       *    f*      *     * f    *                                |
+ i          |       *   f *      *     *  f   *                                |
+ l          |       *     *      *     *   f  *                                |
+ i   0.0057 +       *  f  *      *     *    f *                                +
+ t          |       * f   *      *     *     f*                                |
+ y          |       *     *      *     *      *                                |
+            |       *f    *      *     *      f                                |
+ D          |       *     ********     *      *f                               |
+ e   0.0044 +       f     *      *     *      * f                              +
+ n          |       *     *      *     *      *                                |
+ s          |      f*     *      *     *      *  f                             |
+ i          |       *     *      *     *      *   f                            |
+ t          |     f *     *      *     *      *    f                           |
+ y   0.0030 +       *     *      *     *      ******f                          +
+            |       *     *      *     *      *     *f                         |
+            |    f  *     *      *     *      *     *                          |
+            |       *     *      *     *      *     * f                        |
+            |   f   *     *      *     *      *     *  f                       |
+     0.0016 +********     *      *     *      *     ****ff*                    +
+            |* f    *     *      *     *      *     *     f                    |
+            |*      *     *      *     *      *     *     *f                   |
+            |*      *     *      *     *      *     *     * ff          *******|
+            |*f     *     *      *     *      *     *     *   fff       *     *|
+     0.0003 +*      *     *      *     *      *     *     *      ffffffffffffff+
+            |                                             ***************      |
+            |+------+-----+------+-----+------+-----+-----+------+------+-----+|
+          0.000       54.244      108.488      162.733      216.977      271.221
+                27.122       81.366      135.611     189.855       244.099      
+
+                                Radial distance in meters          
+	Detection Fct/Global/Chi-sq GOF Test 3	
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        27.1            2        2.67        0.168
+   2      27.1        54.2            9        7.24        0.428
+   3      54.2        81.4            6        9.80        1.475
+   4      81.4        108.           10        9.90        0.001
+   5      108.        136.           13        7.99        3.134
+   6      136.        163.            4        5.15        0.258
+   7      163.        190.            2        2.56        0.121
+   8      190.        217.            0        0.94        0.943
+   9      217.        244.            0        0.37        0.372
+  10      244.        271.            1        0.37        1.102
+ -----------------------------------------------------------------
+ Total Chi-square value =     8.0034  Degrees of Freedom =  7.00
+
+Probability of a greater chi-square value, P = 0.33229
+
+ The program has limited capability for pooling.  The user should
+ judge the necessity for pooling and if necessary, do pooling by hand.
+
+ Goodness of Fit Testing with some Pooling
+
+  Cell           Cut           Observed     Expected   Chi-square
+   i            Points          Values       Values       Values
+ -----------------------------------------------------------------
+   1     0.000        27.1            2        2.67        0.168
+   2      27.1        54.2            9        7.24        0.428
+   3      54.2        81.4            6        9.80        1.475
+   4      81.4        108.           10        9.90        0.001
+   5      108.        136.           13        7.99        3.134
+   6      136.        163.            4        5.15        0.258
+   7      163.        271.            3        4.24        0.361
+ -----------------------------------------------------------------
+ Total Chi-square value =     5.8256  Degrees of Freedom =  4.00
+
+Probability of a greater chi-square value, P = 0.21256
+	Density Estimates/Global	
+
+
+ Effort        :    190.0000    
+ # samples     :    96
+ Width         :    271.2211    
+ # observations:    47
+
+ Model  3
+    Uniform key, k(y) = 1/W
+    Cosine adjustments of order(s) :  1, 2
+
+
+              Point        Standard    Percent Coef.        95% Percent
+  Parameter   Estimate       Error      of Variation     Confidence Interval
+  ---------  -----------  -----------  --------------  ----------------------
+    D         6.2358       1.5060          24.15       3.8931       9.9881    
+    N         150.00       36.226          24.15       93.000       240.00    
+  ---------  -----------  -----------  --------------  ----------------------
+
+ Measurement Units                
+ ---------------------------------
+ Density: Numbers/Sq. kilometers 
+     EDR: meters         
+
+ Component Percentages of Var(D)
+ -------------------------------
+ Detection probability   :  46.2
+ Encounter rate          :  53.8
+	Estimation Summary - Encounter rates         	
+
+                         Estimate      %CV     df     95% Confidence Interval
+                        ------------------------------------------------------
+                 n       47.000    
+                 k       96.000    
+                 K       190.00    
+                 n/K    0.24737       17.71    95.00 0.17452      0.35062    
+                 Left    0.0000
+                 Width   271.22    
+	Estimation Summary - Detection probability   	
+
+                         Estimate      %CV     df     95% Confidence Interval
+                        ------------------------------------------------------
+ Uniform/Cosine         
+                 m       2.0000    
+                 LnL    -245.30    
+                 AIC     494.61    
+                 AICc    494.88    
+                 BIC     498.31    
+                 Chi-p  0.21256    
+                 h(0)   0.15839E-03   16.42    45.00 0.11403E-03  0.22000E-03
+                 p      0.17166       16.42    45.00 0.12358      0.23843    
+                 EDR     112.37        8.21    45.00  95.268       132.54    
+	Estimation Summary - Density&Abundance       	
+
+                         Estimate      %CV     df     95% Confidence Interval
+                        ------------------------------------------------------
+ Uniform/Cosine         
+                 D       6.2358       24.15   128.30  3.8931       9.9881    
+                 N       150.00       24.15   128.30  93.000       240.00
```

### Comparing `pyaudisam-0.9.3/tests/refout/dist-order-sens-min/cmd-win7-orig-order/stats.txt` & `pyaudisam-1.0.1/docs/how-it-works/optanlys/SylvAtri-ab-10mn-m-haz-pol-la-ra-ma-90vq_6nr/stats.txt`

 * *Files 12% similar despite different names*

```diff
@@ -1,34 +1,29 @@
-                                                   
-     0     0 1 1   1   423.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   2   104.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   3   208.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   4   2.033654      0.5693950E-01   1.816659       2.276567       103.0000    
-     0     0 1 1   5      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 1   6   500.0000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   1   5.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   4  0.1375615E-03  0.4842835E-01  0.1250775E-03  0.1512915E-03   418.0000    
-     0     0 1 2   5  0.5815582E-01  0.4842835E-01  0.5287804E-01  0.6396037E-01   418.0000    
-     0     0 1 2   6   120.5776      0.2421418E-01   114.9737       126.4546       418.0000    
-     0     0 1 2   2   4647.219          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   7   4647.363          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   8   4667.456          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   9  -2318.610          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  13   1.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  14   3.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  15      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  16   5.000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  17      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2 101   1.730060      0.6882868E-02      0.0000000      0.0000000      0.0000000
-     0     0 1 2 102   1.141592      0.2724335E-01      0.0000000      0.0000000      0.0000000
-     0     0 1 2 103  0.5713539      0.6843474E-01      0.0000000      0.0000000      0.0000000
-     0     0 1 2 104  0.2038293      0.1471647          0.0000000      0.0000000      0.0000000
-     0     0 1 2 105  0.3902836E-01  0.3114501          0.0000000      0.0000000      0.0000000
-     0     0 1 2  10  0.9628050          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  11  0.8000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2  12  0.8000000          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   3  0.8584204          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   3  0.7690719          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 2   3  0.8201779          0.0000000      0.0000000      0.0000000      0.0000000
-     0     0 1 4   1  0.4452398      0.7474899E-01  0.3843891      0.5157234       270.9762    
-     0     0 1 4   2  0.4452398      0.7474899E-01  0.3843891      0.5157234       270.9762    
-     0     0 1 4   3   1069.000      0.7474899E-01   923.0000       1238.000       270.9762    
+                                                   
+     0     0 1 1   1   401.0000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   2   96.00000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   3   190.0000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   4   2.110526      0.6867700E-01   1.841824       2.418429       95.00000    
+     0     0 1 1   5   4.553960          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 1   6   492.3280          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   1   2.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   4  0.1118922E-03  0.7300479E-01  0.9695091E-04  0.1291362E-03   399.0000    
+     0     0 1 2   5  0.7374303E-01  0.7300479E-01  0.6389590E-01  0.8510773E-01   399.0000    
+     0     0 1 2   6   133.6950      0.3650239E-01   124.4400       143.6382       399.0000    
+     0     0 1 2   2   4488.174          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   7   4488.205          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   8   4496.162          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   9  -2242.087          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  13   4.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  14   1.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  15   2.000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  16      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  17      0.0000000      0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2 101   99.03355      0.5326374E-01      0.0000000      0.0000000      0.0000000
+     0     0 1 2 102   3.760158      0.5627085E-01      0.0000000      0.0000000      0.0000000
+     0     0 1 2  10  0.5500754          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  11  0.6000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2  12  0.7000000          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 2   3  0.2500808          0.0000000      0.0000000      0.0000000      0.0000000
+     0     0 1 4   1   37.58468      0.1002309       30.87411       45.75380       330.5204    
+     0     0 1 4   2   37.58468      0.1002309       30.87411       45.75380       330.5204    
+     0     0 1 4   3   902.0000      0.1002309       741.0000       1098.000       330.5204
```

### Comparing `pyaudisam-0.9.3/tests/sensitivity.ipynb` & `pyaudisam-1.0.1/tests/sensitivity.ipynb`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 26% similar despite different names*

```diff
@@ -1,1112 +1,1075 @@
-00000000: 7b0d 0a20 2263 656c 6c73 223a 205b 0d0a  {.. "cells": [..
-00000010: 2020 7b0d 0a20 2020 2263 656c 6c5f 7479    {..   "cell_ty
-00000020: 7065 223a 2022 6d61 726b 646f 776e 222c  pe": "markdown",
-00000030: 0d0a 2020 2022 6d65 7461 6461 7461 223a  ..   "metadata":
-00000040: 207b 7d2c 0d0a 2020 2022 736f 7572 6365   {},..   "source
-00000050: 223a 205b 0d0a 2020 2020 223c 212d 2d20  ": [..    "<!-- 
-00000060: 5461 626c 6520 6465 7320 6d61 7469 c3a8  Table des mati..
-00000070: 7265 7320 6175 746f 6d61 7469 7175 6520  res automatique 
-00000080: 2d2d 3e5c 6e22 2c0d 0a20 2020 2022 3c68  -->\n",..    "<h
-00000090: 3120 636c 6173 733d 2774 6f63 4967 6e6f  1 class='tocIgno
-000000a0: 7265 273e 5365 6e73 6974 6976 6974 7920  re'>Sensitivity 
-000000b0: 7465 7374 7320 286f 6c64 2061 6e64 2075  tests (old and u
-000000c0: 6e6d 6169 6e74 6169 6e65 6429 3c2f 6831  nmaintained)</h1
-000000d0: 3e5c 6e22 2c0d 0a20 2020 2022 5c6e 222c  >\n",..    "\n",
-000000e0: 0d0a 2020 2020 222a 2a70 7961 7564 6973  ..    "**pyaudis
-000000f0: 616d 2a2a 3a20 4175 746f 6d61 7469 6f6e  am**: Automation
-00000100: 206f 6620 4469 7374 616e 6365 2053 616d   of Distance Sam
-00000110: 706c 696e 6720 616e 616c 7973 6573 2077  pling analyses w
-00000120: 6974 6820 5b44 6973 7461 6e63 6520 736f  ith [Distance so
-00000130: 6674 7761 7265 5d28 6874 7470 3a2f 2f64  ftware](http://d
-00000140: 6973 7461 6e63 6573 616d 706c 696e 672e  istancesampling.
-00000150: 6f72 672f 295c 6e22 2c0d 0a20 2020 2022  org/)\n",..    "
-00000160: 5c6e 222c 0d0a 2020 2020 2243 6f70 7972  \n",..    "Copyr
-00000170: 6967 6874 2028 4329 2032 3032 3120 4a65  ight (C) 2021 Je
-00000180: 616e 2d50 6869 6c69 7070 6520 4d65 7572  an-Philippe Meur
-00000190: 6574 5c6e 222c 0d0a 2020 2020 225c 6e22  et\n",..    "\n"
-000001a0: 2c0d 0a20 2020 2022 5468 6973 2070 726f  ,..    "This pro
-000001b0: 6772 616d 2069 7320 6672 6565 2073 6f66  gram is free sof
-000001c0: 7477 6172 653a 2079 6f75 2063 616e 2072  tware: you can r
-000001d0: 6564 6973 7472 6962 7574 6520 6974 2061  edistribute it a
-000001e0: 6e64 2f6f 7220 6d6f 6469 6679 2069 7420  nd/or modify it 
-000001f0: 756e 6465 7220 7468 6520 7465 726d 735c  under the terms\
-00000200: 6e22 2c0d 0a20 2020 2022 6f66 2074 6865  n",..    "of the
-00000210: 2047 4e55 2047 656e 6572 616c 2050 7562   GNU General Pub
-00000220: 6c69 6320 4c69 6365 6e73 6520 6173 2070  lic License as p
-00000230: 7562 6c69 7368 6564 2062 7920 7468 6520  ublished by the 
-00000240: 4672 6565 2053 6f66 7477 6172 6520 466f  Free Software Fo
-00000250: 756e 6461 7469 6f6e 2c5c 6e22 2c0d 0a20  undation,\n",.. 
-00000260: 2020 2022 6569 7468 6572 2076 6572 7369     "either versi
-00000270: 6f6e 2033 206f 6620 7468 6520 4c69 6365  on 3 of the Lice
-00000280: 6e73 652c 206f 7220 2861 7420 796f 7572  nse, or (at your
-00000290: 206f 7074 696f 6e29 2061 6e79 206c 6174   option) any lat
-000002a0: 6572 2076 6572 7369 6f6e 2e5c 6e22 2c0d  er version.\n",.
-000002b0: 0a20 2020 2022 5c6e 222c 0d0a 2020 2020  .    "\n",..    
-000002c0: 2254 6869 7320 7072 6f67 7261 6d20 6973  "This program is
-000002d0: 2064 6973 7472 6962 7574 6564 2069 6e20   distributed in 
-000002e0: 7468 6520 686f 7065 2074 6861 7420 6974  the hope that it
-000002f0: 2077 696c 6c20 6265 2075 7365 6675 6c2c   will be useful,
-00000300: 2062 7574 2057 4954 484f 5554 2041 4e59   but WITHOUT ANY
-00000310: 2057 4152 5241 4e54 593b 5c6e 222c 0d0a   WARRANTY;\n",..
-00000320: 2020 2020 2277 6974 686f 7574 2065 7665      "without eve
-00000330: 6e20 7468 6520 696d 706c 6965 6420 7761  n the implied wa
-00000340: 7272 616e 7479 206f 6620 4d45 5243 4841  rranty of MERCHA
-00000350: 4e54 4142 494c 4954 5920 6f72 2046 4954  NTABILITY or FIT
-00000360: 4e45 5353 2046 4f52 2041 2050 4152 5449  NESS FOR A PARTI
-00000370: 4355 4c41 5220 5055 5250 4f53 452e 5c6e  CULAR PURPOSE.\n
-00000380: 222c 0d0a 2020 2020 2253 6565 2074 6865  ",..    "See the
-00000390: 2047 4e55 2047 656e 6572 616c 2050 7562   GNU General Pub
-000003a0: 6c69 6320 4c69 6365 6e73 6520 666f 7220  lic License for 
-000003b0: 6d6f 7265 2064 6574 6169 6c73 2e5c 6e22  more details.\n"
-000003c0: 2c0d 0a20 2020 2022 5c6e 222c 0d0a 2020  ,..    "\n",..  
-000003d0: 2020 2259 6f75 2073 686f 756c 6420 6861    "You should ha
-000003e0: 7665 2072 6563 6569 7665 6420 6120 636f  ve received a co
-000003f0: 7079 206f 6620 7468 6520 474e 5520 4765  py of the GNU Ge
-00000400: 6e65 7261 6c20 5075 626c 6963 204c 6963  neral Public Lic
-00000410: 656e 7365 2061 6c6f 6e67 2077 6974 6820  ense along with 
-00000420: 7468 6973 2070 726f 6772 616d 2e5c 6e22  this program.\n"
-00000430: 2c0d 0a20 2020 2022 4966 206e 6f74 2c20  ,..    "If not, 
-00000440: 7365 6520 6874 7470 733a 2f2f 7777 772e  see https://www.
-00000450: 676e 752e 6f72 672f 6c69 6365 6e73 6573  gnu.org/licenses
-00000460: 2f2e 5c6e 222c 0d0a 2020 2020 225c 6e22  /.\n",..    "\n"
-00000470: 2c0d 0a20 2020 2022 3c64 6976 2073 7479  ,..    "<div sty
-00000480: 6c65 3d5c 226f 7665 7266 6c6f 772d 793a  le=\"overflow-y:
-00000490: 2061 7574 6f5c 223e 5c6e 222c 0d0a 2020   auto\">\n",..  
-000004a0: 2020 2220 203c 6831 3e54 6162 6c65 2064    "  <h1>Table d
-000004b0: 6573 206d 6174 69c3 a872 6573 3c2f 6831  es mati..res</h1
-000004c0: 3e5c 6e22 2c0d 0a20 2020 2022 2020 3c64  >\n",..    "  <d
-000004d0: 6976 2069 643d 5c22 746f 635c 223e 3c2f  iv id=\"toc\"></
-000004e0: 6469 763e 5c6e 222c 0d0a 2020 2020 223c  div>\n",..    "<
-000004f0: 2f64 6976 3e22 0d0a 2020 205d 0d0a 2020  /div>"..   ]..  
-00000500: 7d2c 0d0a 2020 7b0d 0a20 2020 2263 656c  },..  {..   "cel
-00000510: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
-00000520: 0d0a 2020 2022 6578 6563 7574 696f 6e5f  ..   "execution_
-00000530: 636f 756e 7422 3a20 6e75 6c6c 2c0d 0a20  count": null,.. 
-00000540: 2020 226d 6574 6164 6174 6122 3a20 7b7d    "metadata": {}
-00000550: 2c0d 0a20 2020 226f 7574 7075 7473 223a  ,..   "outputs":
-00000560: 205b 5d2c 0d0a 2020 2022 736f 7572 6365   [],..   "source
-00000570: 223a 205b 0d0a 2020 2020 2225 256a 6176  ": [..    "%%jav
-00000580: 6173 6372 6970 745c 6e22 2c0d 0a20 2020  ascript\n",..   
-00000590: 2022 242e 6765 7453 6372 6970 7428 2769   "$.getScript('i
-000005a0: 7079 7468 6f6e 5f6e 6f74 6562 6f6f 6b5f  python_notebook_
-000005b0: 746f 632e 6a73 2729 220d 0a20 2020 5d0d  toc.js')"..   ].
-000005c0: 0a20 207d 2c0d 0a20 207b 0d0a 2020 2022  .  },..  {..   "
-000005d0: 6365 6c6c 5f74 7970 6522 3a20 226d 6172  cell_type": "mar
-000005e0: 6b64 6f77 6e22 2c0d 0a20 2020 226d 6574  kdown",..   "met
-000005f0: 6164 6174 6122 3a20 7b7d 2c0d 0a20 2020  adata": {},..   
-00000600: 2273 6f75 7263 6522 3a20 5b0d 0a20 2020  "source": [..   
-00000610: 2022 2320 5465 7374 7320 6465 2073 656e   "# Tests de sen
-00000620: 7369 6269 6c69 74c3 a920 6465 204d 4344  sibilit.. de MCD
-00000630: 532e 6578 6520 c3a0 2064 6976 6572 7365  S.exe .. diverse
-00000640: 7320 6368 6f73 6573 220d 0a20 2020 5d0d  s choses"..   ].
-00000650: 0a20 207d 2c0d 0a20 207b 0d0a 2020 2022  .  },..  {..   "
-00000660: 6365 6c6c 5f74 7970 6522 3a20 2263 6f64  cell_type": "cod
-00000670: 6522 2c0d 0a20 2020 2265 7865 6375 7469  e",..   "executi
-00000680: 6f6e 5f63 6f75 6e74 223a 206e 756c 6c2c  on_count": null,
-00000690: 0d0a 2020 2022 6d65 7461 6461 7461 223a  ..   "metadata":
-000006a0: 207b 7d2c 0d0a 2020 2022 6f75 7470 7574   {},..   "output
-000006b0: 7322 3a20 5b5d 2c0d 0a20 2020 2273 6f75  s": [],..   "sou
-000006c0: 7263 6522 3a20 5b0d 0a20 2020 2022 696d  rce": [..    "im
-000006d0: 706f 7274 2073 7973 5c6e 222c 0d0a 2020  port sys\n",..  
-000006e0: 2020 2269 6d70 6f72 7420 6f73 5c6e 222c    "import os\n",
-000006f0: 0d0a 2020 2020 2269 6d70 6f72 7420 7061  ..    "import pa
-00000700: 7468 6c69 6220 6173 2070 6c5c 6e22 2c0d  thlib as pl\n",.
-00000710: 0a20 2020 2022 6672 6f6d 2070 6163 6b61  .    "from packa
-00000720: 6769 6e67 2069 6d70 6f72 7420 7665 7273  ging import vers
-00000730: 696f 6e5c 6e22 2c0d 0a20 2020 2022 5c6e  ion\n",..    "\n
-00000740: 222c 0d0a 2020 2020 2269 6d70 6f72 7420  ",..    "import 
-00000750: 7265 5c6e 222c 0d0a 2020 2020 225c 6e22  re\n",..    "\n"
-00000760: 2c0d 0a20 2020 2022 6672 6f6d 2063 6f6c  ,..    "from col
-00000770: 6c65 6374 696f 6e73 2069 6d70 6f72 7420  lections import 
-00000780: 4f72 6465 7265 6444 6963 7420 6173 206f  OrderedDict as o
-00000790: 6469 6374 5c6e 222c 0d0a 2020 2020 225c  dict\n",..    "\
-000007a0: 6e22 2c0d 0a20 2020 2022 696d 706f 7274  n",..    "import
-000007b0: 206d 6174 685c 6e22 2c0d 0a20 2020 2022   math\n",..    "
-000007c0: 696d 706f 7274 206e 756d 7079 2061 7320  import numpy as 
-000007d0: 6e70 5c6e 222c 0d0a 2020 2020 2269 6d70  np\n",..    "imp
-000007e0: 6f72 7420 7061 6e64 6173 2061 7320 7064  ort pandas as pd
-000007f0: 220d 0a20 2020 5d0d 0a20 207d 2c0d 0a20  "..   ]..  },.. 
-00000800: 207b 0d0a 2020 2022 6365 6c6c 5f74 7970   {..   "cell_typ
-00000810: 6522 3a20 2263 6f64 6522 2c0d 0a20 2020  e": "code",..   
-00000820: 2265 7865 6375 7469 6f6e 5f63 6f75 6e74  "execution_count
-00000830: 223a 206e 756c 6c2c 0d0a 2020 2022 6d65  ": null,..   "me
-00000840: 7461 6461 7461 223a 207b 7d2c 0d0a 2020  tadata": {},..  
-00000850: 2022 6f75 7470 7574 7322 3a20 5b5d 2c0d   "outputs": [],.
-00000860: 0a20 2020 2273 6f75 7263 6522 3a20 5b0d  .   "source": [.
-00000870: 0a20 2020 2022 7379 732e 7061 7468 2e69  .    "sys.path.i
-00000880: 6e73 6572 7428 302c 2027 2e2e 2729 220d  nsert(0, '..')".
-00000890: 0a20 2020 5d0d 0a20 207d 2c0d 0a20 207b  .   ]..  },..  {
-000008a0: 0d0a 2020 2022 6365 6c6c 5f74 7970 6522  ..   "cell_type"
-000008b0: 3a20 2263 6f64 6522 2c0d 0a20 2020 2265  : "code",..   "e
-000008c0: 7865 6375 7469 6f6e 5f63 6f75 6e74 223a  xecution_count":
-000008d0: 206e 756c 6c2c 0d0a 2020 2022 6d65 7461   null,..   "meta
-000008e0: 6461 7461 223a 207b 7d2c 0d0a 2020 2022  data": {},..   "
-000008f0: 6f75 7470 7574 7322 3a20 5b5d 2c0d 0a20  outputs": [],.. 
-00000900: 2020 2273 6f75 7263 6522 3a20 5b0d 0a20    "source": [.. 
-00000910: 2020 2022 696d 706f 7274 2070 7961 7564     "import pyaud
-00000920: 6973 616d 2061 7320 6164 735c 6e22 2c0d  isam as ads\n",.
-00000930: 0a20 2020 2022 5c6e 222c 0d0a 2020 2020  .    "\n",..    
-00000940: 2261 6473 2e72 756e 7469 6d65 220d 0a20  "ads.runtime".. 
-00000950: 2020 5d0d 0a20 207d 2c0d 0a20 207b 0d0a    ]..  },..  {..
-00000960: 2020 2022 6365 6c6c 5f74 7970 6522 3a20     "cell_type": 
-00000970: 2263 6f64 6522 2c0d 0a20 2020 2265 7865  "code",..   "exe
-00000980: 6375 7469 6f6e 5f63 6f75 6e74 223a 206e  cution_count": n
-00000990: 756c 6c2c 0d0a 2020 2022 6d65 7461 6461  ull,..   "metada
-000009a0: 7461 223a 207b 7d2c 0d0a 2020 2022 6f75  ta": {},..   "ou
-000009b0: 7470 7574 7322 3a20 5b5d 2c0d 0a20 2020  tputs": [],..   
-000009c0: 2273 6f75 7263 6522 3a20 5b0d 0a20 2020  "source": [..   
-000009d0: 2022 2320 4372 6561 7465 2074 656d 706f   "# Create tempo
-000009e0: 7261 7279 2064 6972 6563 746f 7279 2069  rary directory i
-000009f0: 6620 6e6f 7420 7965 7420 646f 6e65 2e5c  f not yet done.\
-00000a00: 6e22 2c0d 0a20 2020 2022 746d 7044 6972  n",..    "tmpDir
-00000a10: 203d 2070 6c2e 5061 7468 2827 746d 7027   = pl.Path('tmp'
-00000a20: 295c 6e22 2c0d 0a20 2020 2022 746d 7044  )\n",..    "tmpD
-00000a30: 6972 2e6d 6b64 6972 2865 7869 7374 5f6f  ir.mkdir(exist_o
-00000a40: 6b3d 5472 7565 2922 0d0a 2020 205d 0d0a  k=True)"..   ]..
-00000a50: 2020 7d2c 0d0a 2020 7b0d 0a20 2020 2263    },..  {..   "c
-00000a60: 656c 6c5f 7479 7065 223a 2022 636f 6465  ell_type": "code
-00000a70: 222c 0d0a 2020 2022 6578 6563 7574 696f  ",..   "executio
-00000a80: 6e5f 636f 756e 7422 3a20 6e75 6c6c 2c0d  n_count": null,.
-00000a90: 0a20 2020 226d 6574 6164 6174 6122 3a20  .   "metadata": 
-00000aa0: 7b7d 2c0d 0a20 2020 226f 7574 7075 7473  {},..   "outputs
-00000ab0: 223a 205b 5d2c 0d0a 2020 2022 736f 7572  ": [],..   "sour
-00000ac0: 6365 223a 205b 0d0a 2020 2020 2223 2041  ce": [..    "# A
-00000ad0: 6374 6976 6174 6520 5761 726e 696e 6773  ctivate Warnings
-00000ae0: 2061 7320 4578 6365 7074 696f 6e5c 6e22   as Exception\n"
-00000af0: 2c0d 0a20 2020 2022 2369 6d70 6f72 7420  ,..    "#import 
-00000b00: 7761 726e 696e 6773 5c6e 222c 0d0a 2020  warnings\n",..  
-00000b10: 2020 2223 7761 726e 696e 6773 2e66 696c    "#warnings.fil
-00000b20: 7465 7277 6172 6e69 6e67 7328 2765 7272  terwarnings('err
-00000b30: 6f72 2729 220d 0a20 2020 5d0d 0a20 207d  or')"..   ]..  }
-00000b40: 2c0d 0a20 207b 0d0a 2020 2022 6365 6c6c  ,..  {..   "cell
-00000b50: 5f74 7970 6522 3a20 226d 6172 6b64 6f77  _type": "markdow
-00000b60: 6e22 2c0d 0a20 2020 226d 6574 6164 6174  n",..   "metadat
-00000b70: 6122 3a20 7b7d 2c0d 0a20 2020 2273 6f75  a": {},..   "sou
-00000b80: 7263 6522 3a20 5b0d 0a20 2020 2022 2320  rce": [..    "# 
-00000b90: 436f 6d6d 756e 7322 0d0a 2020 205d 0d0a  Communs"..   ]..
-00000ba0: 2020 7d2c 0d0a 2020 7b0d 0a20 2020 2263    },..  {..   "c
-00000bb0: 656c 6c5f 7479 7065 223a 2022 636f 6465  ell_type": "code
-00000bc0: 222c 0d0a 2020 2022 6578 6563 7574 696f  ",..   "executio
-00000bd0: 6e5f 636f 756e 7422 3a20 6e75 6c6c 2c0d  n_count": null,.
-00000be0: 0a20 2020 226d 6574 6164 6174 6122 3a20  .   "metadata": 
-00000bf0: 7b7d 2c0d 0a20 2020 226f 7574 7075 7473  {},..   "outputs
-00000c00: 223a 205b 5d2c 0d0a 2020 2022 736f 7572  ": [],..   "sour
-00000c10: 6365 223a 205b 0d0a 2020 2020 2223 2041  ce": [..    "# A
-00000c20: 6374 7561 6c20 2f20 7265 6665 7265 6e63  ctual / referenc
-00000c30: 6520 636c 6f73 656e 6573 7320 6d65 6173  e closeness meas
-00000c40: 7572 6520 3a20 2d72 6f75 6e64 286c 6f67  ure : -round(log
-00000c50: 3130 2828 6163 7475 616c 202d 2072 6566  10((actual - ref
-00000c60: 6572 656e 6365 2920 2f20 6d61 7828 6162  erence) / max(ab
-00000c70: 7328 6163 7475 616c 292c 2061 6273 2872  s(actual), abs(r
-00000c80: 6566 6572 656e 6365 2929 292c 2031 295c  eference))), 1)\
-00000c90: 6e22 2c0d 0a20 2020 2022 2320 3d20 436f  n",..    "# = Co
-00000ca0: 6d70 7574 6520 7468 6520 6f72 6465 7273  mpute the orders
-00000cb0: 206f 6620 6d61 676e 6974 7564 6520 7468   of magnitude th
-00000cc0: 6174 2073 6570 6172 6174 6520 7468 6520  at separate the 
-00000cd0: 6469 6666 6572 656e 6365 2066 726f 6d20  difference from 
-00000ce0: 7468 6520 6d61 782e 206f 6620 7468 6520  the max. of the 
-00000cf0: 7477 6f20 7661 6c75 6573 5c6e 222c 0d0a  two values\n",..
-00000d00: 2020 2020 2264 6566 2063 6c6f 7365 6e65      "def closene
-00000d10: 7373 2873 5265 6641 6374 293a 5c6e 222c  ss(sRefAct):\n",
-00000d20: 0d0a 2020 2020 2220 2020 205c 6e22 2c0d  ..    "    \n",.
-00000d30: 0a20 2020 2022 2020 2020 782c 2079 203d  .    "    x, y =
-00000d40: 2073 5265 6641 6374 2e74 6f5f 6c69 7374   sRefAct.to_list
-00000d50: 2829 5c6e 222c 0d0a 2020 2020 2220 2020  ()\n",..    "   
-00000d60: 205c 6e22 2c0d 0a20 2020 2022 2020 2020   \n",..    "    
-00000d70: 2320 5370 6563 6961 6c20 6361 7365 7320  # Special cases 
-00000d80: 7769 7468 2031 204e 614e 2c20 6f72 2031  with 1 NaN, or 1
-00000d90: 206f 7220 6d6f 7265 2069 6e66 203d 3e20   or more inf => 
-00000da0: 616c 6c20 6469 6666 6572 656e 745c 6e22  all different\n"
-00000db0: 2c0d 0a20 2020 2022 2020 2020 6966 206e  ,..    "    if n
-00000dc0: 702e 6973 6e61 6e28 7829 3a5c 6e22 2c0d  p.isnan(x):\n",.
-00000dd0: 0a20 2020 2022 2020 2020 2020 2020 6966  .    "        if
-00000de0: 206e 6f74 206e 702e 6973 6e61 6e28 7929   not np.isnan(y)
-00000df0: 3a5c 6e22 2c0d 0a20 2020 2022 2020 2020  :\n",..    "    
-00000e00: 2020 2020 2020 2020 7265 7475 726e 2030          return 0
-00000e10: 2023 2041 6c6c 2064 6966 6665 7265 6e74   # All different
-00000e20: 5c6e 222c 0d0a 2020 2020 2220 2020 2065  \n",..    "    e
-00000e30: 6c69 6620 6e70 2e69 736e 616e 2879 293a  lif np.isnan(y):
-00000e40: 5c6e 222c 0d0a 2020 2020 2220 2020 2020  \n",..    "     
-00000e50: 2020 2072 6574 7572 6e20 3020 2320 416c     return 0 # Al
-00000e60: 6c20 6469 6666 6572 656e 745c 6e22 2c0d  l different\n",.
-00000e70: 0a20 2020 2022 2020 2020 5c6e 222c 0d0a  .    "    \n",..
-00000e80: 2020 2020 2220 2020 2069 6620 6e70 2e69      "    if np.i
-00000e90: 7369 6e66 2878 2920 6f72 206e 702e 6973  sinf(x) or np.is
-00000ea0: 696e 6628 7929 3a5c 6e22 2c0d 0a20 2020  inf(y):\n",..   
-00000eb0: 2022 2020 2020 2020 2020 7265 7475 726e   "        return
-00000ec0: 2030 2023 2041 6c6c 2064 6966 6665 7265   0 # All differe
-00000ed0: 6e74 5c6e 222c 0d0a 2020 2020 2220 2020  nt\n",..    "   
-00000ee0: 205c 6e22 2c0d 0a20 2020 2022 2020 2020   \n",..    "    
-00000ef0: 2320 4e6f 726d 616c 2063 6173 655c 6e22  # Normal case\n"
-00000f00: 2c0d 0a20 2020 2022 2020 2020 6320 3d20  ,..    "    c = 
-00000f10: 6162 7328 7820 2d20 7929 5c6e 222c 0d0a  abs(x - y)\n",..
-00000f20: 2020 2020 2220 2020 2069 6620 6e6f 7420      "    if not 
-00000f30: 6e70 2e69 736e 616e 2863 2920 616e 6420  np.isnan(c) and 
-00000f40: 6320 213d 2030 3a5c 6e22 2c0d 0a20 2020  c != 0:\n",..   
-00000f50: 2022 2020 2020 2020 2020 6320 3d20 6320   "        c = c 
-00000f60: 2f20 6d61 7828 6162 7328 7829 2c20 6162  / max(abs(x), ab
-00000f70: 7328 7929 295c 6e22 2c0d 0a20 2020 2022  s(y))\n",..    "
-00000f80: 2020 2020 5c6e 222c 0d0a 2020 2020 2220      \n",..    " 
-00000f90: 2020 2072 6574 7572 6e20 726f 756e 6428     return round(
-00000fa0: 2d6e 702e 6c6f 6731 3028 6329 2c20 3129  -np.log10(c), 1)
-00000fb0: 220d 0a20 2020 5d0d 0a20 207d 2c0d 0a20  "..   ]..  },.. 
-00000fc0: 207b 0d0a 2020 2022 6365 6c6c 5f74 7970   {..   "cell_typ
-00000fd0: 6522 3a20 2263 6f64 6522 2c0d 0a20 2020  e": "code",..   
-00000fe0: 2265 7865 6375 7469 6f6e 5f63 6f75 6e74  "execution_count
-00000ff0: 223a 206e 756c 6c2c 0d0a 2020 2022 6d65  ": null,..   "me
-00001000: 7461 6461 7461 223a 207b 7d2c 0d0a 2020  tadata": {},..  
-00001010: 2022 6f75 7470 7574 7322 3a20 5b5d 2c0d   "outputs": [],.
-00001020: 0a20 2020 2273 6f75 7263 6522 3a20 5b0d  .   "source": [.
-00001030: 0a20 2020 2022 2320 436f 6d70 6172 6520  .    "# Compare 
-00001040: 6d75 6c74 6970 6c65 2076 6172 6961 6e74  multiple variant
-00001050: 7320 746f 206f 6e65 206f 6620 7468 656d  s to one of them
-00001060: 2c20 7573 696e 6720 636c 6f73 656e 6573  , using closenes
-00001070: 7320 666e 2061 626f 7665 5c6e 222c 0d0a  s fn above\n",..
-00001080: 2020 2020 2223 2041 696d 6564 2061 7420      "# Aimed at 
-00001090: 6265 696e 6720 7573 6564 2061 7320 7468  being used as th
-000010a0: 6520 666e 2074 6f20 6772 6f75 7062 7928  e fn to groupby(
-000010b0: 2e2e 2e29 2e61 7070 6c79 2866 6e29 2028  ...).apply(fn) (
-000010c0: 7365 6520 6578 616d 706c 6573 2062 656c  see examples bel
-000010d0: 6f77 292e 5c6e 222c 0d0a 2020 2020 2264  ow).\n",..    "d
-000010e0: 6566 2076 6172 6961 6e74 436c 6f73 656e  ef variantClosen
-000010f0: 6573 7328 6466 672c 2069 6e64 6578 436f  ess(dfg, indexCo
-00001100: 6c73 2c20 7265 664c 6162 656c 496e 643d  ls, refLabelInd=
-00001110: 3029 3a5c 6e22 2c0d 0a20 2020 2022 2020  0):\n",..    "  
-00001120: 2020 5c6e 222c 0d0a 2020 2020 2220 2020    \n",..    "   
-00001130: 2023 2042 6163 6b75 7020 616e 6420 7468   # Backup and th
-00001140: 6520 6472 6f70 205c 2269 6e64 6578 696e  e drop \"indexin
-00001150: 675c 2220 636f 6c75 6d6e 7320 3a20 7765  g\" columns : we
-00001160: 2064 6f6e 2774 2063 6865 636b 2063 6c6f   don't check clo
-00001170: 7365 6e65 7373 206f 6e20 7468 656d 5c6e  seness on them\n
-00001180: 222c 0d0a 2020 2020 2220 2020 2064 6667  ",..    "    dfg
-00001190: 6920 3d20 6466 675b 696e 6465 7843 6f6c  i = dfg[indexCol
-000011a0: 735d 2e63 6f70 7928 295c 6e22 2c0d 0a20  s].copy()\n",.. 
-000011b0: 2020 2022 2020 2020 6466 672e 6472 6f70     "    dfg.drop
-000011c0: 2863 6f6c 756d 6e73 3d64 6667 692e 636f  (columns=dfgi.co
-000011d0: 6c75 6d6e 732e 746f 5f6c 6973 7428 292c  lumns.to_list(),
-000011e0: 2069 6e70 6c61 6365 3d54 7275 6529 5c6e   inplace=True)\n
-000011f0: 222c 0d0a 2020 2020 2220 2020 205c 6e22  ",..    "    \n"
-00001200: 2c0d 0a20 2020 2022 2020 2020 2320 436f  ,..    "    # Co
-00001210: 6d70 7574 6520 636c 6f73 656e 6573 7320  mpute closeness 
-00001220: 6f66 2065 6163 6820 726f 7720 746f 2074  of each row to t
-00001230: 6865 2031 7374 206f 6e65 2e5c 6e22 2c0d  he 1st one.\n",.
-00001240: 0a20 2020 2022 2020 2020 6466 6764 203d  .    "    dfgd =
-00001250: 2070 642e 4461 7461 4672 616d 6528 636f   pd.DataFrame(co
-00001260: 6c75 6d6e 733d 6466 672e 636f 6c75 6d6e  lumns=dfg.column
-00001270: 7329 5c6e 222c 0d0a 2020 2020 2220 2020  s)\n",..    "   
-00001280: 2072 6566 4c62 6c20 3d20 6466 672e 696e   refLbl = dfg.in
-00001290: 6465 785b 305d 2023 204c 6162 656c 206f  dex[0] # Label o
-000012a0: 6620 7468 6520 6669 7273 7420 726f 772e  f the first row.
-000012b0: 5c6e 222c 0d0a 2020 2020 2220 2020 2066  \n",..    "    f
-000012c0: 6f72 206c 626c 2069 6e20 6466 672e 696e  or lbl in dfg.in
-000012d0: 6465 783a 5c6e 222c 0d0a 2020 2020 2220  dex:\n",..    " 
-000012e0: 2020 2020 2020 2074 7279 3a5c 6e22 2c0d         try:\n",.
-000012f0: 0a20 2020 2022 2020 2020 2020 2020 2020  .    "          
-00001300: 2020 6466 6764 2e6c 6f63 5b6c 626c 5d20    dfgd.loc[lbl] 
-00001310: 3d20 6466 672e 6c6f 635b 5b72 6566 4c62  = dfg.loc[[refLb
-00001320: 6c2c 206c 626c 5d5d 2e61 7070 6c79 2863  l, lbl]].apply(c
-00001330: 6c6f 7365 6e65 7373 295c 6e22 2c0d 0a20  loseness)\n",.. 
-00001340: 2020 2022 2020 2020 2020 2020 6578 6365     "        exce
-00001350: 7074 3a5c 6e22 2c0d 0a20 2020 2022 2020  pt:\n",..    "  
-00001360: 2020 2020 2020 2020 2020 7072 696e 7428            print(
-00001370: 6c62 6c2c 2072 6566 4c62 6c29 5c6e 222c  lbl, refLbl)\n",
-00001380: 0d0a 2020 2020 2220 2020 2020 2020 2020  ..    "         
-00001390: 2020 2070 7269 6e74 2864 6667 2e6c 6f63     print(dfg.loc
-000013a0: 5b5b 7265 664c 626c 2c20 6c62 6c5d 5d29  [[refLbl, lbl]])
-000013b0: 5c6e 222c 0d0a 2020 2020 2220 2020 2020  \n",..    "     
-000013c0: 2020 2020 2020 2072 6169 7365 5c6e 222c         raise\n",
-000013d0: 0d0a 2020 2020 2220 2020 2020 2020 205c  ..    "        \
-000013e0: 6e22 2c0d 0a20 2020 2022 2020 2020 2320  n",..    "    # 
-000013f0: 5265 7374 6f72 6520 5c22 696e 6465 7869  Restore \"indexi
-00001400: 6e67 5c22 2063 6f6c 756d 6e73 203a 2064  ng\" columns : d
-00001410: 6f6e 652e 5c6e 222c 0d0a 2020 2020 2220  one.\n",..    " 
-00001420: 2020 2072 6574 7572 6e20 6466 6769 2e6a     return dfgi.j
-00001430: 6f69 6e28 6466 6764 2922 0d0a 2020 205d  oin(dfgd)"..   ]
-00001440: 0d0a 2020 7d2c 0d0a 2020 7b0d 0a20 2020  ..  },..  {..   
-00001450: 2263 656c 6c5f 7479 7065 223a 2022 6d61  "cell_type": "ma
-00001460: 726b 646f 776e 222c 0d0a 2020 2022 6d65  rkdown",..   "me
-00001470: 7461 6461 7461 223a 207b 7d2c 0d0a 2020  tadata": {},..  
-00001480: 2022 736f 7572 6365 223a 205b 0d0a 2020   "source": [..  
-00001490: 2020 2223 2053 656e 7369 6269 6c69 74c3    "# Sensibilit.
-000014a0: a920 6465 204d 4344 532e 6578 6520 c3a0  . de MCDS.exe ..
-000014b0: 206c 276f 7264 7265 2064 6573 2064 6f6e   l'ordre des don
-000014c0: 6ec3 a965 7322 0d0a 2020 205d 0d0a 2020  n..es"..   ]..  
-000014d0: 7d2c 0d0a 2020 7b0d 0a20 2020 2263 656c  },..  {..   "cel
-000014e0: 6c5f 7479 7065 223a 2022 6d61 726b 646f  l_type": "markdo
-000014f0: 776e 222c 0d0a 2020 2022 6d65 7461 6461  wn",..   "metada
-00001500: 7461 223a 207b 7d2c 0d0a 2020 2022 736f  ta": {},..   "so
-00001510: 7572 6365 223a 205b 0d0a 2020 2020 2223  urce": [..    "#
-00001520: 2320 612e 2043 6f6e 7374 7275 6374 696f  # a. Constructio
-00001530: 6e20 6465 7320 7661 7269 616e 7465 7320  n des variantes 
-00001540: 6427 616e 616c 7973 6522 0d0a 2020 205d  d'analyse"..   ]
-00001550: 0d0a 2020 7d2c 0d0a 2020 7b0d 0a20 2020  ..  },..  {..   
-00001560: 2263 656c 6c5f 7479 7065 223a 2022 636f  "cell_type": "co
-00001570: 6465 222c 0d0a 2020 2022 6578 6563 7574  de",..   "execut
-00001580: 696f 6e5f 636f 756e 7422 3a20 6e75 6c6c  ion_count": null
-00001590: 2c0d 0a20 2020 226d 6574 6164 6174 6122  ,..   "metadata"
-000015a0: 3a20 7b0d 0a20 2020 2022 7363 726f 6c6c  : {..    "scroll
-000015b0: 6564 223a 2074 7275 650d 0a20 2020 7d2c  ed": true..   },
-000015c0: 0d0a 2020 2022 6f75 7470 7574 7322 3a20  ..   "outputs": 
-000015d0: 5b5d 2c0d 0a20 2020 2273 6f75 7263 6522  [],..   "source"
-000015e0: 3a20 5b0d 0a20 2020 2022 2320 4765 6e65  : [..    "# Gene
-000015f0: 7261 7465 2074 6573 7420 6361 7365 7320  rate test cases 
-00001600: 6465 6669 6e69 7469 6f6e 2063 6f64 6520  definition code 
-00001610: 6672 6f6d 2072 6566 6f75 7420 7265 7375  from refout resu
-00001620: 6c74 7320 6669 6c65 2028 646f 6e27 7420  lts file (don't 
-00001630: 6368 6561 7420 3a20 6f6e 6c79 2069 6e70  cheat : only inp
-00001640: 7574 2063 6f6c 756d 6e73 203a 2d29 5c6e  ut columns :-)\n
-00001650: 222c 0d0a 2020 2020 2263 6173 6549 6443  ",..    "caseIdC
-00001660: 6f6c 7320 3d20 5b27 5370 6563 6965 7327  ols = ['Species'
-00001670: 2c20 2753 616d 706c 6527 2c20 274d 6f64  , 'Sample', 'Mod
-00001680: 656c 272c 2027 4461 7461 4f72 6465 7227  el', 'DataOrder'
-00001690: 5d5c 6e22 2c0d 0a20 2020 2022 5c6e 222c  ]\n",..    "\n",
-000016a0: 0d0a 2020 2020 2264 6641 6e6c 7973 4361  ..    "dfAnlysCa
-000016b0: 7365 7320 3d20 7064 2e44 6174 6146 7261  ses = pd.DataFra
-000016c0: 6d65 2864 6174 613d 5b28 7370 6563 2c20  me(data=[(spec, 
-000016d0: 7361 6d70 2c20 6b65 7946 6e20 2b20 6164  samp, keyFn + ad
-000016e0: 6a53 6572 2c20 6461 7461 4f72 6429 205c  jSer, dataOrd) \
-000016f0: 5c5c 6e22 2c0d 0a20 2020 2022 2020 2020  \\n",..    "    
-00001700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001710: 2020 2020 2020 2020 2020 2020 2020 666f                fo
-00001720: 7220 7370 6563 2069 6e20 5b27 5359 4c41  r spec in ['SYLA
-00001730: 5452 272c 2027 5455 524d 4552 272c 2027  TR', 'TURMER', '
-00001740: 4c55 534d 4547 272c 2027 414c 4141 5256  LUSMEG', 'ALAARV
-00001750: 272c 2027 434f 4c50 414c 272c 5c6e 222c  ', 'COLPAL',\n",
-00001760: 0d0a 2020 2020 2220 2020 2020 2020 2020  ..    "         
-00001770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001790: 2020 2020 2020 2750 4859 434f 4c27 2c20        'PHYCOL', 
-000017a0: 2745 4d42 4349 5427 2c20 2745 4d42 4349  'EMBCIT', 'EMBCI
-000017b0: 5227 2c20 2741 4e54 5452 4927 2c20 274d  R', 'ANTTRI', 'M
-000017c0: 494c 4341 4c27 5d20 5c5c 5c6e 222c 0d0a  ILCAL'] \\\n",..
-000017d0: 2020 2020 2220 2020 2020 2020 2020 2020      "           
-000017e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000017f0: 2020 2020 2020 2066 6f72 2073 616d 7020         for samp 
-00001800: 696e 205b 2741 422d 3130 6d6e 2d74 7464  in ['AB-10mn-ttd
-00001810: 6563 275d 205c 5c5c 6e22 2c0d 0a20 2020  ec'] \\\n",..   
-00001820: 2022 2020 2020 2020 2020 2020 2020 2020   "              
-00001830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001840: 2020 2020 666f 7220 6b65 7946 6e20 696e      for keyFn in
-00001850: 205b 2748 4e6f 272c 2027 556e 6927 2c20   ['HNo', 'Uni', 
-00001860: 2748 617a 275d 205c 5c5c 6e22 2c0d 0a20  'Haz'] \\\n",.. 
-00001870: 2020 2022 2020 2020 2020 2020 2020 2020     "            
+00000000: 7b0a 2022 6365 6c6c 7322 3a20 5b0a 2020  {. "cells": [.  
+00000010: 7b0a 2020 2022 6365 6c6c 5f74 7970 6522  {.   "cell_type"
+00000020: 3a20 226d 6172 6b64 6f77 6e22 2c0a 2020  : "markdown",.  
+00000030: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
+00000040: 0a20 2020 2273 6f75 7263 6522 3a20 5b0a  .   "source": [.
+00000050: 2020 2020 223c 212d 2d20 5461 626c 6520      "<!-- Table 
+00000060: 6465 7320 6d61 7469 c3a8 7265 7320 6175  des mati..res au
+00000070: 746f 6d61 7469 7175 6520 2d2d 3e5c 6e22  tomatique -->\n"
+00000080: 2c0a 2020 2020 223c 6831 2063 6c61 7373  ,.    "<h1 class
+00000090: 3d27 746f 6349 676e 6f72 6527 3e53 656e  ='tocIgnore'>Sen
+000000a0: 7369 7469 7669 7479 2074 6573 7473 2028  sitivity tests (
+000000b0: 6f6c 6420 616e 6420 756e 6d61 696e 7461  old and unmainta
+000000c0: 696e 6564 293c 2f68 313e 5c6e 222c 0a20  ined)</h1>\n",. 
+000000d0: 2020 2022 5c6e 222c 0a20 2020 2022 2a2a     "\n",.    "**
+000000e0: 7079 6175 6469 7361 6d2a 2a3a 2041 7574  pyaudisam**: Aut
+000000f0: 6f6d 6174 696f 6e20 6f66 2044 6973 7461  omation of Dista
+00000100: 6e63 6520 5361 6d70 6c69 6e67 2061 6e61  nce Sampling ana
+00000110: 6c79 7365 7320 7769 7468 205b 4469 7374  lyses with [Dist
+00000120: 616e 6365 2073 6f66 7477 6172 655d 2868  ance software](h
+00000130: 7474 703a 2f2f 6469 7374 616e 6365 7361  ttp://distancesa
+00000140: 6d70 6c69 6e67 2e6f 7267 2f29 5c6e 222c  mpling.org/)\n",
+00000150: 0a20 2020 2022 5c6e 222c 0a20 2020 2022  .    "\n",.    "
+00000160: 436f 7079 7269 6768 7420 2843 2920 3230  Copyright (C) 20
+00000170: 3231 204a 6561 6e2d 5068 696c 6970 7065  21 Jean-Philippe
+00000180: 204d 6575 7265 745c 6e22 2c0a 2020 2020   Meuret\n",.    
+00000190: 225c 6e22 2c0a 2020 2020 2254 6869 7320  "\n",.    "This 
+000001a0: 7072 6f67 7261 6d20 6973 2066 7265 6520  program is free 
+000001b0: 736f 6674 7761 7265 3a20 796f 7520 6361  software: you ca
+000001c0: 6e20 7265 6469 7374 7269 6275 7465 2069  n redistribute i
+000001d0: 7420 616e 642f 6f72 206d 6f64 6966 7920  t and/or modify 
+000001e0: 6974 2075 6e64 6572 2074 6865 2074 6572  it under the ter
+000001f0: 6d73 5c6e 222c 0a20 2020 2022 6f66 2074  ms\n",.    "of t
+00000200: 6865 2047 4e55 2047 656e 6572 616c 2050  he GNU General P
+00000210: 7562 6c69 6320 4c69 6365 6e73 6520 6173  ublic License as
+00000220: 2070 7562 6c69 7368 6564 2062 7920 7468   published by th
+00000230: 6520 4672 6565 2053 6f66 7477 6172 6520  e Free Software 
+00000240: 466f 756e 6461 7469 6f6e 2c5c 6e22 2c0a  Foundation,\n",.
+00000250: 2020 2020 2265 6974 6865 7220 7665 7273      "either vers
+00000260: 696f 6e20 3320 6f66 2074 6865 204c 6963  ion 3 of the Lic
+00000270: 656e 7365 2c20 6f72 2028 6174 2079 6f75  ense, or (at you
+00000280: 7220 6f70 7469 6f6e 2920 616e 7920 6c61  r option) any la
+00000290: 7465 7220 7665 7273 696f 6e2e 5c6e 222c  ter version.\n",
+000002a0: 0a20 2020 2022 5c6e 222c 0a20 2020 2022  .    "\n",.    "
+000002b0: 5468 6973 2070 726f 6772 616d 2069 7320  This program is 
+000002c0: 6469 7374 7269 6275 7465 6420 696e 2074  distributed in t
+000002d0: 6865 2068 6f70 6520 7468 6174 2069 7420  he hope that it 
+000002e0: 7769 6c6c 2062 6520 7573 6566 756c 2c20  will be useful, 
+000002f0: 6275 7420 5749 5448 4f55 5420 414e 5920  but WITHOUT ANY 
+00000300: 5741 5252 414e 5459 3b5c 6e22 2c0a 2020  WARRANTY;\n",.  
+00000310: 2020 2277 6974 686f 7574 2065 7665 6e20    "without even 
+00000320: 7468 6520 696d 706c 6965 6420 7761 7272  the implied warr
+00000330: 616e 7479 206f 6620 4d45 5243 4841 4e54  anty of MERCHANT
+00000340: 4142 494c 4954 5920 6f72 2046 4954 4e45  ABILITY or FITNE
+00000350: 5353 2046 4f52 2041 2050 4152 5449 4355  SS FOR A PARTICU
+00000360: 4c41 5220 5055 5250 4f53 452e 5c6e 222c  LAR PURPOSE.\n",
+00000370: 0a20 2020 2022 5365 6520 7468 6520 474e  .    "See the GN
+00000380: 5520 4765 6e65 7261 6c20 5075 626c 6963  U General Public
+00000390: 204c 6963 656e 7365 2066 6f72 206d 6f72   License for mor
+000003a0: 6520 6465 7461 696c 732e 5c6e 222c 0a20  e details.\n",. 
+000003b0: 2020 2022 5c6e 222c 0a20 2020 2022 596f     "\n",.    "Yo
+000003c0: 7520 7368 6f75 6c64 2068 6176 6520 7265  u should have re
+000003d0: 6365 6976 6564 2061 2063 6f70 7920 6f66  ceived a copy of
+000003e0: 2074 6865 2047 4e55 2047 656e 6572 616c   the GNU General
+000003f0: 2050 7562 6c69 6320 4c69 6365 6e73 6520   Public License 
+00000400: 616c 6f6e 6720 7769 7468 2074 6869 7320  along with this 
+00000410: 7072 6f67 7261 6d2e 5c6e 222c 0a20 2020  program.\n",.   
+00000420: 2022 4966 206e 6f74 2c20 7365 6520 6874   "If not, see ht
+00000430: 7470 733a 2f2f 7777 772e 676e 752e 6f72  tps://www.gnu.or
+00000440: 672f 6c69 6365 6e73 6573 2f2e 5c6e 222c  g/licenses/.\n",
+00000450: 0a20 2020 2022 5c6e 222c 0a20 2020 2022  .    "\n",.    "
+00000460: 3c64 6976 2073 7479 6c65 3d5c 226f 7665  <div style=\"ove
+00000470: 7266 6c6f 772d 793a 2061 7574 6f5c 223e  rflow-y: auto\">
+00000480: 5c6e 222c 0a20 2020 2022 2020 3c68 313e  \n",.    "  <h1>
+00000490: 5461 626c 6520 6465 7320 6d61 7469 c3a8  Table des mati..
+000004a0: 7265 733c 2f68 313e 5c6e 222c 0a20 2020  res</h1>\n",.   
+000004b0: 2022 2020 3c64 6976 2069 643d 5c22 746f   "  <div id=\"to
+000004c0: 635c 223e 3c2f 6469 763e 5c6e 222c 0a20  c\"></div>\n",. 
+000004d0: 2020 2022 3c2f 6469 763e 220a 2020 205d     "</div>".   ]
+000004e0: 0a20 207d 2c0a 2020 7b0a 2020 2022 6365  .  },.  {.   "ce
+000004f0: 6c6c 5f74 7970 6522 3a20 2263 6f64 6522  ll_type": "code"
+00000500: 2c0a 2020 2022 6578 6563 7574 696f 6e5f  ,.   "execution_
+00000510: 636f 756e 7422 3a20 6e75 6c6c 2c0a 2020  count": null,.  
+00000520: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
+00000530: 0a20 2020 226f 7574 7075 7473 223a 205b  .   "outputs": [
+00000540: 5d2c 0a20 2020 2273 6f75 7263 6522 3a20  ],.   "source": 
+00000550: 5b0a 2020 2020 2225 256a 6176 6173 6372  [.    "%%javascr
+00000560: 6970 745c 6e22 2c0a 2020 2020 2224 2e67  ipt\n",.    "$.g
+00000570: 6574 5363 7269 7074 2827 6970 7974 686f  etScript('ipytho
+00000580: 6e5f 6e6f 7465 626f 6f6b 5f74 6f63 2e6a  n_notebook_toc.j
+00000590: 7327 2922 0a20 2020 5d0a 2020 7d2c 0a20  s')".   ].  },. 
+000005a0: 207b 0a20 2020 2263 656c 6c5f 7479 7065   {.   "cell_type
+000005b0: 223a 2022 6d61 726b 646f 776e 222c 0a20  ": "markdown",. 
+000005c0: 2020 226d 6574 6164 6174 6122 3a20 7b7d    "metadata": {}
+000005d0: 2c0a 2020 2022 736f 7572 6365 223a 205b  ,.   "source": [
+000005e0: 0a20 2020 2022 2320 5465 7374 7320 6465  .    "# Tests de
+000005f0: 2073 656e 7369 6269 6c69 74c3 a920 6465   sensibilit.. de
+00000600: 204d 4344 532e 6578 6520 c3a0 2064 6976   MCDS.exe .. div
+00000610: 6572 7365 7320 6368 6f73 6573 220a 2020  erses choses".  
+00000620: 205d 0a20 207d 2c0a 2020 7b0a 2020 2022   ].  },.  {.   "
+00000630: 6365 6c6c 5f74 7970 6522 3a20 2263 6f64  cell_type": "cod
+00000640: 6522 2c0a 2020 2022 6578 6563 7574 696f  e",.   "executio
+00000650: 6e5f 636f 756e 7422 3a20 6e75 6c6c 2c0a  n_count": null,.
+00000660: 2020 2022 6d65 7461 6461 7461 223a 207b     "metadata": {
+00000670: 7d2c 0a20 2020 226f 7574 7075 7473 223a  },.   "outputs":
+00000680: 205b 5d2c 0a20 2020 2273 6f75 7263 6522   [],.   "source"
+00000690: 3a20 5b0a 2020 2020 2269 6d70 6f72 7420  : [.    "import 
+000006a0: 7379 735c 6e22 2c0a 2020 2020 2269 6d70  sys\n",.    "imp
+000006b0: 6f72 7420 6f73 5c6e 222c 0a20 2020 2022  ort os\n",.    "
+000006c0: 696d 706f 7274 2070 6174 686c 6962 2061  import pathlib a
+000006d0: 7320 706c 5c6e 222c 0a20 2020 2022 6672  s pl\n",.    "fr
+000006e0: 6f6d 2070 6163 6b61 6769 6e67 2069 6d70  om packaging imp
+000006f0: 6f72 7420 7665 7273 696f 6e5c 6e22 2c0a  ort version\n",.
+00000700: 2020 2020 225c 6e22 2c0a 2020 2020 2269      "\n",.    "i
+00000710: 6d70 6f72 7420 7265 5c6e 222c 0a20 2020  mport re\n",.   
+00000720: 2022 5c6e 222c 0a20 2020 2022 6672 6f6d   "\n",.    "from
+00000730: 2063 6f6c 6c65 6374 696f 6e73 2069 6d70   collections imp
+00000740: 6f72 7420 4f72 6465 7265 6444 6963 7420  ort OrderedDict 
+00000750: 6173 206f 6469 6374 5c6e 222c 0a20 2020  as odict\n",.   
+00000760: 2022 5c6e 222c 0a20 2020 2022 696d 706f   "\n",.    "impo
+00000770: 7274 206d 6174 685c 6e22 2c0a 2020 2020  rt math\n",.    
+00000780: 2269 6d70 6f72 7420 6e75 6d70 7920 6173  "import numpy as
+00000790: 206e 705c 6e22 2c0a 2020 2020 2269 6d70   np\n",.    "imp
+000007a0: 6f72 7420 7061 6e64 6173 2061 7320 7064  ort pandas as pd
+000007b0: 220a 2020 205d 0a20 207d 2c0a 2020 7b0a  ".   ].  },.  {.
+000007c0: 2020 2022 6365 6c6c 5f74 7970 6522 3a20     "cell_type": 
+000007d0: 2263 6f64 6522 2c0a 2020 2022 6578 6563  "code",.   "exec
+000007e0: 7574 696f 6e5f 636f 756e 7422 3a20 6e75  ution_count": nu
+000007f0: 6c6c 2c0a 2020 2022 6d65 7461 6461 7461  ll,.   "metadata
+00000800: 223a 207b 7d2c 0a20 2020 226f 7574 7075  ": {},.   "outpu
+00000810: 7473 223a 205b 5d2c 0a20 2020 2273 6f75  ts": [],.   "sou
+00000820: 7263 6522 3a20 5b0a 2020 2020 2273 7973  rce": [.    "sys
+00000830: 2e70 6174 682e 696e 7365 7274 2830 2c20  .path.insert(0, 
+00000840: 272e 2e27 2922 0a20 2020 5d0a 2020 7d2c  '..')".   ].  },
+00000850: 0a20 207b 0a20 2020 2263 656c 6c5f 7479  .  {.   "cell_ty
+00000860: 7065 223a 2022 636f 6465 222c 0a20 2020  pe": "code",.   
+00000870: 2265 7865 6375 7469 6f6e 5f63 6f75 6e74  "execution_count
+00000880: 223a 206e 756c 6c2c 0a20 2020 226d 6574  ": null,.   "met
+00000890: 6164 6174 6122 3a20 7b7d 2c0a 2020 2022  adata": {},.   "
+000008a0: 6f75 7470 7574 7322 3a20 5b5d 2c0a 2020  outputs": [],.  
+000008b0: 2022 736f 7572 6365 223a 205b 0a20 2020   "source": [.   
+000008c0: 2022 696d 706f 7274 2070 7961 7564 6973   "import pyaudis
+000008d0: 616d 2061 7320 6164 735c 6e22 2c0a 2020  am as ads\n",.  
+000008e0: 2020 225c 6e22 2c0a 2020 2020 2261 6473    "\n",.    "ads
+000008f0: 2e72 756e 7469 6d65 220a 2020 205d 0a20  .runtime".   ]. 
+00000900: 207d 2c0a 2020 7b0a 2020 2022 6365 6c6c   },.  {.   "cell
+00000910: 5f74 7970 6522 3a20 2263 6f64 6522 2c0a  _type": "code",.
+00000920: 2020 2022 6578 6563 7574 696f 6e5f 636f     "execution_co
+00000930: 756e 7422 3a20 6e75 6c6c 2c0a 2020 2022  unt": null,.   "
+00000940: 6d65 7461 6461 7461 223a 207b 7d2c 0a20  metadata": {},. 
+00000950: 2020 226f 7574 7075 7473 223a 205b 5d2c    "outputs": [],
+00000960: 0a20 2020 2273 6f75 7263 6522 3a20 5b0a  .   "source": [.
+00000970: 2020 2020 2223 2043 7265 6174 6520 7465      "# Create te
+00000980: 6d70 6f72 6172 7920 6469 7265 6374 6f72  mporary director
+00000990: 7920 6966 206e 6f74 2079 6574 2064 6f6e  y if not yet don
+000009a0: 652e 5c6e 222c 0a20 2020 2022 746d 7044  e.\n",.    "tmpD
+000009b0: 6972 203d 2070 6c2e 5061 7468 2827 746d  ir = pl.Path('tm
+000009c0: 7027 295c 6e22 2c0a 2020 2020 2274 6d70  p')\n",.    "tmp
+000009d0: 4469 722e 6d6b 6469 7228 6578 6973 745f  Dir.mkdir(exist_
+000009e0: 6f6b 3d54 7275 6529 220a 2020 205d 0a20  ok=True)".   ]. 
+000009f0: 207d 2c0a 2020 7b0a 2020 2022 6365 6c6c   },.  {.   "cell
+00000a00: 5f74 7970 6522 3a20 2263 6f64 6522 2c0a  _type": "code",.
+00000a10: 2020 2022 6578 6563 7574 696f 6e5f 636f     "execution_co
+00000a20: 756e 7422 3a20 6e75 6c6c 2c0a 2020 2022  unt": null,.   "
+00000a30: 6d65 7461 6461 7461 223a 207b 7d2c 0a20  metadata": {},. 
+00000a40: 2020 226f 7574 7075 7473 223a 205b 5d2c    "outputs": [],
+00000a50: 0a20 2020 2273 6f75 7263 6522 3a20 5b0a  .   "source": [.
+00000a60: 2020 2020 2223 2041 6374 6976 6174 6520      "# Activate 
+00000a70: 5761 726e 696e 6773 2061 7320 4578 6365  Warnings as Exce
+00000a80: 7074 696f 6e5c 6e22 2c0a 2020 2020 2223  ption\n",.    "#
+00000a90: 696d 706f 7274 2077 6172 6e69 6e67 735c  import warnings\
+00000aa0: 6e22 2c0a 2020 2020 2223 7761 726e 696e  n",.    "#warnin
+00000ab0: 6773 2e66 696c 7465 7277 6172 6e69 6e67  gs.filterwarning
+00000ac0: 7328 2765 7272 6f72 2729 220a 2020 205d  s('error')".   ]
+00000ad0: 0a20 207d 2c0a 2020 7b0a 2020 2022 6365  .  },.  {.   "ce
+00000ae0: 6c6c 5f74 7970 6522 3a20 226d 6172 6b64  ll_type": "markd
+00000af0: 6f77 6e22 2c0a 2020 2022 6d65 7461 6461  own",.   "metada
+00000b00: 7461 223a 207b 7d2c 0a20 2020 2273 6f75  ta": {},.   "sou
+00000b10: 7263 6522 3a20 5b0a 2020 2020 2223 2043  rce": [.    "# C
+00000b20: 6f6d 6d75 6e73 220a 2020 205d 0a20 207d  ommuns".   ].  }
+00000b30: 2c0a 2020 7b0a 2020 2022 6365 6c6c 5f74  ,.  {.   "cell_t
+00000b40: 7970 6522 3a20 2263 6f64 6522 2c0a 2020  ype": "code",.  
+00000b50: 2022 6578 6563 7574 696f 6e5f 636f 756e   "execution_coun
+00000b60: 7422 3a20 6e75 6c6c 2c0a 2020 2022 6d65  t": null,.   "me
+00000b70: 7461 6461 7461 223a 207b 7d2c 0a20 2020  tadata": {},.   
+00000b80: 226f 7574 7075 7473 223a 205b 5d2c 0a20  "outputs": [],. 
+00000b90: 2020 2273 6f75 7263 6522 3a20 5b0a 2020    "source": [.  
+00000ba0: 2020 2223 2041 6374 7561 6c20 2f20 7265    "# Actual / re
+00000bb0: 6665 7265 6e63 6520 636c 6f73 656e 6573  ference closenes
+00000bc0: 7320 6d65 6173 7572 6520 3a20 2d72 6f75  s measure : -rou
+00000bd0: 6e64 286c 6f67 3130 2828 6163 7475 616c  nd(log10((actual
+00000be0: 202d 2072 6566 6572 656e 6365 2920 2f20   - reference) / 
+00000bf0: 6d61 7828 6162 7328 6163 7475 616c 292c  max(abs(actual),
+00000c00: 2061 6273 2872 6566 6572 656e 6365 2929   abs(reference))
+00000c10: 292c 2031 295c 6e22 2c0a 2020 2020 2223  ), 1)\n",.    "#
+00000c20: 203d 2043 6f6d 7075 7465 2074 6865 206f   = Compute the o
+00000c30: 7264 6572 7320 6f66 206d 6167 6e69 7475  rders of magnitu
+00000c40: 6465 2074 6861 7420 7365 7061 7261 7465  de that separate
+00000c50: 2074 6865 2064 6966 6665 7265 6e63 6520   the difference 
+00000c60: 6672 6f6d 2074 6865 206d 6178 2e20 6f66  from the max. of
+00000c70: 2074 6865 2074 776f 2076 616c 7565 735c   the two values\
+00000c80: 6e22 2c0a 2020 2020 2264 6566 2063 6c6f  n",.    "def clo
+00000c90: 7365 6e65 7373 2873 5265 6641 6374 293a  seness(sRefAct):
+00000ca0: 5c6e 222c 0a20 2020 2022 2020 2020 5c6e  \n",.    "    \n
+00000cb0: 222c 0a20 2020 2022 2020 2020 782c 2079  ",.    "    x, y
+00000cc0: 203d 2073 5265 6641 6374 2e74 6f5f 6c69   = sRefAct.to_li
+00000cd0: 7374 2829 5c6e 222c 0a20 2020 2022 2020  st()\n",.    "  
+00000ce0: 2020 5c6e 222c 0a20 2020 2022 2020 2020    \n",.    "    
+00000cf0: 2320 5370 6563 6961 6c20 6361 7365 7320  # Special cases 
+00000d00: 7769 7468 2031 204e 614e 2c20 6f72 2031  with 1 NaN, or 1
+00000d10: 206f 7220 6d6f 7265 2069 6e66 203d 3e20   or more inf => 
+00000d20: 616c 6c20 6469 6666 6572 656e 745c 6e22  all different\n"
+00000d30: 2c0a 2020 2020 2220 2020 2069 6620 6e70  ,.    "    if np
+00000d40: 2e69 736e 616e 2878 293a 5c6e 222c 0a20  .isnan(x):\n",. 
+00000d50: 2020 2022 2020 2020 2020 2020 6966 206e     "        if n
+00000d60: 6f74 206e 702e 6973 6e61 6e28 7929 3a5c  ot np.isnan(y):\
+00000d70: 6e22 2c0a 2020 2020 2220 2020 2020 2020  n",.    "       
+00000d80: 2020 2020 2072 6574 7572 6e20 3020 2320       return 0 # 
+00000d90: 416c 6c20 6469 6666 6572 656e 745c 6e22  All different\n"
+00000da0: 2c0a 2020 2020 2220 2020 2065 6c69 6620  ,.    "    elif 
+00000db0: 6e70 2e69 736e 616e 2879 293a 5c6e 222c  np.isnan(y):\n",
+00000dc0: 0a20 2020 2022 2020 2020 2020 2020 7265  .    "        re
+00000dd0: 7475 726e 2030 2023 2041 6c6c 2064 6966  turn 0 # All dif
+00000de0: 6665 7265 6e74 5c6e 222c 0a20 2020 2022  ferent\n",.    "
+00000df0: 2020 2020 5c6e 222c 0a20 2020 2022 2020      \n",.    "  
+00000e00: 2020 6966 206e 702e 6973 696e 6628 7829    if np.isinf(x)
+00000e10: 206f 7220 6e70 2e69 7369 6e66 2879 293a   or np.isinf(y):
+00000e20: 5c6e 222c 0a20 2020 2022 2020 2020 2020  \n",.    "      
+00000e30: 2020 7265 7475 726e 2030 2023 2041 6c6c    return 0 # All
+00000e40: 2064 6966 6665 7265 6e74 5c6e 222c 0a20   different\n",. 
+00000e50: 2020 2022 2020 2020 5c6e 222c 0a20 2020     "    \n",.   
+00000e60: 2022 2020 2020 2320 4e6f 726d 616c 2063   "    # Normal c
+00000e70: 6173 655c 6e22 2c0a 2020 2020 2220 2020  ase\n",.    "   
+00000e80: 2063 203d 2061 6273 2878 202d 2079 295c   c = abs(x - y)\
+00000e90: 6e22 2c0a 2020 2020 2220 2020 2069 6620  n",.    "    if 
+00000ea0: 6e6f 7420 6e70 2e69 736e 616e 2863 2920  not np.isnan(c) 
+00000eb0: 616e 6420 6320 213d 2030 3a5c 6e22 2c0a  and c != 0:\n",.
+00000ec0: 2020 2020 2220 2020 2020 2020 2063 203d      "        c =
+00000ed0: 2063 202f 206d 6178 2861 6273 2878 292c   c / max(abs(x),
+00000ee0: 2061 6273 2879 2929 5c6e 222c 0a20 2020   abs(y))\n",.   
+00000ef0: 2022 2020 2020 5c6e 222c 0a20 2020 2022   "    \n",.    "
+00000f00: 2020 2020 7265 7475 726e 2072 6f75 6e64      return round
+00000f10: 282d 6e70 2e6c 6f67 3130 2863 292c 2031  (-np.log10(c), 1
+00000f20: 2922 0a20 2020 5d0a 2020 7d2c 0a20 207b  )".   ].  },.  {
+00000f30: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
+00000f40: 2022 636f 6465 222c 0a20 2020 2265 7865   "code",.   "exe
+00000f50: 6375 7469 6f6e 5f63 6f75 6e74 223a 206e  cution_count": n
+00000f60: 756c 6c2c 0a20 2020 226d 6574 6164 6174  ull,.   "metadat
+00000f70: 6122 3a20 7b7d 2c0a 2020 2022 6f75 7470  a": {},.   "outp
+00000f80: 7574 7322 3a20 5b5d 2c0a 2020 2022 736f  uts": [],.   "so
+00000f90: 7572 6365 223a 205b 0a20 2020 2022 2320  urce": [.    "# 
+00000fa0: 436f 6d70 6172 6520 6d75 6c74 6970 6c65  Compare multiple
+00000fb0: 2076 6172 6961 6e74 7320 746f 206f 6e65   variants to one
+00000fc0: 206f 6620 7468 656d 2c20 7573 696e 6720   of them, using 
+00000fd0: 636c 6f73 656e 6573 7320 666e 2061 626f  closeness fn abo
+00000fe0: 7665 5c6e 222c 0a20 2020 2022 2320 4169  ve\n",.    "# Ai
+00000ff0: 6d65 6420 6174 2062 6569 6e67 2075 7365  med at being use
+00001000: 6420 6173 2074 6865 2066 6e20 746f 2067  d as the fn to g
+00001010: 726f 7570 6279 282e 2e2e 292e 6170 706c  roupby(...).appl
+00001020: 7928 666e 2920 2873 6565 2065 7861 6d70  y(fn) (see examp
+00001030: 6c65 7320 6265 6c6f 7729 2e5c 6e22 2c0a  les below).\n",.
+00001040: 2020 2020 2264 6566 2076 6172 6961 6e74      "def variant
+00001050: 436c 6f73 656e 6573 7328 6466 672c 2069  Closeness(dfg, i
+00001060: 6e64 6578 436f 6c73 2c20 7265 664c 6162  ndexCols, refLab
+00001070: 656c 496e 643d 3029 3a5c 6e22 2c0a 2020  elInd=0):\n",.  
+00001080: 2020 2220 2020 205c 6e22 2c0a 2020 2020    "    \n",.    
+00001090: 2220 2020 2023 2042 6163 6b75 7020 616e  "    # Backup an
+000010a0: 6420 7468 6520 6472 6f70 205c 2269 6e64  d the drop \"ind
+000010b0: 6578 696e 675c 2220 636f 6c75 6d6e 7320  exing\" columns 
+000010c0: 3a20 7765 2064 6f6e 2774 2063 6865 636b  : we don't check
+000010d0: 2063 6c6f 7365 6e65 7373 206f 6e20 7468   closeness on th
+000010e0: 656d 5c6e 222c 0a20 2020 2022 2020 2020  em\n",.    "    
+000010f0: 6466 6769 203d 2064 6667 5b69 6e64 6578  dfgi = dfg[index
+00001100: 436f 6c73 5d2e 636f 7079 2829 5c6e 222c  Cols].copy()\n",
+00001110: 0a20 2020 2022 2020 2020 6466 672e 6472  .    "    dfg.dr
+00001120: 6f70 2863 6f6c 756d 6e73 3d64 6667 692e  op(columns=dfgi.
+00001130: 636f 6c75 6d6e 732e 746f 5f6c 6973 7428  columns.to_list(
+00001140: 292c 2069 6e70 6c61 6365 3d54 7275 6529  ), inplace=True)
+00001150: 5c6e 222c 0a20 2020 2022 2020 2020 5c6e  \n",.    "    \n
+00001160: 222c 0a20 2020 2022 2020 2020 2320 436f  ",.    "    # Co
+00001170: 6d70 7574 6520 636c 6f73 656e 6573 7320  mpute closeness 
+00001180: 6f66 2065 6163 6820 726f 7720 746f 2074  of each row to t
+00001190: 6865 2031 7374 206f 6e65 2e5c 6e22 2c0a  he 1st one.\n",.
+000011a0: 2020 2020 2220 2020 2064 6667 6420 3d20      "    dfgd = 
+000011b0: 7064 2e44 6174 6146 7261 6d65 2863 6f6c  pd.DataFrame(col
+000011c0: 756d 6e73 3d64 6667 2e63 6f6c 756d 6e73  umns=dfg.columns
+000011d0: 295c 6e22 2c0a 2020 2020 2220 2020 2072  )\n",.    "    r
+000011e0: 6566 4c62 6c20 3d20 6466 672e 696e 6465  efLbl = dfg.inde
+000011f0: 785b 305d 2023 204c 6162 656c 206f 6620  x[0] # Label of 
+00001200: 7468 6520 6669 7273 7420 726f 772e 5c6e  the first row.\n
+00001210: 222c 0a20 2020 2022 2020 2020 666f 7220  ",.    "    for 
+00001220: 6c62 6c20 696e 2064 6667 2e69 6e64 6578  lbl in dfg.index
+00001230: 3a5c 6e22 2c0a 2020 2020 2220 2020 2020  :\n",.    "     
+00001240: 2020 2074 7279 3a5c 6e22 2c0a 2020 2020     try:\n",.    
+00001250: 2220 2020 2020 2020 2020 2020 2064 6667  "            dfg
+00001260: 642e 6c6f 635b 6c62 6c5d 203d 2064 6667  d.loc[lbl] = dfg
+00001270: 2e6c 6f63 5b5b 7265 664c 626c 2c20 6c62  .loc[[refLbl, lb
+00001280: 6c5d 5d2e 6170 706c 7928 636c 6f73 656e  l]].apply(closen
+00001290: 6573 7329 5c6e 222c 0a20 2020 2022 2020  ess)\n",.    "  
+000012a0: 2020 2020 2020 6578 6365 7074 3a5c 6e22        except:\n"
+000012b0: 2c0a 2020 2020 2220 2020 2020 2020 2020  ,.    "         
+000012c0: 2020 2070 7269 6e74 286c 626c 2c20 7265     print(lbl, re
+000012d0: 664c 626c 295c 6e22 2c0a 2020 2020 2220  fLbl)\n",.    " 
+000012e0: 2020 2020 2020 2020 2020 2070 7269 6e74             print
+000012f0: 2864 6667 2e6c 6f63 5b5b 7265 664c 626c  (dfg.loc[[refLbl
+00001300: 2c20 6c62 6c5d 5d29 5c6e 222c 0a20 2020  , lbl]])\n",.   
+00001310: 2022 2020 2020 2020 2020 2020 2020 7261   "            ra
+00001320: 6973 655c 6e22 2c0a 2020 2020 2220 2020  ise\n",.    "   
+00001330: 2020 2020 205c 6e22 2c0a 2020 2020 2220       \n",.    " 
+00001340: 2020 2023 2052 6573 746f 7265 205c 2269     # Restore \"i
+00001350: 6e64 6578 696e 675c 2220 636f 6c75 6d6e  ndexing\" column
+00001360: 7320 3a20 646f 6e65 2e5c 6e22 2c0a 2020  s : done.\n",.  
+00001370: 2020 2220 2020 2072 6574 7572 6e20 6466    "    return df
+00001380: 6769 2e6a 6f69 6e28 6466 6764 2922 0a20  gi.join(dfgd)". 
+00001390: 2020 5d0a 2020 7d2c 0a20 207b 0a20 2020    ].  },.  {.   
+000013a0: 2263 656c 6c5f 7479 7065 223a 2022 6d61  "cell_type": "ma
+000013b0: 726b 646f 776e 222c 0a20 2020 226d 6574  rkdown",.   "met
+000013c0: 6164 6174 6122 3a20 7b7d 2c0a 2020 2022  adata": {},.   "
+000013d0: 736f 7572 6365 223a 205b 0a20 2020 2022  source": [.    "
+000013e0: 2320 5365 6e73 6962 696c 6974 c3a9 2064  # Sensibilit.. d
+000013f0: 6520 4d43 4453 2e65 7865 20c3 a020 6c27  e MCDS.exe .. l'
+00001400: 6f72 6472 6520 6465 7320 646f 6e6e c3a9  ordre des donn..
+00001410: 6573 220a 2020 205d 0a20 207d 2c0a 2020  es".   ].  },.  
+00001420: 7b0a 2020 2022 6365 6c6c 5f74 7970 6522  {.   "cell_type"
+00001430: 3a20 226d 6172 6b64 6f77 6e22 2c0a 2020  : "markdown",.  
+00001440: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
+00001450: 0a20 2020 2273 6f75 7263 6522 3a20 5b0a  .   "source": [.
+00001460: 2020 2020 2223 2320 612e 2043 6f6e 7374      "## a. Const
+00001470: 7275 6374 696f 6e20 6465 7320 7661 7269  ruction des vari
+00001480: 616e 7465 7320 6427 616e 616c 7973 6522  antes d'analyse"
+00001490: 0a20 2020 5d0a 2020 7d2c 0a20 207b 0a20  .   ].  },.  {. 
+000014a0: 2020 2263 656c 6c5f 7479 7065 223a 2022    "cell_type": "
+000014b0: 636f 6465 222c 0a20 2020 2265 7865 6375  code",.   "execu
+000014c0: 7469 6f6e 5f63 6f75 6e74 223a 206e 756c  tion_count": nul
+000014d0: 6c2c 0a20 2020 226d 6574 6164 6174 6122  l,.   "metadata"
+000014e0: 3a20 7b0a 2020 2020 2273 6372 6f6c 6c65  : {.    "scrolle
+000014f0: 6422 3a20 7472 7565 0a20 2020 7d2c 0a20  d": true.   },. 
+00001500: 2020 226f 7574 7075 7473 223a 205b 5d2c    "outputs": [],
+00001510: 0a20 2020 2273 6f75 7263 6522 3a20 5b0a  .   "source": [.
+00001520: 2020 2020 2223 2047 656e 6572 6174 6520      "# Generate 
+00001530: 7465 7374 2063 6173 6573 2064 6566 696e  test cases defin
+00001540: 6974 696f 6e20 636f 6465 2066 726f 6d20  ition code from 
+00001550: 7265 666f 7574 2072 6573 756c 7473 2066  refout results f
+00001560: 696c 6520 2864 6f6e 2774 2063 6865 6174  ile (don't cheat
+00001570: 203a 206f 6e6c 7920 696e 7075 7420 636f   : only input co
+00001580: 6c75 6d6e 7320 3a2d 295c 6e22 2c0a 2020  lumns :-)\n",.  
+00001590: 2020 2263 6173 6549 6443 6f6c 7320 3d20    "caseIdCols = 
+000015a0: 5b27 5370 6563 6965 7327 2c20 2753 616d  ['Species', 'Sam
+000015b0: 706c 6527 2c20 274d 6f64 656c 272c 2027  ple', 'Model', '
+000015c0: 4461 7461 4f72 6465 7227 5d5c 6e22 2c0a  DataOrder']\n",.
+000015d0: 2020 2020 225c 6e22 2c0a 2020 2020 2264      "\n",.    "d
+000015e0: 6641 6e6c 7973 4361 7365 7320 3d20 7064  fAnlysCases = pd
+000015f0: 2e44 6174 6146 7261 6d65 2864 6174 613d  .DataFrame(data=
+00001600: 5b28 7370 6563 2c20 7361 6d70 2c20 6b65  [(spec, samp, ke
+00001610: 7946 6e20 2b20 6164 6a53 6572 2c20 6461  yFn + adjSer, da
+00001620: 7461 4f72 6429 205c 5c5c 6e22 2c0a 2020  taOrd) \\\n",.  
+00001630: 2020 2220 2020 2020 2020 2020 2020 2020    "             
+00001640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001650: 2020 2020 2066 6f72 2073 7065 6320 696e       for spec in
+00001660: 205b 2753 594c 4154 5227 2c20 2754 5552   ['SYLATR', 'TUR
+00001670: 4d45 5227 2c20 274c 5553 4d45 4727 2c20  MER', 'LUSMEG', 
+00001680: 2741 4c41 4152 5627 2c20 2743 4f4c 5041  'ALAARV', 'COLPA
+00001690: 4c27 2c5c 6e22 2c0a 2020 2020 2220 2020  L',\n",.    "   
+000016a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000016b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000016c0: 2020 2020 2020 2020 2020 2020 2750 4859              'PHY
+000016d0: 434f 4c27 2c20 2745 4d42 4349 5427 2c20  COL', 'EMBCIT', 
+000016e0: 2745 4d42 4349 5227 2c20 2741 4e54 5452  'EMBCIR', 'ANTTR
+000016f0: 4927 2c20 274d 494c 4341 4c27 5d20 5c5c  I', 'MILCAL'] \\
+00001700: 5c6e 222c 0a20 2020 2022 2020 2020 2020  \n",.    "      
+00001710: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001720: 2020 2020 2020 2020 2020 2020 666f 7220              for 
+00001730: 7361 6d70 2069 6e20 5b27 4142 2d31 306d  samp in ['AB-10m
+00001740: 6e2d 7474 6465 6327 5d20 5c5c 5c6e 222c  n-ttdec'] \\\n",
+00001750: 0a20 2020 2022 2020 2020 2020 2020 2020  .    "          
+00001760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001770: 2020 2020 2020 2020 666f 7220 6b65 7946          for keyF
+00001780: 6e20 696e 205b 2748 4e6f 272c 2027 556e  n in ['HNo', 'Un
+00001790: 6927 2c20 2748 617a 275d 205c 5c5c 6e22  i', 'Haz'] \\\n"
+000017a0: 2c0a 2020 2020 2220 2020 2020 2020 2020  ,.    "         
+000017b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000017c0: 2020 2020 2020 2020 2066 6f72 2061 646a           for adj
+000017d0: 5365 7220 696e 205b 2743 6f73 272c 2027  Ser in ['Cos', '
+000017e0: 506f 6c27 5d20 5c5c 5c6e 222c 0a20 2020  Pol'] \\\n",.   
+000017f0: 2022 2020 2020 2020 2020 2020 2020 2020   "              
+00001800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001810: 2020 2020 666f 7220 6461 7461 4f72 6420      for dataOrd 
+00001820: 696e 205b 2770 6364 6327 2c20 2020 2320  in ['pcdc',   # 
+00001830: 536f 7274 2062 7920 706f 696e 742c 2061  Sort by point, a
+00001840: 6e64 2069 6e63 7265 6173 696e 6720 6469  nd increasing di
+00001850: 7374 616e 6365 735c 6e22 2c0a 2020 2020  stances\n",.    
+00001860: 2220 2020 2020 2020 2020 2020 2020 2020  "               
+00001870: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001890: 2020 2020 2020 666f 7220 6164 6a53 6572        for adjSer
-000018a0: 2069 6e20 5b27 436f 7327 2c20 2750 6f6c   in ['Cos', 'Pol
-000018b0: 275d 205c 5c5c 6e22 2c0d 0a20 2020 2022  '] \\\n",..    "
-000018c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001890: 2020 2027 7063 6464 272c 2020 2023 2053     'pcdd',   # S
+000018a0: 6f72 7420 6279 2070 6f69 6e74 2c20 616e  ort by point, an
+000018b0: 6420 6465 6372 6561 7369 6e67 2064 6973  d decreasing dis
+000018c0: 7461 6e63 6573 5c6e 222c 0a20 2020 2022  tances\n",.    "
 000018d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000018e0: 2020 666f 7220 6461 7461 4f72 6420 696e    for dataOrd in
-000018f0: 205b 2770 6364 6327 2c20 2020 2320 536f   ['pcdc',   # So
-00001900: 7274 2062 7920 706f 696e 742c 2061 6e64  rt by point, and
-00001910: 2069 6e63 7265 6173 696e 6720 6469 7374   increasing dist
-00001920: 616e 6365 735c 6e22 2c0d 0a20 2020 2022  ances\n",..    "
-00001930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000018e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000018f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001900: 2020 2770 6327 2c20 2020 2020 2320 536f    'pc',     # So
+00001910: 7274 2062 7920 706f 696e 742c 2062 7574  rt by point, but
+00001920: 2064 6973 7461 6e63 6520 6f72 6465 7220   distance order 
+00001930: 756e 746f 7563 6865 645c 6e22 2c0a 2020  untouched\n",.  
+00001940: 2020 2220 2020 2020 2020 2020 2020 2020    "             
 00001950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001960: 2020 2770 6364 6427 2c20 2020 2320 536f    'pcdd',   # So
-00001970: 7274 2062 7920 706f 696e 742c 2061 6e64  rt by point, and
-00001980: 2064 6563 7265 6173 696e 6720 6469 7374   decreasing dist
-00001990: 616e 6365 735c 6e22 2c0d 0a20 2020 2022  ances\n",..    "
-000019a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001960: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001970: 2020 2020 2027 6463 275d 5d2c 2020 2023       'dc']],   #
+00001980: 2053 6f72 7420 6279 2069 6e63 7265 6173   Sort by increas
+00001990: 696e 6720 6469 7374 616e 6365 735c 6e22  ing distances\n"
+000019a0: 2c0a 2020 2020 2220 2020 2020 2020 2020  ,.    "         
 000019b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000019c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000019d0: 2020 2770 6327 2c20 2020 2020 2320 536f    'pc',     # So
-000019e0: 7274 2062 7920 706f 696e 742c 2062 7574  rt by point, but
-000019f0: 2064 6973 7461 6e63 6520 6f72 6465 7220   distance order 
-00001a00: 756e 746f 7563 6865 645c 6e22 2c0d 0a20  untouched\n",.. 
-00001a10: 2020 2022 2020 2020 2020 2020 2020 2020     "            
-00001a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001a40: 2020 2020 2020 2764 6327 5d5d 2c20 2020        'dc']],   
-00001a50: 2320 536f 7274 2062 7920 696e 6372 6561  # Sort by increa
-00001a60: 7369 6e67 2064 6973 7461 6e63 6573 5c6e  sing distances\n
-00001a70: 222c 0d0a 2020 2020 2220 2020 2020 2020  ",..    "       
-00001a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001a90: 2020 2020 2063 6f6c 756d 6e73 3d63 6173       columns=cas
-00001aa0: 6549 6443 6f6c 7329 5c6e 222c 0d0a 2020  eIdCols)\n",..  
-00001ab0: 2020 225c 6e22 2c0d 0a20 2020 2022 6466    "\n",..    "df
-00001ac0: 416e 6c79 7343 6173 6573 5b27 496e 4669  AnlysCases['InFi
-00001ad0: 6c65 4e61 6d65 275d 203d 205c 5c5c 6e22  leName'] = \\\n"
-00001ae0: 2c0d 0a20 2020 2022 2020 2020 6466 416e  ,..    "    dfAn
-00001af0: 6c79 7343 6173 6573 2e61 7070 6c79 286c  lysCases.apply(l
-00001b00: 616d 6264 6120 7352 6f77 3a20 2741 4344  ambda sRow: 'ACD
-00001b10: 4332 3031 392d 5061 7079 7275 732d 7b7d  C2019-Papyrus-{}
-00001b20: 2d7b 7d2d 6469 7374 2e74 7874 272e 666f  -{}-dist.txt'.fo
-00001b30: 726d 6174 2873 526f 772e 5370 6563 6965  rmat(sRow.Specie
-00001b40: 732c 2073 526f 772e 5361 6d70 6c65 292c  s, sRow.Sample),
-00001b50: 5c6e 222c 0d0a 2020 2020 2220 2020 2020  \n",..    "     
-00001b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001b70: 2020 6178 6973 3d27 636f 6c75 6d6e 7327    axis='columns'
-00001b80: 295c 6e22 2c0d 0a20 2020 2022 5c6e 222c  )\n",..    "\n",
-00001b90: 0d0a 2020 2020 2264 6641 6e6c 7973 4361  ..    "dfAnlysCa
-00001ba0: 7365 7322 0d0a 2020 205d 0d0a 2020 7d2c  ses"..   ]..  },
-00001bb0: 0d0a 2020 7b0d 0a20 2020 2263 656c 6c5f  ..  {..   "cell_
-00001bc0: 7479 7065 223a 2022 636f 6465 222c 0d0a  type": "code",..
-00001bd0: 2020 2022 6578 6563 7574 696f 6e5f 636f     "execution_co
-00001be0: 756e 7422 3a20 6e75 6c6c 2c0d 0a20 2020  unt": null,..   
-00001bf0: 226d 6574 6164 6174 6122 3a20 7b7d 2c0d  "metadata": {},.
-00001c00: 0a20 2020 226f 7574 7075 7473 223a 205b  .   "outputs": [
-00001c10: 5d2c 0d0a 2020 2022 736f 7572 6365 223a  ],..   "source":
-00001c20: 205b 0d0a 2020 2020 2261 7373 6572 7420   [..    "assert 
-00001c30: 616c 6c28 706c 2e50 6174 6828 2772 6566  all(pl.Path('ref
-00001c40: 696e 272c 2073 4361 7365 2e49 6e46 696c  in', sCase.InFil
-00001c50: 654e 616d 6529 2e65 7869 7374 7328 2920  eName).exists() 
-00001c60: 5c5c 5c6e 222c 0d0a 2020 2020 2220 2020  \\\n",..    "   
-00001c70: 2020 2020 2020 2020 666f 7220 5f2c 2073          for _, s
-00001c80: 4361 7365 2069 6e20 6466 416e 6c79 7343  Case in dfAnlysC
-00001c90: 6173 6573 2e69 7465 7272 6f77 7328 2929  ases.iterrows())
-00001ca0: 2c20 274f 682c 206f 6820 2e2e 2e20 536f  , 'Oh, oh ... So
-00001cb0: 6d65 206d 6973 7369 6e67 2066 696c 6528  me missing file(
-00001cc0: 7329 2021 2722 0d0a 2020 205d 0d0a 2020  s) !'"..   ]..  
-00001cd0: 7d2c 0d0a 2020 7b0d 0a20 2020 2263 656c  },..  {..   "cel
-00001ce0: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
-00001cf0: 0d0a 2020 2022 6578 6563 7574 696f 6e5f  ..   "execution_
-00001d00: 636f 756e 7422 3a20 6e75 6c6c 2c0d 0a20  count": null,.. 
-00001d10: 2020 226d 6574 6164 6174 6122 3a20 7b7d    "metadata": {}
-00001d20: 2c0d 0a20 2020 226f 7574 7075 7473 223a  ,..   "outputs":
-00001d30: 205b 5d2c 0d0a 2020 2022 736f 7572 6365   [],..   "source
-00001d40: 223a 205b 0d0a 2020 2020 2223 6466 416e  ": [..    "#dfAn
-00001d50: 6c79 7343 6173 6573 203d 2064 6641 6e6c  lysCases = dfAnl
-00001d60: 7973 4361 7365 735b 3a35 5d22 0d0a 2020  ysCases[:5]"..  
-00001d70: 205d 0d0a 2020 7d2c 0d0a 2020 7b0d 0a20   ]..  },..  {.. 
-00001d80: 2020 2263 656c 6c5f 7479 7065 223a 2022    "cell_type": "
-00001d90: 636f 6465 222c 0d0a 2020 2022 6578 6563  code",..   "exec
-00001da0: 7574 696f 6e5f 636f 756e 7422 3a20 6e75  ution_count": nu
-00001db0: 6c6c 2c0d 0a20 2020 226d 6574 6164 6174  ll,..   "metadat
-00001dc0: 6122 3a20 7b7d 2c0d 0a20 2020 226f 7574  a": {},..   "out
-00001dd0: 7075 7473 223a 205b 5d2c 0d0a 2020 2022  puts": [],..   "
-00001de0: 736f 7572 6365 223a 205b 0d0a 2020 2020  source": [..    
-00001df0: 226c 656e 2864 6641 6e6c 7973 4361 7365  "len(dfAnlysCase
-00001e00: 7329 220d 0a20 2020 5d0d 0a20 207d 2c0d  s)"..   ]..  },.
-00001e10: 0a20 207b 0d0a 2020 2022 6365 6c6c 5f74  .  {..   "cell_t
-00001e20: 7970 6522 3a20 226d 6172 6b64 6f77 6e22  ype": "markdown"
-00001e30: 2c0d 0a20 2020 226d 6574 6164 6174 6122  ,..   "metadata"
-00001e40: 3a20 7b7d 2c0d 0a20 2020 2273 6f75 7263  : {},..   "sourc
-00001e50: 6522 3a20 5b0d 0a20 2020 2022 2323 2062  e": [..    "## b
-00001e60: 2e20 4578 c3a9 6375 7469 6f6e 2064 6573  . Ex..cution des
-00001e70: 2061 6e61 6c79 7365 7322 0d0a 2020 205d   analyses"..   ]
-00001e80: 0d0a 2020 7d2c 0d0a 2020 7b0d 0a20 2020  ..  },..  {..   
-00001e90: 2263 656c 6c5f 7479 7065 223a 2022 636f  "cell_type": "co
-00001ea0: 6465 222c 0d0a 2020 2022 6578 6563 7574  de",..   "execut
-00001eb0: 696f 6e5f 636f 756e 7422 3a20 6e75 6c6c  ion_count": null
-00001ec0: 2c0d 0a20 2020 226d 6574 6164 6174 6122  ,..   "metadata"
-00001ed0: 3a20 7b7d 2c0d 0a20 2020 226f 7574 7075  : {},..   "outpu
-00001ee0: 7473 223a 205b 5d2c 0d0a 2020 2022 736f  ts": [],..   "so
-00001ef0: 7572 6365 223a 205b 0d0a 2020 2020 2264  urce": [..    "d
-00001f00: 6563 696d 616c 4669 656c 6473 203d 205b  ecimalFields = [
-00001f10: 2750 6f69 6e74 2074 7261 6e73 6563 742a  'Point transect*
-00001f20: 5375 7276 6579 2065 6666 6f72 7427 2c20  Survey effort', 
-00001f30: 274f 6273 6572 7661 7469 6f6e 2a52 6164  'Observation*Rad
-00001f40: 6961 6c20 6469 7374 616e 6365 275d 220d  ial distance']".
-00001f50: 0a20 2020 5d0d 0a20 207d 2c0d 0a20 207b  .   ]..  },..  {
-00001f60: 0d0a 2020 2022 6365 6c6c 5f74 7970 6522  ..   "cell_type"
-00001f70: 3a20 2263 6f64 6522 2c0d 0a20 2020 2265  : "code",..   "e
-00001f80: 7865 6375 7469 6f6e 5f63 6f75 6e74 223a  xecution_count":
-00001f90: 206e 756c 6c2c 0d0a 2020 2022 6d65 7461   null,..   "meta
-00001fa0: 6461 7461 223a 207b 7d2c 0d0a 2020 2022  data": {},..   "
-00001fb0: 6f75 7470 7574 7322 3a20 5b5d 2c0d 0a20  outputs": [],.. 
-00001fc0: 2020 2273 6f75 7263 6522 3a20 5b0d 0a20    "source": [.. 
-00001fd0: 2020 2022 2320 416e 616c 7973 6973 2065     "# Analysis e
-00001fe0: 6e67 696e 655c 6e22 2c0d 0a20 2020 2022  ngine\n",..    "
-00001ff0: 6d63 6473 203d 2061 6473 2e4d 4344 5345  mcds = ads.MCDSE
-00002000: 6e67 696e 6528 776f 726b 4469 723d 2774  ngine(workDir='t
-00002010: 6d70 2f6d 6364 732d 7365 6e73 272c 5c6e  mp/mcds-sens',\n
-00002020: 222c 0d0a 2020 2020 2220 2020 2020 2020  ",..    "       
-00002030: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-00002040: 6973 7461 6e63 6555 6e69 743d 274d 6574  istanceUnit='Met
-00002050: 6572 272c 2061 7265 6155 6e69 743d 2748  er', areaUnit='H
-00002060: 6563 7461 7265 272c 5c6e 222c 0d0a 2020  ectare',\n",..  
-00002070: 2020 2220 2020 2020 2020 2020 2020 2020    "             
-00002080: 2020 2020 2020 2020 2073 7572 7665 7954           surveyT
-00002090: 7970 653d 2750 6f69 6e74 272c 2064 6973  ype='Point', dis
-000020a0: 7461 6e63 6554 7970 653d 2752 6164 6961  tanceType='Radia
-000020b0: 6c27 2c20 636c 7573 7465 7269 6e67 3d46  l', clustering=F
-000020c0: 616c 7365 2922 0d0a 2020 205d 0d0a 2020  alse)"..   ]..  
-000020d0: 7d2c 0d0a 2020 7b0d 0a20 2020 2263 656c  },..  {..   "cel
-000020e0: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
-000020f0: 0d0a 2020 2022 6578 6563 7574 696f 6e5f  ..   "execution_
-00002100: 636f 756e 7422 3a20 6e75 6c6c 2c0d 0a20  count": null,.. 
-00002110: 2020 226d 6574 6164 6174 6122 3a20 7b7d    "metadata": {}
-00002120: 2c0d 0a20 2020 226f 7574 7075 7473 223a  ,..   "outputs":
-00002130: 205b 5d2c 0d0a 2020 2022 736f 7572 6365   [],..   "source
-00002140: 223a 205b 0d0a 2020 2020 2223 2046 726f  ": [..    "# Fro
-00002150: 7a65 6e20 616e 616c 7973 6973 2070 6172  zen analysis par
-00002160: 616d 6574 6572 7320 2861 2063 686f 6963  ameters (a choic
-00002170: 6520 6865 7265 295c 6e22 2c0d 0a20 2020  e here)\n",..   
-00002180: 2022 4b45 7374 696d 4372 6974 6572 696f   "KEstimCriterio
-00002190: 6e20 3d20 2741 4943 275c 6e22 2c0d 0a20  n = 'AIC'\n",.. 
-000021a0: 2020 2022 4b43 5649 6e74 6572 7661 6c20     "KCVInterval 
-000021b0: 3d20 3935 220d 0a20 2020 5d0d 0a20 207d  = 95"..   ]..  }
-000021c0: 2c0d 0a20 207b 0d0a 2020 2022 6365 6c6c  ,..  {..   "cell
-000021d0: 5f74 7970 6522 3a20 2263 6f64 6522 2c0d  _type": "code",.
-000021e0: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
-000021f0: 6f75 6e74 223a 206e 756c 6c2c 0d0a 2020  ount": null,..  
-00002200: 2022 6d65 7461 6461 7461 223a 207b 0d0a   "metadata": {..
-00002210: 2020 2020 2273 6372 6f6c 6c65 6422 3a20      "scrolled": 
-00002220: 7472 7565 0d0a 2020 207d 2c0d 0a20 2020  true..   },..   
-00002230: 226f 7574 7075 7473 223a 205b 5d2c 0d0a  "outputs": [],..
-00002240: 2020 2022 736f 7572 6365 223a 205b 0d0a     "source": [..
-00002250: 2020 2020 2274 7353 7461 7274 203d 2070      "tsStart = p
-00002260: 642e 5469 6d65 7374 616d 702e 6e6f 7728  d.Timestamp.now(
-00002270: 295c 6e22 2c0d 0a20 2020 2022 7072 696e  )\n",..    "prin
-00002280: 7428 2753 7461 7274 6564 2061 7427 2c20  t('Started at', 
-00002290: 7473 5374 6172 7429 5c6e 222c 0d0a 2020  tsStart)\n",..  
-000022a0: 2020 2270 7269 6e74 2829 5c6e 222c 0d0a    "print()\n",..
-000022b0: 2020 2020 225c 6e22 2c0d 0a20 2020 2022      "\n",..    "
-000022c0: 2320 5275 6e20 616c 6c20 616e 616c 7973  # Run all analys
-000022d0: 6573 5c6e 222c 0d0a 2020 2020 226d 6943  es\n",..    "miC
-000022e0: 7573 7443 6f6c 7320 3d20 7064 2e4d 756c  ustCols = pd.Mul
-000022f0: 7469 496e 6465 782e 6672 6f6d 5f74 7570  tiIndex.from_tup
-00002300: 6c65 7328 5b28 2773 616d 706c 6527 2c20  les([('sample', 
-00002310: 636f 6c2c 2027 5661 6c75 6527 2920 666f  col, 'Value') fo
-00002320: 7220 636f 6c20 696e 2063 6173 6549 6443  r col in caseIdC
-00002330: 6f6c 735d 295c 6e22 2c0d 0a20 2020 2022  ols])\n",..    "
-00002340: 6466 4375 7374 436f 6c54 7261 6e73 203d  dfCustColTrans =
-00002350: 205c 5c5c 6e22 2c0d 0a20 2020 2022 2020   \\\n",..    "  
-00002360: 2020 7064 2e44 6174 6146 7261 6d65 2869    pd.DataFrame(i
-00002370: 6e64 6578 3d6d 6943 7573 7443 6f6c 732c  ndex=miCustCols,
-00002380: 5c6e 222c 0d0a 2020 2020 2220 2020 2020  \n",..    "     
-00002390: 2020 2020 2020 2020 2020 2020 6461 7461              data
-000023a0: 3d64 6963 7428 656e 3d63 6173 6549 6443  =dict(en=caseIdC
-000023b0: 6f6c 732c 2066 723d 5b27 4573 70c3 a863  ols, fr=['Esp..c
-000023c0: 6527 2c20 2745 6368 616e 7469 6c6c 6f6e  e', 'Echantillon
-000023d0: 272c 2027 4d6f 64c3 a86c 6527 2c20 274f  ', 'Mod..le', 'O
-000023e0: 7264 7265 446f 6e6e c3a9 6573 275d 2929  rdreDonn..es']))
-000023f0: 5c6e 222c 0d0a 2020 2020 225c 6e22 2c0d  \n",..    "\n",.
-00002400: 0a20 2020 2022 7265 7375 6c74 7320 3d20  .    "results = 
-00002410: 6164 732e 4d43 4453 416e 616c 7973 6973  ads.MCDSAnalysis
-00002420: 5265 7375 6c74 7353 6574 286d 6943 7573  ResultsSet(miCus
-00002430: 746f 6d43 6f6c 733d 6d69 4375 7374 436f  tomCols=miCustCo
-00002440: 6c73 2c20 6466 4375 7374 6f6d 436f 6c54  ls, dfCustomColT
-00002450: 7261 6e73 3d64 6643 7573 7443 6f6c 5472  rans=dfCustColTr
-00002460: 616e 732c 5c6e 222c 0d0a 2020 2020 2220  ans,\n",..    " 
-00002470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002490: 2020 2020 6469 7374 616e 6365 556e 6974      distanceUnit
-000024a0: 3d27 4d65 7465 7227 2c20 6172 6561 556e  ='Meter', areaUn
-000024b0: 6974 3d27 4865 6374 6172 6527 2c5c 6e22  it='Hectare',\n"
-000024c0: 2c0d 0a20 2020 2022 2020 2020 2020 2020  ,..    "        
-000024d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000024e0: 2020 2020 2020 2020 2020 2020 2073 7572               sur
-000024f0: 7665 7954 7970 653d 2750 6f69 6e74 272c  veyType='Point',
-00002500: 2064 6973 7461 6e63 6554 7970 653d 2752   distanceType='R
-00002510: 6164 6961 6c27 2c20 636c 7573 7465 7269  adial', clusteri
-00002520: 6e67 3d46 616c 7365 295c 6e22 2c0d 0a20  ng=False)\n",.. 
-00002530: 2020 2022 5c6e 222c 0d0a 2020 2020 226c     "\n",..    "l
-00002540: 6173 7449 6e46 696c 654e 616d 6520 3d20  astInFileName = 
-00002550: 2727 5c6e 222c 0d0a 2020 2020 2266 6f72  ''\n",..    "for
-00002560: 2069 6e64 2c20 7343 6173 6520 696e 2064   ind, sCase in d
-00002570: 6641 6e6c 7973 4361 7365 732e 6974 6572  fAnlysCases.iter
-00002580: 726f 7773 2829 3a5c 6e22 2c0d 0a20 2020  rows():\n",..   
-00002590: 2022 2020 2020 5c6e 222c 0d0a 2020 2020   "    \n",..    
-000025a0: 2220 2020 2070 7265 6669 7820 3d20 277b  "    prefix = '{
-000025b0: 7d2d 7b7d 2d7b 7d27 2e66 6f72 6d61 7428  }-{}-{}'.format(
-000025c0: 7343 6173 652e 5370 6563 6965 732c 2073  sCase.Species, s
-000025d0: 4361 7365 2e53 616d 706c 652c 2073 4361  Case.Sample, sCa
-000025e0: 7365 2e44 6174 614f 7264 6572 295c 6e22  se.DataOrder)\n"
-000025f0: 2c0d 0a20 2020 2022 2020 2020 7072 696e  ,..    "    prin
-00002600: 7428 2723 7b3a 3364 7d20 7b7d 207b 7d27  t('#{:3d} {} {}'
-00002610: 2e66 6f72 6d61 7428 696e 642b 312c 2070  .format(ind+1, p
-00002620: 7265 6669 782c 2073 4361 7365 2e4d 6f64  refix, sCase.Mod
-00002630: 656c 292c 2065 6e64 3d27 5c5c 6e27 2a32  el), end='\\n'*2
-00002640: 295c 6e22 2c0d 0a20 2020 2022 2020 2020  )\n",..    "    
-00002650: 5c6e 222c 0d0a 2020 2020 2220 2020 2023  \n",..    "    #
-00002660: 2043 7265 6174 6520 6461 7461 2073 6574   Create data set
-00002670: 2e5c 6e22 2c0d 0a20 2020 2022 2020 2020  .\n",..    "    
-00002680: 6466 496e 4461 7461 203d 2061 6473 2e53  dfInData = ads.S
-00002690: 616d 706c 6544 6174 6153 6574 2e63 7376  ampleDataSet.csv
-000026a0: 3264 6628 6f73 2e70 6174 682e 6a6f 696e  2df(os.path.join
-000026b0: 2827 7265 6669 6e27 2c20 7343 6173 652e  ('refin', sCase.
-000026c0: 496e 4669 6c65 4e61 6d65 292c 2064 6563  InFileName), dec
-000026d0: 436f 6c73 3d64 6563 696d 616c 4669 656c  Cols=decimalFiel
-000026e0: 6473 295c 6e22 2c0d 0a20 2020 2022 2020  ds)\n",..    "  
-000026f0: 2020 736f 7274 436f 6c73 203d 206c 6973    sortCols = lis
-00002700: 7428 295c 6e22 2c0d 0a20 2020 2022 2020  t()\n",..    "  
-00002710: 2020 736f 7274 4173 6367 203d 206c 6973    sortAscg = lis
-00002720: 7428 295c 6e22 2c0d 0a20 2020 2022 2020  t()\n",..    "  
-00002730: 2020 666f 7220 7372 7420 696e 205b 7343    for srt in [sC
-00002740: 6173 652e 4461 7461 4f72 6465 725b 693a  ase.DataOrder[i:
-00002750: 692b 325d 2066 6f72 2069 2069 6e20 7261  i+2] for i in ra
-00002760: 6e67 6528 302c 206c 656e 2873 4361 7365  nge(0, len(sCase
-00002770: 2e44 6174 614f 7264 6572 292c 2032 295d  .DataOrder), 2)]
-00002780: 3a5c 6e22 2c0d 0a20 2020 2022 2020 2020  :\n",..    "    
-00002790: 2020 2020 6173 7365 7274 2073 7274 5b30      assert srt[0
-000027a0: 5d20 696e 2027 7064 2720 616e 6420 7372  ] in 'pd' and sr
-000027b0: 745b 315d 2069 6e20 2763 6427 5c6e 222c  t[1] in 'cd'\n",
-000027c0: 0d0a 2020 2020 2220 2020 2020 2020 2069  ..    "        i
-000027d0: 6620 7372 745b 305d 203d 3d20 2770 273a  f srt[0] == 'p':
-000027e0: 5c6e 222c 0d0a 2020 2020 2220 2020 2020  \n",..    "     
-000027f0: 2020 2020 2020 2073 6f72 7443 6f6c 732e         sortCols.
-00002800: 6170 7065 6e64 2827 506f 696e 7420 7472  append('Point tr
-00002810: 616e 7365 6374 2a4c 6162 656c 2729 5c6e  ansect*Label')\n
-00002820: 222c 0d0a 2020 2020 2220 2020 2020 2020  ",..    "       
-00002830: 2065 6c73 653a 2023 2027 6427 5c6e 222c   else: # 'd'\n",
-00002840: 0d0a 2020 2020 2220 2020 2020 2020 2020  ..    "         
-00002850: 2020 2073 6f72 7443 6f6c 732e 6170 7065     sortCols.appe
-00002860: 6e64 2827 4f62 7365 7276 6174 696f 6e2a  nd('Observation*
-00002870: 5261 6469 616c 2064 6973 7461 6e63 6527  Radial distance'
-00002880: 295c 6e22 2c0d 0a20 2020 2022 2020 2020  )\n",..    "    
-00002890: 2020 2020 736f 7274 4173 6367 2e61 7070      sortAscg.app
-000028a0: 656e 6428 7372 745b 315d 203d 3d20 2763  end(srt[1] == 'c
-000028b0: 2729 5c6e 222c 0d0a 2020 2020 2220 2020  ')\n",..    "   
-000028c0: 2064 6649 6e44 6174 612e 736f 7274 5f76   dfInData.sort_v
-000028d0: 616c 7565 7328 6279 3d73 6f72 7443 6f6c  alues(by=sortCol
-000028e0: 732c 2061 7363 656e 6469 6e67 3d73 6f72  s, ascending=sor
-000028f0: 7441 7363 672c 2069 6e70 6c61 6365 3d54  tAscg, inplace=T
-00002900: 7275 6529 5c6e 222c 0d0a 2020 2020 2220  rue)\n",..    " 
-00002910: 2020 2073 6473 203d 2061 6473 2e53 616d     sds = ads.Sam
-00002920: 706c 6544 6174 6153 6574 2864 6649 6e44  pleDataSet(dfInD
-00002930: 6174 612c 2064 6563 696d 616c 4669 656c  ata, decimalFiel
-00002940: 6473 3d64 6563 696d 616c 4669 656c 6473  ds=decimalFields
-00002950: 295c 6e22 2c0d 0a20 2020 2022 2020 2020  )\n",..    "    
-00002960: 2020 2020 5c6e 222c 0d0a 2020 2020 2220      \n",..    " 
-00002970: 2020 2023 2052 756e 2061 6e61 6c79 7369     # Run analysi
-00002980: 735c 6e22 2c0d 0a20 2020 2022 2020 2020  s\n",..    "    
-00002990: 616e 616c 7973 6973 203d 2061 6473 2e4d  analysis = ads.M
-000029a0: 4344 5341 6e61 6c79 7369 7328 656e 6769  CDSAnalysis(engi
-000029b0: 6e65 3d6d 6364 732c 2073 616d 706c 6544  ne=mcds, sampleD
-000029c0: 6174 6153 6574 3d73 6473 2c20 6e61 6d65  ataSet=sds, name
-000029d0: 3d70 7265 6669 782c 5c6e 222c 0d0a 2020  =prefix,\n",..  
-000029e0: 2020 2220 2020 2020 2020 2020 2020 2020    "             
-000029f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002a00: 2020 2065 7374 696d 4b65 7946 6e3d 7343     estimKeyFn=sC
-00002a10: 6173 652e 4d6f 6465 6c5b 3a33 5d2e 7570  ase.Model[:3].up
-00002a20: 7065 7228 292c 2065 7374 696d 4164 6a75  per(), estimAdju
-00002a30: 7374 466e 3d73 4361 7365 2e4d 6f64 656c  stFn=sCase.Model
-00002a40: 5b33 3a5d 2e75 7070 6572 2829 2c5c 6e22  [3:].upper(),\n"
-00002a50: 2c0d 0a20 2020 2022 2020 2020 2020 2020  ,..    "        
-00002a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002a70: 2020 2020 2020 2020 6573 7469 6d43 7269          estimCri
-00002a80: 7465 7269 6f6e 3d4b 4573 7469 6d43 7269  terion=KEstimCri
-00002a90: 7465 7269 6f6e 2c20 6376 496e 7465 7276  terion, cvInterv
-00002aa0: 616c 3d4b 4356 496e 7465 7276 616c 295c  al=KCVInterval)\
-00002ab0: 6e22 2c0d 0a20 2020 2022 2020 2020 7352  n",..    "    sR
-00002ac0: 6573 756c 7420 3d20 616e 616c 7973 6973  esult = analysis
-00002ad0: 2e73 7562 6d69 7428 292e 6765 7452 6573  .submit().getRes
-00002ae0: 756c 7473 2829 5c6e 222c 0d0a 2020 2020  ults()\n",..    
-00002af0: 225c 6e22 2c0d 0a20 2020 2022 2020 2020  "\n",..    "    
-00002b00: 2320 5361 7665 2072 6573 756c 7473 5c6e  # Save results\n
-00002b10: 222c 0d0a 2020 2020 2220 2020 2073 4865  ",..    "    sHe
-00002b20: 6164 203d 2070 642e 5365 7269 6573 2864  ad = pd.Series(d
-00002b30: 6174 613d 5b73 4361 7365 5b63 6f6c 5d20  ata=[sCase[col] 
-00002b40: 666f 7220 636f 6c20 696e 2073 4361 7365  for col in sCase
-00002b50: 2e69 6e64 6578 5b3a 6c65 6e28 6361 7365  .index[:len(case
-00002b60: 4964 436f 6c73 295d 5d2c 2069 6e64 6578  IdCols)]], index
-00002b70: 3d6d 6943 7573 7443 6f6c 7329 5c6e 222c  =miCustCols)\n",
-00002b80: 0d0a 2020 2020 225c 6e22 2c0d 0a20 2020  ..    "\n",..   
-00002b90: 2022 2020 2020 7265 7375 6c74 732e 6170   "    results.ap
-00002ba0: 7065 6e64 2873 5265 7375 6c74 2c20 7343  pend(sResult, sC
-00002bb0: 7573 746f 6d48 6561 643d 7348 6561 6429  ustomHead=sHead)
-00002bc0: 5c6e 222c 0d0a 2020 2020 2220 2020 205c  \n",..    "    \
-00002bd0: 6e22 2c0d 0a20 2020 2022 7473 456e 6420  n",..    "tsEnd 
-00002be0: 3d20 7064 2e54 696d 6573 7461 6d70 2e6e  = pd.Timestamp.n
-00002bf0: 6f77 2829 5c6e 222c 0d0a 2020 2020 2270  ow()\n",..    "p
-00002c00: 7269 6e74 2827 4669 6e69 7368 6564 2061  rint('Finished a
-00002c10: 7427 2c20 7473 456e 642c 2027 3a20 6475  t', tsEnd, ': du
-00002c20: 7261 7469 6f6e 272c 2073 7472 2874 7345  ration', str(tsE
-00002c30: 6e64 202d 2074 7353 7461 7274 292e 7265  nd - tsStart).re
-00002c40: 706c 6163 6528 2730 2064 6179 7320 272c  place('0 days ',
-00002c50: 2027 2729 2922 0d0a 2020 205d 0d0a 2020   ''))"..   ]..  
-00002c60: 7d2c 0d0a 2020 7b0d 0a20 2020 2263 656c  },..  {..   "cel
-00002c70: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
-00002c80: 0d0a 2020 2022 6578 6563 7574 696f 6e5f  ..   "execution_
-00002c90: 636f 756e 7422 3a20 6e75 6c6c 2c0d 0a20  count": null,.. 
-00002ca0: 2020 226d 6574 6164 6174 6122 3a20 7b0d    "metadata": {.
-00002cb0: 0a20 2020 2022 7363 726f 6c6c 6564 223a  .    "scrolled":
-00002cc0: 2066 616c 7365 0d0a 2020 207d 2c0d 0a20   false..   },.. 
-00002cd0: 2020 226f 7574 7075 7473 223a 205b 5d2c    "outputs": [],
-00002ce0: 0d0a 2020 2022 736f 7572 6365 223a 205b  ..   "source": [
-00002cf0: 0d0a 2020 2020 2223 2053 6176 6520 616e  ..    "# Save an
-00002d00: 616c 7973 6973 2072 6573 756c 7473 5c6e  alysis results\n
-00002d10: 222c 0d0a 2020 2020 2264 6652 6573 203d  ",..    "dfRes =
-00002d20: 2072 6573 756c 7473 2e64 6644 6174 615c   results.dfData\
-00002d30: 6e22 2c0d 0a20 2020 2022 5c6e 222c 0d0a  n",..    "\n",..
-00002d40: 2020 2020 2264 6652 6573 2e68 6561 6428      "dfRes.head(
-00002d50: 2922 0d0a 2020 205d 0d0a 2020 7d2c 0d0a  )"..   ]..  },..
-00002d60: 2020 7b0d 0a20 2020 2263 656c 6c5f 7479    {..   "cell_ty
-00002d70: 7065 223a 2022 6d61 726b 646f 776e 222c  pe": "markdown",
-00002d80: 0d0a 2020 2022 6d65 7461 6461 7461 223a  ..   "metadata":
-00002d90: 207b 7d2c 0d0a 2020 2022 736f 7572 6365   {},..   "source
-00002da0: 223a 205b 0d0a 2020 2020 2223 2320 632e  ": [..    "## c.
-00002db0: 2043 6f6d 7061 7261 6973 6f6e 2064 6573   Comparaison des
-00002dc0: 2072 c3a9 7375 6c74 6174 7320 c3a0 206c   r..sultats .. l
-00002dd0: 6120 72c3 a966 c3a9 7265 6e63 655c 6e22  a r..f..rence\n"
-00002de0: 2c0d 0a20 2020 2022 5c6e 222c 0d0a 2020  ,..    "\n",..  
-00002df0: 2020 2228 706f 7572 2063 6861 7175 6520    "(pour chaque 
-00002e00: 6772 6f75 7065 207b 2065 7370 c3a8 6365  groupe { esp..ce
-00002e10: 2c20 c3a9 6368 616e 7469 6c6c 6f6e 2c20  , ..chantillon, 
-00002e20: 6d6f 64c3 a86c 6520 7d2c 206c 6120 31c3  mod..le }, la 1.
-00002e30: a872 6520 7661 7269 616e 7465 2064 6520  .re variante de 
-00002e40: 7472 6929 220d 0a20 2020 5d0d 0a20 207d  tri)"..   ]..  }
-00002e50: 2c0d 0a20 207b 0d0a 2020 2022 6365 6c6c  ,..  {..   "cell
-00002e60: 5f74 7970 6522 3a20 2263 6f64 6522 2c0d  _type": "code",.
-00002e70: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
-00002e80: 6f75 6e74 223a 206e 756c 6c2c 0d0a 2020  ount": null,..  
-00002e90: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
-00002ea0: 0d0a 2020 2022 6f75 7470 7574 7322 3a20  ..   "outputs": 
-00002eb0: 5b5d 2c0d 0a20 2020 2273 6f75 7263 6522  [],..   "source"
-00002ec0: 3a20 5b0d 0a20 2020 2022 2320 5265 6d6f  : [..    "# Remo
-00002ed0: 7665 2075 7365 6c65 7373 2063 6f6c 756d  ve useless colum
-00002ee0: 6e73 2066 6f72 2063 6f6d 7061 7269 736f  ns for compariso
-00002ef0: 6e5c 6e22 2c0d 0a20 2020 2022 6466 5265  n\n",..    "dfRe
-00002f00: 7334 6320 3d20 6466 5265 732e 636f 7079  s4c = dfRes.copy
-00002f10: 2829 5c6e 222c 0d0a 2020 2020 2264 6652  ()\n",..    "dfR
-00002f20: 6573 3463 2e64 726f 7028 636f 6c75 6d6e  es4c.drop(column
-00002f30: 733d 5b28 2772 756e 206f 7574 7075 7427  s=[('run output'
-00002f40: 2c20 2772 756e 2074 696d 6527 2c20 2756  , 'run time', 'V
-00002f50: 616c 7565 2729 2c20 2827 7275 6e20 6f75  alue'), ('run ou
-00002f60: 7470 7574 272c 2027 7275 6e20 666f 6c64  tput', 'run fold
-00002f70: 6572 272c 2027 5661 6c75 6527 292c 5c6e  er', 'Value'),\n
-00002f80: 222c 0d0a 2020 2020 2220 2020 2020 2020  ",..    "       
-00002f90: 2020 2020 2020 2020 2020 2020 2020 2028                 (
-00002fa0: 2764 6574 6563 7469 6f6e 2070 726f 6261  'detection proba
-00002fb0: 6269 6c69 7479 272c 2027 6b65 7920 6675  bility', 'key fu
-00002fc0: 6e63 7469 6f6e 2074 7970 6527 2c20 2756  nction type', 'V
-00002fd0: 616c 7565 2729 2c5c 6e22 2c0d 0a20 2020  alue'),\n",..   
-00002fe0: 2022 2020 2020 2020 2020 2020 2020 2020   "              
-00002ff0: 2020 2020 2020 2020 2827 6465 7465 6374          ('detect
-00003000: 696f 6e20 7072 6f62 6162 696c 6974 7927  ion probability'
-00003010: 2c20 2761 646a 7573 746d 656e 7420 7365  , 'adjustment se
-00003020: 7269 6573 2074 7970 6527 2c20 2756 616c  ries type', 'Val
-00003030: 7565 2729 5d2c 2069 6e70 6c61 6365 3d54  ue')], inplace=T
-00003040: 7275 6529 220d 0a20 2020 5d0d 0a20 207d  rue)"..   ]..  }
-00003050: 2c0d 0a20 207b 0d0a 2020 2022 6365 6c6c  ,..  {..   "cell
-00003060: 5f74 7970 6522 3a20 2263 6f64 6522 2c0d  _type": "code",.
-00003070: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
-00003080: 6f75 6e74 223a 206e 756c 6c2c 0d0a 2020  ount": null,..  
-00003090: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
-000030a0: 0d0a 2020 2022 6f75 7470 7574 7322 3a20  ..   "outputs": 
-000030b0: 5b5d 2c0d 0a20 2020 2273 6f75 7263 6522  [],..   "source"
-000030c0: 3a20 5b0d 0a20 2020 2022 2320 436f 6d70  : [..    "# Comp
-000030d0: 6172 6520 6461 7461 206f 7264 6572 2076  are data order v
-000030e0: 6172 6961 6e74 2072 6573 756c 7473 5c6e  ariant results\n
-000030f0: 222c 0d0a 2020 2020 226d 6947 726f 7570  ",..    "miGroup
-00003100: 436f 6c73 203d 205c 5c5c 6e22 2c0d 0a20  Cols = \\\n",.. 
-00003110: 2020 2022 2020 2020 7064 2e4d 756c 7469     "    pd.Multi
-00003120: 496e 6465 782e 6672 6f6d 5f74 7570 6c65  Index.from_tuple
-00003130: 7328 5b28 2773 616d 706c 6527 2c20 636f  s([('sample', co
-00003140: 6c2c 2027 5661 6c75 6527 2920 666f 7220  l, 'Value') for 
-00003150: 636f 6c20 696e 2063 6173 6549 6443 6f6c  col in caseIdCol
-00003160: 7320 6966 2063 6f6c 2021 3d20 2744 6174  s if col != 'Dat
-00003170: 614f 7264 6572 275d 2920 5c5c 5c6e 222c  aOrder']) \\\n",
-00003180: 0d0a 2020 2020 2220 2020 2020 2020 2020  ..    "         
-00003190: 2020 2020 2020 2020 2e61 7070 656e 6428          .append(
-000031a0: 7064 2e4d 756c 7469 496e 6465 782e 6672  pd.MultiIndex.fr
-000031b0: 6f6d 5f74 7570 6c65 7328 5b28 2770 6172  om_tuples([('par
-000031c0: 616d 6574 6572 7327 2c20 636f 6c2c 2027  ameters', col, '
-000031d0: 5661 6c75 6527 2920 5c5c 5c6e 222c 0d0a  Value') \\\n",..
-000031e0: 2020 2020 2220 2020 2020 2020 2020 2020      "           
-000031f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003210: 2020 2020 2020 2020 2066 6f72 2063 6f6c           for col
-00003220: 2069 6e20 6466 5265 735b 2770 6172 616d   in dfRes['param
-00003230: 6574 6572 7327 5d2e 636f 6c75 6d6e 732e  eters'].columns.
-00003240: 6765 745f 6c65 7665 6c5f 7661 6c75 6573  get_level_values
-00003250: 2830 295d 2929 5c6e 222c 0d0a 2020 2020  (0)]))\n",..    
-00003260: 2269 6e64 6578 436f 6c73 203d 206d 6947  "indexCols = miG
-00003270: 726f 7570 436f 6c73 2e74 6f5f 6c69 7374  roupCols.to_list
-00003280: 2829 202b 205b 2827 7361 6d70 6c65 272c  () + [('sample',
-00003290: 2027 4461 7461 4f72 6465 7227 2c20 2756   'DataOrder', 'V
-000032a0: 616c 7565 2729 5d5c 6e22 2c0d 0a20 2020  alue')]\n",..   
-000032b0: 2022 6466 5265 6c44 6966 203d 2064 6652   "dfRelDif = dfR
-000032c0: 6573 3463 2e67 726f 7570 6279 286d 6947  es4c.groupby(miG
-000032d0: 726f 7570 436f 6c73 2e74 6f5f 6c69 7374  roupCols.to_list
-000032e0: 2829 292e 6170 706c 7928 7661 7269 616e  ()).apply(varian
-000032f0: 7443 6c6f 7365 6e65 7373 2c20 696e 6465  tCloseness, inde
-00003300: 7843 6f6c 733d 696e 6465 7843 6f6c 732c  xCols=indexCols,
-00003310: 2072 6566 4c61 6265 6c49 6e64 3d30 295c   refLabelInd=0)\
-00003320: 6e22 2c0d 0a20 2020 2022 5c6e 222c 0d0a  n",..    "\n",..
-00003330: 2020 2020 2264 6652 656c 4469 6622 0d0a      "dfRelDif"..
-00003340: 2020 205d 0d0a 2020 7d2c 0d0a 2020 7b0d     ]..  },..  {.
-00003350: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
-00003360: 2022 6d61 726b 646f 776e 222c 0d0a 2020   "markdown",..  
-00003370: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
-00003380: 0d0a 2020 2022 736f 7572 6365 223a 205b  ..   "source": [
-00003390: 0d0a 2020 2020 2223 2320 642e 2053 6175  ..    "## d. Sau
-000033a0: 7665 6761 7264 6520 6465 7320 72c3 a973  vegarde des r..s
-000033b0: 756c 7461 7473 2e22 0d0a 2020 205d 0d0a  ultats."..   ]..
-000033c0: 2020 7d2c 0d0a 2020 7b0d 0a20 2020 2263    },..  {..   "c
-000033d0: 656c 6c5f 7479 7065 223a 2022 636f 6465  ell_type": "code
-000033e0: 222c 0d0a 2020 2022 6578 6563 7574 696f  ",..   "executio
-000033f0: 6e5f 636f 756e 7422 3a20 6e75 6c6c 2c0d  n_count": null,.
-00003400: 0a20 2020 226d 6574 6164 6174 6122 3a20  .   "metadata": 
-00003410: 7b7d 2c0d 0a20 2020 226f 7574 7075 7473  {},..   "outputs
-00003420: 223a 205b 5d2c 0d0a 2020 2022 736f 7572  ": [],..   "sour
-00003430: 6365 223a 205b 0d0a 2020 2020 2272 6573  ce": [..    "res
-00003440: 4669 6c65 4e61 6d65 203d 206f 732e 7061  FileName = os.pa
-00003450: 7468 2e6a 6f69 6e28 6d63 6473 2e77 6f72  th.join(mcds.wor
-00003460: 6b44 6972 2c20 2741 4344 4332 3031 392d  kDir, 'ACDC2019-
-00003470: 5061 7079 7275 732d 6175 746f 2d73 656e  Papyrus-auto-sen
-00003480: 732d 6461 7461 2d6f 7264 6572 2d72 6573  s-data-order-res
-00003490: 756c 7473 2e78 6c73 7827 295c 6e22 2c0d  ults.xlsx')\n",.
-000034a0: 0a20 2020 2022 5c6e 222c 0d0a 2020 2020  .    "\n",..    
-000034b0: 2277 6974 6820 7064 2e45 7863 656c 5772  "with pd.ExcelWr
-000034c0: 6974 6572 2872 6573 4669 6c65 4e61 6d65  iter(resFileName
-000034d0: 2920 6173 2078 6c73 7857 7269 7465 723a  ) as xlsxWriter:
-000034e0: 5c6e 222c 0d0a 2020 2020 225c 6e22 2c0d  \n",..    "\n",.
-000034f0: 0a20 2020 2022 2020 2020 6466 5265 732e  .    "    dfRes.
-00003500: 746f 5f65 7863 656c 2878 6c73 7857 7269  to_excel(xlsxWri
-00003510: 7465 722c 2073 6865 6574 5f6e 616d 653d  ter, sheet_name=
-00003520: 2752 6177 5265 7375 6c74 7327 2c20 696e  'RawResults', in
-00003530: 6465 783d 5472 7565 295c 6e22 2c0d 0a20  dex=True)\n",.. 
-00003540: 2020 2022 2020 2020 6466 5265 6c44 6966     "    dfRelDif
-00003550: 2e74 6f5f 6578 6365 6c28 786c 7378 5772  .to_excel(xlsxWr
-00003560: 6974 6572 2c20 7368 6565 745f 6e61 6d65  iter, sheet_name
-00003570: 3d27 4469 6666 3252 6566 272c 2069 6e64  ='Diff2Ref', ind
-00003580: 6578 3d54 7275 6529 220d 0a20 2020 5d0d  ex=True)"..   ].
-00003590: 0a20 207d 2c0d 0a20 207b 0d0a 2020 2022  .  },..  {..   "
-000035a0: 6365 6c6c 5f74 7970 6522 3a20 2263 6f64  cell_type": "cod
-000035b0: 6522 2c0d 0a20 2020 2265 7865 6375 7469  e",..   "executi
-000035c0: 6f6e 5f63 6f75 6e74 223a 206e 756c 6c2c  on_count": null,
-000035d0: 0d0a 2020 2022 6d65 7461 6461 7461 223a  ..   "metadata":
-000035e0: 207b 7d2c 0d0a 2020 2022 6f75 7470 7574   {},..   "output
-000035f0: 7322 3a20 5b5d 2c0d 0a20 2020 2273 6f75  s": [],..   "sou
-00003600: 7263 6522 3a20 5b5d 0d0a 2020 7d2c 0d0a  rce": []..  },..
-00003610: 2020 7b0d 0a20 2020 2263 656c 6c5f 7479    {..   "cell_ty
-00003620: 7065 223a 2022 6d61 726b 646f 776e 222c  pe": "markdown",
-00003630: 0d0a 2020 2022 6d65 7461 6461 7461 223a  ..   "metadata":
-00003640: 207b 7d2c 0d0a 2020 2022 736f 7572 6365   {},..   "source
-00003650: 223a 205b 0d0a 2020 2020 2223 2053 656e  ": [..    "# Sen
-00003660: 7369 6269 6c69 74c3 a920 6465 204d 4344  sibilit.. de MCD
-00003670: 532e 6578 6520 c3a0 206c 276f 7264 7265  S.exe .. l'ordre
-00003680: 2064 6573 2064 6f6e 6ec3 a965 7320 2862   des donn..es (b
-00003690: 6973 295c 6e22 2c0d 0a20 2020 2022 5c6e  is)\n",..    "\n
-000036a0: 222c 0d0a 2020 2020 2243 6f6e 7374 7275  ",..    "Constru
-000036b0: 6374 696f 6e20 7365 6d69 2d6d 616e 7565  ction semi-manue
-000036c0: 6c6c 6520 6427 756e 2065 7865 6d70 6c65  lle d'un exemple
-000036d0: 2064 6520 7461 696c 6c65 2072 c3a9 6475   de taille r..du
-000036e0: 6974 6520 736f 756d 6973 6520 c3a0 2045  ite soumise .. E
-000036f0: 7269 6320 5265 7873 7461 6474 203a 5c6e  ric Rexstadt :\n
-00003700: 222c 0d0a 2020 2020 2220 2043 662e 2072  ",..    "  Cf. r
-00003710: 6566 6f75 742f 6469 7374 2d6f 7264 6572  efout/dist-order
-00003720: 2d73 656e 732d 6d69 6e2f 6469 7374 2d6f  -sens-min/dist-o
-00003730: 7264 6572 2d73 656e 732e 6f64 7422 0d0a  rder-sens.odt"..
-00003740: 2020 205d 0d0a 2020 7d2c 0d0a 2020 7b0d     ]..  },..  {.
-00003750: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
-00003760: 2022 6d61 726b 646f 776e 222c 0d0a 2020   "markdown",..  
-00003770: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
-00003780: 0d0a 2020 2022 736f 7572 6365 223a 205b  ..   "source": [
-00003790: 0d0a 2020 2020 2223 204f 7264 7265 2064  ..    "# Ordre d
-000037a0: 6573 2064 6f6e 6ec3 a965 7320 67c3 a96e  es donn..es g..n
-000037b0: c3a9 72c3 a965 7320 7061 7220 4469 7374  ..r..es par Dist
-000037c0: 616e 6365 2037 2070 6f75 7220 4d43 4453  ance 7 pour MCDS
-000037d0: 2e65 7865 220d 0a20 2020 5d0d 0a20 207d  .exe"..   ]..  }
-000037e0: 2c0d 0a20 207b 0d0a 2020 2022 6365 6c6c  ,..  {..   "cell
-000037f0: 5f74 7970 6522 3a20 2263 6f64 6522 2c0d  _type": "code",.
-00003800: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
-00003810: 6f75 6e74 223a 206e 756c 6c2c 0d0a 2020  ount": null,..  
-00003820: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
-00003830: 0d0a 2020 2022 6f75 7470 7574 7322 3a20  ..   "outputs": 
-00003840: 5b5d 2c0d 0a20 2020 2273 6f75 7263 6522  [],..   "source"
-00003850: 3a20 5b0d 0a20 2020 2022 6466 203d 2070  : [..    "df = p
-00003860: 642e 7265 6164 5f63 7376 2827 7265 666f  d.read_csv('refo
-00003870: 7574 2f64 6973 742d 6f72 6465 722d 7365  ut/dist-order-se
-00003880: 6e73 2d6d 696e 2f63 6d64 2d77 696e 372d  ns-min/cmd-win7-
-00003890: 6469 7374 2d6f 7264 6572 2f64 6174 612e  dist-order/data.
-000038a0: 7478 7427 2c20 7365 703d 275c 5c74 272c  txt', sep='\\t',
-000038b0: 5c6e 222c 0d0a 2020 2020 2220 2020 2020  \n",..    "     
-000038c0: 2020 2020 2020 2020 2020 2020 6e61 6d65              name
-000038d0: 733d 5b27 7265 6769 6f6e 272c 2027 6172  s=['region', 'ar
-000038e0: 6561 272c 2027 706f 696e 7427 2c20 2765  ea', 'point', 'e
-000038f0: 6666 6f72 7427 2c20 2764 6973 7461 6e63  ffort', 'distanc
-00003900: 6527 5d29 5c6e 222c 0d0a 2020 2020 2264  e'])\n",..    "d
-00003910: 662e 6865 6164 2832 3029 220d 0a20 2020  f.head(20)"..   
-00003920: 5d0d 0a20 207d 2c0d 0a20 207b 0d0a 2020  ]..  },..  {..  
-00003930: 2022 6365 6c6c 5f74 7970 6522 3a20 2263   "cell_type": "c
-00003940: 6f64 6522 2c0d 0a20 2020 2265 7865 6375  ode",..   "execu
-00003950: 7469 6f6e 5f63 6f75 6e74 223a 206e 756c  tion_count": nul
-00003960: 6c2c 0d0a 2020 2022 6d65 7461 6461 7461  l,..   "metadata
-00003970: 223a 207b 7d2c 0d0a 2020 2022 6f75 7470  ": {},..   "outp
-00003980: 7574 7322 3a20 5b5d 2c0d 0a20 2020 2273  uts": [],..   "s
-00003990: 6f75 7263 6522 3a20 5b0d 0a20 2020 2022  ource": [..    "
-000039a0: 6466 5b27 6e70 6f69 6e74 275d 203d 2064  df['npoint'] = d
-000039b0: 662e 706f 696e 742e 6170 706c 7928 6c61  f.point.apply(la
-000039c0: 6d62 6461 2073 3a20 696e 7428 732e 7370  mbda s: int(s.sp
-000039d0: 6c69 7428 2720 2729 5b31 5d29 2922 0d0a  lit(' ')[1]))"..
-000039e0: 2020 205d 0d0a 2020 7d2c 0d0a 2020 7b0d     ]..  },..  {.
-000039f0: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
-00003a00: 2022 636f 6465 222c 0d0a 2020 2022 6578   "code",..   "ex
-00003a10: 6563 7574 696f 6e5f 636f 756e 7422 3a20  ecution_count": 
-00003a20: 6e75 6c6c 2c0d 0a20 2020 226d 6574 6164  null,..   "metad
-00003a30: 6174 6122 3a20 7b7d 2c0d 0a20 2020 226f  ata": {},..   "o
-00003a40: 7574 7075 7473 223a 205b 5d2c 0d0a 2020  utputs": [],..  
-00003a50: 2022 736f 7572 6365 223a 205b 0d0a 2020   "source": [..  
-00003a60: 2020 2223 2043 6861 6e67 656d 656e 7420    "# Changement 
-00003a70: 6465 206c 276f 7264 7265 203a 2074 7269  de l'ordre : tri
-00003a80: 2070 6172 2070 6f69 6e74 2065 7420 7061   par point et pa
-00003a90: 7220 6469 7374 616e 6365 7320 6372 6f69  r distances croi
-00003aa0: 7373 616e 7465 735c 6e22 2c0d 0a20 2020  ssantes\n",..   
-00003ab0: 2022 6466 2e73 6f72 745f 7661 6c75 6573   "df.sort_values
-00003ac0: 2862 793d 5b27 6e70 6f69 6e74 272c 2027  (by=['npoint', '
-00003ad0: 6469 7374 616e 6365 275d 2c20 696e 706c  distance'], inpl
-00003ae0: 6163 653d 5472 7565 295c 6e22 2c0d 0a20  ace=True)\n",.. 
-00003af0: 2020 2022 6466 2e68 6561 6428 3230 2922     "df.head(20)"
-00003b00: 0d0a 2020 205d 0d0a 2020 7d2c 0d0a 2020  ..   ]..  },..  
-00003b10: 7b0d 0a20 2020 2263 656c 6c5f 7479 7065  {..   "cell_type
-00003b20: 223a 2022 636f 6465 222c 0d0a 2020 2022  ": "code",..   "
-00003b30: 6578 6563 7574 696f 6e5f 636f 756e 7422  execution_count"
-00003b40: 3a20 6e75 6c6c 2c0d 0a20 2020 226d 6574  : null,..   "met
-00003b50: 6164 6174 6122 3a20 7b7d 2c0d 0a20 2020  adata": {},..   
-00003b60: 226f 7574 7075 7473 223a 205b 5d2c 0d0a  "outputs": [],..
-00003b70: 2020 2022 736f 7572 6365 223a 205b 0d0a     "source": [..
-00003b80: 2020 2020 2270 6c2e 5061 7468 2827 7265      "pl.Path('re
-00003b90: 666f 7574 2f64 6973 742d 6f72 6465 722d  fout/dist-order-
-00003ba0: 7365 6e73 2d6d 696e 2f63 6d64 2d77 696e  sens-min/cmd-win
-00003bb0: 372d 736f 7274 6564 2d6f 7264 6572 2729  7-sorted-order')
-00003bc0: 2e6d 6b64 6972 2865 7869 7374 5f6f 6b3d  .mkdir(exist_ok=
-00003bd0: 5472 7565 295c 6e22 2c0d 0a20 2020 2022  True)\n",..    "
-00003be0: 6466 5b5b 2772 6567 696f 6e27 2c20 2761  df[['region', 'a
-00003bf0: 7265 6127 2c20 2770 6f69 6e74 272c 2027  rea', 'point', '
-00003c00: 6566 666f 7274 272c 2027 6469 7374 616e  effort', 'distan
-00003c10: 6365 275d 5d20 5c5c 5c6e 222c 0d0a 2020  ce']] \\\n",..  
-00003c20: 2020 2220 202e 746f 5f63 7376 2827 7265    "  .to_csv('re
-00003c30: 666f 7574 2f64 6973 742d 6f72 6465 722d  fout/dist-order-
-00003c40: 7365 6e73 2d6d 696e 2f63 6d64 2d77 696e  sens-min/cmd-win
-00003c50: 372d 736f 7274 6564 2d6f 7264 6572 2f64  7-sorted-order/d
-00003c60: 6174 612e 7478 7427 2c20 7365 703d 275c  ata.txt', sep='\
-00003c70: 5c74 272c 2069 6e64 6578 3d46 616c 7365  \t', index=False
-00003c80: 2c20 6865 6164 6572 3d46 616c 7365 2922  , header=False)"
-00003c90: 0d0a 2020 205d 0d0a 2020 7d2c 0d0a 2020  ..   ]..  },..  
-00003ca0: 7b0d 0a20 2020 2263 656c 6c5f 7479 7065  {..   "cell_type
-00003cb0: 223a 2022 636f 6465 222c 0d0a 2020 2022  ": "code",..   "
-00003cc0: 6578 6563 7574 696f 6e5f 636f 756e 7422  execution_count"
-00003cd0: 3a20 6e75 6c6c 2c0d 0a20 2020 226d 6574  : null,..   "met
-00003ce0: 6164 6174 6122 3a20 7b7d 2c0d 0a20 2020  adata": {},..   
-00003cf0: 226f 7574 7075 7473 223a 205b 5d2c 0d0a  "outputs": [],..
-00003d00: 2020 2022 736f 7572 6365 223a 205b 5d0d     "source": [].
-00003d10: 0a20 207d 2c0d 0a20 207b 0d0a 2020 2022  .  },..  {..   "
-00003d20: 6365 6c6c 5f74 7970 6522 3a20 226d 6172  cell_type": "mar
-00003d30: 6b64 6f77 6e22 2c0d 0a20 2020 226d 6574  kdown",..   "met
-00003d40: 6164 6174 6122 3a20 7b7d 2c0d 0a20 2020  adata": {},..   
-00003d50: 2273 6f75 7263 6522 3a20 5b0d 0a20 2020  "source": [..   
-00003d60: 2022 2320 4f72 6472 6520 6465 7320 646f   "# Ordre des do
-00003d70: 6e6e c3a9 6573 2065 6e20 656e 7472 c3a9  nn..es en entr..
-00003d80: 6520 6465 2064 6973 7461 6e63 6522 0d0a  e de distance"..
-00003d90: 2020 205d 0d0a 2020 7d2c 0d0a 2020 7b0d     ]..  },..  {.
-00003da0: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
-00003db0: 2022 636f 6465 222c 0d0a 2020 2022 6578   "code",..   "ex
-00003dc0: 6563 7574 696f 6e5f 636f 756e 7422 3a20  ecution_count": 
-00003dd0: 6e75 6c6c 2c0d 0a20 2020 226d 6574 6164  null,..   "metad
-00003de0: 6174 6122 3a20 7b7d 2c0d 0a20 2020 226f  ata": {},..   "o
-00003df0: 7574 7075 7473 223a 205b 5d2c 0d0a 2020  utputs": [],..  
-00003e00: 2022 736f 7572 6365 223a 205b 0d0a 2020   "source": [..  
-00003e10: 2020 2223 204d c3aa 6d65 2066 6963 6869    "# M..me fichi
-00003e20: 6572 2065 7861 6374 656d 656e 7420 7175  er exactement qu
-00003e30: 6520 7265 666f 7574 2f64 6973 742d 6f72  e refout/dist-or
-00003e40: 6465 722d 7365 6e73 2d6d 696e 2f69 6d70  der-sens-min/imp
-00003e50: 6f72 742d 6461 7461 2d73 6574 2e74 7874  ort-data-set.txt
-00003e60: 206e 6f72 6d61 6c65 6d65 6e74 2e5c 6e22   normalement.\n"
-00003e70: 2c0d 0a20 2020 2022 6466 203d 2070 642e  ,..    "df = pd.
-00003e80: 7265 6164 5f63 7376 2827 7265 6669 6e2f  read_csv('refin/
-00003e90: 4143 4443 3230 3139 2d50 6170 7972 7573  ACDC2019-Papyrus
-00003ea0: 2d54 5552 4d45 522d 4142 2d31 306d 6e2d  -TURMER-AB-10mn-
-00003eb0: 3164 6563 2d64 6973 742e 7478 7427 2c20  1dec-dist.txt', 
-00003ec0: 7365 703d 275c 5c74 272c 2068 6561 6465  sep='\\t', heade
-00003ed0: 723d 3029 5c6e 222c 0d0a 2020 2020 2264  r=0)\n",..    "d
-00003ee0: 662e 6865 6164 2832 3029 220d 0a20 2020  f.head(20)"..   
-00003ef0: 5d0d 0a20 207d 2c0d 0a20 207b 0d0a 2020  ]..  },..  {..  
-00003f00: 2022 6365 6c6c 5f74 7970 6522 3a20 2263   "cell_type": "c
-00003f10: 6f64 6522 2c0d 0a20 2020 2265 7865 6375  ode",..   "execu
-00003f20: 7469 6f6e 5f63 6f75 6e74 223a 206e 756c  tion_count": nul
-00003f30: 6c2c 0d0a 2020 2022 6d65 7461 6461 7461  l,..   "metadata
-00003f40: 223a 207b 7d2c 0d0a 2020 2022 6f75 7470  ": {},..   "outp
-00003f50: 7574 7322 3a20 5b5d 2c0d 0a20 2020 2273  uts": [],..   "s
-00003f60: 6f75 7263 6522 3a20 5b0d 0a20 2020 2022  ource": [..    "
-00003f70: 2320 4368 616e 6765 6d65 6e74 2064 6520  # Changement de 
-00003f80: 6c27 6f72 6472 6520 3a20 7472 6920 7061  l'ordre : tri pa
-00003f90: 7220 7061 7220 6469 7374 616e 6365 7320  r par distances 
-00003fa0: 616c 7068 6162 c3a9 7469 7175 6573 2063  alphab..tiques c
-00003fb0: 726f 6973 7361 6e74 6573 2c20 6f75 692c  roissantes, oui,
-00003fc0: 206f 7569 2028 656e 2069 676e 6f72 616e   oui (en ignoran
-00003fd0: 7420 6c65 7320 706f 696e 7473 295c 6e22  t les points)\n"
-00003fe0: 2c0d 0a20 2020 2022 2320 4275 743a 2056  ,..    "# But: V
-00003ff0: 6f69 7220 7369 2044 6973 7461 6e63 6520  oir si Distance 
-00004000: 7265 636c 6173 7365 2061 7574 7420 7061  reclasse autt pa
-00004010: 7220 706f 696e 745c 6e22 2c0d 0a20 2020  r point\n",..   
-00004020: 2022 6466 2e73 6f72 745f 7661 6c75 6573   "df.sort_values
-00004030: 2862 793d 5b27 4f62 7365 7276 6174 696f  (by=['Observatio
-00004040: 6e2a 5261 6469 616c 2064 6973 7461 6e63  n*Radial distanc
-00004050: 6527 5d2c 2069 6e70 6c61 6365 3d54 7275  e'], inplace=Tru
-00004060: 6529 5c6e 222c 0d0a 2020 2020 2264 662e  e)\n",..    "df.
-00004070: 6865 6164 2832 3029 220d 0a20 2020 5d0d  head(20)"..   ].
-00004080: 0a20 207d 2c0d 0a20 207b 0d0a 2020 2022  .  },..  {..   "
-00004090: 6365 6c6c 5f74 7970 6522 3a20 2263 6f64  cell_type": "cod
-000040a0: 6522 2c0d 0a20 2020 2265 7865 6375 7469  e",..   "executi
-000040b0: 6f6e 5f63 6f75 6e74 223a 206e 756c 6c2c  on_count": null,
-000040c0: 0d0a 2020 2022 6d65 7461 6461 7461 223a  ..   "metadata":
-000040d0: 207b 7d2c 0d0a 2020 2022 6f75 7470 7574   {},..   "output
-000040e0: 7322 3a20 5b5d 2c0d 0a20 2020 2273 6f75  s": [],..   "sou
-000040f0: 7263 6522 3a20 5b0d 0a20 2020 2022 6466  rce": [..    "df
-00004100: 2e74 6f5f 6373 7628 2774 6d70 2f41 4344  .to_csv('tmp/ACD
-00004110: 4332 3031 392d 5061 7079 7275 732d 5455  C2019-Papyrus-TU
-00004120: 524d 4552 2d41 422d 3130 6d6e 2d31 6465  RMER-AB-10mn-1de
-00004130: 632d 7472 6961 6c70 6861 2d64 6973 742e  c-trialpha-dist.
-00004140: 7478 7427 2c20 7365 703d 275c 5c74 272c  txt', sep='\\t',
-00004150: 2069 6e64 6578 3d46 616c 7365 2922 0d0a   index=False)"..
-00004160: 2020 205d 0d0a 2020 7d2c 0d0a 2020 7b0d     ]..  },..  {.
-00004170: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
-00004180: 2022 636f 6465 222c 0d0a 2020 2022 6578   "code",..   "ex
-00004190: 6563 7574 696f 6e5f 636f 756e 7422 3a20  ecution_count": 
-000041a0: 6e75 6c6c 2c0d 0a20 2020 226d 6574 6164  null,..   "metad
-000041b0: 6174 6122 3a20 7b7d 2c0d 0a20 2020 226f  ata": {},..   "o
-000041c0: 7574 7075 7473 223a 205b 5d2c 0d0a 2020  utputs": [],..  
-000041d0: 2022 736f 7572 6365 223a 205b 5d0d 0a20   "source": [].. 
-000041e0: 207d 2c0d 0a20 207b 0d0a 2020 2022 6365   },..  {..   "ce
-000041f0: 6c6c 5f74 7970 6522 3a20 2263 6f64 6522  ll_type": "code"
-00004200: 2c0d 0a20 2020 2265 7865 6375 7469 6f6e  ,..   "execution
-00004210: 5f63 6f75 6e74 223a 206e 756c 6c2c 0d0a  _count": null,..
-00004220: 2020 2022 6d65 7461 6461 7461 223a 207b     "metadata": {
-00004230: 7d2c 0d0a 2020 2022 6f75 7470 7574 7322  },..   "outputs"
-00004240: 3a20 5b5d 2c0d 0a20 2020 2273 6f75 7263  : [],..   "sourc
-00004250: 6522 3a20 5b5d 0d0a 2020 7d2c 0d0a 2020  e": []..  },..  
-00004260: 7b0d 0a20 2020 2263 656c 6c5f 7479 7065  {..   "cell_type
-00004270: 223a 2022 6d61 726b 646f 776e 222c 0d0a  ": "markdown",..
-00004280: 2020 2022 6d65 7461 6461 7461 223a 207b     "metadata": {
-00004290: 7d2c 0d0a 2020 2022 736f 7572 6365 223a  },..   "source":
-000042a0: 205b 0d0a 2020 2020 2223 2042 6163 20c3   [..    "# Bac .
-000042b0: a020 7361 626c 6522 0d0a 2020 205d 0d0a  . sable"..   ]..
-000042c0: 2020 7d2c 0d0a 2020 7b0d 0a20 2020 2263    },..  {..   "c
-000042d0: 656c 6c5f 7479 7065 223a 2022 636f 6465  ell_type": "code
-000042e0: 222c 0d0a 2020 2022 6578 6563 7574 696f  ",..   "executio
-000042f0: 6e5f 636f 756e 7422 3a20 6e75 6c6c 2c0d  n_count": null,.
-00004300: 0a20 2020 226d 6574 6164 6174 6122 3a20  .   "metadata": 
-00004310: 7b7d 2c0d 0a20 2020 226f 7574 7075 7473  {},..   "outputs
-00004320: 223a 205b 5d2c 0d0a 2020 2022 736f 7572  ": [],..   "sour
-00004330: 6365 223a 205b 5d0d 0a20 207d 2c0d 0a20  ce": []..  },.. 
-00004340: 207b 0d0a 2020 2022 6365 6c6c 5f74 7970   {..   "cell_typ
-00004350: 6522 3a20 2263 6f64 6522 2c0d 0a20 2020  e": "code",..   
-00004360: 2265 7865 6375 7469 6f6e 5f63 6f75 6e74  "execution_count
-00004370: 223a 206e 756c 6c2c 0d0a 2020 2022 6d65  ": null,..   "me
-00004380: 7461 6461 7461 223a 207b 7d2c 0d0a 2020  tadata": {},..  
-00004390: 2022 6f75 7470 7574 7322 3a20 5b5d 2c0d   "outputs": [],.
-000043a0: 0a20 2020 2273 6f75 7263 6522 3a20 5b5d  .   "source": []
-000043b0: 0d0a 2020 7d0d 0a20 5d2c 0d0a 2022 6d65  ..  }.. ],.. "me
-000043c0: 7461 6461 7461 223a 207b 0d0a 2020 226b  tadata": {..  "k
-000043d0: 6572 6e65 6c73 7065 6322 3a20 7b0d 0a20  ernelspec": {.. 
-000043e0: 2020 2264 6973 706c 6179 5f6e 616d 6522    "display_name"
-000043f0: 3a20 2250 7974 686f 6e20 3322 2c0d 0a20  : "Python 3",.. 
-00004400: 2020 226c 616e 6775 6167 6522 3a20 2270    "language": "p
-00004410: 7974 686f 6e22 2c0d 0a20 2020 226e 616d  ython",..   "nam
-00004420: 6522 3a20 2270 7974 686f 6e33 220d 0a20  e": "python3".. 
-00004430: 207d 2c0d 0a20 2022 6c61 6e67 7561 6765   },..  "language
-00004440: 5f69 6e66 6f22 3a20 7b0d 0a20 2020 2263  _info": {..   "c
-00004450: 6f64 656d 6972 726f 725f 6d6f 6465 223a  odemirror_mode":
-00004460: 207b 0d0a 2020 2020 226e 616d 6522 3a20   {..    "name": 
-00004470: 2269 7079 7468 6f6e 222c 0d0a 2020 2020  "ipython",..    
-00004480: 2276 6572 7369 6f6e 223a 2033 0d0a 2020  "version": 3..  
-00004490: 207d 2c0d 0a20 2020 2266 696c 655f 6578   },..   "file_ex
-000044a0: 7465 6e73 696f 6e22 3a20 222e 7079 222c  tension": ".py",
-000044b0: 0d0a 2020 2022 6d69 6d65 7479 7065 223a  ..   "mimetype":
-000044c0: 2022 7465 7874 2f78 2d70 7974 686f 6e22   "text/x-python"
-000044d0: 2c0d 0a20 2020 226e 616d 6522 3a20 2270  ,..   "name": "p
-000044e0: 7974 686f 6e22 2c0d 0a20 2020 226e 6263  ython",..   "nbc
-000044f0: 6f6e 7665 7274 5f65 7870 6f72 7465 7222  onvert_exporter"
-00004500: 3a20 2270 7974 686f 6e22 2c0d 0a20 2020  : "python",..   
-00004510: 2270 7967 6d65 6e74 735f 6c65 7865 7222  "pygments_lexer"
-00004520: 3a20 2269 7079 7468 6f6e 3322 2c0d 0a20  : "ipython3",.. 
-00004530: 2020 2276 6572 7369 6f6e 223a 2022 332e    "version": "3.
-00004540: 382e 3222 0d0a 2020 7d0d 0a20 7d2c 0d0a  8.2"..  }.. },..
-00004550: 2022 6e62 666f 726d 6174 223a 2034 2c0d   "nbformat": 4,.
-00004560: 0a20 226e 6266 6f72 6d61 745f 6d69 6e6f  . "nbformat_mino
-00004570: 7222 3a20 320d 0a7d 0d0a                 r": 2..}..
+000019c0: 2020 2063 6f6c 756d 6e73 3d63 6173 6549     columns=caseI
+000019d0: 6443 6f6c 7329 5c6e 222c 0a20 2020 2022  dCols)\n",.    "
+000019e0: 5c6e 222c 0a20 2020 2022 6466 416e 6c79  \n",.    "dfAnly
+000019f0: 7343 6173 6573 5b27 496e 4669 6c65 4e61  sCases['InFileNa
+00001a00: 6d65 275d 203d 205c 5c5c 6e22 2c0a 2020  me'] = \\\n",.  
+00001a10: 2020 2220 2020 2064 6641 6e6c 7973 4361    "    dfAnlysCa
+00001a20: 7365 732e 6170 706c 7928 6c61 6d62 6461  ses.apply(lambda
+00001a30: 2073 526f 773a 2027 4143 4443 3230 3139   sRow: 'ACDC2019
+00001a40: 2d50 6170 7972 7573 2d7b 7d2d 7b7d 2d64  -Papyrus-{}-{}-d
+00001a50: 6973 742e 7478 7427 2e66 6f72 6d61 7428  ist.txt'.format(
+00001a60: 7352 6f77 2e53 7065 6369 6573 2c20 7352  sRow.Species, sR
+00001a70: 6f77 2e53 616d 706c 6529 2c5c 6e22 2c0a  ow.Sample),\n",.
+00001a80: 2020 2020 2220 2020 2020 2020 2020 2020      "           
+00001a90: 2020 2020 2020 2020 2020 2020 6178 6973              axis
+00001aa0: 3d27 636f 6c75 6d6e 7327 295c 6e22 2c0a  ='columns')\n",.
+00001ab0: 2020 2020 225c 6e22 2c0a 2020 2020 2264      "\n",.    "d
+00001ac0: 6641 6e6c 7973 4361 7365 7322 0a20 2020  fAnlysCases".   
+00001ad0: 5d0a 2020 7d2c 0a20 207b 0a20 2020 2263  ].  },.  {.   "c
+00001ae0: 656c 6c5f 7479 7065 223a 2022 636f 6465  ell_type": "code
+00001af0: 222c 0a20 2020 2265 7865 6375 7469 6f6e  ",.   "execution
+00001b00: 5f63 6f75 6e74 223a 206e 756c 6c2c 0a20  _count": null,. 
+00001b10: 2020 226d 6574 6164 6174 6122 3a20 7b7d    "metadata": {}
+00001b20: 2c0a 2020 2022 6f75 7470 7574 7322 3a20  ,.   "outputs": 
+00001b30: 5b5d 2c0a 2020 2022 736f 7572 6365 223a  [],.   "source":
+00001b40: 205b 0a20 2020 2022 6173 7365 7274 2061   [.    "assert a
+00001b50: 6c6c 2870 6c2e 5061 7468 2827 7265 6669  ll(pl.Path('refi
+00001b60: 6e27 2c20 7343 6173 652e 496e 4669 6c65  n', sCase.InFile
+00001b70: 4e61 6d65 292e 6578 6973 7473 2829 205c  Name).exists() \
+00001b80: 5c5c 6e22 2c0a 2020 2020 2220 2020 2020  \\n",.    "     
+00001b90: 2020 2020 2020 666f 7220 5f2c 2073 4361        for _, sCa
+00001ba0: 7365 2069 6e20 6466 416e 6c79 7343 6173  se in dfAnlysCas
+00001bb0: 6573 2e69 7465 7272 6f77 7328 2929 2c20  es.iterrows()), 
+00001bc0: 274f 682c 206f 6820 2e2e 2e20 536f 6d65  'Oh, oh ... Some
+00001bd0: 206d 6973 7369 6e67 2066 696c 6528 7329   missing file(s)
+00001be0: 2021 2722 0a20 2020 5d0a 2020 7d2c 0a20   !'".   ].  },. 
+00001bf0: 207b 0a20 2020 2263 656c 6c5f 7479 7065   {.   "cell_type
+00001c00: 223a 2022 636f 6465 222c 0a20 2020 2265  ": "code",.   "e
+00001c10: 7865 6375 7469 6f6e 5f63 6f75 6e74 223a  xecution_count":
+00001c20: 206e 756c 6c2c 0a20 2020 226d 6574 6164   null,.   "metad
+00001c30: 6174 6122 3a20 7b7d 2c0a 2020 2022 6f75  ata": {},.   "ou
+00001c40: 7470 7574 7322 3a20 5b5d 2c0a 2020 2022  tputs": [],.   "
+00001c50: 736f 7572 6365 223a 205b 0a20 2020 2022  source": [.    "
+00001c60: 2364 6641 6e6c 7973 4361 7365 7320 3d20  #dfAnlysCases = 
+00001c70: 6466 416e 6c79 7343 6173 6573 5b3a 355d  dfAnlysCases[:5]
+00001c80: 220a 2020 205d 0a20 207d 2c0a 2020 7b0a  ".   ].  },.  {.
+00001c90: 2020 2022 6365 6c6c 5f74 7970 6522 3a20     "cell_type": 
+00001ca0: 2263 6f64 6522 2c0a 2020 2022 6578 6563  "code",.   "exec
+00001cb0: 7574 696f 6e5f 636f 756e 7422 3a20 6e75  ution_count": nu
+00001cc0: 6c6c 2c0a 2020 2022 6d65 7461 6461 7461  ll,.   "metadata
+00001cd0: 223a 207b 7d2c 0a20 2020 226f 7574 7075  ": {},.   "outpu
+00001ce0: 7473 223a 205b 5d2c 0a20 2020 2273 6f75  ts": [],.   "sou
+00001cf0: 7263 6522 3a20 5b0a 2020 2020 226c 656e  rce": [.    "len
+00001d00: 2864 6641 6e6c 7973 4361 7365 7329 220a  (dfAnlysCases)".
+00001d10: 2020 205d 0a20 207d 2c0a 2020 7b0a 2020     ].  },.  {.  
+00001d20: 2022 6365 6c6c 5f74 7970 6522 3a20 226d   "cell_type": "m
+00001d30: 6172 6b64 6f77 6e22 2c0a 2020 2022 6d65  arkdown",.   "me
+00001d40: 7461 6461 7461 223a 207b 7d2c 0a20 2020  tadata": {},.   
+00001d50: 2273 6f75 7263 6522 3a20 5b0a 2020 2020  "source": [.    
+00001d60: 2223 2320 622e 2045 78c3 a963 7574 696f  "## b. Ex..cutio
+00001d70: 6e20 6465 7320 616e 616c 7973 6573 220a  n des analyses".
+00001d80: 2020 205d 0a20 207d 2c0a 2020 7b0a 2020     ].  },.  {.  
+00001d90: 2022 6365 6c6c 5f74 7970 6522 3a20 2263   "cell_type": "c
+00001da0: 6f64 6522 2c0a 2020 2022 6578 6563 7574  ode",.   "execut
+00001db0: 696f 6e5f 636f 756e 7422 3a20 6e75 6c6c  ion_count": null
+00001dc0: 2c0a 2020 2022 6d65 7461 6461 7461 223a  ,.   "metadata":
+00001dd0: 207b 7d2c 0a20 2020 226f 7574 7075 7473   {},.   "outputs
+00001de0: 223a 205b 5d2c 0a20 2020 2273 6f75 7263  ": [],.   "sourc
+00001df0: 6522 3a20 5b0a 2020 2020 2264 6563 696d  e": [.    "decim
+00001e00: 616c 4669 656c 6473 203d 205b 2750 6f69  alFields = ['Poi
+00001e10: 6e74 2074 7261 6e73 6563 742a 5375 7276  nt transect*Surv
+00001e20: 6579 2065 6666 6f72 7427 2c20 274f 6273  ey effort', 'Obs
+00001e30: 6572 7661 7469 6f6e 2a52 6164 6961 6c20  ervation*Radial 
+00001e40: 6469 7374 616e 6365 275d 220a 2020 205d  distance']".   ]
+00001e50: 0a20 207d 2c0a 2020 7b0a 2020 2022 6365  .  },.  {.   "ce
+00001e60: 6c6c 5f74 7970 6522 3a20 2263 6f64 6522  ll_type": "code"
+00001e70: 2c0a 2020 2022 6578 6563 7574 696f 6e5f  ,.   "execution_
+00001e80: 636f 756e 7422 3a20 6e75 6c6c 2c0a 2020  count": null,.  
+00001e90: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
+00001ea0: 0a20 2020 226f 7574 7075 7473 223a 205b  .   "outputs": [
+00001eb0: 5d2c 0a20 2020 2273 6f75 7263 6522 3a20  ],.   "source": 
+00001ec0: 5b0a 2020 2020 2223 2041 6e61 6c79 7369  [.    "# Analysi
+00001ed0: 7320 656e 6769 6e65 5c6e 222c 0a20 2020  s engine\n",.   
+00001ee0: 2022 6d63 6473 203d 2061 6473 2e4d 4344   "mcds = ads.MCD
+00001ef0: 5345 6e67 696e 6528 776f 726b 4469 723d  SEngine(workDir=
+00001f00: 2774 6d70 2f6d 6364 732d 7365 6e73 272c  'tmp/mcds-sens',
+00001f10: 5c6e 222c 0a20 2020 2022 2020 2020 2020  \n",.    "      
+00001f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001f30: 6469 7374 616e 6365 556e 6974 3d27 4d65  distanceUnit='Me
+00001f40: 7465 7227 2c20 6172 6561 556e 6974 3d27  ter', areaUnit='
+00001f50: 4865 6374 6172 6527 2c5c 6e22 2c0a 2020  Hectare',\n",.  
+00001f60: 2020 2220 2020 2020 2020 2020 2020 2020    "             
+00001f70: 2020 2020 2020 2020 2073 7572 7665 7954           surveyT
+00001f80: 7970 653d 2750 6f69 6e74 272c 2064 6973  ype='Point', dis
+00001f90: 7461 6e63 6554 7970 653d 2752 6164 6961  tanceType='Radia
+00001fa0: 6c27 2c20 636c 7573 7465 7269 6e67 3d46  l', clustering=F
+00001fb0: 616c 7365 2922 0a20 2020 5d0a 2020 7d2c  alse)".   ].  },
+00001fc0: 0a20 207b 0a20 2020 2263 656c 6c5f 7479  .  {.   "cell_ty
+00001fd0: 7065 223a 2022 636f 6465 222c 0a20 2020  pe": "code",.   
+00001fe0: 2265 7865 6375 7469 6f6e 5f63 6f75 6e74  "execution_count
+00001ff0: 223a 206e 756c 6c2c 0a20 2020 226d 6574  ": null,.   "met
+00002000: 6164 6174 6122 3a20 7b7d 2c0a 2020 2022  adata": {},.   "
+00002010: 6f75 7470 7574 7322 3a20 5b5d 2c0a 2020  outputs": [],.  
+00002020: 2022 736f 7572 6365 223a 205b 0a20 2020   "source": [.   
+00002030: 2022 2320 4672 6f7a 656e 2061 6e61 6c79   "# Frozen analy
+00002040: 7369 7320 7061 7261 6d65 7465 7273 2028  sis parameters (
+00002050: 6120 6368 6f69 6365 2068 6572 6529 5c6e  a choice here)\n
+00002060: 222c 0a20 2020 2022 4b45 7374 696d 4372  ",.    "KEstimCr
+00002070: 6974 6572 696f 6e20 3d20 2741 4943 275c  iterion = 'AIC'\
+00002080: 6e22 2c0a 2020 2020 224b 4356 496e 7465  n",.    "KCVInte
+00002090: 7276 616c 203d 2039 3522 0a20 2020 5d0a  rval = 95".   ].
+000020a0: 2020 7d2c 0a20 207b 0a20 2020 2263 656c    },.  {.   "cel
+000020b0: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
+000020c0: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
+000020d0: 6f75 6e74 223a 206e 756c 6c2c 0a20 2020  ount": null,.   
+000020e0: 226d 6574 6164 6174 6122 3a20 7b0a 2020  "metadata": {.  
+000020f0: 2020 2273 6372 6f6c 6c65 6422 3a20 7472    "scrolled": tr
+00002100: 7565 0a20 2020 7d2c 0a20 2020 226f 7574  ue.   },.   "out
+00002110: 7075 7473 223a 205b 5d2c 0a20 2020 2273  puts": [],.   "s
+00002120: 6f75 7263 6522 3a20 5b0a 2020 2020 2274  ource": [.    "t
+00002130: 7353 7461 7274 203d 2070 642e 5469 6d65  sStart = pd.Time
+00002140: 7374 616d 702e 6e6f 7728 295c 6e22 2c0a  stamp.now()\n",.
+00002150: 2020 2020 2270 7269 6e74 2827 5374 6172      "print('Star
+00002160: 7465 6420 6174 272c 2074 7353 7461 7274  ted at', tsStart
+00002170: 295c 6e22 2c0a 2020 2020 2270 7269 6e74  )\n",.    "print
+00002180: 2829 5c6e 222c 0a20 2020 2022 5c6e 222c  ()\n",.    "\n",
+00002190: 0a20 2020 2022 2320 5275 6e20 616c 6c20  .    "# Run all 
+000021a0: 616e 616c 7973 6573 5c6e 222c 0a20 2020  analyses\n",.   
+000021b0: 2022 6d69 4375 7374 436f 6c73 203d 2070   "miCustCols = p
+000021c0: 642e 4d75 6c74 6949 6e64 6578 2e66 726f  d.MultiIndex.fro
+000021d0: 6d5f 7475 706c 6573 285b 2827 7361 6d70  m_tuples([('samp
+000021e0: 6c65 272c 2063 6f6c 2c20 2756 616c 7565  le', col, 'Value
+000021f0: 2729 2066 6f72 2063 6f6c 2069 6e20 6361  ') for col in ca
+00002200: 7365 4964 436f 6c73 5d29 5c6e 222c 0a20  seIdCols])\n",. 
+00002210: 2020 2022 6466 4375 7374 436f 6c54 7261     "dfCustColTra
+00002220: 6e73 203d 205c 5c5c 6e22 2c0a 2020 2020  ns = \\\n",.    
+00002230: 2220 2020 2070 642e 4461 7461 4672 616d  "    pd.DataFram
+00002240: 6528 696e 6465 783d 6d69 4375 7374 436f  e(index=miCustCo
+00002250: 6c73 2c5c 6e22 2c0a 2020 2020 2220 2020  ls,\n",.    "   
+00002260: 2020 2020 2020 2020 2020 2020 2020 6461                da
+00002270: 7461 3d64 6963 7428 656e 3d63 6173 6549  ta=dict(en=caseI
+00002280: 6443 6f6c 732c 2066 723d 5b27 4573 70c3  dCols, fr=['Esp.
+00002290: a863 6527 2c20 2745 6368 616e 7469 6c6c  .ce', 'Echantill
+000022a0: 6f6e 272c 2027 4d6f 64c3 a86c 6527 2c20  on', 'Mod..le', 
+000022b0: 274f 7264 7265 446f 6e6e c3a9 6573 275d  'OrdreDonn..es']
+000022c0: 2929 5c6e 222c 0a20 2020 2022 5c6e 222c  ))\n",.    "\n",
+000022d0: 0a20 2020 2022 7265 7375 6c74 7320 3d20  .    "results = 
+000022e0: 6164 732e 4d43 4453 416e 616c 7973 6973  ads.MCDSAnalysis
+000022f0: 5265 7375 6c74 7353 6574 286d 6943 7573  ResultsSet(miCus
+00002300: 746f 6d43 6f6c 733d 6d69 4375 7374 436f  tomCols=miCustCo
+00002310: 6c73 2c20 6466 4375 7374 6f6d 436f 6c54  ls, dfCustomColT
+00002320: 7261 6e73 3d64 6643 7573 7443 6f6c 5472  rans=dfCustColTr
+00002330: 616e 732c 5c6e 222c 0a20 2020 2022 2020  ans,\n",.    "  
+00002340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002360: 2020 2064 6973 7461 6e63 6555 6e69 743d     distanceUnit=
+00002370: 274d 6574 6572 272c 2061 7265 6155 6e69  'Meter', areaUni
+00002380: 743d 2748 6563 7461 7265 272c 5c6e 222c  t='Hectare',\n",
+00002390: 0a20 2020 2022 2020 2020 2020 2020 2020  .    "          
+000023a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000023b0: 2020 2020 2020 2020 2020 2073 7572 7665             surve
+000023c0: 7954 7970 653d 2750 6f69 6e74 272c 2064  yType='Point', d
+000023d0: 6973 7461 6e63 6554 7970 653d 2752 6164  istanceType='Rad
+000023e0: 6961 6c27 2c20 636c 7573 7465 7269 6e67  ial', clustering
+000023f0: 3d46 616c 7365 295c 6e22 2c0a 2020 2020  =False)\n",.    
+00002400: 225c 6e22 2c0a 2020 2020 226c 6173 7449  "\n",.    "lastI
+00002410: 6e46 696c 654e 616d 6520 3d20 2727 5c6e  nFileName = ''\n
+00002420: 222c 0a20 2020 2022 666f 7220 696e 642c  ",.    "for ind,
+00002430: 2073 4361 7365 2069 6e20 6466 416e 6c79   sCase in dfAnly
+00002440: 7343 6173 6573 2e69 7465 7272 6f77 7328  sCases.iterrows(
+00002450: 293a 5c6e 222c 0a20 2020 2022 2020 2020  ):\n",.    "    
+00002460: 5c6e 222c 0a20 2020 2022 2020 2020 7072  \n",.    "    pr
+00002470: 6566 6978 203d 2027 7b7d 2d7b 7d2d 7b7d  efix = '{}-{}-{}
+00002480: 272e 666f 726d 6174 2873 4361 7365 2e53  '.format(sCase.S
+00002490: 7065 6369 6573 2c20 7343 6173 652e 5361  pecies, sCase.Sa
+000024a0: 6d70 6c65 2c20 7343 6173 652e 4461 7461  mple, sCase.Data
+000024b0: 4f72 6465 7229 5c6e 222c 0a20 2020 2022  Order)\n",.    "
+000024c0: 2020 2020 7072 696e 7428 2723 7b3a 3364      print('#{:3d
+000024d0: 7d20 7b7d 207b 7d27 2e66 6f72 6d61 7428  } {} {}'.format(
+000024e0: 696e 642b 312c 2070 7265 6669 782c 2073  ind+1, prefix, s
+000024f0: 4361 7365 2e4d 6f64 656c 292c 2065 6e64  Case.Model), end
+00002500: 3d27 5c5c 6e27 2a32 295c 6e22 2c0a 2020  ='\\n'*2)\n",.  
+00002510: 2020 2220 2020 205c 6e22 2c0a 2020 2020    "    \n",.    
+00002520: 2220 2020 2023 2043 7265 6174 6520 6461  "    # Create da
+00002530: 7461 2073 6574 2e5c 6e22 2c0a 2020 2020  ta set.\n",.    
+00002540: 2220 2020 2064 6649 6e44 6174 6120 3d20  "    dfInData = 
+00002550: 6164 732e 5361 6d70 6c65 4461 7461 5365  ads.SampleDataSe
+00002560: 742e 6373 7632 6466 286f 732e 7061 7468  t.csv2df(os.path
+00002570: 2e6a 6f69 6e28 2772 6566 696e 272c 2073  .join('refin', s
+00002580: 4361 7365 2e49 6e46 696c 654e 616d 6529  Case.InFileName)
+00002590: 2c20 6465 6343 6f6c 733d 6465 6369 6d61  , decCols=decima
+000025a0: 6c46 6965 6c64 7329 5c6e 222c 0a20 2020  lFields)\n",.   
+000025b0: 2022 2020 2020 736f 7274 436f 6c73 203d   "    sortCols =
+000025c0: 206c 6973 7428 295c 6e22 2c0a 2020 2020   list()\n",.    
+000025d0: 2220 2020 2073 6f72 7441 7363 6720 3d20  "    sortAscg = 
+000025e0: 6c69 7374 2829 5c6e 222c 0a20 2020 2022  list()\n",.    "
+000025f0: 2020 2020 666f 7220 7372 7420 696e 205b      for srt in [
+00002600: 7343 6173 652e 4461 7461 4f72 6465 725b  sCase.DataOrder[
+00002610: 693a 692b 325d 2066 6f72 2069 2069 6e20  i:i+2] for i in 
+00002620: 7261 6e67 6528 302c 206c 656e 2873 4361  range(0, len(sCa
+00002630: 7365 2e44 6174 614f 7264 6572 292c 2032  se.DataOrder), 2
+00002640: 295d 3a5c 6e22 2c0a 2020 2020 2220 2020  )]:\n",.    "   
+00002650: 2020 2020 2061 7373 6572 7420 7372 745b       assert srt[
+00002660: 305d 2069 6e20 2770 6427 2061 6e64 2073  0] in 'pd' and s
+00002670: 7274 5b31 5d20 696e 2027 6364 275c 6e22  rt[1] in 'cd'\n"
+00002680: 2c0a 2020 2020 2220 2020 2020 2020 2069  ,.    "        i
+00002690: 6620 7372 745b 305d 203d 3d20 2770 273a  f srt[0] == 'p':
+000026a0: 5c6e 222c 0a20 2020 2022 2020 2020 2020  \n",.    "      
+000026b0: 2020 2020 2020 736f 7274 436f 6c73 2e61        sortCols.a
+000026c0: 7070 656e 6428 2750 6f69 6e74 2074 7261  ppend('Point tra
+000026d0: 6e73 6563 742a 4c61 6265 6c27 295c 6e22  nsect*Label')\n"
+000026e0: 2c0a 2020 2020 2220 2020 2020 2020 2065  ,.    "        e
+000026f0: 6c73 653a 2023 2027 6427 5c6e 222c 0a20  lse: # 'd'\n",. 
+00002700: 2020 2022 2020 2020 2020 2020 2020 2020     "            
+00002710: 736f 7274 436f 6c73 2e61 7070 656e 6428  sortCols.append(
+00002720: 274f 6273 6572 7661 7469 6f6e 2a52 6164  'Observation*Rad
+00002730: 6961 6c20 6469 7374 616e 6365 2729 5c6e  ial distance')\n
+00002740: 222c 0a20 2020 2022 2020 2020 2020 2020  ",.    "        
+00002750: 736f 7274 4173 6367 2e61 7070 656e 6428  sortAscg.append(
+00002760: 7372 745b 315d 203d 3d20 2763 2729 5c6e  srt[1] == 'c')\n
+00002770: 222c 0a20 2020 2022 2020 2020 6466 496e  ",.    "    dfIn
+00002780: 4461 7461 2e73 6f72 745f 7661 6c75 6573  Data.sort_values
+00002790: 2862 793d 736f 7274 436f 6c73 2c20 6173  (by=sortCols, as
+000027a0: 6365 6e64 696e 673d 736f 7274 4173 6367  cending=sortAscg
+000027b0: 2c20 696e 706c 6163 653d 5472 7565 295c  , inplace=True)\
+000027c0: 6e22 2c0a 2020 2020 2220 2020 2073 6473  n",.    "    sds
+000027d0: 203d 2061 6473 2e53 616d 706c 6544 6174   = ads.SampleDat
+000027e0: 6153 6574 2864 6649 6e44 6174 612c 2064  aSet(dfInData, d
+000027f0: 6563 696d 616c 4669 656c 6473 3d64 6563  ecimalFields=dec
+00002800: 696d 616c 4669 656c 6473 295c 6e22 2c0a  imalFields)\n",.
+00002810: 2020 2020 2220 2020 2020 2020 205c 6e22      "        \n"
+00002820: 2c0a 2020 2020 2220 2020 2023 2052 756e  ,.    "    # Run
+00002830: 2061 6e61 6c79 7369 735c 6e22 2c0a 2020   analysis\n",.  
+00002840: 2020 2220 2020 2061 6e61 6c79 7369 7320    "    analysis 
+00002850: 3d20 6164 732e 4d43 4453 416e 616c 7973  = ads.MCDSAnalys
+00002860: 6973 2865 6e67 696e 653d 6d63 6473 2c20  is(engine=mcds, 
+00002870: 7361 6d70 6c65 4461 7461 5365 743d 7364  sampleDataSet=sd
+00002880: 732c 206e 616d 653d 7072 6566 6978 2c5c  s, name=prefix,\
+00002890: 6e22 2c0a 2020 2020 2220 2020 2020 2020  n",.    "       
+000028a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000028b0: 2020 2020 2020 2020 2065 7374 696d 4b65           estimKe
+000028c0: 7946 6e3d 7343 6173 652e 4d6f 6465 6c5b  yFn=sCase.Model[
+000028d0: 3a33 5d2e 7570 7065 7228 292c 2065 7374  :3].upper(), est
+000028e0: 696d 4164 6a75 7374 466e 3d73 4361 7365  imAdjustFn=sCase
+000028f0: 2e4d 6f64 656c 5b33 3a5d 2e75 7070 6572  .Model[3:].upper
+00002900: 2829 2c5c 6e22 2c0a 2020 2020 2220 2020  (),\n",.    "   
+00002910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002920: 2020 2020 2020 2020 2020 2020 2065 7374               est
+00002930: 696d 4372 6974 6572 696f 6e3d 4b45 7374  imCriterion=KEst
+00002940: 696d 4372 6974 6572 696f 6e2c 2063 7649  imCriterion, cvI
+00002950: 6e74 6572 7661 6c3d 4b43 5649 6e74 6572  nterval=KCVInter
+00002960: 7661 6c29 5c6e 222c 0a20 2020 2022 2020  val)\n",.    "  
+00002970: 2020 7352 6573 756c 7420 3d20 616e 616c    sResult = anal
+00002980: 7973 6973 2e73 7562 6d69 7428 292e 6765  ysis.submit().ge
+00002990: 7452 6573 756c 7473 2829 5c6e 222c 0a20  tResults()\n",. 
+000029a0: 2020 2022 5c6e 222c 0a20 2020 2022 2020     "\n",.    "  
+000029b0: 2020 2320 5361 7665 2072 6573 756c 7473    # Save results
+000029c0: 5c6e 222c 0a20 2020 2022 2020 2020 7348  \n",.    "    sH
+000029d0: 6561 6420 3d20 7064 2e53 6572 6965 7328  ead = pd.Series(
+000029e0: 6461 7461 3d5b 7343 6173 655b 636f 6c5d  data=[sCase[col]
+000029f0: 2066 6f72 2063 6f6c 2069 6e20 7343 6173   for col in sCas
+00002a00: 652e 696e 6465 785b 3a6c 656e 2863 6173  e.index[:len(cas
+00002a10: 6549 6443 6f6c 7329 5d5d 2c20 696e 6465  eIdCols)]], inde
+00002a20: 783d 6d69 4375 7374 436f 6c73 295c 6e22  x=miCustCols)\n"
+00002a30: 2c0a 2020 2020 225c 6e22 2c0a 2020 2020  ,.    "\n",.    
+00002a40: 2220 2020 2072 6573 756c 7473 2e61 7070  "    results.app
+00002a50: 656e 6428 7352 6573 756c 742c 2073 4375  end(sResult, sCu
+00002a60: 7374 6f6d 4865 6164 3d73 4865 6164 295c  stomHead=sHead)\
+00002a70: 6e22 2c0a 2020 2020 2220 2020 205c 6e22  n",.    "    \n"
+00002a80: 2c0a 2020 2020 2274 7345 6e64 203d 2070  ,.    "tsEnd = p
+00002a90: 642e 5469 6d65 7374 616d 702e 6e6f 7728  d.Timestamp.now(
+00002aa0: 295c 6e22 2c0a 2020 2020 2270 7269 6e74  )\n",.    "print
+00002ab0: 2827 4669 6e69 7368 6564 2061 7427 2c20  ('Finished at', 
+00002ac0: 7473 456e 642c 2027 3a20 6475 7261 7469  tsEnd, ': durati
+00002ad0: 6f6e 272c 2073 7472 2874 7345 6e64 202d  on', str(tsEnd -
+00002ae0: 2074 7353 7461 7274 292e 7265 706c 6163   tsStart).replac
+00002af0: 6528 2730 2064 6179 7320 272c 2027 2729  e('0 days ', '')
+00002b00: 2922 0a20 2020 5d0a 2020 7d2c 0a20 207b  )".   ].  },.  {
+00002b10: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
+00002b20: 2022 636f 6465 222c 0a20 2020 2265 7865   "code",.   "exe
+00002b30: 6375 7469 6f6e 5f63 6f75 6e74 223a 206e  cution_count": n
+00002b40: 756c 6c2c 0a20 2020 226d 6574 6164 6174  ull,.   "metadat
+00002b50: 6122 3a20 7b0a 2020 2020 2273 6372 6f6c  a": {.    "scrol
+00002b60: 6c65 6422 3a20 6661 6c73 650a 2020 207d  led": false.   }
+00002b70: 2c0a 2020 2022 6f75 7470 7574 7322 3a20  ,.   "outputs": 
+00002b80: 5b5d 2c0a 2020 2022 736f 7572 6365 223a  [],.   "source":
+00002b90: 205b 0a20 2020 2022 2320 5361 7665 2061   [.    "# Save a
+00002ba0: 6e61 6c79 7369 7320 7265 7375 6c74 735c  nalysis results\
+00002bb0: 6e22 2c0a 2020 2020 2264 6652 6573 203d  n",.    "dfRes =
+00002bc0: 2072 6573 756c 7473 2e64 6644 6174 615c   results.dfData\
+00002bd0: 6e22 2c0a 2020 2020 225c 6e22 2c0a 2020  n",.    "\n",.  
+00002be0: 2020 2264 6652 6573 2e68 6561 6428 2922    "dfRes.head()"
+00002bf0: 0a20 2020 5d0a 2020 7d2c 0a20 207b 0a20  .   ].  },.  {. 
+00002c00: 2020 2263 656c 6c5f 7479 7065 223a 2022    "cell_type": "
+00002c10: 6d61 726b 646f 776e 222c 0a20 2020 226d  markdown",.   "m
+00002c20: 6574 6164 6174 6122 3a20 7b7d 2c0a 2020  etadata": {},.  
+00002c30: 2022 736f 7572 6365 223a 205b 0a20 2020   "source": [.   
+00002c40: 2022 2323 2063 2e20 436f 6d70 6172 6169   "## c. Comparai
+00002c50: 736f 6e20 6465 7320 72c3 a973 756c 7461  son des r..sulta
+00002c60: 7473 20c3 a020 6c61 2072 c3a9 66c3 a972  ts .. la r..f..r
+00002c70: 656e 6365 5c6e 222c 0a20 2020 2022 5c6e  ence\n",.    "\n
+00002c80: 222c 0a20 2020 2022 2870 6f75 7220 6368  ",.    "(pour ch
+00002c90: 6171 7565 2067 726f 7570 6520 7b20 6573  aque groupe { es
+00002ca0: 70c3 a863 652c 20c3 a963 6861 6e74 696c  p..ce, ..chantil
+00002cb0: 6c6f 6e2c 206d 6f64 c3a8 6c65 207d 2c20  lon, mod..le }, 
+00002cc0: 6c61 2031 c3a8 7265 2076 6172 6961 6e74  la 1..re variant
+00002cd0: 6520 6465 2074 7269 2922 0a20 2020 5d0a  e de tri)".   ].
+00002ce0: 2020 7d2c 0a20 207b 0a20 2020 2263 656c    },.  {.   "cel
+00002cf0: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
+00002d00: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
+00002d10: 6f75 6e74 223a 206e 756c 6c2c 0a20 2020  ount": null,.   
+00002d20: 226d 6574 6164 6174 6122 3a20 7b7d 2c0a  "metadata": {},.
+00002d30: 2020 2022 6f75 7470 7574 7322 3a20 5b5d     "outputs": []
+00002d40: 2c0a 2020 2022 736f 7572 6365 223a 205b  ,.   "source": [
+00002d50: 0a20 2020 2022 2320 5265 6d6f 7665 2075  .    "# Remove u
+00002d60: 7365 6c65 7373 2063 6f6c 756d 6e73 2066  seless columns f
+00002d70: 6f72 2063 6f6d 7061 7269 736f 6e5c 6e22  or comparison\n"
+00002d80: 2c0a 2020 2020 2264 6652 6573 3463 203d  ,.    "dfRes4c =
+00002d90: 2064 6652 6573 2e63 6f70 7928 295c 6e22   dfRes.copy()\n"
+00002da0: 2c0a 2020 2020 2264 6652 6573 3463 2e64  ,.    "dfRes4c.d
+00002db0: 726f 7028 636f 6c75 6d6e 733d 5b28 2772  rop(columns=[('r
+00002dc0: 756e 206f 7574 7075 7427 2c20 2772 756e  un output', 'run
+00002dd0: 2074 696d 6527 2c20 2756 616c 7565 2729   time', 'Value')
+00002de0: 2c20 2827 7275 6e20 6f75 7470 7574 272c  , ('run output',
+00002df0: 2027 7275 6e20 666f 6c64 6572 272c 2027   'run folder', '
+00002e00: 5661 6c75 6527 292c 5c6e 222c 0a20 2020  Value'),\n",.   
+00002e10: 2022 2020 2020 2020 2020 2020 2020 2020   "              
+00002e20: 2020 2020 2020 2020 2827 6465 7465 6374          ('detect
+00002e30: 696f 6e20 7072 6f62 6162 696c 6974 7927  ion probability'
+00002e40: 2c20 276b 6579 2066 756e 6374 696f 6e20  , 'key function 
+00002e50: 7479 7065 272c 2027 5661 6c75 6527 292c  type', 'Value'),
+00002e60: 5c6e 222c 0a20 2020 2022 2020 2020 2020  \n",.    "      
+00002e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002e80: 2827 6465 7465 6374 696f 6e20 7072 6f62  ('detection prob
+00002e90: 6162 696c 6974 7927 2c20 2761 646a 7573  ability', 'adjus
+00002ea0: 746d 656e 7420 7365 7269 6573 2074 7970  tment series typ
+00002eb0: 6527 2c20 2756 616c 7565 2729 5d2c 2069  e', 'Value')], i
+00002ec0: 6e70 6c61 6365 3d54 7275 6529 220a 2020  nplace=True)".  
+00002ed0: 205d 0a20 207d 2c0a 2020 7b0a 2020 2022   ].  },.  {.   "
+00002ee0: 6365 6c6c 5f74 7970 6522 3a20 2263 6f64  cell_type": "cod
+00002ef0: 6522 2c0a 2020 2022 6578 6563 7574 696f  e",.   "executio
+00002f00: 6e5f 636f 756e 7422 3a20 6e75 6c6c 2c0a  n_count": null,.
+00002f10: 2020 2022 6d65 7461 6461 7461 223a 207b     "metadata": {
+00002f20: 7d2c 0a20 2020 226f 7574 7075 7473 223a  },.   "outputs":
+00002f30: 205b 5d2c 0a20 2020 2273 6f75 7263 6522   [],.   "source"
+00002f40: 3a20 5b0a 2020 2020 2223 2043 6f6d 7061  : [.    "# Compa
+00002f50: 7265 2064 6174 6120 6f72 6465 7220 7661  re data order va
+00002f60: 7269 616e 7420 7265 7375 6c74 735c 6e22  riant results\n"
+00002f70: 2c0a 2020 2020 226d 6947 726f 7570 436f  ,.    "miGroupCo
+00002f80: 6c73 203d 205c 5c5c 6e22 2c0a 2020 2020  ls = \\\n",.    
+00002f90: 2220 2020 2070 642e 4d75 6c74 6949 6e64  "    pd.MultiInd
+00002fa0: 6578 2e66 726f 6d5f 7475 706c 6573 285b  ex.from_tuples([
+00002fb0: 2827 7361 6d70 6c65 272c 2063 6f6c 2c20  ('sample', col, 
+00002fc0: 2756 616c 7565 2729 2066 6f72 2063 6f6c  'Value') for col
+00002fd0: 2069 6e20 6361 7365 4964 436f 6c73 2069   in caseIdCols i
+00002fe0: 6620 636f 6c20 213d 2027 4461 7461 4f72  f col != 'DataOr
+00002ff0: 6465 7227 5d29 205c 5c5c 6e22 2c0a 2020  der']) \\\n",.  
+00003000: 2020 2220 2020 2020 2020 2020 2020 2020    "             
+00003010: 2020 2020 2e61 7070 656e 6428 7064 2e4d      .append(pd.M
+00003020: 756c 7469 496e 6465 782e 6672 6f6d 5f74  ultiIndex.from_t
+00003030: 7570 6c65 7328 5b28 2770 6172 616d 6574  uples([('paramet
+00003040: 6572 7327 2c20 636f 6c2c 2027 5661 6c75  ers', col, 'Valu
+00003050: 6527 2920 5c5c 5c6e 222c 0a20 2020 2022  e') \\\n",.    "
+00003060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003090: 2020 2020 666f 7220 636f 6c20 696e 2064      for col in d
+000030a0: 6652 6573 5b27 7061 7261 6d65 7465 7273  fRes['parameters
+000030b0: 275d 2e63 6f6c 756d 6e73 2e67 6574 5f6c  '].columns.get_l
+000030c0: 6576 656c 5f76 616c 7565 7328 3029 5d29  evel_values(0)])
+000030d0: 295c 6e22 2c0a 2020 2020 2269 6e64 6578  )\n",.    "index
+000030e0: 436f 6c73 203d 206d 6947 726f 7570 436f  Cols = miGroupCo
+000030f0: 6c73 2e74 6f5f 6c69 7374 2829 202b 205b  ls.to_list() + [
+00003100: 2827 7361 6d70 6c65 272c 2027 4461 7461  ('sample', 'Data
+00003110: 4f72 6465 7227 2c20 2756 616c 7565 2729  Order', 'Value')
+00003120: 5d5c 6e22 2c0a 2020 2020 2264 6652 656c  ]\n",.    "dfRel
+00003130: 4469 6620 3d20 6466 5265 7334 632e 6772  Dif = dfRes4c.gr
+00003140: 6f75 7062 7928 6d69 4772 6f75 7043 6f6c  oupby(miGroupCol
+00003150: 732e 746f 5f6c 6973 7428 2929 2e61 7070  s.to_list()).app
+00003160: 6c79 2876 6172 6961 6e74 436c 6f73 656e  ly(variantClosen
+00003170: 6573 732c 2069 6e64 6578 436f 6c73 3d69  ess, indexCols=i
+00003180: 6e64 6578 436f 6c73 2c20 7265 664c 6162  ndexCols, refLab
+00003190: 656c 496e 643d 3029 5c6e 222c 0a20 2020  elInd=0)\n",.   
+000031a0: 2022 5c6e 222c 0a20 2020 2022 6466 5265   "\n",.    "dfRe
+000031b0: 6c44 6966 220a 2020 205d 0a20 207d 2c0a  lDif".   ].  },.
+000031c0: 2020 7b0a 2020 2022 6365 6c6c 5f74 7970    {.   "cell_typ
+000031d0: 6522 3a20 226d 6172 6b64 6f77 6e22 2c0a  e": "markdown",.
+000031e0: 2020 2022 6d65 7461 6461 7461 223a 207b     "metadata": {
+000031f0: 7d2c 0a20 2020 2273 6f75 7263 6522 3a20  },.   "source": 
+00003200: 5b0a 2020 2020 2223 2320 642e 2053 6175  [.    "## d. Sau
+00003210: 7665 6761 7264 6520 6465 7320 72c3 a973  vegarde des r..s
+00003220: 756c 7461 7473 2e22 0a20 2020 5d0a 2020  ultats.".   ].  
+00003230: 7d2c 0a20 207b 0a20 2020 2263 656c 6c5f  },.  {.   "cell_
+00003240: 7479 7065 223a 2022 636f 6465 222c 0a20  type": "code",. 
+00003250: 2020 2265 7865 6375 7469 6f6e 5f63 6f75    "execution_cou
+00003260: 6e74 223a 206e 756c 6c2c 0a20 2020 226d  nt": null,.   "m
+00003270: 6574 6164 6174 6122 3a20 7b7d 2c0a 2020  etadata": {},.  
+00003280: 2022 6f75 7470 7574 7322 3a20 5b5d 2c0a   "outputs": [],.
+00003290: 2020 2022 736f 7572 6365 223a 205b 0a20     "source": [. 
+000032a0: 2020 2022 7265 7346 696c 654e 616d 6520     "resFileName 
+000032b0: 3d20 6f73 2e70 6174 682e 6a6f 696e 286d  = os.path.join(m
+000032c0: 6364 732e 776f 726b 4469 722c 2027 4143  cds.workDir, 'AC
+000032d0: 4443 3230 3139 2d50 6170 7972 7573 2d61  DC2019-Papyrus-a
+000032e0: 7574 6f2d 7365 6e73 2d64 6174 612d 6f72  uto-sens-data-or
+000032f0: 6465 722d 7265 7375 6c74 732e 786c 7378  der-results.xlsx
+00003300: 2729 5c6e 222c 0a20 2020 2022 5c6e 222c  ')\n",.    "\n",
+00003310: 0a20 2020 2022 7769 7468 2070 642e 4578  .    "with pd.Ex
+00003320: 6365 6c57 7269 7465 7228 7265 7346 696c  celWriter(resFil
+00003330: 654e 616d 6529 2061 7320 786c 7378 5772  eName) as xlsxWr
+00003340: 6974 6572 3a5c 6e22 2c0a 2020 2020 225c  iter:\n",.    "\
+00003350: 6e22 2c0a 2020 2020 2220 2020 2064 6652  n",.    "    dfR
+00003360: 6573 2e74 6f5f 6578 6365 6c28 786c 7378  es.to_excel(xlsx
+00003370: 5772 6974 6572 2c20 7368 6565 745f 6e61  Writer, sheet_na
+00003380: 6d65 3d27 5261 7752 6573 756c 7473 272c  me='RawResults',
+00003390: 2069 6e64 6578 3d54 7275 6529 5c6e 222c   index=True)\n",
+000033a0: 0a20 2020 2022 2020 2020 6466 5265 6c44  .    "    dfRelD
+000033b0: 6966 2e74 6f5f 6578 6365 6c28 786c 7378  if.to_excel(xlsx
+000033c0: 5772 6974 6572 2c20 7368 6565 745f 6e61  Writer, sheet_na
+000033d0: 6d65 3d27 4469 6666 3252 6566 272c 2069  me='Diff2Ref', i
+000033e0: 6e64 6578 3d54 7275 6529 220a 2020 205d  ndex=True)".   ]
+000033f0: 0a20 207d 2c0a 2020 7b0a 2020 2022 6365  .  },.  {.   "ce
+00003400: 6c6c 5f74 7970 6522 3a20 2263 6f64 6522  ll_type": "code"
+00003410: 2c0a 2020 2022 6578 6563 7574 696f 6e5f  ,.   "execution_
+00003420: 636f 756e 7422 3a20 6e75 6c6c 2c0a 2020  count": null,.  
+00003430: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
+00003440: 0a20 2020 226f 7574 7075 7473 223a 205b  .   "outputs": [
+00003450: 5d2c 0a20 2020 2273 6f75 7263 6522 3a20  ],.   "source": 
+00003460: 5b5d 0a20 207d 2c0a 2020 7b0a 2020 2022  [].  },.  {.   "
+00003470: 6365 6c6c 5f74 7970 6522 3a20 226d 6172  cell_type": "mar
+00003480: 6b64 6f77 6e22 2c0a 2020 2022 6d65 7461  kdown",.   "meta
+00003490: 6461 7461 223a 207b 7d2c 0a20 2020 2273  data": {},.   "s
+000034a0: 6f75 7263 6522 3a20 5b0a 2020 2020 2223  ource": [.    "#
+000034b0: 2053 656e 7369 6269 6c69 74c3 a920 6465   Sensibilit.. de
+000034c0: 204d 4344 532e 6578 6520 c3a0 206c 276f   MCDS.exe .. l'o
+000034d0: 7264 7265 2064 6573 2064 6f6e 6ec3 a965  rdre des donn..e
+000034e0: 7320 2862 6973 295c 6e22 2c0a 2020 2020  s (bis)\n",.    
+000034f0: 225c 6e22 2c0a 2020 2020 2243 6f6e 7374  "\n",.    "Const
+00003500: 7275 6374 696f 6e20 7365 6d69 2d6d 616e  ruction semi-man
+00003510: 7565 6c6c 6520 6427 756e 2065 7865 6d70  uelle d'un exemp
+00003520: 6c65 2064 6520 7461 696c 6c65 2072 c3a9  le de taille r..
+00003530: 6475 6974 6520 736f 756d 6973 6520 c3a0  duite soumise ..
+00003540: 2045 7269 6320 5265 7873 7461 6474 203a   Eric Rexstadt :
+00003550: 5c6e 222c 0a20 2020 2022 2020 4366 2e20  \n",.    "  Cf. 
+00003560: 7265 666f 7574 2f64 6973 742d 6f72 6465  refout/dist-orde
+00003570: 722d 7365 6e73 2d6d 696e 2f64 6973 742d  r-sens-min/dist-
+00003580: 6f72 6465 722d 7365 6e73 2e6f 6474 220a  order-sens.odt".
+00003590: 2020 205d 0a20 207d 2c0a 2020 7b0a 2020     ].  },.  {.  
+000035a0: 2022 6365 6c6c 5f74 7970 6522 3a20 226d   "cell_type": "m
+000035b0: 6172 6b64 6f77 6e22 2c0a 2020 2022 6d65  arkdown",.   "me
+000035c0: 7461 6461 7461 223a 207b 7d2c 0a20 2020  tadata": {},.   
+000035d0: 2273 6f75 7263 6522 3a20 5b0a 2020 2020  "source": [.    
+000035e0: 2223 204f 7264 7265 2064 6573 2064 6f6e  "# Ordre des don
+000035f0: 6ec3 a965 7320 67c3 a96e c3a9 72c3 a965  n..es g..n..r..e
+00003600: 7320 7061 7220 4469 7374 616e 6365 2037  s par Distance 7
+00003610: 2070 6f75 7220 4d43 4453 2e65 7865 220a   pour MCDS.exe".
+00003620: 2020 205d 0a20 207d 2c0a 2020 7b0a 2020     ].  },.  {.  
+00003630: 2022 6365 6c6c 5f74 7970 6522 3a20 2263   "cell_type": "c
+00003640: 6f64 6522 2c0a 2020 2022 6578 6563 7574  ode",.   "execut
+00003650: 696f 6e5f 636f 756e 7422 3a20 6e75 6c6c  ion_count": null
+00003660: 2c0a 2020 2022 6d65 7461 6461 7461 223a  ,.   "metadata":
+00003670: 207b 7d2c 0a20 2020 226f 7574 7075 7473   {},.   "outputs
+00003680: 223a 205b 5d2c 0a20 2020 2273 6f75 7263  ": [],.   "sourc
+00003690: 6522 3a20 5b0a 2020 2020 2264 6620 3d20  e": [.    "df = 
+000036a0: 7064 2e72 6561 645f 6373 7628 2772 6566  pd.read_csv('ref
+000036b0: 6f75 742f 6469 7374 2d6f 7264 6572 2d73  out/dist-order-s
+000036c0: 656e 732d 6d69 6e2f 636d 642d 7769 6e37  ens-min/cmd-win7
+000036d0: 2d64 6973 742d 6f72 6465 722f 6461 7461  -dist-order/data
+000036e0: 2e74 7874 272c 2073 6570 3d27 5c5c 7427  .txt', sep='\\t'
+000036f0: 2c5c 6e22 2c0a 2020 2020 2220 2020 2020  ,\n",.    "     
+00003700: 2020 2020 2020 2020 2020 2020 6e61 6d65              name
+00003710: 733d 5b27 7265 6769 6f6e 272c 2027 6172  s=['region', 'ar
+00003720: 6561 272c 2027 706f 696e 7427 2c20 2765  ea', 'point', 'e
+00003730: 6666 6f72 7427 2c20 2764 6973 7461 6e63  ffort', 'distanc
+00003740: 6527 5d29 5c6e 222c 0a20 2020 2022 6466  e'])\n",.    "df
+00003750: 2e68 6561 6428 3230 2922 0a20 2020 5d0a  .head(20)".   ].
+00003760: 2020 7d2c 0a20 207b 0a20 2020 2263 656c    },.  {.   "cel
+00003770: 6c5f 7479 7065 223a 2022 636f 6465 222c  l_type": "code",
+00003780: 0a20 2020 2265 7865 6375 7469 6f6e 5f63  .   "execution_c
+00003790: 6f75 6e74 223a 206e 756c 6c2c 0a20 2020  ount": null,.   
+000037a0: 226d 6574 6164 6174 6122 3a20 7b7d 2c0a  "metadata": {},.
+000037b0: 2020 2022 6f75 7470 7574 7322 3a20 5b5d     "outputs": []
+000037c0: 2c0a 2020 2022 736f 7572 6365 223a 205b  ,.   "source": [
+000037d0: 0a20 2020 2022 6466 5b27 6e70 6f69 6e74  .    "df['npoint
+000037e0: 275d 203d 2064 662e 706f 696e 742e 6170  '] = df.point.ap
+000037f0: 706c 7928 6c61 6d62 6461 2073 3a20 696e  ply(lambda s: in
+00003800: 7428 732e 7370 6c69 7428 2720 2729 5b31  t(s.split(' ')[1
+00003810: 5d29 2922 0a20 2020 5d0a 2020 7d2c 0a20  ]))".   ].  },. 
+00003820: 207b 0a20 2020 2263 656c 6c5f 7479 7065   {.   "cell_type
+00003830: 223a 2022 636f 6465 222c 0a20 2020 2265  ": "code",.   "e
+00003840: 7865 6375 7469 6f6e 5f63 6f75 6e74 223a  xecution_count":
+00003850: 206e 756c 6c2c 0a20 2020 226d 6574 6164   null,.   "metad
+00003860: 6174 6122 3a20 7b7d 2c0a 2020 2022 6f75  ata": {},.   "ou
+00003870: 7470 7574 7322 3a20 5b5d 2c0a 2020 2022  tputs": [],.   "
+00003880: 736f 7572 6365 223a 205b 0a20 2020 2022  source": [.    "
+00003890: 2320 4368 616e 6765 6d65 6e74 2064 6520  # Changement de 
+000038a0: 6c27 6f72 6472 6520 3a20 7472 6920 7061  l'ordre : tri pa
+000038b0: 7220 706f 696e 7420 6574 2070 6172 2064  r point et par d
+000038c0: 6973 7461 6e63 6573 2063 726f 6973 7361  istances croissa
+000038d0: 6e74 6573 5c6e 222c 0a20 2020 2022 6466  ntes\n",.    "df
+000038e0: 2e73 6f72 745f 7661 6c75 6573 2862 793d  .sort_values(by=
+000038f0: 5b27 6e70 6f69 6e74 272c 2027 6469 7374  ['npoint', 'dist
+00003900: 616e 6365 275d 2c20 696e 706c 6163 653d  ance'], inplace=
+00003910: 5472 7565 295c 6e22 2c0a 2020 2020 2264  True)\n",.    "d
+00003920: 662e 6865 6164 2832 3029 220a 2020 205d  f.head(20)".   ]
+00003930: 0a20 207d 2c0a 2020 7b0a 2020 2022 6365  .  },.  {.   "ce
+00003940: 6c6c 5f74 7970 6522 3a20 2263 6f64 6522  ll_type": "code"
+00003950: 2c0a 2020 2022 6578 6563 7574 696f 6e5f  ,.   "execution_
+00003960: 636f 756e 7422 3a20 6e75 6c6c 2c0a 2020  count": null,.  
+00003970: 2022 6d65 7461 6461 7461 223a 207b 7d2c   "metadata": {},
+00003980: 0a20 2020 226f 7574 7075 7473 223a 205b  .   "outputs": [
+00003990: 5d2c 0a20 2020 2273 6f75 7263 6522 3a20  ],.   "source": 
+000039a0: 5b0a 2020 2020 2270 6c2e 5061 7468 2827  [.    "pl.Path('
+000039b0: 7265 666f 7574 2f64 6973 742d 6f72 6465  refout/dist-orde
+000039c0: 722d 7365 6e73 2d6d 696e 2f63 6d64 2d77  r-sens-min/cmd-w
+000039d0: 696e 372d 736f 7274 6564 2d6f 7264 6572  in7-sorted-order
+000039e0: 2729 2e6d 6b64 6972 2865 7869 7374 5f6f  ').mkdir(exist_o
+000039f0: 6b3d 5472 7565 295c 6e22 2c0a 2020 2020  k=True)\n",.    
+00003a00: 2264 665b 5b27 7265 6769 6f6e 272c 2027  "df[['region', '
+00003a10: 6172 6561 272c 2027 706f 696e 7427 2c20  area', 'point', 
+00003a20: 2765 6666 6f72 7427 2c20 2764 6973 7461  'effort', 'dista
+00003a30: 6e63 6527 5d5d 205c 5c5c 6e22 2c0a 2020  nce']] \\\n",.  
+00003a40: 2020 2220 202e 746f 5f63 7376 2827 7265    "  .to_csv('re
+00003a50: 666f 7574 2f64 6973 742d 6f72 6465 722d  fout/dist-order-
+00003a60: 7365 6e73 2d6d 696e 2f63 6d64 2d77 696e  sens-min/cmd-win
+00003a70: 372d 736f 7274 6564 2d6f 7264 6572 2f64  7-sorted-order/d
+00003a80: 6174 612e 7478 7427 2c20 7365 703d 275c  ata.txt', sep='\
+00003a90: 5c74 272c 2069 6e64 6578 3d46 616c 7365  \t', index=False
+00003aa0: 2c20 6865 6164 6572 3d46 616c 7365 2922  , header=False)"
+00003ab0: 0a20 2020 5d0a 2020 7d2c 0a20 207b 0a20  .   ].  },.  {. 
+00003ac0: 2020 2263 656c 6c5f 7479 7065 223a 2022    "cell_type": "
+00003ad0: 636f 6465 222c 0a20 2020 2265 7865 6375  code",.   "execu
+00003ae0: 7469 6f6e 5f63 6f75 6e74 223a 206e 756c  tion_count": nul
+00003af0: 6c2c 0a20 2020 226d 6574 6164 6174 6122  l,.   "metadata"
+00003b00: 3a20 7b7d 2c0a 2020 2022 6f75 7470 7574  : {},.   "output
+00003b10: 7322 3a20 5b5d 2c0a 2020 2022 736f 7572  s": [],.   "sour
+00003b20: 6365 223a 205b 5d0a 2020 7d2c 0a20 207b  ce": [].  },.  {
+00003b30: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
+00003b40: 2022 6d61 726b 646f 776e 222c 0a20 2020   "markdown",.   
+00003b50: 226d 6574 6164 6174 6122 3a20 7b7d 2c0a  "metadata": {},.
+00003b60: 2020 2022 736f 7572 6365 223a 205b 0a20     "source": [. 
+00003b70: 2020 2022 2320 4f72 6472 6520 6465 7320     "# Ordre des 
+00003b80: 646f 6e6e c3a9 6573 2065 6e20 656e 7472  donn..es en entr
+00003b90: c3a9 6520 6465 2064 6973 7461 6e63 6522  ..e de distance"
+00003ba0: 0a20 2020 5d0a 2020 7d2c 0a20 207b 0a20  .   ].  },.  {. 
+00003bb0: 2020 2263 656c 6c5f 7479 7065 223a 2022    "cell_type": "
+00003bc0: 636f 6465 222c 0a20 2020 2265 7865 6375  code",.   "execu
+00003bd0: 7469 6f6e 5f63 6f75 6e74 223a 206e 756c  tion_count": nul
+00003be0: 6c2c 0a20 2020 226d 6574 6164 6174 6122  l,.   "metadata"
+00003bf0: 3a20 7b7d 2c0a 2020 2022 6f75 7470 7574  : {},.   "output
+00003c00: 7322 3a20 5b5d 2c0a 2020 2022 736f 7572  s": [],.   "sour
+00003c10: 6365 223a 205b 0a20 2020 2022 2320 4dc3  ce": [.    "# M.
+00003c20: aa6d 6520 6669 6368 6965 7220 6578 6163  .me fichier exac
+00003c30: 7465 6d65 6e74 2071 7565 2072 6566 6f75  tement que refou
+00003c40: 742f 6469 7374 2d6f 7264 6572 2d73 656e  t/dist-order-sen
+00003c50: 732d 6d69 6e2f 696d 706f 7274 2d64 6174  s-min/import-dat
+00003c60: 612d 7365 742e 7478 7420 6e6f 726d 616c  a-set.txt normal
+00003c70: 656d 656e 742e 5c6e 222c 0a20 2020 2022  ement.\n",.    "
+00003c80: 6466 203d 2070 642e 7265 6164 5f63 7376  df = pd.read_csv
+00003c90: 2827 7265 6669 6e2f 4143 4443 3230 3139  ('refin/ACDC2019
+00003ca0: 2d50 6170 7972 7573 2d54 5552 4d45 522d  -Papyrus-TURMER-
+00003cb0: 4142 2d31 306d 6e2d 3164 6563 2d64 6973  AB-10mn-1dec-dis
+00003cc0: 742e 7478 7427 2c20 7365 703d 275c 5c74  t.txt', sep='\\t
+00003cd0: 272c 2068 6561 6465 723d 3029 5c6e 222c  ', header=0)\n",
+00003ce0: 0a20 2020 2022 6466 2e68 6561 6428 3230  .    "df.head(20
+00003cf0: 2922 0a20 2020 5d0a 2020 7d2c 0a20 207b  )".   ].  },.  {
+00003d00: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
+00003d10: 2022 636f 6465 222c 0a20 2020 2265 7865   "code",.   "exe
+00003d20: 6375 7469 6f6e 5f63 6f75 6e74 223a 206e  cution_count": n
+00003d30: 756c 6c2c 0a20 2020 226d 6574 6164 6174  ull,.   "metadat
+00003d40: 6122 3a20 7b7d 2c0a 2020 2022 6f75 7470  a": {},.   "outp
+00003d50: 7574 7322 3a20 5b5d 2c0a 2020 2022 736f  uts": [],.   "so
+00003d60: 7572 6365 223a 205b 0a20 2020 2022 2320  urce": [.    "# 
+00003d70: 4368 616e 6765 6d65 6e74 2064 6520 6c27  Changement de l'
+00003d80: 6f72 6472 6520 3a20 7472 6920 7061 7220  ordre : tri par 
+00003d90: 7061 7220 6469 7374 616e 6365 7320 616c  par distances al
+00003da0: 7068 6162 c3a9 7469 7175 6573 2063 726f  phab..tiques cro
+00003db0: 6973 7361 6e74 6573 2c20 6f75 692c 206f  issantes, oui, o
+00003dc0: 7569 2028 656e 2069 676e 6f72 616e 7420  ui (en ignorant 
+00003dd0: 6c65 7320 706f 696e 7473 295c 6e22 2c0a  les points)\n",.
+00003de0: 2020 2020 2223 2042 7574 3a20 566f 6972      "# But: Voir
+00003df0: 2073 6920 4469 7374 616e 6365 2072 6563   si Distance rec
+00003e00: 6c61 7373 6520 6175 7474 2070 6172 2070  lasse autt par p
+00003e10: 6f69 6e74 5c6e 222c 0a20 2020 2022 6466  oint\n",.    "df
+00003e20: 2e73 6f72 745f 7661 6c75 6573 2862 793d  .sort_values(by=
+00003e30: 5b27 4f62 7365 7276 6174 696f 6e2a 5261  ['Observation*Ra
+00003e40: 6469 616c 2064 6973 7461 6e63 6527 5d2c  dial distance'],
+00003e50: 2069 6e70 6c61 6365 3d54 7275 6529 5c6e   inplace=True)\n
+00003e60: 222c 0a20 2020 2022 6466 2e68 6561 6428  ",.    "df.head(
+00003e70: 3230 2922 0a20 2020 5d0a 2020 7d2c 0a20  20)".   ].  },. 
+00003e80: 207b 0a20 2020 2263 656c 6c5f 7479 7065   {.   "cell_type
+00003e90: 223a 2022 636f 6465 222c 0a20 2020 2265  ": "code",.   "e
+00003ea0: 7865 6375 7469 6f6e 5f63 6f75 6e74 223a  xecution_count":
+00003eb0: 206e 756c 6c2c 0a20 2020 226d 6574 6164   null,.   "metad
+00003ec0: 6174 6122 3a20 7b7d 2c0a 2020 2022 6f75  ata": {},.   "ou
+00003ed0: 7470 7574 7322 3a20 5b5d 2c0a 2020 2022  tputs": [],.   "
+00003ee0: 736f 7572 6365 223a 205b 0a20 2020 2022  source": [.    "
+00003ef0: 6466 2e74 6f5f 6373 7628 2774 6d70 2f41  df.to_csv('tmp/A
+00003f00: 4344 4332 3031 392d 5061 7079 7275 732d  CDC2019-Papyrus-
+00003f10: 5455 524d 4552 2d41 422d 3130 6d6e 2d31  TURMER-AB-10mn-1
+00003f20: 6465 632d 7472 6961 6c70 6861 2d64 6973  dec-trialpha-dis
+00003f30: 742e 7478 7427 2c20 7365 703d 275c 5c74  t.txt', sep='\\t
+00003f40: 272c 2069 6e64 6578 3d46 616c 7365 2922  ', index=False)"
+00003f50: 0a20 2020 5d0a 2020 7d2c 0a20 207b 0a20  .   ].  },.  {. 
+00003f60: 2020 2263 656c 6c5f 7479 7065 223a 2022    "cell_type": "
+00003f70: 636f 6465 222c 0a20 2020 2265 7865 6375  code",.   "execu
+00003f80: 7469 6f6e 5f63 6f75 6e74 223a 206e 756c  tion_count": nul
+00003f90: 6c2c 0a20 2020 226d 6574 6164 6174 6122  l,.   "metadata"
+00003fa0: 3a20 7b7d 2c0a 2020 2022 6f75 7470 7574  : {},.   "output
+00003fb0: 7322 3a20 5b5d 2c0a 2020 2022 736f 7572  s": [],.   "sour
+00003fc0: 6365 223a 205b 5d0a 2020 7d2c 0a20 207b  ce": [].  },.  {
+00003fd0: 0a20 2020 2263 656c 6c5f 7479 7065 223a  .   "cell_type":
+00003fe0: 2022 636f 6465 222c 0a20 2020 2265 7865   "code",.   "exe
+00003ff0: 6375 7469 6f6e 5f63 6f75 6e74 223a 206e  cution_count": n
+00004000: 756c 6c2c 0a20 2020 226d 6574 6164 6174  ull,.   "metadat
+00004010: 6122 3a20 7b7d 2c0a 2020 2022 6f75 7470  a": {},.   "outp
+00004020: 7574 7322 3a20 5b5d 2c0a 2020 2022 736f  uts": [],.   "so
+00004030: 7572 6365 223a 205b 5d0a 2020 7d2c 0a20  urce": [].  },. 
+00004040: 207b 0a20 2020 2263 656c 6c5f 7479 7065   {.   "cell_type
+00004050: 223a 2022 6d61 726b 646f 776e 222c 0a20  ": "markdown",. 
+00004060: 2020 226d 6574 6164 6174 6122 3a20 7b7d    "metadata": {}
+00004070: 2c0a 2020 2022 736f 7572 6365 223a 205b  ,.   "source": [
+00004080: 0a20 2020 2022 2320 4261 6320 c3a0 2073  .    "# Bac .. s
+00004090: 6162 6c65 220a 2020 205d 0a20 207d 2c0a  able".   ].  },.
+000040a0: 2020 7b0a 2020 2022 6365 6c6c 5f74 7970    {.   "cell_typ
+000040b0: 6522 3a20 2263 6f64 6522 2c0a 2020 2022  e": "code",.   "
+000040c0: 6578 6563 7574 696f 6e5f 636f 756e 7422  execution_count"
+000040d0: 3a20 6e75 6c6c 2c0a 2020 2022 6d65 7461  : null,.   "meta
+000040e0: 6461 7461 223a 207b 7d2c 0a20 2020 226f  data": {},.   "o
+000040f0: 7574 7075 7473 223a 205b 5d2c 0a20 2020  utputs": [],.   
+00004100: 2273 6f75 7263 6522 3a20 5b5d 0a20 207d  "source": [].  }
+00004110: 2c0a 2020 7b0a 2020 2022 6365 6c6c 5f74  ,.  {.   "cell_t
+00004120: 7970 6522 3a20 2263 6f64 6522 2c0a 2020  ype": "code",.  
+00004130: 2022 6578 6563 7574 696f 6e5f 636f 756e   "execution_coun
+00004140: 7422 3a20 6e75 6c6c 2c0a 2020 2022 6d65  t": null,.   "me
+00004150: 7461 6461 7461 223a 207b 7d2c 0a20 2020  tadata": {},.   
+00004160: 226f 7574 7075 7473 223a 205b 5d2c 0a20  "outputs": [],. 
+00004170: 2020 2273 6f75 7263 6522 3a20 5b5d 0a20    "source": []. 
+00004180: 207d 0a20 5d2c 0a20 226d 6574 6164 6174   }. ],. "metadat
+00004190: 6122 3a20 7b0a 2020 226b 6572 6e65 6c73  a": {.  "kernels
+000041a0: 7065 6322 3a20 7b0a 2020 2022 6469 7370  pec": {.   "disp
+000041b0: 6c61 795f 6e61 6d65 223a 2022 5079 7468  lay_name": "Pyth
+000041c0: 6f6e 2033 222c 0a20 2020 226c 616e 6775  on 3",.   "langu
+000041d0: 6167 6522 3a20 2270 7974 686f 6e22 2c0a  age": "python",.
+000041e0: 2020 2022 6e61 6d65 223a 2022 7079 7468     "name": "pyth
+000041f0: 6f6e 3322 0a20 207d 2c0a 2020 226c 616e  on3".  },.  "lan
+00004200: 6775 6167 655f 696e 666f 223a 207b 0a20  guage_info": {. 
+00004210: 2020 2263 6f64 656d 6972 726f 725f 6d6f    "codemirror_mo
+00004220: 6465 223a 207b 0a20 2020 2022 6e61 6d65  de": {.    "name
+00004230: 223a 2022 6970 7974 686f 6e22 2c0a 2020  ": "ipython",.  
+00004240: 2020 2276 6572 7369 6f6e 223a 2033 0a20    "version": 3. 
+00004250: 2020 7d2c 0a20 2020 2266 696c 655f 6578    },.   "file_ex
+00004260: 7465 6e73 696f 6e22 3a20 222e 7079 222c  tension": ".py",
+00004270: 0a20 2020 226d 696d 6574 7970 6522 3a20  .   "mimetype": 
+00004280: 2274 6578 742f 782d 7079 7468 6f6e 222c  "text/x-python",
+00004290: 0a20 2020 226e 616d 6522 3a20 2270 7974  .   "name": "pyt
+000042a0: 686f 6e22 2c0a 2020 2022 6e62 636f 6e76  hon",.   "nbconv
+000042b0: 6572 745f 6578 706f 7274 6572 223a 2022  ert_exporter": "
+000042c0: 7079 7468 6f6e 222c 0a20 2020 2270 7967  python",.   "pyg
+000042d0: 6d65 6e74 735f 6c65 7865 7222 3a20 2269  ments_lexer": "i
+000042e0: 7079 7468 6f6e 3322 2c0a 2020 2022 7665  python3",.   "ve
+000042f0: 7273 696f 6e22 3a20 2233 2e38 2e32 220a  rsion": "3.8.2".
+00004300: 2020 7d0a 207d 2c0a 2022 6e62 666f 726d    }. },. "nbform
+00004310: 6174 223a 2034 2c0a 2022 6e62 666f 726d  at": 4,. "nbform
+00004320: 6174 5f6d 696e 6f72 223a 2032 0a7d 0a    at_minor": 2.}.
```

### Comparing `pyaudisam-0.9.3/tests/unint_data_test.py` & `pyaudisam-1.0.1/tests/unint_data_test.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,617 +1,617 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret, Sylvain Sainnier
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Automated unit and integration tests for "data" submodule
-
-# To run : simply run "pytest" or "python <this file>" in current folder
-#          and check standard output ; and ./tmp/unit-int-test.{datetime}.log for details
-
-import sys
-import pathlib as pl
-import pandas as pd
-import numpy as np
-
-import pyaudisam as ads
-
-import pytest
-
-
-# The "tests" folder.
-KTestSrcPath = pl.Path(__file__).parent
-
-# Temporary work folder (created once in conftest.py if needed).
-tmpDir = KTestSrcPath / 'tmp'
-
-# Logger for this module.
-logger = ads.logger('unt.dat')
-
-###############################################################################
-#                         Actions to be done before any test                  #
-###############################################################################
-def test_init():
-
-    # Configure logging.
-    logFilePathName = tmpDir / 'unt-dat.{}.log'.format(pd.Timestamp.now().strftime('%Y%m%d%H%M'))
-    ads.log.configure(loggers=[dict(name='matplotlib', level=ads.WARNING),
-                               dict(name='ads', level=ads.INFO),
-                               # dict(name='ads.dat', level=ads.WARNING),  # Uncomment to limit log ouput
-                               dict(name='ads.eng', level=ads.INFO2),
-                               dict(name='unt.dat', level=ads.DEBUG)],
-                      handlers=[logFilePathName], reset=True)
-
-    # Show testing configuration (traceability).
-    logger.info('PyAuDiSam {} from {}'
-                .format(ads.__version__, pl.Path(ads.__path__[0]).resolve().as_posix()))
-    logger.info('Python environment:')
-    logger.info('*  {}: {}'.format(sys.implementation.name, sys.version))
-    logger.info('* platform: {}'.format(sys.platform))
-    for module in ['pytest', 'pandas', 'numpy']:
-        logger.info('* {:>8s}: {}'.format(module, sys.modules[module].__version__))
-
-    logger.info('Testing pyaudisam.data ...')
-
-###############################################################################
-#                         Input Data Preparation                              #
-###############################################################################
-#   Generate DataFrame (returned) and other format files  from .ods source
-#   and return a list of sources (4 files and 1 DataFrame)
-
-KSrcRefin = KTestSrcPath / 'refin'
-KSrcRefout = KTestSrcPath / 'refout'
-
-def sources():
-
-    dfPapAlaArv = pd.read_excel(KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods')
-
-    dfPapAlaArv.to_csv(tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.csv', sep='\t', index=False)
-    dfPapAlaArv.to_excel(tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xls', index=False)
-
-    # DataSet from multiple sources from various formats (same columns).
-    # For test, list of require to contain 1 DataFrame, and one or several
-    # source files (one for each different extension)
-    # DataFrame and all files need to contain the same data
-    sources = [KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods',  # Need for module odfpy
-               KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',  # Need for module openpyxl (or xlrd)
-               tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xls',  # No need for xlwt(openpyxl seems OK)
-               tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.csv',
-               dfPapAlaArv]
-    return sources
-
-
-# same as above => to allow using pytest
-@pytest.fixture
-def sources_fxt():
-
-    return sources()
-
-
-###############################################################################
-#                          Miscellaneous Tools                                #
-###############################################################################
-def male2bool(s):
-    return False if pd.isnull(s.MALE) or s.MALE.lower() != 'oui' else True
-
-
-def sex2bool(s):
-    return False if pd.isnull(s.SEXE) or s.SEXE.lower() != 'oui' else True
-
-
-def newval2othercol(s):
-    return "New Value"
-
-
-###############################################################################
-#                                Test Cases                                   #
-###############################################################################
-# test DataSet - method "Ctor", getter "dfData", and "__len__"
-    # from various sources: .ads, .xslx, .xls, .csv and from DataFrame
-def test_DataSet_Ctor_len(sources_fxt):
-
-    # gather source DataFrame (used for checking size of DataSet)
-    srcDf = [src for src in sources_fxt if isinstance(src, pd.DataFrame)][0]
-
-    # list of awaited columns form input DataFrame with associated types
-    dTypes = {'ZONE': 'object', 'HA': 'int', 'POINT': 'int', 'ESPECE': 'object',
-              'DISTANCE': 'float', 'MALE': 'object', 'DATE': 'object', 'OBSERVATEUR': 'object',
-              'PASSAGE': 'object', 'NOMBRE': 'float', 'EFFORT': 'int'}
-
-    ds = ads.DataSet(sources_fxt, importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'],
-                     sheet='Sheet1', skipRows=None, separator='\t')
-
-    # checking size of DataFrame
-    assert ds.dfData.size == srcDf.size * len(sources_fxt), \
-        'Error: test_DataSet_Ctor_len: DataSet Ctor: Size of DataFrame "DataFrame.dfData" inconsistent with sources'
-    # checking list of columns of DataFrame
-    assert sorted(ds.columns) == sorted(dTypes.keys()), \
-        'Error: test_DataSet_Ctor_len: Ctor: Columns list from "DataFrame.dfData" mismatched with columns list of \
-        the source file'
-    # checking column dtypes of DataFrame
-    assert all(typ.name.startswith(dTypes[col]) for col, typ in ds.dfData.dtypes.items()), \
-        'Error: test_DataSet_Ctor_len: DataSet Ctor: types of data from the "DataFrame.dfData" mismatched awaited \
-        data types'
-
-    logger.info0('PASS (test_DataSet_Ctor_len) => DATASET => Constructor, getter "dfData" and \
-    method "__len__"\n(Includes _csv2df, _fromDataFrame, _fromDataFile)')
-
-
-# test setter dfData
-def test_DataSet_dfData_getter_setter(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
-
-    # Check Exception raising with dfData setter call
-    with pytest.raises(NotImplementedError) as e:
-        ds.dfData = pd.DataFrame()
-    logger.info0('PASS (test_DataSet_dfData_getter_setter) => EXCEPTION RAISED AS AWAITED with the following \
-    Exception message:\n{}'.format(e))
-
-
-# test DataSet - methoc "empty"
-def test_DataSet_empty():
-
-    emptyDf = pd.DataFrame()
-    emptyDs = ads.DataSet(emptyDf)
-    assert emptyDs.empty, 'Error: test_DataSet_empty: DataFrame from DataSet should be empty'
-
-    ds = pd.DataFrame(data={'col1': [1, 2]})
-    assert not ds.empty, 'Error: test_DataSet_empty: DataFrame from DataSet should not be empty'
-
-    logger.info0('PASS (test_DataSet_empty) => DATASET => method "empty"')
-
-
-# test DataSet - method "columns"
-def test_DataSet_columns(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
-
-    assert (ds.columns == ds.dfData.columns).all(), 'Error: test_DataSet_columns: columns: Issue occurred with \
-    method "DataSet.columns"'
-
-    logger.info0('PASS (test_DataSet_columns) => DATASET => method "columns"')
-
-
-# test columns renaming
-def test_DataSet_renameColumns(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, dRenameCols={'NOMBRE': 'INDIVIDUS', 'MALE': 'SEXE'},
-                     importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'], sheet='Sheet1', skipRows=None, separator='\t')
-
-    assert all([(col not in ds.columns) for col in ['MALE', 'NOMBRE']]) \
-        and all([(col in ds.columns) for col in ['SEXE', 'INDIVIDUS']]), 'Error: test_DataSet_renameColumns: \
-        renameColumns: Issue occurred with renaming process: columns "NOMBRE" and "MALE" should have been \
-        renamed "INDIVIDUS" and "SEXE"'
-
-    logger.info0('PASS (test_DataSet_renameColumns) => DATASET => method "renameColumns"\n(Includes _csv2df, \
-    _fromDataFrame, _fromDataFile)')
-
-
-# test columns addition, recomputing, and combination addition/recomputing
-# (_addComputedColumns) for existing or new column
-def test_DataSet_addComputedColumns(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
-
-    # Checking addition of a columns
-    ds = ads.DataSet(ds.dfData, dComputeCols={'NEWCOL': ''})
-    assert 'NEWCOL' in ds.columns, 'Error: test_DataSet_addComputedColumns: _addComputedColumns: Issue occurred with \
-    simple addition of a new column. Columns "NEWCOL" should have been added'
-
-    # Checking re-computing a columns
-    #   Checking initial data type of 'MALE' column values are not boolean
-    assert ds.dfData['MALE'].dtype != bool, 'Error: test_DataSet_addComputedColumns: Pre-test: Values from MALE \
-    column should not be bool'
-    #   Checking data type of 'MALE' column modified to boolean (dComputeCols)
-    ds = ads.DataSet(ds.dfData, dComputeCols={'MALE': male2bool})
-    assert ds.dfData['MALE'].dtype == bool, \
-        'Error: test_DataSet_addComputedColumns: _addComputedColumns: Issue occurred with recomputing existing \
-        column process. Values from MALE column should be bool'
-
-    # Checking adding a new column with computing process
-    #   Checking data type of 'NEWCOL' colonne was added with boolean (dComputeCols)
-    ds = ads.DataSet(ds.dfData, dComputeCols={'OTHERCOL': newval2othercol})
-    assert ds.dfData.OTHERCOL[0] == 'New Value', 'Error: test_DataSet_addComputedColumns: _addComputedColumns: \
-    Issue occurred with process of adding a new AND computed column.'
-
-    # Checking adding a new column with computing process and rename it
-    #   a new column also added to prepare following test step
-    ds = ads.DataSet(sources_fxt, dRenameCols={'MALE': 'SEXE'}, dComputeCols={'MALE': sex2bool, 'NEWCOL': ''},
-                     sheet='Sheet1', skipRows=None, separator='\t')
-    print(ds.columns)
-    print(ds.dfData.SEXE)
-    assert ds.dfData['SEXE'].dtype == bool, \
-        'Error: test_DataSet_addComputedColumns: _addComputedColumns: Issue occurred with process of adding and \
-        renaming a new AND computed column.'
-
-    logger.info0('PASS (test_DataSet_renameColumns) => DATASET => methods "addColumns" and "_addComputedColumns": \
-    \n\t\t\t\t\t\taddition, recomputing, and combination addition/recomputing for existing or new column checked \
-    \n\t\t\t\t\t\t(Includes _csv2df, _fromDataFrame, _fromDataFile)')
-
-
-# test method "dfSubData"
-def test_DataSet_dfSubData(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
-
-    # subset = None
-    df = ds.dfSubData()
-    assert df.compare(ds.dfData).empty, 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "None", \
-    df should be a copy of source DataSet.'
-    assert id(df) == id(ds.dfData), 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "copy = False \
-    (default)", subset of DataFrame should be a reference to the subset of source DataFrame, not a copy of data.'
-
-    df = ds.dfSubData(copy=True)
-    assert df.compare(ds.dfData).empty, 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "None", \
-    df should be a copy of source DataSet.'
-    assert id(df) != id(ds.dfData), 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "copy = True", \
-    subset of DataFrame should be a copy of data, not a reference to the subset of source DataFrame.'
-
-    # columns = as an empty list
-    df = ds.dfSubData(columns=[])
-    assert df.compare(ds.dfData).empty, 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "columns=[]", \
-    df should be equal source DataSet.'
-
-    # columns = as list
-    df = ds.dfSubData(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])
-    assert df.compare(ds.dfData.reindex(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])).empty, \
-        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=list(...)".'
-
-    # columns = as pandas.Index
-    df = ds.dfSubData(columns=pd.Index(['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']))
-    assert df.compare(ds.dfData.reindex(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])).empty, \
-        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=Index(...)".'
-
-    # columns = as other (str)
-    # Check Exception raising
-    with pytest.raises(Exception) as e:
-        df = ds.dfSubData(columns='POINT')
-    logger.info0('PASS (test_DataSet_dfSubData) => EXCEPTION RAISED AS AWAITED with the \
-    following Exception message:\n{}'.format(e))
-
-    # TODO: columns with MultiIndexed-columned dataset
-
-    # columns = as an empty list
-    df = ds.dfSubData(index=[])
-    assert df.empty, \
-        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "index=[]", df should be empty.'
-
-    # index = as list
-    df = ds.dfSubData(index=[1, 4, 7, 9, 38])
-    assert df.compare(ds.dfData.loc[[1, 4, 7, 9, 38]]).empty, \
-        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=list(...)".'
-
-    # index = as pandas.Index
-    df = ds.dfSubData(index=pd.Index([1, 4, 7, 9, 38]))
-    assert df.compare(ds.dfData.loc[[1, 4, 7, 9, 38]]).empty, \
-        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=Index(...)".'
-
-    # index = as range
-    df = ds.dfSubData(index=range(1, 300, 3))
-    assert df.compare(ds.dfData.loc[range(1, 300, 3)]).empty, \
-        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=range(...)".'
-
-    logger.info0('PASS (test_DataSet_dfSubData) => DATASET => method "dfSubData"')
-
-
-# test columns deletion
-def test_DataSet_dropColumns(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
-
-    # gather source DataFrame (used for checking size of DataSet)
-    srcDf = [src for src in sources_fxt if isinstance(src, pd.DataFrame)][0]
-
-    # Checking deletion of columns
-    dropCols = ['ZONE', 'HA', 'OBSERVATEUR']
-
-    ds.dropColumns(dropCols)
-
-    assert ds.columns.to_list() == ['POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE', 'PASSAGE', 'NOMBRE', 'EFFORT'], \
-        'Error: test_DataSet_dropColumns: dropColumns: Issue occurred with dropping columns: one or more \
-        columns were not drop.'
-
-    assert len(ds) == len(srcDf) * len(sources_fxt),  \
-        'Error: test_DataSet_dropColumns: dropColumns: Issue occurred: inconsistent size of the DataFrame next \
-        to columns dropping.'
-
-    logger.info0('PASS (test_DataSet_dropColumns) => DATASET => method "dropColumns"')
-
-
-# test rows deletion
-def test_DataSet_dropRows(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, sheet='Sheet1', dComputeCols={'MALE': male2bool}, skipRows=None, separator='\t')
-
-    # gather source DataFrame (used for checking size of DataSet)
-    srcDf = [src for src in sources_fxt if isinstance(src, pd.DataFrame)][0]
-
-    # deletion of rows with no distance noted
-    ds.dropRows(ds.dfData.DISTANCE.isnull())
-
-    # check result DataFrame is same size than entry with distance not null in source DataFrame
-    assert len(ds) == len(srcDf[srcDf.DISTANCE.notnull()]) * len(sources_fxt), \
-        'Error: test_DataSet_dropRows: dropRows: Issue occurred. Size of resulted DataFrame \
-        not consistent after deletion process.'
-
-    # check nb of rows are consistent with data before/after deletion (refer source files data)
-    assert ds.dfData.MALE.value_counts()[True] == ds.dfData.NOMBRE.sum() == srcDf.NOMBRE.sum() * len(sources_fxt), \
-        'Error: test_DataSet_dropRows: dropRows: Issue occurred. Comparison of numbers of data in resulted DataFrame \
-        not consistentafter deletion process.'
-
-    logger.info0('PASS (test_DataSet_dropRows) => DATASET => method "dropRows"')
-
-
-# DATASET TESTS
-# test methods "toExcel", "toOpenDoc", "toPickle", "compareDataFrames"
-def test_DataSet_toFiles(sources_fxt):
-
-    ds = ads.DataSet(sources_fxt, importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'],
-                     dRenameCols={'NOMBRE': 'INDIVIDUS'}, dComputeCols={'MALE': male2bool},
-                     sheet='Sheet1', skipRows=None, separator='\t')
-    ds.dropColumns(['ZONE', 'HA', 'OBSERVATEUR'])
-    ds.dropRows(ds.dfData.DISTANCE.isnull())
-    # => toExcel, toOpenDoc, toPickle, compareDataFrames
-    closenessThreshold = 15  # => max relative delta = 1e-15
-    subsetCols = ['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']
-    filePathName = tmpDir / 'dataset-uni.ods'
-    dfRef = ds.dfSubData(columns=subsetCols).reset_index(drop=True)
-
-    for fpn in [filePathName, filePathName.with_suffix('.xlsx'), filePathName.with_suffix('.xls'),
-                filePathName.with_suffix('.pickle'), filePathName.with_suffix('.pickle.xz')]:
-
-        print(fpn.as_posix(), end=' : ')
-        if fpn.suffix == '.ods':
-            ds.toOpenDoc(fpn, sheetName='utest', subset=subsetCols, index=False)
-        elif fpn.suffix in ['.xlsx', '.xls']:
-            ds.toExcel(fpn, sheetName='utest', subset=subsetCols, index=False)
-        elif fpn.suffix in ['.pickle', '.xz']:
-            ds.toPickle(fpn, subset=subsetCols, index=False)
-        assert fpn.is_file(), 'Error: test_DataSet_toFiles'
-
-        if fpn.suffix in ['.ods', '.xlsx', '.xls']:
-            df = pd.read_excel(fpn, sheet_name='utest')
-        elif fpn.suffix in ['.pickle', '.xz']:
-            df = pd.read_pickle(fpn)
-            df.reset_index(drop=True, inplace=True)
-        assert ds.compareDataFrames(df.reset_index(), dfRef.reset_index(),
-                                    subsetCols=['POINT', 'DISTANCE', 'INDIVIDUS', 'EFFORT'],
-                                    indexCols=['index'], dropCloser=closenessThreshold, dropNans=True).empty
-        print('1e-{} comparison OK (df.equals(dfRef) is {}, df.compare(dfRef) {}empty)'.
-              format(closenessThreshold, df.equals(dfRef), '' if df.compare(dfRef).empty else 'not')), \
-            'Error: test_DataSet_toFiles'
-
-    logger.info0('PASS (test_DataSet_toFiles) => DATASET => method "toExcel",'
-                 ' "toOpenDoc", "toPickle", "compareDataFrames"')
-
-
-# Test of the base function for comparison (test from static hard-coded data, not from loaded DataSets)
-# method "_closeness" used for
-def test_closeness():
-    # creation of empty DataSet
-    ds = ads.DataSet(pd.DataFrame())
-
-    values = [np.nan, -np.inf,
-              -1.0e12, -1.0e5, -1.0-1e-5, -1.0, -1.0+1e-5, -1.0e-8,
-              0.0, 1.0e-8, 1.0, 1.0e5, 1.0e12, np.inf]
-    aClose = np.ndarray(shape=(len(values), len(values)))
-
-    for r in range(len(values)):
-        for c in range(len(values)):
-            try:
-                aClose[r, c] = ds._closeness(pd.Series([values[r], values[c]]))
-            except Exception as exc:
-                print(exc, r, c, values[r], values[c])
-                raise
-
-    # Infinite closeness on the diagonal (except for nan and +/-inf)
-    assert all(np.isnan(values[i]) or np.isinf(values[i]) or np.isinf(aClose[i, i]) for i in range(len(values))), \
-           'Error: test_closeness: Inequality on the diagonal'
-
-    # No infinite proximity elsewhere
-    assert all(r == c or not np.isinf(aClose[r, c]) for r in range(len(values)) for c in range(len(values))), \
-           'Error: test_closeness: No equality should be found outside the diagonal'
-
-    # Good closeness around -1 only
-    whereClose = [i for i in range(len(values)) if abs(values[i] + 1) <= 1.0e-5]
-    assert all(aClose[r, c] > 4 for r in whereClose for c in whereClose), \
-        'Error: test_closeness: Unexpectedly bad closeness around -1'
-
-    logger.info0('PASS (test_closeness) => DATASET => method "_closeness"')
-
-
-# Comparison (from other files data sources, the same as for ResultsSet.compare below, but through DataSet)
-# => compare, compareDataFrames, _toHashable, _closeness
-def test_compare():
-    # a. Loading of Distance 7 and values to compare issued from PyAuDiSam
-    dsDist = ads.DataSet(KSrcRefin / 'ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods',
-                         sheet='RefDist73', skipRows=[3], headerRows=[0, 1, 2], indexCols=0)
-
-    dsAuto = ads.DataSet(KSrcRefin / 'ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods',
-                         sheet='ActAuto', skipRows=[3], headerRows=[0, 1, 2], indexCols=0)
-
-    # b. Index columns to be compared
-    indexCols = [('sample', 'AnlysNum', 'Value')] \
-                + [('sample', col, 'Value') for col in ['Species', 'Periods', 'Prec.', 'Duration']] \
-                + [('model', 'Model', 'Value')] \
-                + [('parameters', 'left truncation distance', 'Value'),
-                   ('parameters', 'right truncation distance', 'Value'),
-                   ('parameters', 'model fitting distance cut points', 'Value'),
-                   ('parameters', 'distance discretisation cut points', 'Value')]
-
-    # # c. Columns to be compared (DeltaDCV and DeltaAIC were removed as results are dependant of a set of ran analyses,
-    # #    different between reference and PyAuDiSam run.
-    subsetCols = [col for col in dsDist.dfData.columns.to_list()
-                  if col not in indexCols + [('run output', 'run time', 'Value'),
-                                             ('density/abundance', 'density of animals', 'Delta Cv'),
-                                             ('detection probability', 'Delta AIC', 'Value')]]
-
-    # # d. "Exact" Comparison : no line pass (majority of epsilons due to IO ODS)
-    dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols)
-    assert len(dfRelDiff) == len(dsDist), 'Error: test_compare: compare: Issue occurred. Exact row by row comparison \
-    of both DataSets should fail for all rows.'
-
-    # # e. Comparison with 10**-16 accuracy : almost all lines pass, but 3 (majority of epsilons due to IO ODS)
-    dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=16, dropNans=True)
-    assert len(dfRelDiff) == 3, 'Error: test_compare: compare: Issue occurred. Row by row comparison of both DataSet \
-    with accuracy of 10**-16 should fail for all rows but 3.'
-    print(dfRelDiff)
-
-    # # e. Comparison with 10**-5 accuracy : all lines pass
-    dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=5, dropNans=True)
-    assert len(dfRelDiff) == 2,  'Error: test_compare: compare: Issue occurred. Row by row comparison of both DataSet \
-    with accuracy of 10**5 should pass for all rows.'
-
-    logger.info0('PASS (test_compare) => DATASET => method "_closeness" and "compare"')
-
-
-# SAMPLEDATASET TESTS
-# test methods "toExcel", "toOpenDoc", "toPickle", "compareDataFrames"
-def test_SDS_Ctor():
-    # test: creation of SDS from DataFrame source.
-    dfData = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],
-                          data=[('2019-05-13', 3.5, 'TURMER', 23, 2, 83),
-                                ('2019-05-15', np.nan, 'TURMER', 23, 2, 27.355),
-                                ('2019-05-13', 0, 'ALAARV', 29, 2, 56.85),
-                                ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3, 7.2),
-                                ('2019-06-01', 2, 'PHICOL', 12, 1, np.nan),
-                                ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),
-                                ])
-    dfData['Region'] = 'ACDC'
-    dfData['Surface'] = '2400'
-
-    sds = ads.SampleDataSet(source=dfData, decimalFields=['Effort', 'Distance', 'TrucDec'])
-
-    assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \
-        'Error: test_SDS_Ctor: Some strings found in declared decimal fields ... any decimal format issue ?'
-
-    assert sds.columns.equals(dfData.columns), 'Error: test_SDS_Ctor: inconsistency with columns list \
-    of SDS vs source DataFrame'
-    assert len(sds) == len(dfData),  'Error: test_SDS_Ctor: inconsistency with size of SDS vs source DataFrame'
-    assert sds.dfData.Distance.notnull().sum() == 4, 'Error: test_SDS_Ctor: issue with NaN values \
-    that should be considered a null'
-
-    # test: empty SDS raises exception
-    emptyDf = pd.DataFrame()
-    with pytest.raises(Exception) as e_info:
-        ads.SampleDataSet(emptyDf)
-    logger.info0('PASS (test_SDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'.
-                 format(e_info.value))
-
-    # test: exception raised if SDS created with DataFrame < 5 columns
-    dfData = pd.DataFrame(columns=['Col1', 'Col2', 'Col3', 'Col4'], data=[(1, 2, 3, 4)])
-    with pytest.raises(Exception) as e_info:
-        sds = ads.SampleDataSet(source=dfData)
-    logger.info0('PASS (test_SDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'.
-                 format(e_info.value))
-
-    # Excel source (path as simple string)
-    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',
-                            decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])
-
-    assert sds.columns.to_list() == ['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',
-                                     'OBSERVATEUR', 'PASSAGE', 'NOMBRE', 'EFFORT'], \
-        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
-    assert len(sds) == 256, 'Error: test_SDS_Ctor: inconsistency with size of the resulting SDS. \
-    It should contain 256 rows'
-    assert sds.dfData.NOMBRE.sum() == 217, 'Error: test_SDS_Ctor: inconsistency with data loaded in SDS: \
-    Sum of values from column "NOMBRE" should be 217'
-
-    # Libre / Open Office source (path as simple string)
-    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods',
-                            decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])
-
-    assert sds.columns.to_list() == ['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',
-                                     'OBSERVATEUR', 'PASSAGE', 'NOMBRE', 'EFFORT'], \
-        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
-    assert len(sds) == 256, 'Error: inconsistency with size of the resulting SDS. It should contain 256 rows'
-    assert sds.dfData.NOMBRE.sum() == 217, 'Error: test_SDS_Ctor: inconsistency with data loaded in SDS: Sum of values \
-    from column "NOMBRE" should be 217'
-
-    # CSV source with ',' as decimal point (path as pl.Path)
-    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-TURMER-AB-5mn-1dec-dist.txt',
-                            decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])
-
-    assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \
-        'Error: test_SDS_Ctor: Some strings found in declared decimal fields ... any decimal format issue ?'
-
-    assert sds.columns.to_list() == ['Region*Label', 'Region*Area', 'Point transect*Label',
-                                     'Point transect*Survey effort', 'Observation*Radial distance'], \
-        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
-    assert len(sds) == 330, 'Error: test_SDS_Ctor: inconsistency with size of the resulting SDS. \
-    It should contain 330 rows'
-    assert sds.dfData['Observation*Radial distance'].notnull().sum() == 324, 'Error: test_SDS_Ctor: inconsistency with \
-    data loaded in SDS: Sum of values from column "Observation*Radial distance" should be 324'
-
-    # CSV source with '.' as decimal point
-    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt',
-                            decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])
-
-    assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \
-        'Error: test_SDS_Ctor: Some strings found in declared decimal fields ... any decimal format issue ?'
-
-    assert sds.columns.to_list() == ['Region*Label', 'Region*Area', 'Point transect*Label',
-                                     'Point transect*Survey effort', 'Observation*Radial distance'], \
-        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
-    assert len(sds) == 256,  'Error: test_SDS_Ctor: inconsistency with size of the resulting SDS. \
-    It should contain 256 rows'
-    assert sds.dfData['Observation*Radial distance'].notnull().sum() == 217, 'Error: test_SDS_Ctor: inconsistency with \
-    data loaded in SDS: Sum of values from column "Observation*Radial distance" should be 217'
-
-    logger.info0('PASS (test_SDS_Ctor) => SampleDATASET => Constructor')
-
-###############################################################################
-#                         Actions to be done after all tests                  #
-###############################################################################
-def test_final():
-
-    logger.info('Done testing pyaudisam.data.')
-
-
-if __name__ == '__main__':
-
-    run = True
-    # Run auto-tests (exit(0) if OK, 1 if not).
-    rc = -1
-
-    if run:
-        try:
-            test_init()
-
-            # Tests for DataSet
-            test_DataSet_Ctor_len(sources())
-            test_DataSet_dfData_getter_setter(sources())
-            test_DataSet_empty()
-            test_DataSet_columns(sources())
-            test_DataSet_renameColumns(sources())
-            test_DataSet_addComputedColumns(sources())
-            test_DataSet_dropColumns(sources())
-            test_DataSet_dfSubData(sources())
-            test_DataSet_dropRows(sources())
-            test_DataSet_toFiles(sources())
-            test_closeness()
-            test_compare()
-
-            # Tests for SampleDataSet
-            test_SDS_Ctor()
-
-            # Success !
-            rc = 0
-
-        except Exception as exc:
-            logger.exception('Exception: ' + str(exc))
-            rc = 1
-
-    logger.info('Done unit integration testing pyaudisam.data: {} (code: {})'
-                .format({-1: 'Not run', 0: 'Success'}.get(rc, 'Error'), rc))
-    sys.exit(rc)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret, Sylvain Sainnier
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Automated unit and integration tests for "data" submodule
+
+# To run : simply run "pytest" or "python <this file>" in current folder
+#          and check standard output ; and ./tmp/unit-int-test.{datetime}.log for details
+
+import sys
+import pathlib as pl
+import pandas as pd
+import numpy as np
+
+import pyaudisam as ads
+
+import pytest
+
+
+# The "tests" folder.
+KTestSrcPath = pl.Path(__file__).parent
+
+# Temporary work folder (created once in conftest.py if needed).
+tmpDir = KTestSrcPath / 'tmp'
+
+# Logger for this module.
+logger = ads.logger('unt.dat')
+
+###############################################################################
+#                         Actions to be done before any test                  #
+###############################################################################
+def test_init():
+
+    # Configure logging.
+    logFilePathName = tmpDir / 'unt-dat.{}.log'.format(pd.Timestamp.now().strftime('%Y%m%d%H%M'))
+    ads.log.configure(loggers=[dict(name='matplotlib', level=ads.WARNING),
+                               dict(name='ads', level=ads.INFO),
+                               # dict(name='ads.dat', level=ads.WARNING),  # Uncomment to limit log ouput
+                               dict(name='ads.eng', level=ads.INFO2),
+                               dict(name='unt.dat', level=ads.DEBUG)],
+                      handlers=[logFilePathName], reset=True)
+
+    # Show testing configuration (traceability).
+    logger.info('PyAuDiSam {} from {}'
+                .format(ads.__version__, pl.Path(ads.__path__[0]).resolve().as_posix()))
+    logger.info('Python environment:')
+    logger.info('*  {}: {}'.format(sys.implementation.name, sys.version))
+    logger.info('* platform: {}'.format(sys.platform))
+    for module in ['pytest', 'pandas', 'numpy']:
+        logger.info('* {:>8s}: {}'.format(module, sys.modules[module].__version__))
+
+    logger.info('Testing pyaudisam.data ...')
+
+###############################################################################
+#                         Input Data Preparation                              #
+###############################################################################
+#   Generate DataFrame (returned) and other format files  from .ods source
+#   and return a list of sources (4 files and 1 DataFrame)
+
+KSrcRefin = KTestSrcPath / 'refin'
+KSrcRefout = KTestSrcPath / 'refout'
+
+def sources():
+
+    dfPapAlaArv = pd.read_excel(KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods')
+
+    dfPapAlaArv.to_csv(tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.csv', sep='\t', index=False)
+    dfPapAlaArv.to_excel(tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xls', index=False)
+
+    # DataSet from multiple sources from various formats (same columns).
+    # For test, list of require to contain 1 DataFrame, and one or several
+    # source files (one for each different extension)
+    # DataFrame and all files need to contain the same data
+    sources = [KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods',  # Need for module odfpy
+               KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',  # Need for module openpyxl (or xlrd)
+               tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xls',  # No need for xlwt(openpyxl seems OK)
+               tmpDir / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.csv',
+               dfPapAlaArv]
+    return sources
+
+
+# same as above => to allow using pytest
+@pytest.fixture
+def sources_fxt():
+
+    return sources()
+
+
+###############################################################################
+#                          Miscellaneous Tools                                #
+###############################################################################
+def male2bool(s):
+    return False if pd.isnull(s.MALE) or s.MALE.lower() != 'oui' else True
+
+
+def sex2bool(s):
+    return False if pd.isnull(s.SEXE) or s.SEXE.lower() != 'oui' else True
+
+
+def newval2othercol(s):
+    return "New Value"
+
+
+###############################################################################
+#                                Test Cases                                   #
+###############################################################################
+# test DataSet - method "Ctor", getter "dfData", and "__len__"
+    # from various sources: .ads, .xslx, .xls, .csv and from DataFrame
+def test_DataSet_Ctor_len(sources_fxt):
+
+    # gather source DataFrame (used for checking size of DataSet)
+    srcDf = [src for src in sources_fxt if isinstance(src, pd.DataFrame)][0]
+
+    # list of awaited columns form input DataFrame with associated types
+    dTypes = {'ZONE': 'object', 'HA': 'int', 'POINT': 'int', 'ESPECE': 'object',
+              'DISTANCE': 'float', 'MALE': 'object', 'DATE': 'object', 'OBSERVATEUR': 'object',
+              'PASSAGE': 'object', 'NOMBRE': 'float', 'EFFORT': 'int'}
+
+    ds = ads.DataSet(sources_fxt, importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'],
+                     sheet='Sheet1', skipRows=None, separator='\t')
+
+    # checking size of DataFrame
+    assert ds.dfData.size == srcDf.size * len(sources_fxt), \
+        'Error: test_DataSet_Ctor_len: DataSet Ctor: Size of DataFrame "DataFrame.dfData" inconsistent with sources'
+    # checking list of columns of DataFrame
+    assert sorted(ds.columns) == sorted(dTypes.keys()), \
+        'Error: test_DataSet_Ctor_len: Ctor: Columns list from "DataFrame.dfData" mismatched with columns list of \
+        the source file'
+    # checking column dtypes of DataFrame
+    assert all(typ.name.startswith(dTypes[col]) for col, typ in ds.dfData.dtypes.items()), \
+        'Error: test_DataSet_Ctor_len: DataSet Ctor: types of data from the "DataFrame.dfData" mismatched awaited \
+        data types'
+
+    logger.info0('PASS (test_DataSet_Ctor_len) => DATASET => Constructor, getter "dfData" and \
+    method "__len__"\n(Includes _csv2df, _fromDataFrame, _fromDataFile)')
+
+
+# test setter dfData
+def test_DataSet_dfData_getter_setter(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
+
+    # Check Exception raising with dfData setter call
+    with pytest.raises(NotImplementedError) as e:
+        ds.dfData = pd.DataFrame()
+    logger.info0('PASS (test_DataSet_dfData_getter_setter) => EXCEPTION RAISED AS AWAITED with the following \
+    Exception message:\n{}'.format(e))
+
+
+# test DataSet - methoc "empty"
+def test_DataSet_empty():
+
+    emptyDf = pd.DataFrame()
+    emptyDs = ads.DataSet(emptyDf)
+    assert emptyDs.empty, 'Error: test_DataSet_empty: DataFrame from DataSet should be empty'
+
+    ds = pd.DataFrame(data={'col1': [1, 2]})
+    assert not ds.empty, 'Error: test_DataSet_empty: DataFrame from DataSet should not be empty'
+
+    logger.info0('PASS (test_DataSet_empty) => DATASET => method "empty"')
+
+
+# test DataSet - method "columns"
+def test_DataSet_columns(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
+
+    assert (ds.columns == ds.dfData.columns).all(), 'Error: test_DataSet_columns: columns: Issue occurred with \
+    method "DataSet.columns"'
+
+    logger.info0('PASS (test_DataSet_columns) => DATASET => method "columns"')
+
+
+# test columns renaming
+def test_DataSet_renameColumns(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, dRenameCols={'NOMBRE': 'INDIVIDUS', 'MALE': 'SEXE'},
+                     importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'], sheet='Sheet1', skipRows=None, separator='\t')
+
+    assert all([(col not in ds.columns) for col in ['MALE', 'NOMBRE']]) \
+        and all([(col in ds.columns) for col in ['SEXE', 'INDIVIDUS']]), 'Error: test_DataSet_renameColumns: \
+        renameColumns: Issue occurred with renaming process: columns "NOMBRE" and "MALE" should have been \
+        renamed "INDIVIDUS" and "SEXE"'
+
+    logger.info0('PASS (test_DataSet_renameColumns) => DATASET => method "renameColumns"\n(Includes _csv2df, \
+    _fromDataFrame, _fromDataFile)')
+
+
+# test columns addition, recomputing, and combination addition/recomputing
+# (_addComputedColumns) for existing or new column
+def test_DataSet_addComputedColumns(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
+
+    # Checking addition of a columns
+    ds = ads.DataSet(ds.dfData, dComputeCols={'NEWCOL': ''})
+    assert 'NEWCOL' in ds.columns, 'Error: test_DataSet_addComputedColumns: _addComputedColumns: Issue occurred with \
+    simple addition of a new column. Columns "NEWCOL" should have been added'
+
+    # Checking re-computing a columns
+    #   Checking initial data type of 'MALE' column values are not boolean
+    assert ds.dfData['MALE'].dtype != bool, 'Error: test_DataSet_addComputedColumns: Pre-test: Values from MALE \
+    column should not be bool'
+    #   Checking data type of 'MALE' column modified to boolean (dComputeCols)
+    ds = ads.DataSet(ds.dfData, dComputeCols={'MALE': male2bool})
+    assert ds.dfData['MALE'].dtype == bool, \
+        'Error: test_DataSet_addComputedColumns: _addComputedColumns: Issue occurred with recomputing existing \
+        column process. Values from MALE column should be bool'
+
+    # Checking adding a new column with computing process
+    #   Checking data type of 'NEWCOL' colonne was added with boolean (dComputeCols)
+    ds = ads.DataSet(ds.dfData, dComputeCols={'OTHERCOL': newval2othercol})
+    assert ds.dfData.OTHERCOL[0] == 'New Value', 'Error: test_DataSet_addComputedColumns: _addComputedColumns: \
+    Issue occurred with process of adding a new AND computed column.'
+
+    # Checking adding a new column with computing process and rename it
+    #   a new column also added to prepare following test step
+    ds = ads.DataSet(sources_fxt, dRenameCols={'MALE': 'SEXE'}, dComputeCols={'MALE': sex2bool, 'NEWCOL': ''},
+                     sheet='Sheet1', skipRows=None, separator='\t')
+    print(ds.columns)
+    print(ds.dfData.SEXE)
+    assert ds.dfData['SEXE'].dtype == bool, \
+        'Error: test_DataSet_addComputedColumns: _addComputedColumns: Issue occurred with process of adding and \
+        renaming a new AND computed column.'
+
+    logger.info0('PASS (test_DataSet_renameColumns) => DATASET => methods "addColumns" and "_addComputedColumns": \
+    \n\t\t\t\t\t\taddition, recomputing, and combination addition/recomputing for existing or new column checked \
+    \n\t\t\t\t\t\t(Includes _csv2df, _fromDataFrame, _fromDataFile)')
+
+
+# test method "dfSubData"
+def test_DataSet_dfSubData(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
+
+    # subset = None
+    df = ds.dfSubData()
+    assert df.compare(ds.dfData).empty, 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "None", \
+    df should be a copy of source DataSet.'
+    assert id(df) == id(ds.dfData), 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "copy = False \
+    (default)", subset of DataFrame should be a reference to the subset of source DataFrame, not a copy of data.'
+
+    df = ds.dfSubData(copy=True)
+    assert df.compare(ds.dfData).empty, 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "None", \
+    df should be a copy of source DataSet.'
+    assert id(df) != id(ds.dfData), 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "copy = True", \
+    subset of DataFrame should be a copy of data, not a reference to the subset of source DataFrame.'
+
+    # columns = as an empty list
+    df = ds.dfSubData(columns=[])
+    assert df.compare(ds.dfData).empty, 'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "columns=[]", \
+    df should be equal source DataSet.'
+
+    # columns = as list
+    df = ds.dfSubData(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])
+    assert df.compare(ds.dfData.reindex(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])).empty, \
+        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=list(...)".'
+
+    # columns = as pandas.Index
+    df = ds.dfSubData(columns=pd.Index(['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']))
+    assert df.compare(ds.dfData.reindex(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])).empty, \
+        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=Index(...)".'
+
+    # columns = as other (str)
+    # Check Exception raising
+    with pytest.raises(Exception) as e:
+        df = ds.dfSubData(columns='POINT')
+    logger.info0('PASS (test_DataSet_dfSubData) => EXCEPTION RAISED AS AWAITED with the \
+    following Exception message:\n{}'.format(e))
+
+    # TODO: columns with MultiIndexed-columned dataset
+
+    # columns = as an empty list
+    df = ds.dfSubData(index=[])
+    assert df.empty, \
+        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred. With "index=[]", df should be empty.'
+
+    # index = as list
+    df = ds.dfSubData(index=[1, 4, 7, 9, 38])
+    assert df.compare(ds.dfData.loc[[1, 4, 7, 9, 38]]).empty, \
+        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=list(...)".'
+
+    # index = as pandas.Index
+    df = ds.dfSubData(index=pd.Index([1, 4, 7, 9, 38]))
+    assert df.compare(ds.dfData.loc[[1, 4, 7, 9, 38]]).empty, \
+        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=Index(...)".'
+
+    # index = as range
+    df = ds.dfSubData(index=range(1, 300, 3))
+    assert df.compare(ds.dfData.loc[range(1, 300, 3)]).empty, \
+        'Error: test_DataSet_dfSubData: dfSubData: Issue occurred with "columns=range(...)".'
+
+    logger.info0('PASS (test_DataSet_dfSubData) => DATASET => method "dfSubData"')
+
+
+# test columns deletion
+def test_DataSet_dropColumns(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, sheet='Sheet1', skipRows=None, separator='\t')
+
+    # gather source DataFrame (used for checking size of DataSet)
+    srcDf = [src for src in sources_fxt if isinstance(src, pd.DataFrame)][0]
+
+    # Checking deletion of columns
+    dropCols = ['ZONE', 'HA', 'OBSERVATEUR']
+
+    ds.dropColumns(dropCols)
+
+    assert ds.columns.to_list() == ['POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE', 'PASSAGE', 'NOMBRE', 'EFFORT'], \
+        'Error: test_DataSet_dropColumns: dropColumns: Issue occurred with dropping columns: one or more \
+        columns were not drop.'
+
+    assert len(ds) == len(srcDf) * len(sources_fxt),  \
+        'Error: test_DataSet_dropColumns: dropColumns: Issue occurred: inconsistent size of the DataFrame next \
+        to columns dropping.'
+
+    logger.info0('PASS (test_DataSet_dropColumns) => DATASET => method "dropColumns"')
+
+
+# test rows deletion
+def test_DataSet_dropRows(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, sheet='Sheet1', dComputeCols={'MALE': male2bool}, skipRows=None, separator='\t')
+
+    # gather source DataFrame (used for checking size of DataSet)
+    srcDf = [src for src in sources_fxt if isinstance(src, pd.DataFrame)][0]
+
+    # deletion of rows with no distance noted
+    ds.dropRows(ds.dfData.DISTANCE.isnull())
+
+    # check result DataFrame is same size than entry with distance not null in source DataFrame
+    assert len(ds) == len(srcDf[srcDf.DISTANCE.notnull()]) * len(sources_fxt), \
+        'Error: test_DataSet_dropRows: dropRows: Issue occurred. Size of resulted DataFrame \
+        not consistent after deletion process.'
+
+    # check nb of rows are consistent with data before/after deletion (refer source files data)
+    assert ds.dfData.MALE.value_counts()[True] == ds.dfData.NOMBRE.sum() == srcDf.NOMBRE.sum() * len(sources_fxt), \
+        'Error: test_DataSet_dropRows: dropRows: Issue occurred. Comparison of numbers of data in resulted DataFrame \
+        not consistentafter deletion process.'
+
+    logger.info0('PASS (test_DataSet_dropRows) => DATASET => method "dropRows"')
+
+
+# DATASET TESTS
+# test methods "toExcel", "toOpenDoc", "toPickle", "compareDataFrames"
+def test_DataSet_toFiles(sources_fxt):
+
+    ds = ads.DataSet(sources_fxt, importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'],
+                     dRenameCols={'NOMBRE': 'INDIVIDUS'}, dComputeCols={'MALE': male2bool},
+                     sheet='Sheet1', skipRows=None, separator='\t')
+    ds.dropColumns(['ZONE', 'HA', 'OBSERVATEUR'])
+    ds.dropRows(ds.dfData.DISTANCE.isnull())
+    # => toExcel, toOpenDoc, toPickle, compareDataFrames
+    closenessThreshold = 15  # => max relative delta = 1e-15
+    subsetCols = ['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']
+    filePathName = tmpDir / 'dataset-uni.ods'
+    dfRef = ds.dfSubData(columns=subsetCols).reset_index(drop=True)
+
+    for fpn in [filePathName, filePathName.with_suffix('.xlsx'), filePathName.with_suffix('.xls'),
+                filePathName.with_suffix('.pickle'), filePathName.with_suffix('.pickle.xz')]:
+
+        print(fpn.as_posix(), end=' : ')
+        if fpn.suffix == '.ods':
+            ds.toOpenDoc(fpn, sheetName='utest', subset=subsetCols, index=False)
+        elif fpn.suffix in ['.xlsx', '.xls']:
+            ds.toExcel(fpn, sheetName='utest', subset=subsetCols, index=False)
+        elif fpn.suffix in ['.pickle', '.xz']:
+            ds.toPickle(fpn, subset=subsetCols, index=False)
+        assert fpn.is_file(), 'Error: test_DataSet_toFiles'
+
+        if fpn.suffix in ['.ods', '.xlsx', '.xls']:
+            df = pd.read_excel(fpn, sheet_name='utest')
+        elif fpn.suffix in ['.pickle', '.xz']:
+            df = pd.read_pickle(fpn)
+            df.reset_index(drop=True, inplace=True)
+        assert ds.compareDataFrames(df.reset_index(), dfRef.reset_index(),
+                                    subsetCols=['POINT', 'DISTANCE', 'INDIVIDUS', 'EFFORT'],
+                                    indexCols=['index'], dropCloser=closenessThreshold, dropNans=True).empty
+        print('1e-{} comparison OK (df.equals(dfRef) is {}, df.compare(dfRef) {}empty)'.
+              format(closenessThreshold, df.equals(dfRef), '' if df.compare(dfRef).empty else 'not')), \
+            'Error: test_DataSet_toFiles'
+
+    logger.info0('PASS (test_DataSet_toFiles) => DATASET => method "toExcel",'
+                 ' "toOpenDoc", "toPickle", "compareDataFrames"')
+
+
+# Test of the base function for comparison (test from static hard-coded data, not from loaded DataSets)
+# method "_closeness" used for
+def test_closeness():
+    # creation of empty DataSet
+    ds = ads.DataSet(pd.DataFrame())
+
+    values = [np.nan, -np.inf,
+              -1.0e12, -1.0e5, -1.0-1e-5, -1.0, -1.0+1e-5, -1.0e-8,
+              0.0, 1.0e-8, 1.0, 1.0e5, 1.0e12, np.inf]
+    aClose = np.ndarray(shape=(len(values), len(values)))
+
+    for r in range(len(values)):
+        for c in range(len(values)):
+            try:
+                aClose[r, c] = ds._closeness(pd.Series([values[r], values[c]]))
+            except Exception as exc:
+                print(exc, r, c, values[r], values[c])
+                raise
+
+    # Infinite closeness on the diagonal (except for nan and +/-inf)
+    assert all(np.isnan(values[i]) or np.isinf(values[i]) or np.isinf(aClose[i, i]) for i in range(len(values))), \
+           'Error: test_closeness: Inequality on the diagonal'
+
+    # No infinite proximity elsewhere
+    assert all(r == c or not np.isinf(aClose[r, c]) for r in range(len(values)) for c in range(len(values))), \
+           'Error: test_closeness: No equality should be found outside the diagonal'
+
+    # Good closeness around -1 only
+    whereClose = [i for i in range(len(values)) if abs(values[i] + 1) <= 1.0e-5]
+    assert all(aClose[r, c] > 4 for r in whereClose for c in whereClose), \
+        'Error: test_closeness: Unexpectedly bad closeness around -1'
+
+    logger.info0('PASS (test_closeness) => DATASET => method "_closeness"')
+
+
+# Comparison (from other files data sources, the same as for ResultsSet.compare below, but through DataSet)
+# => compare, compareDataFrames, _toHashable, _closeness
+def test_compare():
+    # a. Loading of Distance 7 and values to compare issued from PyAuDiSam
+    dsDist = ads.DataSet(KSrcRefin / 'ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods',
+                         sheet='RefDist73', skipRows=[3], headerRows=[0, 1, 2], indexCols=0)
+
+    dsAuto = ads.DataSet(KSrcRefin / 'ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods',
+                         sheet='ActAuto', skipRows=[3], headerRows=[0, 1, 2], indexCols=0)
+
+    # b. Index columns to be compared
+    indexCols = [('sample', 'AnlysNum', 'Value')] \
+                + [('sample', col, 'Value') for col in ['Species', 'Periods', 'Prec.', 'Duration']] \
+                + [('model', 'Model', 'Value')] \
+                + [('parameters', 'left truncation distance', 'Value'),
+                   ('parameters', 'right truncation distance', 'Value'),
+                   ('parameters', 'model fitting distance cut points', 'Value'),
+                   ('parameters', 'distance discretisation cut points', 'Value')]
+
+    # # c. Columns to be compared (DeltaDCV and DeltaAIC were removed as results are dependant of a set of ran analyses,
+    # #    different between reference and PyAuDiSam run.
+    subsetCols = [col for col in dsDist.dfData.columns.to_list()
+                  if col not in indexCols + [('run output', 'run time', 'Value'),
+                                             ('density/abundance', 'density of animals', 'Delta Cv'),
+                                             ('detection probability', 'Delta AIC', 'Value')]]
+
+    # # d. "Exact" Comparison : no line pass (majority of epsilons due to IO ODS)
+    dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols)
+    assert len(dfRelDiff) == len(dsDist), 'Error: test_compare: compare: Issue occurred. Exact row by row comparison \
+    of both DataSets should fail for all rows.'
+
+    # # e. Comparison with 10**-16 accuracy : almost all lines pass, but 3 (majority of epsilons due to IO ODS)
+    dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=16, dropNans=True)
+    assert len(dfRelDiff) == 3, 'Error: test_compare: compare: Issue occurred. Row by row comparison of both DataSet \
+    with accuracy of 10**-16 should fail for all rows but 3.'
+    print(dfRelDiff)
+
+    # # e. Comparison with 10**-5 accuracy : all lines pass
+    dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=5, dropNans=True)
+    assert len(dfRelDiff) == 2,  'Error: test_compare: compare: Issue occurred. Row by row comparison of both DataSet \
+    with accuracy of 10**5 should pass for all rows.'
+
+    logger.info0('PASS (test_compare) => DATASET => method "_closeness" and "compare"')
+
+
+# SAMPLEDATASET TESTS
+# test methods "toExcel", "toOpenDoc", "toPickle", "compareDataFrames"
+def test_SDS_Ctor():
+    # test: creation of SDS from DataFrame source.
+    dfData = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],
+                          data=[('2019-05-13', 3.5, 'TURMER', 23, 2, 83),
+                                ('2019-05-15', np.nan, 'TURMER', 23, 2, 27.355),
+                                ('2019-05-13', 0, 'ALAARV', 29, 2, 56.85),
+                                ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3, 7.2),
+                                ('2019-06-01', 2, 'PHICOL', 12, 1, np.nan),
+                                ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),
+                                ])
+    dfData['Region'] = 'ACDC'
+    dfData['Surface'] = '2400'
+
+    sds = ads.SampleDataSet(source=dfData, decimalFields=['Effort', 'Distance', 'TrucDec'])
+
+    assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \
+        'Error: test_SDS_Ctor: Some strings found in declared decimal fields ... any decimal format issue ?'
+
+    assert sds.columns.equals(dfData.columns), 'Error: test_SDS_Ctor: inconsistency with columns list \
+    of SDS vs source DataFrame'
+    assert len(sds) == len(dfData),  'Error: test_SDS_Ctor: inconsistency with size of SDS vs source DataFrame'
+    assert sds.dfData.Distance.notnull().sum() == 4, 'Error: test_SDS_Ctor: issue with NaN values \
+    that should be considered a null'
+
+    # test: empty SDS raises exception
+    emptyDf = pd.DataFrame()
+    with pytest.raises(Exception) as e_info:
+        ads.SampleDataSet(emptyDf)
+    logger.info0('PASS (test_SDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'.
+                 format(e_info.value))
+
+    # test: exception raised if SDS created with DataFrame < 5 columns
+    dfData = pd.DataFrame(columns=['Col1', 'Col2', 'Col3', 'Col4'], data=[(1, 2, 3, 4)])
+    with pytest.raises(Exception) as e_info:
+        sds = ads.SampleDataSet(source=dfData)
+    logger.info0('PASS (test_SDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'.
+                 format(e_info.value))
+
+    # Excel source (path as simple string)
+    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',
+                            decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])
+
+    assert sds.columns.to_list() == ['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',
+                                     'OBSERVATEUR', 'PASSAGE', 'NOMBRE', 'EFFORT'], \
+        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
+    assert len(sds) == 256, 'Error: test_SDS_Ctor: inconsistency with size of the resulting SDS. \
+    It should contain 256 rows'
+    assert sds.dfData.NOMBRE.sum() == 217, 'Error: test_SDS_Ctor: inconsistency with data loaded in SDS: \
+    Sum of values from column "NOMBRE" should be 217'
+
+    # Libre / Open Office source (path as simple string)
+    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods',
+                            decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])
+
+    assert sds.columns.to_list() == ['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',
+                                     'OBSERVATEUR', 'PASSAGE', 'NOMBRE', 'EFFORT'], \
+        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
+    assert len(sds) == 256, 'Error: inconsistency with size of the resulting SDS. It should contain 256 rows'
+    assert sds.dfData.NOMBRE.sum() == 217, 'Error: test_SDS_Ctor: inconsistency with data loaded in SDS: Sum of values \
+    from column "NOMBRE" should be 217'
+
+    # CSV source with ',' as decimal point (path as pl.Path)
+    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-TURMER-AB-5mn-1dec-dist.txt',
+                            decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])
+
+    assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \
+        'Error: test_SDS_Ctor: Some strings found in declared decimal fields ... any decimal format issue ?'
+
+    assert sds.columns.to_list() == ['Region*Label', 'Region*Area', 'Point transect*Label',
+                                     'Point transect*Survey effort', 'Observation*Radial distance'], \
+        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
+    assert len(sds) == 330, 'Error: test_SDS_Ctor: inconsistency with size of the resulting SDS. \
+    It should contain 330 rows'
+    assert sds.dfData['Observation*Radial distance'].notnull().sum() == 324, 'Error: test_SDS_Ctor: inconsistency with \
+    data loaded in SDS: Sum of values from column "Observation*Radial distance" should be 324'
+
+    # CSV source with '.' as decimal point
+    sds = ads.SampleDataSet(source=KSrcRefin / 'ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt',
+                            decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])
+
+    assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \
+        'Error: test_SDS_Ctor: Some strings found in declared decimal fields ... any decimal format issue ?'
+
+    assert sds.columns.to_list() == ['Region*Label', 'Region*Area', 'Point transect*Label',
+                                     'Point transect*Survey effort', 'Observation*Radial distance'], \
+        'Error: test_SDS_Ctor: inconsistency with columns list of SDS vs source file'
+    assert len(sds) == 256,  'Error: test_SDS_Ctor: inconsistency with size of the resulting SDS. \
+    It should contain 256 rows'
+    assert sds.dfData['Observation*Radial distance'].notnull().sum() == 217, 'Error: test_SDS_Ctor: inconsistency with \
+    data loaded in SDS: Sum of values from column "Observation*Radial distance" should be 217'
+
+    logger.info0('PASS (test_SDS_Ctor) => SampleDATASET => Constructor')
+
+###############################################################################
+#                         Actions to be done after all tests                  #
+###############################################################################
+def test_final():
+
+    logger.info('Done testing pyaudisam.data.')
+
+
+if __name__ == '__main__':
+
+    run = True
+    # Run auto-tests (exit(0) if OK, 1 if not).
+    rc = -1
+
+    if run:
+        try:
+            test_init()
+
+            # Tests for DataSet
+            test_DataSet_Ctor_len(sources())
+            test_DataSet_dfData_getter_setter(sources())
+            test_DataSet_empty()
+            test_DataSet_columns(sources())
+            test_DataSet_renameColumns(sources())
+            test_DataSet_addComputedColumns(sources())
+            test_DataSet_dropColumns(sources())
+            test_DataSet_dfSubData(sources())
+            test_DataSet_dropRows(sources())
+            test_DataSet_toFiles(sources())
+            test_closeness()
+            test_compare()
+
+            # Tests for SampleDataSet
+            test_SDS_Ctor()
+
+            # Success !
+            rc = 0
+
+        except Exception as exc:
+            logger.exception('Exception: ' + str(exc))
+            rc = 1
+
+    logger.info('Done unit integration testing pyaudisam.data: {} (code: {})'
+                .format({-1: 'Not run', 0: 'Success'}.get(rc, 'Error'), rc))
+    sys.exit(rc)
```

### Comparing `pyaudisam-0.9.3/tests/unint_engine_test.py` & `pyaudisam-1.0.1/tests/unint_engine_test.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,378 +1,378 @@
-# coding: utf-8
-
-# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
-
-# Copyright (C) 2021 Jean-Philippe Meuret, Sylvain Sainnier
-
-# This program is free software: you can redistribute it and/or modify it under the terms
-# of the GNU General Public License as published by the Free Software Foundation,
-# either version 3 of the License, or (at your option) any later version.
-# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU General Public License for more details.
-# You should have received a copy of the GNU General Public License along with this program.
-# If not, see https://www.gnu.org/licenses/.
-
-# Automated unit and integration tests for "engine" submodule
-
-# To run : simply run "pytest" or "python <this file>" in current folder
-#          and check standard output ; and tmp/unit-int-test.{datetime}.log for details
-
-import sys
-import re
-import shutil
-import time
-import pathlib as pl
-import pandas as pd
-import numpy as np
-import logging
-
-import pyaudisam as ads
-
-import pytest
-
-
-# The "tests" folder.
-KTestSrcPath = pl.Path(__file__).parent
-
-# Temporary work folder (created once in conftest.py if needed).
-tmpDir = KTestSrcPath / 'tmp'
-
-# Logger for this module.
-logger = ads.logger('unt.eng')
-
-###############################################################################
-#                         Actions to be done before any test                  #
-###############################################################################
-def test_init():
-
-    # Configure logging.
-    logFilePathName = tmpDir / 'unt-eng.{}.log'.format(pd.Timestamp.now().strftime('%Y%m%d%H%M'))
-    ads.log.configure(loggers=[dict(name='matplotlib', level=ads.WARNING),
-                               dict(name='ads', level=ads.INFO),
-                               #dict(name='ads.dat', level=ads.WARNING),  # Uncomment to limit log ouput
-                               dict(name='ads.eng', level=ads.INFO2),
-                               dict(name='unt.eng', level=ads.DEBUG)],
-                      handlers=[logFilePathName], reset=True)
-
-    # Show testing configuration (traçability).
-    logger.info('PyAuDiSam {} from {}'
-                .format(ads.__version__, pl.Path(ads.__path__[0]).resolve().as_posix()))
-    logger.info('Python environment:')
-    logger.info('*  {}: {}'.format(sys.implementation.name, sys.version))
-    logger.info('* platform: {}'.format(sys.platform))
-    for module in ['pytest', 'pandas', 'numpy']:  # 'lxml', 'scipy', 'pyproj', 'shapely']:
-        logger.info('* {:>8s}: {}'.format(module, sys.modules[module].__version__))
-
-    logger.info('Testing pyaudisam.engine ...')
-
-###############################################################################
-#                         Input Data Preparation                              #
-###############################################################################
-#   Generate a short DataFrame (returned) for test purpose
-#   and return a list of sources (4 files and 1 DataFrame)
-def shortDF():
-
-    shortDF = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],
-                           data=[('2019-05-13', 3.5, 'TURMER', 23, 2, 83),
-                                 ('2019-05-15', np.nan, 'TURMER', 23, 2, 27.355),
-                                 ('2019-05-13', 0, 'ALAARV', 29, 2, 56.85),
-                                 ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3, 7.2),
-                                 ('2019-06-01', 2, 'PHICOL', 12, 1, np.nan),
-                                 ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),
-                                 ])
-    shortDF['Region'] = 'ACDC'
-    shortDF['Surface'] = '2400'
-
-    return shortDF
-
-
-# same as above => to allow using pytest
-@pytest.fixture
-def shortDF_fxt():
-
-    return shortDF()
-
-
-###############################################################################
-#                          Miscellaneous Tools                                #
-###############################################################################
-###############################################################################
-#                                Test Cases                                   #
-###############################################################################
-def test_executableNotFound():
-
-    with pytest.raises(Exception) as e_info:
-        ads.MCDSEngine().findExecutable('WrongName')
-    logger.info0('PASS (test_executableNotFound) => EXCEPTION RAISED AS AWAITED with the following \
-    Exception message:\n{}'.format(e_info.value))
-
-
-def test_MCDS_Ctor():
-
-    # test Exception raising with one of unsupported characters in workdir string (space) - first way
-    with pytest.raises(Exception) as e_info:
-        ads.MCDSEngine(workDir=tmpDir.as_posix() + '/test out')  # Simple string path
-    logger.info0('PASS (test_MCDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'
-                 .format(e_info.value))
-
-    # test Exception raising with one of unsupported characters in workdir string (space) - second way
-    with pytest.raises(Exception) as e_info:
-        ads.MCDSEngine(workDir=tmpDir / 'test out')  # pl.Path path
-    logger.info0('PASS (test_MCDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'
-                 .format(e_info.value))
-
-    # test preferred method to initiate MCDSEngine
-    assert ads.MCDSEngine(workDir=tmpDir / 'mcds-out', runMethod='os.system'), 'MCDS Engine: \
-    Non identified issue at engine initiation'
-
-    # test previous (older) method to initiate MCDSEngine
-    assert ads.MCDSEngine(workDir=tmpDir / 'mcds-out'), 'MCDS Engine: Non identified issue at engine initiation'
-
-    # test Specs DataFrames not empty
-    # TODO: improve for deeper test ???
-    assert not any([ads.MCDSEngine().statRowSpecs().empty, ads.MCDSEngine().statModSpecs().empty,
-                    ads.MCDSEngine().statModCols().empty, ads.MCDSEngine().statModNotes().empty,
-                    ads.MCDSEngine().statModColTrans().empty]), 'Specs DataFrames: issue occurred with initialization \
-                    from external files'
-
-    logger.info0('PASS => MCDSEngine => Constructor and methods "_run", "_runThroughOSSystem", \
-    "_runThroughSubProcessRun", "loadStatSpecs" and setters for related class variables ')
-
-
-def test_MCDS_setupRunFolder():
-
-    runDir = ads.MCDSEngine().setupRunFolder(runPrefix=' t,e.s:t; ( )_setupRunFolder')
-
-    assert not re.search('[ ,.:;()/]', str(runDir)), 'Error: test_MCDS_setupRunFolde: Setted up directory: unsupported \
-    caracters should have been cleaned up'
-    assert runDir.exists(), 'Error: test_MCDS_setupRunFolde: temporary directory not created'
-    # clean-up: tmp directory deleted
-    runDir.rmdir()
-
-    logger.info0('PASS => MCDSEngine => method "setupRunFolder"')
-
-
-def test_buildExportTable(shortDF_fxt):
-
-    eng = ads.MCDSEngine()
-    dfData = shortDF_fxt  # load source test data
-    dfData.drop(['Surface'], axis=1, inplace=True)  # to create missing field
-
-    # test exception raised if at least one field missing vs DSEngine.ImportFieldAliasREs
-    with pytest.raises(Exception) as e_info:
-        dfExport, extraFields = eng.buildExportTable(ads.SampleDataSet(dfData),
-                                                     withExtraFields=True, decPoint='.')
-    logger.info0('PASS (test_buildExportTable) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'
-                 .format(e_info.value))
-
-    # missing field added
-    dfData['Surface'] = '2400'
-
-    dfExport, extraFields = ads.MCDSEngine().buildExportTable(ads.SampleDataSet(dfData),
-                                                              withExtraFields=False, decPoint='.')
-
-    # withExtraFields=False, number/order of columns same as for DSEngine.ImportFieldAliasREs, and
-    # comparison of source and resulting DataFrames'size
-    assert dfExport.columns.to_list() == ['Region', 'Surface', 'Point', 'Effort', 'Distance'] and \
-           dfExport.size == dfData.loc[:, ['Region', 'Surface', 'Point', 'Effort', 'Distance']].size, 'Error: \
-           test_buildExportTable: issue with resulting exported DataFrame (size, columns list or columns order). \
-           With option "withExtraFields=False"'
-
-    dfExport, extraFields = ads.MCDSEngine().buildExportTable(ads.SampleDataSet(dfData, decimalFields=['TrucDec']),
-                                                              withExtraFields=True, decPoint='.')
-
-    # withExtraFields=True, number/order of columns same as for DSEngine.ImportFieldAliasREs + extraFields columns, and
-    # comparison of source and resulting DataFrames'size
-    assert dfExport.columns.to_list() == \
-           ['Region', 'Surface', 'Point', 'Effort', 'Distance', 'Date', 'TrucDec', 'Espece'] and \
-           dfExport.size == dfData.size, 'Error: test_buildExportTable: issue with resulting exported DataFrame \
-           (size, columns list or columns order). With option "withExtraFields=True"'
-
-    # test all decimal fields (those defined by MCDSEngine and by SampleDataSet) where changed to string type,
-    # with '' instead of 'NaN'
-    for col in ['Effort', 'Distance', 'TrucDec']:
-        assert dfExport[col].compare(dfData[col].apply(lambda x: '' if np.isnan(x) else str(x))).empty, 'Error: \
-       test_buildExportTable: issue with decimal fields: values of exported DataFrame should have same values than \
-       source, but as string type (and NaN changed to \'\')'
-
-    logger.info0('PASS => MCDSEngine => methods "buildExportTable", "matchDataFields" and "safeFloat2Str"')
-
-
-def test_buildDataFile(shortDF_fxt):
-
-    eng = ads.MCDSEngine(workDir=tmpDir / 'mcds-out')
-    runDir = eng.setupRunFolder(runPrefix='uni')
-    dfData = shortDF_fxt  # load source test data
-
-    # export data to file
-    dataFileName = eng.buildDataFile(runDir, ads.SampleDataSet(dfData))
-    # gather exported data, with columns indexed
-    dfFiled = pd.read_csv(dataFileName, sep='\t', header=None)
-    dfFiled.set_axis(['Region', 'Surface', 'Point', 'Effort', 'Distance'], axis=1, inplace=True)
-    # prepare dfData for comparison - change type of 'Surface' to integer (due to data type as read by pd.read_csv)
-    dfData.Surface = dfData.Surface.apply(int)
-    # test exported data match to source
-    assert dfFiled.compare(dfData[['Region', 'Surface', 'Point', 'Effort', 'Distance']]).empty, 'Error: \
-    test_buildDataFile: data exported to file do not match to source data'
-
-    # clean-up: 'mcds-out' directory and content deleted
-    shutil.rmtree(tmpDir / 'mcds-out')
-
-    logger.info0('PASS => MCDSEngine => methods "buildDataFile"')
-
-
-def test_buildCmdFile():
-
-    # values selected for the test (t_values)
-    t_estimKeyFn = 'HNORMAL'
-    t_estimAdjustFn = 'COSINE'
-    t_estimCriterion = 'AIC'
-    t_cvInterval = 95
-    eng = ads.MCDSEngine(workDir=tmpDir / 'mcds-out')
-    runDir = eng.setupRunFolder(runPrefix='uni')
-    cmdFileName = eng.buildCmdFile(estimKeyFn=t_estimKeyFn, estimAdjustFn=t_estimAdjustFn,
-                                   estimCriterion=t_estimCriterion, cvInterval=t_cvInterval, runDir=runDir)
-    # read cmd.txt file
-    with open(cmdFileName, 'r') as cmdFile:
-        lines = cmdFile.readlines()
-
-    # check recorded param match with selected param in buildCmdFile method
-    # TODO: default parameter may ne checked
-    for line in lines:
-        if re.match('^Estimator', line):
-            val = re.search('/Key=(.+) /Adjust=(.+) /Criterion=(.+);$', line)
-            assert val.group(1) == t_estimKeyFn and val.group(2) == t_estimAdjustFn and \
-                   val.group(3) == t_estimCriterion, 'Error: test_buildCmdFile: issue with parameter \
-                   recorded in cmd.txt (estimKeyFn, estimAdjustFn and estimCriterion)'
-        elif re.match('^Confidence', line):
-            val = re.search('=(.+)(;)$', line)
-            assert val.group(1) == str(t_cvInterval), 'Error: test_buildCmdFile: issue with parameter \
-                   recorded in cmd.txt (cvInterval)'
-
-    # clean-up: 'mcds-out' directory and content deleted
-    shutil.rmtree(tmpDir / 'mcds-out')
-
-    logger.info0('PASS => MCDSEngine => methods "test_buildCmdFile"')
-
-
-def test__run(shortDF_fxt):
-
-    # init MCDSEngine
-    eng = ads.MCDSEngine(workDir=tmpDir / 'mcds-out', runMethod='os.system')
-    # Prepare temporary working folder
-    runDir = eng.setupRunFolder(runPrefix='uni')
-    # Prepare SampleDataSet and data.txt file
-    sds = ads.SampleDataSet(source=shortDF_fxt, decimalFields=['Effort', 'Distance', 'TrucDec'])
-    eng.buildDataFile(sampleDataSet=sds, runDir=runDir)
-    # Prepare cmd.txt file
-    cmdFileName = eng.buildCmdFile(estimKeyFn='HNORMAL', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95,
-                                   runDir=runDir)
-
-    # Debug mode - os.system method
-    runStatus, startTime, elapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=False, method=eng.runMethod)
-    # test appropriate outputs
-    assert runStatus == 0 and startTime is pd.NaT and elapsedTime == 0, 'Error: test__run: issue occured with \
-    debug mode (forReal=False ; runMethod=os.system)'
-
-    # Debug mode - subprocess.run method
-    runStatus, startTime, elapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=False,
-                                                 method='subprocess.run')
-    # test appropriate outputs
-    assert runStatus == 0 and startTime is pd.NaT and elapsedTime == 0, 'Error: test__run: issue occured with \
-    debug mode (forReal=False ; runMethod=subprocess.run)'
-
-    # JUST TESTING No Exceptions raised (no specific tests)
-    # run with Warning Status (2) as for JPM test in Nootebook "unintests.ipynb"
-    # Real mode -  os.system method
-    runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
-                                                    method=eng.runMethod)
-
-    # Real mode -  subprocess.run method
-    runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
-                                                    method='subprocess.run')
-
-    # Timeout
-    runStatus, startTime, engElapsedTime = \
-        eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method='subprocess.run', timeOut=0.01)
-    assert runStatus == 555, 'Error: runStatus should be 555 (refer MCDSEngine doc)'
-
-    # Measure of performances (low level analysis execution)
-    # BE CAREFULL: time.process_time() uses relative time for comparison only of codes among the same environment
-    # NOT A REAL TIME reference
-    timePerf = pd.DataFrame(columns=['OSS', 'SPR'], index=list('Cycle' + str(i) for i in range(1, 11)))
-
-    i = 0
-    while i < 10:
-        j = 0
-        start = time.perf_counter()
-        while j < 5:
-            runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
-                                                            method=eng.runMethod)
-            j += 1
-        end = time.perf_counter()
-
-        timePerf.iloc[i, 0] = end - start
-        i += 1
-
-    i = 0
-    while i < 10:
-        j = 0
-        start = time.perf_counter()
-        while j < 5:
-            runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
-                                                            method='subprocess.run')
-            j += 1
-        end = time.perf_counter()
-
-        timePerf.iloc[i, 1] = end - start
-        i += 1
-
-    timePerf['OSS-faster'] = timePerf['OSS'] < timePerf['SPR']
-    timePerf['%_vs_OSS'] = ((timePerf['SPR'] - timePerf['OSS']) / timePerf['OSS']) * 100
-
-    logger.info0('\n\nPerformance: 10 loops of 5 runs (OSS = "os.system" ; SPR = "subprocess.run")\n')
-    logger.info0(f'\n{timePerf.to_markdown(floatfmt=".2f")}\n')
-    logger.info0(f'For "os.system": Mean +/- Std Dev = {timePerf["OSS"].mean():.2f} +/- {timePerf["OSS"].std():.2f}')
-    logger.info0(
-        f'For "subprocess.run": Mean +/- Std Dev = {timePerf["SPR"].mean():.2f} +/- {timePerf["SPR"].std():.2f}\n\n')
-
-    logger.info0('PASS => MCDSEngine => methods "_run", "_runThroughOSSystem" and "_runThroughSubProcessRun"')
-
-
-###############################################################################
-#                         Actions to be done after all tests                  #
-###############################################################################
-def test_final():
-
-    logger.info('Done testing pyaudisam.engine.')
-
-
-if __name__ == '__main__':
-
-    run = True
-    # Run auto-tests (exit(0) if OK, 1 if not).
-    rc = -1
-
-    if run:
-        try:
-            test_init()
-
-            test_executableNotFound()
-            test_MCDS_Ctor()
-            test_MCDS_setupRunFolder()
-            test_buildExportTable(shortDF())
-            test_buildDataFile(shortDF())
-            test_buildCmdFile()
-            test__run(shortDF())
-
-            # Success !
-            rc = 0
-
-        except Exception as exc:
-            logger.exception('Exception: ' + str(exc))
-            rc = 1
-
-    logger.info('Done unit integration testing ads: {} (code: {})'
-                .format({-1: 'Not run', 0: 'Success'}.get(rc, 'Error'), rc))
-    sys.exit(rc)
+# coding: utf-8
+
+# PyAuDiSam: Automation of Distance Sampling analyses with Distance software (http://distancesampling.org/)
+
+# Copyright (C) 2021 Jean-Philippe Meuret, Sylvain Sainnier
+
+# This program is free software: you can redistribute it and/or modify it under the terms
+# of the GNU General Public License as published by the Free Software Foundation,
+# either version 3 of the License, or (at your option) any later version.
+# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+# See the GNU General Public License for more details.
+# You should have received a copy of the GNU General Public License along with this program.
+# If not, see https://www.gnu.org/licenses/.
+
+# Automated unit and integration tests for "engine" submodule
+
+# To run : simply run "pytest" or "python <this file>" in current folder
+#          and check standard output ; and tmp/unit-int-test.{datetime}.log for details
+
+import sys
+import re
+import shutil
+import time
+import pathlib as pl
+import pandas as pd
+import numpy as np
+import logging
+
+import pyaudisam as ads
+
+import pytest
+
+
+# The "tests" folder.
+KTestSrcPath = pl.Path(__file__).parent
+
+# Temporary work folder (created once in conftest.py if needed).
+tmpDir = KTestSrcPath / 'tmp'
+
+# Logger for this module.
+logger = ads.logger('unt.eng')
+
+###############################################################################
+#                         Actions to be done before any test                  #
+###############################################################################
+def test_init():
+
+    # Configure logging.
+    logFilePathName = tmpDir / 'unt-eng.{}.log'.format(pd.Timestamp.now().strftime('%Y%m%d%H%M'))
+    ads.log.configure(loggers=[dict(name='matplotlib', level=ads.WARNING),
+                               dict(name='ads', level=ads.INFO),
+                               #dict(name='ads.dat', level=ads.WARNING),  # Uncomment to limit log ouput
+                               dict(name='ads.eng', level=ads.INFO2),
+                               dict(name='unt.eng', level=ads.DEBUG)],
+                      handlers=[logFilePathName], reset=True)
+
+    # Show testing configuration (traçability).
+    logger.info('PyAuDiSam {} from {}'
+                .format(ads.__version__, pl.Path(ads.__path__[0]).resolve().as_posix()))
+    logger.info('Python environment:')
+    logger.info('*  {}: {}'.format(sys.implementation.name, sys.version))
+    logger.info('* platform: {}'.format(sys.platform))
+    for module in ['pytest', 'pandas', 'numpy']:  # 'lxml', 'scipy', 'pyproj', 'shapely']:
+        logger.info('* {:>8s}: {}'.format(module, sys.modules[module].__version__))
+
+    logger.info('Testing pyaudisam.engine ...')
+
+###############################################################################
+#                         Input Data Preparation                              #
+###############################################################################
+#   Generate a short DataFrame (returned) for test purpose
+#   and return a list of sources (4 files and 1 DataFrame)
+def shortDF():
+
+    shortDF = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],
+                           data=[('2019-05-13', 3.5, 'TURMER', 23, 2, 83),
+                                 ('2019-05-15', np.nan, 'TURMER', 23, 2, 27.355),
+                                 ('2019-05-13', 0, 'ALAARV', 29, 2, 56.85),
+                                 ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3, 7.2),
+                                 ('2019-06-01', 2, 'PHICOL', 12, 1, np.nan),
+                                 ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),
+                                 ])
+    shortDF['Region'] = 'ACDC'
+    shortDF['Surface'] = '2400'
+
+    return shortDF
+
+
+# same as above => to allow using pytest
+@pytest.fixture
+def shortDF_fxt():
+
+    return shortDF()
+
+
+###############################################################################
+#                          Miscellaneous Tools                                #
+###############################################################################
+###############################################################################
+#                                Test Cases                                   #
+###############################################################################
+def test_executableNotFound():
+
+    with pytest.raises(Exception) as e_info:
+        ads.MCDSEngine().findExecutable('WrongName')
+    logger.info0('PASS (test_executableNotFound) => EXCEPTION RAISED AS AWAITED with the following \
+    Exception message:\n{}'.format(e_info.value))
+
+
+def test_MCDS_Ctor():
+
+    # test Exception raising with one of unsupported characters in workdir string (space) - first way
+    with pytest.raises(Exception) as e_info:
+        ads.MCDSEngine(workDir=tmpDir.as_posix() + '/test out')  # Simple string path
+    logger.info0('PASS (test_MCDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'
+                 .format(e_info.value))
+
+    # test Exception raising with one of unsupported characters in workdir string (space) - second way
+    with pytest.raises(Exception) as e_info:
+        ads.MCDSEngine(workDir=tmpDir / 'test out')  # pl.Path path
+    logger.info0('PASS (test_MCDS_Ctor) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'
+                 .format(e_info.value))
+
+    # test preferred method to initiate MCDSEngine
+    assert ads.MCDSEngine(workDir=tmpDir / 'mcds-out', runMethod='os.system'), 'MCDS Engine: \
+    Non identified issue at engine initiation'
+
+    # test previous (older) method to initiate MCDSEngine
+    assert ads.MCDSEngine(workDir=tmpDir / 'mcds-out'), 'MCDS Engine: Non identified issue at engine initiation'
+
+    # test Specs DataFrames not empty
+    # TODO: improve for deeper test ???
+    assert not any([ads.MCDSEngine().statRowSpecs().empty, ads.MCDSEngine().statModSpecs().empty,
+                    ads.MCDSEngine().statModCols().empty, ads.MCDSEngine().statModNotes().empty,
+                    ads.MCDSEngine().statModColTrans().empty]), 'Specs DataFrames: issue occurred with initialization \
+                    from external files'
+
+    logger.info0('PASS => MCDSEngine => Constructor and methods "_run", "_runThroughOSSystem", \
+    "_runThroughSubProcessRun", "loadStatSpecs" and setters for related class variables ')
+
+
+def test_MCDS_setupRunFolder():
+
+    runDir = ads.MCDSEngine().setupRunFolder(runPrefix=' t,e.s:t; ( )_setupRunFolder')
+
+    assert not re.search('[ ,.:;()/]', str(runDir)), 'Error: test_MCDS_setupRunFolde: Setted up directory: unsupported \
+    caracters should have been cleaned up'
+    assert runDir.exists(), 'Error: test_MCDS_setupRunFolde: temporary directory not created'
+    # clean-up: tmp directory deleted
+    runDir.rmdir()
+
+    logger.info0('PASS => MCDSEngine => method "setupRunFolder"')
+
+
+def test_buildExportTable(shortDF_fxt):
+
+    eng = ads.MCDSEngine()
+    dfData = shortDF_fxt  # load source test data
+    dfData.drop(['Surface'], axis=1, inplace=True)  # to create missing field
+
+    # test exception raised if at least one field missing vs DSEngine.ImportFieldAliasREs
+    with pytest.raises(Exception) as e_info:
+        dfExport, extraFields = eng.buildExportTable(ads.SampleDataSet(dfData),
+                                                     withExtraFields=True, decPoint='.')
+    logger.info0('PASS (test_buildExportTable) => EXCEPTION RAISED AS AWAITED with the following Exception message:\n{}'
+                 .format(e_info.value))
+
+    # missing field added
+    dfData['Surface'] = '2400'
+
+    dfExport, extraFields = ads.MCDSEngine().buildExportTable(ads.SampleDataSet(dfData),
+                                                              withExtraFields=False, decPoint='.')
+
+    # withExtraFields=False, number/order of columns same as for DSEngine.ImportFieldAliasREs, and
+    # comparison of source and resulting DataFrames'size
+    assert dfExport.columns.to_list() == ['Region', 'Surface', 'Point', 'Effort', 'Distance'] and \
+           dfExport.size == dfData.loc[:, ['Region', 'Surface', 'Point', 'Effort', 'Distance']].size, 'Error: \
+           test_buildExportTable: issue with resulting exported DataFrame (size, columns list or columns order). \
+           With option "withExtraFields=False"'
+
+    dfExport, extraFields = ads.MCDSEngine().buildExportTable(ads.SampleDataSet(dfData, decimalFields=['TrucDec']),
+                                                              withExtraFields=True, decPoint='.')
+
+    # withExtraFields=True, number/order of columns same as for DSEngine.ImportFieldAliasREs + extraFields columns, and
+    # comparison of source and resulting DataFrames'size
+    assert dfExport.columns.to_list() == \
+           ['Region', 'Surface', 'Point', 'Effort', 'Distance', 'Date', 'TrucDec', 'Espece'] and \
+           dfExport.size == dfData.size, 'Error: test_buildExportTable: issue with resulting exported DataFrame \
+           (size, columns list or columns order). With option "withExtraFields=True"'
+
+    # test all decimal fields (those defined by MCDSEngine and by SampleDataSet) where changed to string type,
+    # with '' instead of 'NaN'
+    for col in ['Effort', 'Distance', 'TrucDec']:
+        assert dfExport[col].compare(dfData[col].apply(lambda x: '' if np.isnan(x) else str(x))).empty, 'Error: \
+       test_buildExportTable: issue with decimal fields: values of exported DataFrame should have same values than \
+       source, but as string type (and NaN changed to \'\')'
+
+    logger.info0('PASS => MCDSEngine => methods "buildExportTable", "matchDataFields" and "safeFloat2Str"')
+
+
+def test_buildDataFile(shortDF_fxt):
+
+    eng = ads.MCDSEngine(workDir=tmpDir / 'mcds-out')
+    runDir = eng.setupRunFolder(runPrefix='uni')
+    dfData = shortDF_fxt  # load source test data
+
+    # export data to file
+    dataFileName = eng.buildDataFile(runDir, ads.SampleDataSet(dfData))
+    # gather exported data, with columns indexed
+    dfFiled = pd.read_csv(dataFileName, sep='\t', header=None)
+    dfFiled.set_axis(['Region', 'Surface', 'Point', 'Effort', 'Distance'], axis=1, inplace=True)
+    # prepare dfData for comparison - change type of 'Surface' to integer (due to data type as read by pd.read_csv)
+    dfData.Surface = dfData.Surface.apply(int)
+    # test exported data match to source
+    assert dfFiled.compare(dfData[['Region', 'Surface', 'Point', 'Effort', 'Distance']]).empty, 'Error: \
+    test_buildDataFile: data exported to file do not match to source data'
+
+    # clean-up: 'mcds-out' directory and content deleted
+    shutil.rmtree(tmpDir / 'mcds-out')
+
+    logger.info0('PASS => MCDSEngine => methods "buildDataFile"')
+
+
+def test_buildCmdFile():
+
+    # values selected for the test (t_values)
+    t_estimKeyFn = 'HNORMAL'
+    t_estimAdjustFn = 'COSINE'
+    t_estimCriterion = 'AIC'
+    t_cvInterval = 95
+    eng = ads.MCDSEngine(workDir=tmpDir / 'mcds-out')
+    runDir = eng.setupRunFolder(runPrefix='uni')
+    cmdFileName = eng.buildCmdFile(estimKeyFn=t_estimKeyFn, estimAdjustFn=t_estimAdjustFn,
+                                   estimCriterion=t_estimCriterion, cvInterval=t_cvInterval, runDir=runDir)
+    # read cmd.txt file
+    with open(cmdFileName, 'r') as cmdFile:
+        lines = cmdFile.readlines()
+
+    # check recorded param match with selected param in buildCmdFile method
+    # TODO: default parameter may ne checked
+    for line in lines:
+        if re.match('^Estimator', line):
+            val = re.search('/Key=(.+) /Adjust=(.+) /Criterion=(.+);$', line)
+            assert val.group(1) == t_estimKeyFn and val.group(2) == t_estimAdjustFn and \
+                   val.group(3) == t_estimCriterion, 'Error: test_buildCmdFile: issue with parameter \
+                   recorded in cmd.txt (estimKeyFn, estimAdjustFn and estimCriterion)'
+        elif re.match('^Confidence', line):
+            val = re.search('=(.+)(;)$', line)
+            assert val.group(1) == str(t_cvInterval), 'Error: test_buildCmdFile: issue with parameter \
+                   recorded in cmd.txt (cvInterval)'
+
+    # clean-up: 'mcds-out' directory and content deleted
+    shutil.rmtree(tmpDir / 'mcds-out')
+
+    logger.info0('PASS => MCDSEngine => methods "test_buildCmdFile"')
+
+
+def test__run(shortDF_fxt):
+
+    # init MCDSEngine
+    eng = ads.MCDSEngine(workDir=tmpDir / 'mcds-out', runMethod='os.system')
+    # Prepare temporary working folder
+    runDir = eng.setupRunFolder(runPrefix='uni')
+    # Prepare SampleDataSet and data.txt file
+    sds = ads.SampleDataSet(source=shortDF_fxt, decimalFields=['Effort', 'Distance', 'TrucDec'])
+    eng.buildDataFile(sampleDataSet=sds, runDir=runDir)
+    # Prepare cmd.txt file
+    cmdFileName = eng.buildCmdFile(estimKeyFn='HNORMAL', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95,
+                                   runDir=runDir)
+
+    # Debug mode - os.system method
+    runStatus, startTime, elapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=False, method=eng.runMethod)
+    # test appropriate outputs
+    assert runStatus == 0 and startTime is pd.NaT and elapsedTime == 0, 'Error: test__run: issue occured with \
+    debug mode (forReal=False ; runMethod=os.system)'
+
+    # Debug mode - subprocess.run method
+    runStatus, startTime, elapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=False,
+                                                 method='subprocess.run')
+    # test appropriate outputs
+    assert runStatus == 0 and startTime is pd.NaT and elapsedTime == 0, 'Error: test__run: issue occured with \
+    debug mode (forReal=False ; runMethod=subprocess.run)'
+
+    # JUST TESTING No Exceptions raised (no specific tests)
+    # run with Warning Status (2) as for JPM test in Nootebook "unintests.ipynb"
+    # Real mode -  os.system method
+    runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
+                                                    method=eng.runMethod)
+
+    # Real mode -  subprocess.run method
+    runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
+                                                    method='subprocess.run')
+
+    # Timeout
+    runStatus, startTime, engElapsedTime = \
+        eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method='subprocess.run', timeOut=0.01)
+    assert runStatus == 555, 'Error: runStatus should be 555 (refer MCDSEngine doc)'
+
+    # Measure of performances (low level analysis execution)
+    # BE CAREFULL: time.process_time() uses relative time for comparison only of codes among the same environment
+    # NOT A REAL TIME reference
+    timePerf = pd.DataFrame(columns=['OSS', 'SPR'], index=list('Cycle' + str(i) for i in range(1, 11)))
+
+    i = 0
+    while i < 10:
+        j = 0
+        start = time.perf_counter()
+        while j < 5:
+            runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
+                                                            method=eng.runMethod)
+            j += 1
+        end = time.perf_counter()
+
+        timePerf.iloc[i, 0] = end - start
+        i += 1
+
+    i = 0
+    while i < 10:
+        j = 0
+        start = time.perf_counter()
+        while j < 5:
+            runStatus, startTime, engElapsedTime = eng._run(eng.ExeFilePathName, cmdFileName, forReal=True,
+                                                            method='subprocess.run')
+            j += 1
+        end = time.perf_counter()
+
+        timePerf.iloc[i, 1] = end - start
+        i += 1
+
+    timePerf['OSS-faster'] = timePerf['OSS'] < timePerf['SPR']
+    timePerf['%_vs_OSS'] = ((timePerf['SPR'] - timePerf['OSS']) / timePerf['OSS']) * 100
+
+    logger.info0('\n\nPerformance: 10 loops of 5 runs (OSS = "os.system" ; SPR = "subprocess.run")\n')
+    logger.info0(f'\n{timePerf.to_markdown(floatfmt=".2f")}\n')
+    logger.info0(f'For "os.system": Mean +/- Std Dev = {timePerf["OSS"].mean():.2f} +/- {timePerf["OSS"].std():.2f}')
+    logger.info0(
+        f'For "subprocess.run": Mean +/- Std Dev = {timePerf["SPR"].mean():.2f} +/- {timePerf["SPR"].std():.2f}\n\n')
+
+    logger.info0('PASS => MCDSEngine => methods "_run", "_runThroughOSSystem" and "_runThroughSubProcessRun"')
+
+
+###############################################################################
+#                         Actions to be done after all tests                  #
+###############################################################################
+def test_final():
+
+    logger.info('Done testing pyaudisam.engine.')
+
+
+if __name__ == '__main__':
+
+    run = True
+    # Run auto-tests (exit(0) if OK, 1 if not).
+    rc = -1
+
+    if run:
+        try:
+            test_init()
+
+            test_executableNotFound()
+            test_MCDS_Ctor()
+            test_MCDS_setupRunFolder()
+            test_buildExportTable(shortDF())
+            test_buildDataFile(shortDF())
+            test_buildCmdFile()
+            test__run(shortDF())
+
+            # Success !
+            rc = 0
+
+        except Exception as exc:
+            logger.exception('Exception: ' + str(exc))
+            rc = 1
+
+    logger.info('Done unit integration testing ads: {} (code: {})'
+                .format({-1: 'Not run', 0: 'Success'}.get(rc, 'Error'), rc))
+    sys.exit(rc)
```

### Comparing `pyaudisam-0.9.3/tests/unintests.ipynb` & `pyaudisam-1.0.1/tests/unintests.ipynb`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/valarchives.ipynb` & `pyaudisam-1.0.1/tests/valarchives.ipynb`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/valtests-ds-params.py` & `pyaudisam-1.0.1/tests/valtests-ds-params.py`

 * *Files identical despite different names*

### Comparing `pyaudisam-0.9.3/tests/valtests.ipynb` & `pyaudisam-1.0.1/tests/valtests.ipynb`

 * *Files identical despite different names*

