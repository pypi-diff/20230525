# Comparing `tmp/fmot-1.5.6-py3-none-any.whl.zip` & `tmp/fmot-1.5.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,160 +1,160 @@
-Zip file size: 222191 bytes, number of entries: 158
--rwxr-xr-x  2.0 unx     1355 b- defN 23-Apr-22 01:51 fmot/ENV_REQUIREMENTS.sh
--rwxr-xr-x  2.0 unx      138 b- defN 23-Apr-22 01:51 fmot/PY_REQUIREMENTS
--rw-r--r--  2.0 unx        6 b- defN 23-Apr-22 01:51 fmot/VERSION
--rw-r--r--  2.0 unx      648 b- defN 23-Apr-22 01:51 fmot/__init__.py
--rw-r--r--  2.0 unx      414 b- defN 23-Apr-22 01:51 fmot/_open_docs.py
--rw-r--r--  2.0 unx     1611 b- defN 23-Apr-22 01:51 fmot/_supported_ops.py
--rw-r--r--  2.0 unx     1654 b- defN 23-Apr-22 01:51 fmot/configure.py
--rw-r--r--  2.0 unx      929 b- defN 23-Apr-22 01:51 fmot/exceptions.py
--rw-r--r--  2.0 unx     1821 b- defN 23-Apr-22 01:51 fmot/execute_code.py
--rw-r--r--  2.0 unx       61 b- defN 23-Apr-22 01:51 fmot/functional.py
--rw-r--r--  2.0 unx     4945 b- defN 23-Apr-22 01:51 fmot/torchscript_utils.py
--rw-r--r--  2.0 unx       83 b- defN 23-Apr-22 01:51 fmot/beta/__init__.py
--rw-r--r--  2.0 unx     4169 b- defN 23-Apr-22 01:51 fmot/beta/amp.py
--rw-r--r--  2.0 unx    10740 b- defN 23-Apr-22 01:51 fmot/beta/auto_multiprecision.py
--rw-r--r--  2.0 unx     1166 b- defN 23-Apr-22 01:51 fmot/beta/quantize_part.py
--rw-r--r--  2.0 unx      200 b- defN 23-Apr-22 01:51 fmot/convert/__init__.py
--rw-r--r--  2.0 unx     2900 b- defN 23-Apr-22 01:51 fmot/convert/_convert_to_qat.py
--rw-r--r--  2.0 unx    15168 b- defN 23-Apr-22 01:51 fmot/convert/conversion_api.py
--rw-r--r--  2.0 unx     2410 b- defN 23-Apr-22 01:51 fmot/convert/default_mappings.py
--rw-r--r--  2.0 unx     9684 b- defN 23-Apr-22 01:51 fmot/convert/default_patchings.py
--rw-r--r--  2.0 unx      547 b- defN 23-Apr-22 01:51 fmot/convert/default_substitutions.py
--rw-r--r--  2.0 unx     4562 b- defN 23-Apr-22 01:51 fmot/convert/lut_registry.py
--rw-r--r--  2.0 unx     3998 b- defN 23-Apr-22 01:51 fmot/convert/mapping.py
--rw-r--r--  2.0 unx     2831 b- defN 23-Apr-22 01:51 fmot/convert/optimizer.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-22 01:51 fmot/convert/param_manager.py
--rw-r--r--  2.0 unx    24680 b- defN 23-Apr-22 01:51 fmot/convert/patch_ir.py
--rw-r--r--  2.0 unx     4175 b- defN 23-Apr-22 01:51 fmot/convert/patching.py
--rw-r--r--  2.0 unx     3244 b- defN 23-Apr-22 01:51 fmot/convert/prune_reparametrization.py
--rw-r--r--  2.0 unx     2963 b- defN 23-Apr-22 01:51 fmot/convert/quantizer_manager.py
--rw-r--r--  2.0 unx     2121 b- defN 23-Apr-22 01:51 fmot/convert/substitution.py
--rw-r--r--  2.0 unx      199 b- defN 23-Apr-22 01:51 fmot/fqir/__init__.py
--rw-r--r--  2.0 unx      660 b- defN 23-Apr-22 01:51 fmot/fqir/utils.py
--rw-r--r--  2.0 unx       35 b- defN 23-Apr-22 01:51 fmot/fqir/graph_proto/__init__.py
--rw-r--r--  2.0 unx    13950 b- defN 23-Apr-22 01:51 fmot/fqir/graph_proto/graph_proto.py
--rw-r--r--  2.0 unx     5303 b- defN 23-Apr-22 01:51 fmot/fqir/graph_proto/graph_runtime.py
--rw-r--r--  2.0 unx      115 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/__init__.py
--rw-r--r--  2.0 unx     5761 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/node_base.py
--rw-r--r--  2.0 unx     6920 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/node_proto.py
--rw-r--r--  2.0 unx     2207 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/opcount.py
--rw-r--r--  2.0 unx    20518 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/opcounters.py
--rw-r--r--  2.0 unx     7008 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/optype_base.py
--rw-r--r--  2.0 unx    30178 b- defN 23-Apr-22 01:51 fmot/fqir/nodes/optypes.py
--rw-r--r--  2.0 unx      583 b- defN 23-Apr-22 01:51 fmot/fqir/passes/__init__.py
--rw-r--r--  2.0 unx     7322 b- defN 23-Apr-22 01:51 fmot/fqir/passes/batchdim_removal.py
--rw-r--r--  2.0 unx     1606 b- defN 23-Apr-22 01:51 fmot/fqir/passes/cleanup.py
--rw-r--r--  2.0 unx      463 b- defN 23-Apr-22 01:51 fmot/fqir/passes/dimtag_removal.py
--rw-r--r--  2.0 unx     1160 b- defN 23-Apr-22 01:51 fmot/fqir/passes/helpers.py
--rw-r--r--  2.0 unx    19971 b- defN 23-Apr-22 01:51 fmot/fqir/passes/kernelize_lstm.py
--rw-r--r--  2.0 unx     5496 b- defN 23-Apr-22 01:51 fmot/fqir/passes/kernelize_red_broad.py
--rw-r--r--  2.0 unx     2039 b- defN 23-Apr-22 01:51 fmot/fqir/passes/kernelize_temporal_unfold.py
--rw-r--r--  2.0 unx       94 b- defN 23-Apr-22 01:51 fmot/fqir/variables/__init__.py
--rw-r--r--  2.0 unx     6255 b- defN 23-Apr-22 01:51 fmot/fqir/variables/tensor_proto.py
--rw-r--r--  2.0 unx     2064 b- defN 23-Apr-22 01:51 fmot/fqir/variables/variable_base.py
--rw-r--r--  2.0 unx      563 b- defN 23-Apr-22 01:51 fmot/nn/__init__.py
--rw-r--r--  2.0 unx     8663 b- defN 23-Apr-22 01:51 fmot/nn/atomics.py
--rw-r--r--  2.0 unx    12272 b- defN 23-Apr-22 01:51 fmot/nn/composites.py
--rw-r--r--  2.0 unx    27568 b- defN 23-Apr-22 01:51 fmot/nn/conv1d.py
--rw-r--r--  2.0 unx     1844 b- defN 23-Apr-22 01:51 fmot/nn/derived_param.py
--rw-r--r--  2.0 unx    21993 b- defN 23-Apr-22 01:51 fmot/nn/femtornn.py
--rw-r--r--  2.0 unx    17526 b- defN 23-Apr-22 01:51 fmot/nn/fft.py
--rw-r--r--  2.0 unx    34238 b- defN 23-Apr-22 01:51 fmot/nn/sequenced_rnn.py
--rw-r--r--  2.0 unx     5305 b- defN 23-Apr-22 01:51 fmot/nn/sequencer.py
--rw-r--r--  2.0 unx    11986 b- defN 23-Apr-22 01:51 fmot/nn/signal_processing.py
--rw-r--r--  2.0 unx     3742 b- defN 23-Apr-22 01:51 fmot/nn/sliding_attention.py
--rw-r--r--  2.0 unx     9256 b- defN 23-Apr-22 01:51 fmot/nn/sparsifiers.py
--rw-r--r--  2.0 unx     2294 b- defN 23-Apr-22 01:51 fmot/nn/sru.py
--rw-r--r--  2.0 unx     8916 b- defN 23-Apr-22 01:51 fmot/nn/stft.py
--rw-r--r--  2.0 unx     1525 b- defN 23-Apr-22 01:51 fmot/nn/super_structures.py
--rw-r--r--  2.0 unx     1430 b- defN 23-Apr-22 01:51 fmot/nn/temporal_unfold.py
--rw-r--r--  2.0 unx      192 b- defN 23-Apr-22 01:51 fmot/qat/__init__.py
--rw-r--r--  2.0 unx     4059 b- defN 23-Apr-22 01:51 fmot/qat/annotated_tensors.py
--rw-r--r--  2.0 unx     1659 b- defN 23-Apr-22 01:51 fmot/qat/bitwidths.py
--rw-r--r--  2.0 unx     4381 b- defN 23-Apr-22 01:51 fmot/qat/control.py
--rw-r--r--  2.0 unx     2684 b- defN 23-Apr-22 01:51 fmot/qat/fake_quantization.py
--rw-r--r--  2.0 unx      328 b- defN 23-Apr-22 01:51 fmot/qat/nn/__init__.py
--rw-r--r--  2.0 unx      276 b- defN 23-Apr-22 01:51 fmot/qat/nn/_utils.py
--rw-r--r--  2.0 unx     3783 b- defN 23-Apr-22 01:51 fmot/qat/nn/act_density.py
--rw-r--r--  2.0 unx    28153 b- defN 23-Apr-22 01:51 fmot/qat/nn/atomics.py
--rw-r--r--  2.0 unx     9198 b- defN 23-Apr-22 01:51 fmot/qat/nn/composites.py
--rw-r--r--  2.0 unx    10828 b- defN 23-Apr-22 01:51 fmot/qat/nn/density_matmul.py
--rw-r--r--  2.0 unx     2102 b- defN 23-Apr-22 01:51 fmot/qat/nn/derived_param.py
--rw-r--r--  2.0 unx     8923 b- defN 23-Apr-22 01:51 fmot/qat/nn/linear.py
--rw-r--r--  2.0 unx    19303 b- defN 23-Apr-22 01:51 fmot/qat/nn/lstm.py
--rw-r--r--  2.0 unx    14400 b- defN 23-Apr-22 01:51 fmot/qat/nn/luts.py
--rw-r--r--  2.0 unx     3922 b- defN 23-Apr-22 01:51 fmot/qat/nn/norm.py
--rw-r--r--  2.0 unx     1350 b- defN 23-Apr-22 01:51 fmot/qat/nn/quant_wrap.py
--rw-r--r--  2.0 unx    22679 b- defN 23-Apr-22 01:51 fmot/qat/nn/quantizers.py
--rw-r--r--  2.0 unx     1992 b- defN 23-Apr-22 01:51 fmot/qat/nn/sequencer.py
--rw-r--r--  2.0 unx     2283 b- defN 23-Apr-22 01:51 fmot/qat/nn/temporal_unfold.py
--rw-r--r--  2.0 unx     1007 b- defN 23-Apr-22 01:51 fmot/qat/nn/tuning_eps.py
--rw-r--r--  2.0 unx       90 b- defN 23-Apr-22 01:51 fmot/sparse/__init__.py
--rw-r--r--  2.0 unx     2694 b- defN 23-Apr-22 01:51 fmot/sparse/act_pruning.py
--rw-r--r--  2.0 unx     7357 b- defN 23-Apr-22 01:51 fmot/sparse/pruning.py
--rw-r--r--  2.0 unx     4024 b- defN 23-Apr-22 01:51 fmot/sparse/pruning_schedulers.py
--rw-r--r--  2.0 unx     7162 b- defN 23-Apr-22 01:51 fmot/sparse/pruning_utils.py
--rw-r--r--  2.0 unx      196 b- defN 23-Apr-22 01:51 fmot/test/__init__.py
--rw-r--r--  2.0 unx     5861 b- defN 23-Apr-22 01:51 fmot/test/ds_tc_resnet.py
--rw-r--r--  2.0 unx     3000 b- defN 23-Apr-22 01:51 fmot/test/kws_test_library.py
--rw-r--r--  2.0 unx      936 b- defN 23-Apr-22 01:51 fmot/test/test_automp.py
--rw-r--r--  2.0 unx     1373 b- defN 23-Apr-22 01:51 fmot/test/test_cqt.py
--rw-r--r--  2.0 unx     1688 b- defN 23-Apr-22 01:51 fmot/test/test_cuda.py
--rw-r--r--  2.0 unx     3878 b- defN 23-Apr-22 01:51 fmot/test/test_density_tracking.py
--rw-r--r--  2.0 unx     1134 b- defN 23-Apr-22 01:51 fmot/test/test_diagnosis.py
--rw-r--r--  2.0 unx     3433 b- defN 23-Apr-22 01:51 fmot/test/test_dim_anno.py
--rw-r--r--  2.0 unx      958 b- defN 23-Apr-22 01:51 fmot/test/test_dropout.py
--rw-r--r--  2.0 unx    14304 b- defN 23-Apr-22 01:51 fmot/test/test_features.py
--rw-r--r--  2.0 unx      722 b- defN 23-Apr-22 01:51 fmot/test/test_fft.py
--rw-r--r--  2.0 unx     1664 b- defN 23-Apr-22 01:51 fmot/test/test_ifft.py
--rw-r--r--  2.0 unx      378 b- defN 23-Apr-22 01:51 fmot/test/test_layernorm.py
--rw-r--r--  2.0 unx     1412 b- defN 23-Apr-22 01:51 fmot/test/test_lut_grads.py
--rw-r--r--  2.0 unx     3581 b- defN 23-Apr-22 01:51 fmot/test/test_nn_stft.py
--rw-r--r--  2.0 unx     2347 b- defN 23-Apr-22 01:51 fmot/test/test_observers.py
--rw-r--r--  2.0 unx      310 b- defN 23-Apr-22 01:51 fmot/test/test_package.py
--rw-r--r--  2.0 unx     1240 b- defN 23-Apr-22 01:51 fmot/test/test_param_reuse.py
--rw-r--r--  2.0 unx      830 b- defN 23-Apr-22 01:51 fmot/test/test_precision_mod.py
--rw-r--r--  2.0 unx     6440 b- defN 23-Apr-22 01:51 fmot/test/test_pruning.py
--rw-r--r--  2.0 unx    25171 b- defN 23-Apr-22 01:51 fmot/test/test_sequencers.py
--rw-r--r--  2.0 unx     2017 b- defN 23-Apr-22 01:51 fmot/test/test_serialization.py
--rw-r--r--  2.0 unx     4308 b- defN 23-Apr-22 01:51 fmot/test/test_stft.py
--rw-r--r--  2.0 unx     1322 b- defN 23-Apr-22 01:51 fmot/test/test_temporal_unfold.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-22 01:51 fmot/test/test_fqir/__init__.py
--rw-r--r--  2.0 unx     1657 b- defN 23-Apr-22 01:51 fmot/test/test_fqir/test_opchecks.py
--rw-r--r--  2.0 unx     1138 b- defN 23-Apr-22 01:51 fmot/test/test_fqir/test_readme.py
--rw-r--r--  2.0 unx    15396 b- defN 23-Apr-22 01:51 fmot/test/test_fqir/test_single_ops.py
--rw-r--r--  2.0 unx      147 b- defN 23-Apr-22 01:51 fmot/test/utm/__init__.py
--rw-r--r--  2.0 unx    13543 b- defN 23-Apr-22 01:51 fmot/test/utm/atomic_test_library.py
--rw-r--r--  2.0 unx     5731 b- defN 23-Apr-22 01:51 fmot/test/utm/feedforward_test_library.py
--rw-r--r--  2.0 unx      990 b- defN 23-Apr-22 01:51 fmot/test/utm/get_utms.py
--rw-r--r--  2.0 unx     2284 b- defN 23-Apr-22 01:51 fmot/test/utm/plot_errors.py
--rw-r--r--  2.0 unx     3603 b- defN 23-Apr-22 01:51 fmot/test/utm/quant_tolerances.py
--rw-r--r--  2.0 unx    10015 b- defN 23-Apr-22 01:51 fmot/test/utm/rnn_test_library.py
--rw-r--r--  2.0 unx      173 b- defN 23-Apr-22 01:51 fmot/test/utm/test_utm_converted_runtime.py
--rw-r--r--  2.0 unx      340 b- defN 23-Apr-22 01:51 fmot/test/utm/test_utm_fqir_runtime.py
--rw-r--r--  2.0 unx      316 b- defN 23-Apr-22 01:51 fmot/test/utm/test_utm_mixed_precision.py
--rw-r--r--  2.0 unx      587 b- defN 23-Apr-22 01:51 fmot/test/utm/test_utm_quantization_error.py
--rw-r--r--  2.0 unx    12337 b- defN 23-Apr-22 01:51 fmot/test/utm/unittest_objects.py
--rw-r--r--  2.0 unx       90 b- defN 23-Apr-22 01:51 fmot/tracing/__init__.py
--rw-r--r--  2.0 unx     2699 b- defN 23-Apr-22 01:51 fmot/tracing/compare_fqir.py
--rw-r--r--  2.0 unx     1542 b- defN 23-Apr-22 01:51 fmot/tracing/oplinks_v1.py
--rw-r--r--  2.0 unx    23682 b- defN 23-Apr-22 01:51 fmot/tracing/tracing.py
--rw-r--r--  2.0 unx      365 b- defN 23-Apr-22 01:51 fmot/tracing/tracing_blacklist.py
--rw-r--r--  2.0 unx     2017 b- defN 23-Apr-22 01:51 fmot/tracing/utils.py
--rw-r--r--  2.0 unx      325 b- defN 23-Apr-22 01:51 fmot/utils/__init__.py
--rw-r--r--  2.0 unx     3941 b- defN 23-Apr-22 01:51 fmot/utils/activity.py
--rw-r--r--  2.0 unx     3089 b- defN 23-Apr-22 01:51 fmot/utils/conv1d_utils.py
--rw-r--r--  2.0 unx     2245 b- defN 23-Apr-22 01:51 fmot/utils/param_manager.py
--rw-r--r--  2.0 unx     6744 b- defN 23-Apr-22 01:51 fmot/utils/quant_diagnostic.py
--rw-r--r--  2.0 unx     1792 b- defN 23-Apr-22 01:51 fmot/utils/quantizer_manager.py
--rw-r--r--  2.0 unx      357 b- defN 23-Apr-22 01:51 fmot/utils/rich_attr.py
--rw-r--r--  2.0 unx     8372 b- defN 23-Apr-22 01:51 fmot/utils/saturation_opt.py
--rw-r--r--  2.0 unx     1986 b- defN 23-Apr-22 01:51 fmot/utils/serialization.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-22 01:51 fmot/utils/quant_tools/__init__.py
--rw-r--r--  2.0 unx     4545 b- defN 23-Apr-22 01:51 fmot/utils/quant_tools/diagnosis.py
--rwxr-xr-x  2.0 unx       36 b- defN 23-Apr-22 01:55 fmot-1.5.6.dist-info/LICENSE
--rw-r--r--  2.0 unx     3668 b- defN 23-Apr-22 01:55 fmot-1.5.6.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-22 01:55 fmot-1.5.6.dist-info/WHEEL
--rw-r--r--  2.0 unx        5 b- defN 23-Apr-22 01:55 fmot-1.5.6.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    13156 b- defN 23-Apr-22 01:55 fmot-1.5.6.dist-info/RECORD
-158 files, 820588 bytes uncompressed, 201679 bytes compressed:  75.4%
+Zip file size: 223568 bytes, number of entries: 158
+-rwxr-xr-x  2.0 unx     1355 b- defN 23-May-25 17:07 fmot/ENV_REQUIREMENTS.sh
+-rwxr-xr-x  2.0 unx      138 b- defN 23-May-25 17:07 fmot/PY_REQUIREMENTS
+-rw-r--r--  2.0 unx        6 b- defN 23-May-25 17:07 fmot/VERSION
+-rw-r--r--  2.0 unx      648 b- defN 23-May-25 17:07 fmot/__init__.py
+-rw-r--r--  2.0 unx      414 b- defN 23-May-25 17:07 fmot/_open_docs.py
+-rw-r--r--  2.0 unx     1611 b- defN 23-May-25 17:07 fmot/_supported_ops.py
+-rw-r--r--  2.0 unx     1654 b- defN 23-May-25 17:07 fmot/configure.py
+-rw-r--r--  2.0 unx      929 b- defN 23-May-25 17:07 fmot/exceptions.py
+-rw-r--r--  2.0 unx     1821 b- defN 23-May-25 17:07 fmot/execute_code.py
+-rw-r--r--  2.0 unx       61 b- defN 23-May-25 17:07 fmot/functional.py
+-rw-r--r--  2.0 unx     8975 b- defN 23-May-25 17:07 fmot/torchscript_utils.py
+-rw-r--r--  2.0 unx       83 b- defN 23-May-25 17:07 fmot/beta/__init__.py
+-rw-r--r--  2.0 unx     4169 b- defN 23-May-25 17:07 fmot/beta/amp.py
+-rw-r--r--  2.0 unx    10740 b- defN 23-May-25 17:07 fmot/beta/auto_multiprecision.py
+-rw-r--r--  2.0 unx     1166 b- defN 23-May-25 17:07 fmot/beta/quantize_part.py
+-rw-r--r--  2.0 unx      200 b- defN 23-May-25 17:07 fmot/convert/__init__.py
+-rw-r--r--  2.0 unx     2900 b- defN 23-May-25 17:07 fmot/convert/_convert_to_qat.py
+-rw-r--r--  2.0 unx    15168 b- defN 23-May-25 17:07 fmot/convert/conversion_api.py
+-rw-r--r--  2.0 unx     2410 b- defN 23-May-25 17:07 fmot/convert/default_mappings.py
+-rw-r--r--  2.0 unx     9684 b- defN 23-May-25 17:07 fmot/convert/default_patchings.py
+-rw-r--r--  2.0 unx      547 b- defN 23-May-25 17:07 fmot/convert/default_substitutions.py
+-rw-r--r--  2.0 unx     4562 b- defN 23-May-25 17:07 fmot/convert/lut_registry.py
+-rw-r--r--  2.0 unx     3998 b- defN 23-May-25 17:07 fmot/convert/mapping.py
+-rw-r--r--  2.0 unx     2831 b- defN 23-May-25 17:07 fmot/convert/optimizer.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-25 17:07 fmot/convert/param_manager.py
+-rw-r--r--  2.0 unx    24680 b- defN 23-May-25 17:07 fmot/convert/patch_ir.py
+-rw-r--r--  2.0 unx     4402 b- defN 23-May-25 17:07 fmot/convert/patching.py
+-rw-r--r--  2.0 unx     3244 b- defN 23-May-25 17:07 fmot/convert/prune_reparametrization.py
+-rw-r--r--  2.0 unx     2963 b- defN 23-May-25 17:07 fmot/convert/quantizer_manager.py
+-rw-r--r--  2.0 unx     2121 b- defN 23-May-25 17:07 fmot/convert/substitution.py
+-rw-r--r--  2.0 unx      199 b- defN 23-May-25 17:07 fmot/fqir/__init__.py
+-rw-r--r--  2.0 unx      660 b- defN 23-May-25 17:07 fmot/fqir/utils.py
+-rw-r--r--  2.0 unx       35 b- defN 23-May-25 17:07 fmot/fqir/graph_proto/__init__.py
+-rw-r--r--  2.0 unx    13950 b- defN 23-May-25 17:07 fmot/fqir/graph_proto/graph_proto.py
+-rw-r--r--  2.0 unx     5303 b- defN 23-May-25 17:07 fmot/fqir/graph_proto/graph_runtime.py
+-rw-r--r--  2.0 unx      115 b- defN 23-May-25 17:07 fmot/fqir/nodes/__init__.py
+-rw-r--r--  2.0 unx     5761 b- defN 23-May-25 17:07 fmot/fqir/nodes/node_base.py
+-rw-r--r--  2.0 unx     6920 b- defN 23-May-25 17:07 fmot/fqir/nodes/node_proto.py
+-rw-r--r--  2.0 unx     2207 b- defN 23-May-25 17:07 fmot/fqir/nodes/opcount.py
+-rw-r--r--  2.0 unx    20518 b- defN 23-May-25 17:07 fmot/fqir/nodes/opcounters.py
+-rw-r--r--  2.0 unx     7008 b- defN 23-May-25 17:07 fmot/fqir/nodes/optype_base.py
+-rw-r--r--  2.0 unx    30178 b- defN 23-May-25 17:07 fmot/fqir/nodes/optypes.py
+-rw-r--r--  2.0 unx      583 b- defN 23-May-25 17:07 fmot/fqir/passes/__init__.py
+-rw-r--r--  2.0 unx     7322 b- defN 23-May-25 17:07 fmot/fqir/passes/batchdim_removal.py
+-rw-r--r--  2.0 unx     1606 b- defN 23-May-25 17:07 fmot/fqir/passes/cleanup.py
+-rw-r--r--  2.0 unx      463 b- defN 23-May-25 17:07 fmot/fqir/passes/dimtag_removal.py
+-rw-r--r--  2.0 unx     1160 b- defN 23-May-25 17:07 fmot/fqir/passes/helpers.py
+-rw-r--r--  2.0 unx    19971 b- defN 23-May-25 17:07 fmot/fqir/passes/kernelize_lstm.py
+-rw-r--r--  2.0 unx     5496 b- defN 23-May-25 17:07 fmot/fqir/passes/kernelize_red_broad.py
+-rw-r--r--  2.0 unx     2039 b- defN 23-May-25 17:07 fmot/fqir/passes/kernelize_temporal_unfold.py
+-rw-r--r--  2.0 unx       94 b- defN 23-May-25 17:07 fmot/fqir/variables/__init__.py
+-rw-r--r--  2.0 unx     6255 b- defN 23-May-25 17:07 fmot/fqir/variables/tensor_proto.py
+-rw-r--r--  2.0 unx     2064 b- defN 23-May-25 17:07 fmot/fqir/variables/variable_base.py
+-rw-r--r--  2.0 unx      563 b- defN 23-May-25 17:07 fmot/nn/__init__.py
+-rw-r--r--  2.0 unx     8663 b- defN 23-May-25 17:07 fmot/nn/atomics.py
+-rw-r--r--  2.0 unx    12410 b- defN 23-May-25 17:07 fmot/nn/composites.py
+-rw-r--r--  2.0 unx    27568 b- defN 23-May-25 17:07 fmot/nn/conv1d.py
+-rw-r--r--  2.0 unx     1844 b- defN 23-May-25 17:07 fmot/nn/derived_param.py
+-rw-r--r--  2.0 unx    21993 b- defN 23-May-25 17:07 fmot/nn/femtornn.py
+-rw-r--r--  2.0 unx    16064 b- defN 23-May-25 17:07 fmot/nn/fft.py
+-rw-r--r--  2.0 unx    34238 b- defN 23-May-25 17:07 fmot/nn/sequenced_rnn.py
+-rw-r--r--  2.0 unx     5305 b- defN 23-May-25 17:07 fmot/nn/sequencer.py
+-rw-r--r--  2.0 unx    12970 b- defN 23-May-25 17:07 fmot/nn/signal_processing.py
+-rw-r--r--  2.0 unx     3742 b- defN 23-May-25 17:07 fmot/nn/sliding_attention.py
+-rw-r--r--  2.0 unx     9256 b- defN 23-May-25 17:07 fmot/nn/sparsifiers.py
+-rw-r--r--  2.0 unx     2294 b- defN 23-May-25 17:07 fmot/nn/sru.py
+-rw-r--r--  2.0 unx     8916 b- defN 23-May-25 17:07 fmot/nn/stft.py
+-rw-r--r--  2.0 unx     1525 b- defN 23-May-25 17:07 fmot/nn/super_structures.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-May-25 17:07 fmot/nn/temporal_unfold.py
+-rw-r--r--  2.0 unx      192 b- defN 23-May-25 17:07 fmot/qat/__init__.py
+-rw-r--r--  2.0 unx     4059 b- defN 23-May-25 17:07 fmot/qat/annotated_tensors.py
+-rw-r--r--  2.0 unx     1659 b- defN 23-May-25 17:07 fmot/qat/bitwidths.py
+-rw-r--r--  2.0 unx     4381 b- defN 23-May-25 17:07 fmot/qat/control.py
+-rw-r--r--  2.0 unx     2684 b- defN 23-May-25 17:07 fmot/qat/fake_quantization.py
+-rw-r--r--  2.0 unx      328 b- defN 23-May-25 17:07 fmot/qat/nn/__init__.py
+-rw-r--r--  2.0 unx      276 b- defN 23-May-25 17:07 fmot/qat/nn/_utils.py
+-rw-r--r--  2.0 unx     3783 b- defN 23-May-25 17:07 fmot/qat/nn/act_density.py
+-rw-r--r--  2.0 unx    28277 b- defN 23-May-25 17:07 fmot/qat/nn/atomics.py
+-rw-r--r--  2.0 unx     9336 b- defN 23-May-25 17:07 fmot/qat/nn/composites.py
+-rw-r--r--  2.0 unx    10828 b- defN 23-May-25 17:07 fmot/qat/nn/density_matmul.py
+-rw-r--r--  2.0 unx     2102 b- defN 23-May-25 17:07 fmot/qat/nn/derived_param.py
+-rw-r--r--  2.0 unx     8923 b- defN 23-May-25 17:07 fmot/qat/nn/linear.py
+-rw-r--r--  2.0 unx    19303 b- defN 23-May-25 17:07 fmot/qat/nn/lstm.py
+-rw-r--r--  2.0 unx    14400 b- defN 23-May-25 17:07 fmot/qat/nn/luts.py
+-rw-r--r--  2.0 unx     3922 b- defN 23-May-25 17:07 fmot/qat/nn/norm.py
+-rw-r--r--  2.0 unx     1350 b- defN 23-May-25 17:07 fmot/qat/nn/quant_wrap.py
+-rw-r--r--  2.0 unx    22679 b- defN 23-May-25 17:07 fmot/qat/nn/quantizers.py
+-rw-r--r--  2.0 unx     1992 b- defN 23-May-25 17:07 fmot/qat/nn/sequencer.py
+-rw-r--r--  2.0 unx     2283 b- defN 23-May-25 17:07 fmot/qat/nn/temporal_unfold.py
+-rw-r--r--  2.0 unx     1007 b- defN 23-May-25 17:07 fmot/qat/nn/tuning_eps.py
+-rw-r--r--  2.0 unx       90 b- defN 23-May-25 17:07 fmot/sparse/__init__.py
+-rw-r--r--  2.0 unx     2694 b- defN 23-May-25 17:07 fmot/sparse/act_pruning.py
+-rw-r--r--  2.0 unx     7357 b- defN 23-May-25 17:07 fmot/sparse/pruning.py
+-rw-r--r--  2.0 unx     4024 b- defN 23-May-25 17:07 fmot/sparse/pruning_schedulers.py
+-rw-r--r--  2.0 unx     7162 b- defN 23-May-25 17:07 fmot/sparse/pruning_utils.py
+-rw-r--r--  2.0 unx      196 b- defN 23-May-25 17:07 fmot/test/__init__.py
+-rw-r--r--  2.0 unx     5861 b- defN 23-May-25 17:07 fmot/test/ds_tc_resnet.py
+-rw-r--r--  2.0 unx     3000 b- defN 23-May-25 17:07 fmot/test/kws_test_library.py
+-rw-r--r--  2.0 unx      936 b- defN 23-May-25 17:07 fmot/test/test_automp.py
+-rw-r--r--  2.0 unx     1373 b- defN 23-May-25 17:07 fmot/test/test_cqt.py
+-rw-r--r--  2.0 unx     1688 b- defN 23-May-25 17:07 fmot/test/test_cuda.py
+-rw-r--r--  2.0 unx     3878 b- defN 23-May-25 17:07 fmot/test/test_density_tracking.py
+-rw-r--r--  2.0 unx     1134 b- defN 23-May-25 17:07 fmot/test/test_diagnosis.py
+-rw-r--r--  2.0 unx     3433 b- defN 23-May-25 17:07 fmot/test/test_dim_anno.py
+-rw-r--r--  2.0 unx      958 b- defN 23-May-25 17:07 fmot/test/test_dropout.py
+-rw-r--r--  2.0 unx    14698 b- defN 23-May-25 17:07 fmot/test/test_features.py
+-rw-r--r--  2.0 unx      722 b- defN 23-May-25 17:07 fmot/test/test_fft.py
+-rw-r--r--  2.0 unx     1664 b- defN 23-May-25 17:07 fmot/test/test_ifft.py
+-rw-r--r--  2.0 unx      378 b- defN 23-May-25 17:07 fmot/test/test_layernorm.py
+-rw-r--r--  2.0 unx     1412 b- defN 23-May-25 17:07 fmot/test/test_lut_grads.py
+-rw-r--r--  2.0 unx     3581 b- defN 23-May-25 17:07 fmot/test/test_nn_stft.py
+-rw-r--r--  2.0 unx     2347 b- defN 23-May-25 17:07 fmot/test/test_observers.py
+-rw-r--r--  2.0 unx      310 b- defN 23-May-25 17:07 fmot/test/test_package.py
+-rw-r--r--  2.0 unx     1240 b- defN 23-May-25 17:07 fmot/test/test_param_reuse.py
+-rw-r--r--  2.0 unx      830 b- defN 23-May-25 17:07 fmot/test/test_precision_mod.py
+-rw-r--r--  2.0 unx     6440 b- defN 23-May-25 17:07 fmot/test/test_pruning.py
+-rw-r--r--  2.0 unx    25171 b- defN 23-May-25 17:07 fmot/test/test_sequencers.py
+-rw-r--r--  2.0 unx     2381 b- defN 23-May-25 17:07 fmot/test/test_serialization.py
+-rw-r--r--  2.0 unx     4308 b- defN 23-May-25 17:07 fmot/test/test_stft.py
+-rw-r--r--  2.0 unx     1322 b- defN 23-May-25 17:07 fmot/test/test_temporal_unfold.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-25 17:07 fmot/test/test_fqir/__init__.py
+-rw-r--r--  2.0 unx     1657 b- defN 23-May-25 17:07 fmot/test/test_fqir/test_opchecks.py
+-rw-r--r--  2.0 unx     1138 b- defN 23-May-25 17:07 fmot/test/test_fqir/test_readme.py
+-rw-r--r--  2.0 unx    15396 b- defN 23-May-25 17:07 fmot/test/test_fqir/test_single_ops.py
+-rw-r--r--  2.0 unx      147 b- defN 23-May-25 17:07 fmot/test/utm/__init__.py
+-rw-r--r--  2.0 unx    13543 b- defN 23-May-25 17:07 fmot/test/utm/atomic_test_library.py
+-rw-r--r--  2.0 unx     5731 b- defN 23-May-25 17:07 fmot/test/utm/feedforward_test_library.py
+-rw-r--r--  2.0 unx      990 b- defN 23-May-25 17:07 fmot/test/utm/get_utms.py
+-rw-r--r--  2.0 unx     2284 b- defN 23-May-25 17:07 fmot/test/utm/plot_errors.py
+-rw-r--r--  2.0 unx     3603 b- defN 23-May-25 17:07 fmot/test/utm/quant_tolerances.py
+-rw-r--r--  2.0 unx    10015 b- defN 23-May-25 17:07 fmot/test/utm/rnn_test_library.py
+-rw-r--r--  2.0 unx      173 b- defN 23-May-25 17:07 fmot/test/utm/test_utm_converted_runtime.py
+-rw-r--r--  2.0 unx      340 b- defN 23-May-25 17:07 fmot/test/utm/test_utm_fqir_runtime.py
+-rw-r--r--  2.0 unx      316 b- defN 23-May-25 17:07 fmot/test/utm/test_utm_mixed_precision.py
+-rw-r--r--  2.0 unx      587 b- defN 23-May-25 17:07 fmot/test/utm/test_utm_quantization_error.py
+-rw-r--r--  2.0 unx    12337 b- defN 23-May-25 17:07 fmot/test/utm/unittest_objects.py
+-rw-r--r--  2.0 unx       90 b- defN 23-May-25 17:07 fmot/tracing/__init__.py
+-rw-r--r--  2.0 unx     2699 b- defN 23-May-25 17:07 fmot/tracing/compare_fqir.py
+-rw-r--r--  2.0 unx     1542 b- defN 23-May-25 17:07 fmot/tracing/oplinks_v1.py
+-rw-r--r--  2.0 unx    23682 b- defN 23-May-25 17:07 fmot/tracing/tracing.py
+-rw-r--r--  2.0 unx      365 b- defN 23-May-25 17:07 fmot/tracing/tracing_blacklist.py
+-rw-r--r--  2.0 unx     2017 b- defN 23-May-25 17:07 fmot/tracing/utils.py
+-rw-r--r--  2.0 unx      325 b- defN 23-May-25 17:07 fmot/utils/__init__.py
+-rw-r--r--  2.0 unx     3941 b- defN 23-May-25 17:07 fmot/utils/activity.py
+-rw-r--r--  2.0 unx     3089 b- defN 23-May-25 17:07 fmot/utils/conv1d_utils.py
+-rw-r--r--  2.0 unx     2245 b- defN 23-May-25 17:07 fmot/utils/param_manager.py
+-rw-r--r--  2.0 unx     6744 b- defN 23-May-25 17:07 fmot/utils/quant_diagnostic.py
+-rw-r--r--  2.0 unx     1792 b- defN 23-May-25 17:07 fmot/utils/quantizer_manager.py
+-rw-r--r--  2.0 unx      357 b- defN 23-May-25 17:07 fmot/utils/rich_attr.py
+-rw-r--r--  2.0 unx     8372 b- defN 23-May-25 17:07 fmot/utils/saturation_opt.py
+-rw-r--r--  2.0 unx     1986 b- defN 23-May-25 17:07 fmot/utils/serialization.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-25 17:07 fmot/utils/quant_tools/__init__.py
+-rw-r--r--  2.0 unx     4545 b- defN 23-May-25 17:07 fmot/utils/quant_tools/diagnosis.py
+-rwxr-xr-x  2.0 unx       36 b- defN 23-May-25 17:11 fmot-1.5.8.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3668 b- defN 23-May-25 17:11 fmot-1.5.8.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-25 17:11 fmot-1.5.8.dist-info/WHEEL
+-rw-r--r--  2.0 unx        5 b- defN 23-May-25 17:11 fmot-1.5.8.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    13156 b- defN 23-May-25 17:11 fmot-1.5.8.dist-info/RECORD
+158 files, 825525 bytes uncompressed, 203056 bytes compressed:  75.4%
```

## zipnote {}

```diff
@@ -453,23 +453,23 @@
 
 Filename: fmot/utils/quant_tools/__init__.py
 Comment: 
 
 Filename: fmot/utils/quant_tools/diagnosis.py
 Comment: 
 
-Filename: fmot-1.5.6.dist-info/LICENSE
+Filename: fmot-1.5.8.dist-info/LICENSE
 Comment: 
 
-Filename: fmot-1.5.6.dist-info/METADATA
+Filename: fmot-1.5.8.dist-info/METADATA
 Comment: 
 
-Filename: fmot-1.5.6.dist-info/WHEEL
+Filename: fmot-1.5.8.dist-info/WHEEL
 Comment: 
 
-Filename: fmot-1.5.6.dist-info/top_level.txt
+Filename: fmot-1.5.8.dist-info/top_level.txt
 Comment: 
 
-Filename: fmot-1.5.6.dist-info/RECORD
+Filename: fmot-1.5.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fmot/VERSION

```diff
@@ -1 +1 @@
-1.5.6
+1.5.8
```

## fmot/torchscript_utils.py

```diff
@@ -69,31 +69,61 @@
 def get_list(x, module=None):
     node = x.node()
     return [get_value(xx, module=module) for xx in node.inputs()]
 
 def isaten(node):
     return node.kind().startswith('aten::')
 
+def isblock(node):
+    return len(list(node.blocks())) > 0
+
 def hasaten(graph):
     if graph is None:
         return False
     return any([isaten(node) for node in graph.nodes()])
 
+def hassubblocks(graph):
+    return any([isblock(node) for node in graph.nodes()])
+
 def isgetparam(node):
     return node.kind() == 'prim::GetAttr' and istensor(node.output())
 
 def isgetattr(node):
     return node.kind() == 'prim::GetAttr' and not istensor(node.output())
 
 def isgetattrmodule(node):
     return isgetattr(node) and ismodule(node.output())
 
 def iscallmethod(node):
     return node.kind() == 'prim::CallMethod'
 
+def getcallmethod(node):
+    if not iscallmethod(node):
+        raise ValueError(f'node was expected to be prim::CallMethod, was {node.kind()} instead')
+    
+    attrs = parse_attributes(node)
+    return attrs['name']
+
+def calls_allowed_methods(node, allowed_methods: list):
+    """checks if a prim::CallMethod node calls one of the allowed methods"""
+    methodname = getcallmethod(node)
+    if methodname in allowed_methods:
+        return True
+    elif methodname.split('__')[0] in allowed_methods:
+        return True
+    else:
+        return False
+
+def assert_calls_allowed_methods(graph, allowed_methods: list):
+    for node in itergraph(graph):
+        if iscallmethod(node):
+            if not calls_allowed_methods(node, allowed_methods):
+                raise ValueError(f'Node {node} calls method "{getcallmethod(node)}", which is not one of {allowed_methods}. Please contain your forward logic'
+                                 f' in forward methods.\n\n{graph}')
+
 def isprimcallmethod(node):
     return node.kind() == 'prim::PythonOp' and node.pyname() == 'forward'
 
 def islistconstruct(node, tensorlist=True):
     result = node.kind() == 'prim::ListConstruct'
     if result and tensorlist:
         result = istensorlist(node.output())
@@ -153,18 +183,112 @@
 def getuserfuncname(node):
     assert isuserfunction(node)
     funcname = get_function_name(node)
     assert funcname.startswith('__torch__')
     funcname = funcname.split('__torch__.')[1]
     return funcname
 
+def ispermissiveaten(node):
+    """For a given node, returns True if:
+    
+    1. the node is not an aten operation
+    
+    OR if it is an aten operation, it will still return True if one
+    of the following is satisfied:
+        2. the inputs are all integers
+        3. the node is an append, slice, or getitem applied to a tensorlist
+        4. the node is aten::len
+    """
+
+    # 1: True if not aten
+    if not isaten(node):
+        return True
+
+
+    # 2. True if all arguments are ints
+    input_types = set()
+    for input in node.inputs():
+        input_types.add(input.type().str())
+    if len(input_types) == 1:
+        if list(input_types)[0] == 'int':
+            # print(f'Node applied to just ints {node}')
+            return True
+        
+    # 3. True if it is append, slice, or getitem on a Tensor[]
+
+    # 3.a. list __getitem__
+    if node.kind() == 'aten::__getitem__':
+        x, idx = node.inputs()
+        assert idx.type().str() == 'int'
+        if x.type().str().startswith('Tensor[]'):
+            # print(f'TensorList.__getitem__: {node}')
+            return True
+        
+    # 3.b. list append
+    if node.kind() == 'aten::append':
+        base, __ = node.inputs()
+        if base.type().str().startswith('Tensor[]'):
+            # print(f'TensorList.append: {node}')
+            return True
+        
+    # 3.c. list slice
+    if node.kind() == 'aten::slice':
+        # note: not checking types of slice start/end/delta arguments,
+        # just the base input
+        base, __, __, __ = node.inputs()
+        if base.type().str().startswith('Tensor[]'):
+            # print(f'TensorList.slice: {node}')
+            return True
+        
+    # 4. aten::len is okay
+    if node.kind() == 'aten::len':
+        return True
+    
+    return False
+
 def hasfunctional(graph):
     return any([isfunctional(node) for node in graph.nodes()])
 
 def hasuserfunction(graph):
     return any([isuserfunction(node) for node in graph.nodes()])
 
 def needspatching(graph):
     if graph is None:
         raise ValueError('Could not get graph')
     else:
-        return any([hasgetparam(graph), hasaten(graph), hasfunctional(graph), hasuserfunction(graph)])
+        if any([hasgetparam(graph), hasaten(graph), hasfunctional(graph), hasuserfunction(graph)]):
+            if hassubblocks(graph):
+                raise NotImplementedError('Graph has conditional blocks, not supported at this time')
+            
+            assert_calls_allowed_methods(graph, ['forward'])
+
+            return True
+
+def itergraph(graph):
+    for node in graph.nodes():
+        yield node
+        for blk in node.blocks():
+            for sub_node in itergraph(blk):
+                yield sub_node
+
+def issuperstructure(graph):
+    """
+    A SuperStructure is a module that satisfies the following constraints:
+
+    1. No direct parameter accesses in the module
+    2. No arithmetic aten operations are used, but some aten ops are still allowed 
+        (permissive aten operation check allows for aten ops that act on collections of tensors, like aten::append)
+    3. No functional operations
+    """
+
+    for node in itergraph(graph):
+        if isgetparam(node):
+            return False
+        elif not ispermissiveaten(node):
+            return False
+        elif isfunctional(node) or isuserfunction(node):
+            return False
+    assert_calls_allowed_methods(graph, ['forward'])
+        
+    return True
+
+
```

## fmot/convert/patching.py

```diff
@@ -19,23 +19,29 @@
         module (Module): Module to be checked
         mappings (dict): Dictionary of mappings
     """
     if isinstance(module, (nn.ModuleList, nn.ModuleDict)):
         return False
     if type(module) in mappings:
         return False
-    # If our module is (inherited from) a Super Structure, we basically want to
+    # If our module is (inherited from) a SuperStructure, we basically want to
     # skip over all aten/functional operations for the moment,
     # sublayers will still be patched later
     elif any(issubclass(type(module), super_struct) for super_struct in SUPERSTRUCT_DIC):
         return False
     elif issubclass(type(module), ProtectedModule):
         return False
+    
+    # torchscript utils now has its own way to check for super-structure that doens't rely on the user directly 
+    # annotating it themselves
+    graph = get_graph(module, step=isinstance(module, Sequencer))
+    if tsutils.issuperstructure(graph):
+        return False
     else:
-        return tsutils.needspatching(get_graph(module, step=isinstance(module, Sequencer)))
+        return tsutils.needspatching(graph)
 
 def verbose_printout(parent_name, child_name, patch, original_model):
     name = parent_name
     if child_name is not None:
         name += '.' + child_name
     if not isinstance(original_model, Sequencer):
         model_code = inspect.getsource(original_model.forward)
```

## fmot/nn/composites.py

```diff
@@ -400,30 +400,33 @@
     `max_abs` is a running maximum input seen by the layer.
     
     Arguments:
         alpha (float): exponential smoothing coefficient, for updating `running_max`
         eps (float): ratio between max_abs and epsilon"""
     def __init__(self, alpha=0.99, eps=1/128):
         super().__init__()
-        self.running_max = 0
+        self.register_buffer('running_max', torch.tensor(0))
         self.alpha = alpha
         self.eps = eps
 
     @torch.jit.ignore()
     def epsilon(self):
         return self.running_max * self.eps
 
     @torch.jit.ignore()
     @torch.no_grad()
     def update(self, x):
-        xmax = torch.max(x).cpu().item()
-        if self.running_max == 0:
-            self.running_max = xmax
-        else:
-            self.running_max = self.alpha * self.running_max + (1-self.alpha) * xmax
+        """ Updates the running max during training
+        """
+        if self.training:
+            xmax = torch.max(x).detach()
+            if self.running_max == 0:
+                self.running_max = xmax
+            else:
+                self.running_max = self.alpha * self.running_max + (1-self.alpha) * xmax
 
     def forward(self, x):
         self.update(x)
         return x + self.epsilon()
 
 class Softmax(nn.Module):
     def __init__(self, size: int, dim=-1):
```

## fmot/nn/fft.py

```diff
@@ -131,15 +131,15 @@
         self.perm = nn.Parameter(get_partial_bit_reversal_matrix(n_fft, n_stages),
                 requires_grad=False)
 
     def forward(self, x):
         y = torch.matmul(x, self.perm)
         return torch.chunk(y, self.n_chunks, dim=-1)
 
-class FFT(SuperStructure):
+class FFT(nn.Module):
     """Sparse decomposition of FFT.
 
     Arguments:
         n_fft (int): FFT size
         n_stages (int): number of decomposition stages.
     """
     def __init__(self, n_fft: int, n_stages: int):
@@ -150,84 +150,64 @@
         else:
             dft_class = DFT
 
         self.n_fft = n_fft
         self.n_stages = n_stages
         assert n_fft / 2**n_stages % 1 == 0, f'Cannot decompose {n_fft} with {n_stages} power-of-2 stages'
 
-        dft_parallelism = min(2**n_stages, DFT_PARALLELISM)
-        assert dft_parallelism <= 2**n_stages
-        self.dft_parallelism = dft_parallelism
-        self.twiddle_parallelism = TWIDDLE_PARALLELISM
-
-        self.dfts = nn.ModuleList()
-        for _ in range(DFT_PARALLELISM):
-            self.dfts.append(dft_class(n_fft//2**n_stages))
+        self.dft = dft_class(n_fft//2**n_stages)
+
+        self.permuter: Optional[nn.Module] = None
+        self.twiddle_states: Optional[nn.ModuleList] = None
 
         if n_stages > 0:
             self.permuter = FFTPermuter(n_fft, n_stages)
             self.twiddle_stages = nn.ModuleList()
 
             stage_size = n_fft//2**(n_stages-1)
             num_calls = n_fft//stage_size
             for _ in range(n_stages):
-                twiddle_list = nn.ModuleList()
-                for j in range(min(num_calls, TWIDDLE_PARALLELISM)):
-                    twiddle_list.append(FFTwiddle(stage_size, inv=False))
-                self.twiddle_stages.append(twiddle_list)
+                self.twiddle_stages.append(FFTwiddle(stage_size, inv=False))
                 stage_size = stage_size * 2
                 num_calls = num_calls//2
 
-        else:
-            self.permuter = None
-            self.twiddle_stages = None
-
         # quantization configs
         self.observe: bool = False
         self.quantize: bool = False
 
-    @torch.jit.ignore
     def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
         """
         Arguments:
             x (Tensor): real-valued tensor of shape (*, n_fft)
         Returns:
             - (re, im): two tensors of shape (*, n_fft)
                 The first holds the real-part, and the second holds the imaginary part
         """
 
-        # Fallback to fast implementation of FFT if 
-        # not in quantization or observation mode
-        if not (self.observe or self.quantize or hasattr(x, 'dimensions')):
-            y = torch.fft.fft(x, dim=-1, n=self.n_fft)
-            real, imag = y.real, y.imag
-            return real, imag
-
         if self.permuter is None:
-            return self.dfts[0](x)
+            return self.dft(x)
 
         else:
             perms = self.permuter(x)
 
             sub_dfts = [[],[],[],[]] # stores even_real, even_imag, odd_real, odd_imag sets
 
             for j, (x_even, x_odd) in enumerate(zip(perms[::2], perms[1::2])):
 
-                ev_r, ev_i = self.dfts[2*j % self.dft_parallelism](x_even)
-                od_r, od_i = self.dfts[(2*j+1) % self.dft_parallelism](x_odd)
+                ev_r, ev_i = self.dft(x_even)
+                od_r, od_i = self.dft(x_odd)
 
                 sub_dfts[0].append(ev_r)
                 sub_dfts[1].append(ev_i)
                 sub_dfts[2].append(od_r)
                 sub_dfts[3].append(od_i)
 
-            for twiddle_list in self.twiddle_stages:
+            for twiddler in self.twiddle_stages:
                 new_sub_dfts = [[],[],[],[]]
-                for i, (ev_r, ev_i, od_r, od_i) in enumerate(zip(*sub_dfts)):
-                    twiddler = twiddle_list[i%len(twiddle_list)]
+                for i, (ev_r, ev_i, od_r, od_i) in enumerate(zip(sub_dfts[0], sub_dfts[1], sub_dfts[2], sub_dfts[3])):
                     real, imag = twiddler(ev_r, ev_i, od_r, od_i)
                     if i % 2 == 0:
                         new_sub_dfts[0].append(real)
                         new_sub_dfts[1].append(imag)
                     else:
                         new_sub_dfts[2].append(real)
                         new_sub_dfts[3].append(imag)
@@ -343,82 +323,71 @@
         self.n_fft = n_fft
         self.n_stages = n_stages
         self.factor = 2**(n_stages)
 
     def forward(self, x):
         return x / self.factor
     
-class IFFT(SuperStructure):
+class IFFT(nn.Module):
     def __init__(self, n_fft, n_stages):
         super().__init__()
 
         self.n_fft = n_fft
         self.n_stages = n_stages
         assert n_fft / 2**n_stages % 1 == 0, f'Cannot decompose {n_fft} with {n_stages} power-of-2 stages'
 
-        dft_parallelism = min(2**n_stages, DFT_PARALLELISM)
-        assert dft_parallelism <= 2**n_stages
-        self.dft_parallelism = dft_parallelism
-        self.twiddle_parallelism = TWIDDLE_PARALLELISM
-
-        self.idfts = nn.ModuleList()
-        for _ in range(dft_parallelism):
-            self.idfts.append(IDFT(n_fft//2**n_stages))
+
+        self.idft = IDFT(n_fft//2**n_stages)
 
         if n_stages > 0:
             self.permuter = FFTPermuter(n_fft, n_stages)
             self.twiddle_stages = nn.ModuleList()
 
             stage_size = n_fft//2**(n_stages-1)
             num_calls = n_fft//stage_size
             for _ in range(n_stages):
-                twiddle_list = nn.ModuleList()
-                for j in range(min(num_calls, self.twiddle_parallelism)):
-                    twiddle_list.append(FFTwiddle(stage_size, inv=True))
-                self.twiddle_stages.append(twiddle_list)
+                self.twiddle_stages.append(FFTwiddle(stage_size, inv=True))
                 stage_size = stage_size * 2
                 num_calls = num_calls//2
             
             self.normalizer = IFFTNormalizer(n_fft, n_stages)
 
         else:
             self.permuter = None
             self.twiddle_stages = None
 
         # quantization configs
         self.observe: bool = False
         self.quantize: bool = False
 
-    @torch.jit.ignore
     def forward(self, re, im):
 
         if self.permuter is None:
-            return self.idfts[0](re, im)
+            return self.idft(re, im)
 
         else:
             re_perms = self.permuter(re)
             im_perms = self.permuter(im)
 
             sub_dfts = [[],[],[],[]] # stores even_real, even_imag, odd_real, odd_imag sets
 
             for j, (x_even_re, x_odd_re, x_even_im, x_odd_im) in enumerate(zip(
                 re_perms[::2], re_perms[1::2], im_perms[::2], im_perms[1::2])):
 
-                ev_r, ev_i = self.idfts[2*j % self.dft_parallelism](x_even_re, x_even_im)
-                od_r, od_i = self.idfts[(2*j+1) % self.dft_parallelism](x_odd_re, x_odd_im)
+                ev_r, ev_i = self.idft(x_even_re, x_even_im)
+                od_r, od_i = self.idft(x_odd_re, x_odd_im)
 
                 sub_dfts[0].append(ev_r)
                 sub_dfts[1].append(ev_i)
                 sub_dfts[2].append(od_r)
                 sub_dfts[3].append(od_i)
 
-            for twiddle_list in self.twiddle_stages:
+            for twiddler in self.twiddle_stages:
                 new_sub_dfts = [[],[],[],[]]
-                for i, (ev_r, ev_i, od_r, od_i) in enumerate(zip(*sub_dfts)):
-                    twiddler = twiddle_list[i%len(twiddle_list)]
+                for i, (ev_r, ev_i, od_r, od_i) in enumerate(zip(sub_dfts[0], sub_dfts[1], sub_dfts[2], sub_dfts[3])):
                     real, imag = twiddler(ev_r, ev_i, od_r, od_i)
                     if i % 2 == 0:
                         new_sub_dfts[0].append(real)
                         new_sub_dfts[1].append(imag)
                     else:
                         new_sub_dfts[2].append(real)
                         new_sub_dfts[3].append(imag)
```

## fmot/nn/signal_processing.py

```diff
@@ -344,7 +344,32 @@
         super().__init__()
         weight = get_mel_matrix(sr, n_fft, n_mels, fmin, fmax, **kwargs)
         self.weight = nn.Parameter(weight.t(), requires_grad=False)
 
     def forward(self, x):
         """"""
         return torch.matmul(x, self.weight)
+
+class MelTranspose(nn.Linear):
+    r"""
+    Project Mel-Frequency bins back into FFT bins.
+
+    Args:
+        sr (int): audio sampling rate (in Hz)
+        n_fft (int): number of FFT frequencies
+        n_mels (int): number of mel-frequencies to create
+        fmin (float): lowest frequency (in Hz), default is 0
+        fmax (float): maximum frequency (in Hz). If :attr:`None`, the Nyquist frequency
+            :attr:`sr/2.0` is used. Default is :attr:`None`.
+
+    Shape:
+        - Input: :math:`(*, C_{in})` where :math:`*` is any number of dimensions and
+          :math:`C_{in} = \lfloor \text{n_dft}/2 + 1 \rfloor`
+        - Output: :math:`(*, \text{n_mels})`
+    """
+    def __init__(self, sr, n_fft, n_mels, fmin=0.0, fmax=None):
+        super().__init__(
+            out_features=n_fft//2 + 1,
+            in_features=n_mels,
+            bias=False)
+        mat = get_mel_matrix(sr, n_fft, n_mels, fmin, fmax).T
+        self.weight = nn.Parameter(mat, requires_grad=False)
```

## fmot/qat/nn/atomics.py

```diff
@@ -573,21 +573,24 @@
             i += 1
 
 class Cat(nn.Module):
     def __init__(self, dim, bitwidth, observer):
         super().__init__()
         self.cat = BareCat(dim)
         self.requantizers = nn.ModuleList()
+        self.q_group = quantizers.PrecisionConstraint()
         self._built = False
         self._obs = observer
         self._bw = bitwidth
 
     def _build(self, N):
         for __ in range(N):
-            self.requantizers.append(Requantize(self._bw, self._obs))
+            req = Requantize(self._bw, self._obs)
+            self.requantizers.append(req)
+            self.q_group.recursively_add(req)
         quantizers.share_observer(self.requantizers)
         self._built = True
 
     @check_for_annotations
     def forward(self, tensors):
         if not self._built:
             N = len(tensors)
```

## fmot/qat/nn/composites.py

```diff
@@ -192,29 +192,32 @@
 
 class TuningEpsilon(nn.Module):
     def __init__(self, bitwidth, running_max=0, observer=quantizers.DEFAULT_OBSERVERS['default'],
         eps=1/128, alpha=.99):
         super().__init__()
         self.eps = eps
         self.alpha = alpha
-        self.running_max = running_max
+        self.register_buffer('running_max', torch.tensor(running_max))
         self.add = atomics.VIAdd(self.epsilon(), bitwidth, observer=observer)
     
     @torch.jit.ignore()
     def epsilon(self):
         return self.running_max * self.eps
 
     @torch.jit.ignore()
     @torch.no_grad()
     def update(self, x):
-        xmax = torch.max(x).cpu().item()
-        if self.running_max == 0:
-            self.running_max = xmax
-        else:
-            self.running_max = self.alpha * self.running_max + (1-self.alpha) * xmax
+        """ Updates the running max during training
+        """
+        if self.training:
+            xmax = torch.max(x).detach()
+            if self.running_max == 0:
+                self.running_max = xmax
+            else:
+                self.running_max = self.alpha * self.running_max + (1-self.alpha) * xmax
 
     def forward(self, x):
         self.update(x)
         self.add.imm.data = torch.ones_like(self.add.imm.data)*self.epsilon()
         return self.add(x)
 
     @classmethod
```

## fmot/test/test_features.py

```diff
@@ -365,10 +365,22 @@
     #     assert(table['model.model.1.weight']['density'] == 1.)
     #     assert(table['model.model.1.weight']['memory'] == 8 * 8 / 8) # nb_param * bitwidth / 8
     #     prune.l1_unstructured(cmodel.model.model[1], 'weight', 0.5)
     #     table = cmodel.get_parameter_table()
     #     assert(table['model.model.1.weight']['density'] == .5)
     #     assert(table['model.model.1.weight']['memory'] == 4.0)
 
+    def test_tuneps_eval(self):
+        """ Tests that TuningEpsilon running_mean only gets updated during training.
+        """
+        tuneps = fmot.nn.TuningEpsilon(eps=0.25)
+        input = torch.tensor([8, 8, 8])
+        _ = tuneps(input)
+        assert(tuneps.epsilon() == 2.)
+        tuneps.eval()
+        _ = tuneps(torch.tensor([10, 10, 10]))
+        assert (tuneps.epsilon() == 2.)
+
+
 if __name__ == "__main__":
     test = TestFeatures()
     test.test_logic_param2quant()
```

## fmot/test/test_serialization.py

```diff
@@ -50,7 +50,17 @@
         cmodel_orig = ConvertedModel(model_orig, batch_dim=0)
         quant_inputs = [torch.randn(1, 3) for _ in range(2)]
         cmodel_orig.quantize(quant_inputs)
         pretrained_dict = cmodel_orig.state_dict()
 
         cmodel = ConvertedModel(model_orig, batch_dim=0)
         self.assertRaises(Exception, fmot.load_state_dict, cmodel, pretrained_dict)
+
+    def test_tuneps(self):
+        r''' Tests that TuningEpsilon running max appears in the state dict
+        '''
+        tuneps = fmot.nn.TuningEpsilon(eps=0.25)
+        input = torch.tensor([8, 8, 8])
+        with torch.no_grad():
+            _ = tuneps(input)
+        assert('running_max' in tuneps.state_dict().keys())
+        assert(tuneps.epsilon() == 2.)
```

## Comparing `fmot-1.5.6.dist-info/METADATA` & `fmot-1.5.8.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: fmot
-Version: 1.5.6
+Version: 1.5.8
 Summary: Femtosense Model Optimization Toolkit
 Home-page: https://github.com/femtosense/fmot
 Author: Femtosense
 Author-email: info@femtosense.ai
 Project-URL: Source, https://github.com/femtosense/fmot
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
```

## Comparing `fmot-1.5.6.dist-info/RECORD` & `fmot-1.5.8.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 fmot/ENV_REQUIREMENTS.sh,sha256=qMNutpKLRCQMv70HRJhShmXAqDr_wic-NIn5kponsZI,1355
 fmot/PY_REQUIREMENTS,sha256=z2FbJTW3SjQ0E2dCkcq0KPEm_BNn8ndfhHbief-E2Bk,138
-fmot/VERSION,sha256=2rZOBsCBfXe0-IcmGDLLs0ugI8rHx01y2BBoVTZS0MU,6
+fmot/VERSION,sha256=9teVmEct-Ay1LJI-CQnJUsTAZ7uL62gwMEbPtj7X6tg,6
 fmot/__init__.py,sha256=RjjwgFf7ZUWZxzWp23xViTOM3kfLXjsqWjO8-oAxsUw,648
 fmot/_open_docs.py,sha256=jdnE5Bokv9I7byUaw0fNRq6RqMCD_g4pPKmgBUDSIW4,414
 fmot/_supported_ops.py,sha256=_nWGLwheCOoDAW0hUMiNWX_V7C7q0ROHJWz4ojK9YJw,1611
 fmot/configure.py,sha256=fjW_mDYAmrOSrpd9cNDOJH0NEXRnGhvScKfkCwUSNkI,1654
 fmot/exceptions.py,sha256=k_hnhQvLqQzELA5yV8aDDnik6CxJkAXmfzeHbnaV3mc,929
 fmot/execute_code.py,sha256=eW227iyCxiDMKjCK4ruK5-vd_tYNrgL1a7KkRNJ1Q_U,1821
 fmot/functional.py,sha256=DH98rAmladME98oV5QNtiKicWbMVANGXJgSXGVYb548,61
-fmot/torchscript_utils.py,sha256=lca5a5S3-Wzp-_Lvfp6kHanFc48yG2Qagio3PbpM1jI,4945
+fmot/torchscript_utils.py,sha256=f9gJJ-g6XxCH99x4bSHGhgqBpYzGO3JJKqbFjuuR1ig,8975
 fmot/beta/__init__.py,sha256=LRcqC5g3JZ7ejVWFpadlnU_noRqkKPnDzCku_nq9vww,83
 fmot/beta/amp.py,sha256=OGJAi1h5gLWziv_kk8k9XpFYGRy0v2fRIZYz_0nzjuk,4169
 fmot/beta/auto_multiprecision.py,sha256=BbtXHvvZKeAIw6nu5NDGhk_umUAE7-8CVl9xn8lfkpE,10740
 fmot/beta/quantize_part.py,sha256=YgxUqZnUZCVFCW0Y75vP0Tr3cbmiI8QhXPWn2cT1iEc,1166
 fmot/convert/__init__.py,sha256=WAoTNbpBhUZz1QSCvGLGvv2g0Eyy1tjsYYsofYxCBNQ,200
 fmot/convert/_convert_to_qat.py,sha256=yNhnn3sz2JTQElnipPzFAK8Ba6qHE7LDcGs0JFU9L6w,2900
 fmot/convert/conversion_api.py,sha256=LugKItwCxLIANvVidfZSRqXad44-2o1mertYNkBZriw,15168
@@ -20,15 +20,15 @@
 fmot/convert/default_patchings.py,sha256=I7s_UHPIsCZvKy-cSmjxTDFg6rqeyKnpK6Vgm4k8ym0,9684
 fmot/convert/default_substitutions.py,sha256=NNMuIMLdzYaWG8KsLRfxgIp9ckptDMcfl08BB3m_abo,547
 fmot/convert/lut_registry.py,sha256=aTOnVsCoeWHorT9P7PyVuGNHc1qNzTMdqWO-NYt7Ezg,4562
 fmot/convert/mapping.py,sha256=XgZDs5MrG1UXwPpkcF-yg0lE3TC-UYdislOzr6my9fs,3998
 fmot/convert/optimizer.py,sha256=VOLVJTJTVdv3vznN0qIAyErzVFD6V36jLJqhe0naxqw,2831
 fmot/convert/param_manager.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fmot/convert/patch_ir.py,sha256=Y29mfiw33ebIBRtaElkiZTwl-UbI92kDedro_73-wxM,24680
-fmot/convert/patching.py,sha256=Nmp9DEfjCnyWfFpIfSLW63Tmf3i45vkLIeYZBr90AVA,4175
+fmot/convert/patching.py,sha256=td05vg1rmCYy9Rt9cVioZrtj2A1DWuKrN7dwe75hIXc,4402
 fmot/convert/prune_reparametrization.py,sha256=z_cDRGmCuffym2YNvtz8cifQSWKxSWdU0c3-g4L4L44,3244
 fmot/convert/quantizer_manager.py,sha256=8vCSWq-u_GT1xk4gFV4_W_wP7rlWkeSa7FaMGWuJROU,2963
 fmot/convert/substitution.py,sha256=XHx-XSdBFBgkdUwlnGYun7w8j9akzo1A3AQnzoZ3EUQ,2121
 fmot/fqir/__init__.py,sha256=FBYLaAL1u5sgX05hVkhnmdEpXb6laAYxllKybjjv0hg,199
 fmot/fqir/utils.py,sha256=nz_dstBc5nRB2Qx1q8-c-eQ_jNlKUU0VwTUEhPzxxNM,660
 fmot/fqir/graph_proto/__init__.py,sha256=Z8uP9WEhcJk5RvoZEVYruJVL_JT4ZgZev3kygnXboDs,35
 fmot/fqir/graph_proto/graph_proto.py,sha256=OYnujquygeIAR8p3_SV3xMefNRCW1NNXzWBM7JBRuBc,13950
@@ -49,38 +49,38 @@
 fmot/fqir/passes/kernelize_red_broad.py,sha256=Y9TqT0dVra4p6IpjS78QP7gHOM4cwTkRPpkXYAaTYBc,5496
 fmot/fqir/passes/kernelize_temporal_unfold.py,sha256=ihZcChdCiwC18coYPPnXymx9p2wgWEmIPhj2zTT3qCg,2039
 fmot/fqir/variables/__init__.py,sha256=24krQW55auJPC_q0hoV71-RD9AOWWiaWyOnazLkrtMM,94
 fmot/fqir/variables/tensor_proto.py,sha256=EvQwq6KJOfx8D1HBTRxio9sEsy71Ny_kkcpARmp9-XQ,6255
 fmot/fqir/variables/variable_base.py,sha256=4d1UdjK0IU4MIaNZqzW7TO3ePZwdZHAURakw5otuB60,2064
 fmot/nn/__init__.py,sha256=TI8l3MAV6pa-l3_oHVQ0az0hTES9xgmuPb7JUjOI68g,563
 fmot/nn/atomics.py,sha256=E_FxbKKigyz06vydochdiBRgzyZk8KA-i-fvl9B5xic,8663
-fmot/nn/composites.py,sha256=MkjKot4gqbmOdWDfH5qVkizq3_grqv_2Un5lMUAsqXc,12272
+fmot/nn/composites.py,sha256=TxbLKHe5aQBGCQJllKu3mYk3UZvu_c_TlwOf0hOik78,12410
 fmot/nn/conv1d.py,sha256=nPPLo_S2GYHLJhsKbaE0t6QVzSt92I3lnop3uFiSPvk,27568
 fmot/nn/derived_param.py,sha256=wLgyVPScJp0VHsHWRNq5TtgShES2vjZPQDlYtCNZEAo,1844
 fmot/nn/femtornn.py,sha256=T7XtDFWoURSKCMfMBZfJp1WablYNxvbxSNWBzEAdG0M,21993
-fmot/nn/fft.py,sha256=J4Rlcko7K_sprpeIT5ptniXzyjsLPEkoqAnzzPNNPhc,17526
+fmot/nn/fft.py,sha256=eOQyQTcAAIbHsJTtMbyU7yJXC01LNhhde-szUMQriWg,16064
 fmot/nn/sequenced_rnn.py,sha256=yO6YpwSyQdxBCELHGFVILFNZmHlp5njYxHOCXW9yKao,34238
 fmot/nn/sequencer.py,sha256=9gCgumIvuO71PajoEfcZbi9XGTzqywTDNeEn62xKTP8,5305
-fmot/nn/signal_processing.py,sha256=_eghECb8TGkaUeLznM9vmCO0MkQNNGVebCUUHexHBqw,11986
+fmot/nn/signal_processing.py,sha256=NtDXtTzEJkTT8MNuXoQLOA582H4n_tWr56hJmBoLTfk,12970
 fmot/nn/sliding_attention.py,sha256=Bw-2gno_1IExLnmGqiNVIs2_Ee4z4mDRdevxyHp9B24,3742
 fmot/nn/sparsifiers.py,sha256=iXrs6RgTThGDeXGrEqQzI4cC-VxPdzUdIEKqe9tApFc,9256
 fmot/nn/sru.py,sha256=etnDGB85LxrJsDvN9gecAdOuRB5J2p96dfYIz9tuAos,2294
 fmot/nn/stft.py,sha256=ycbGfbzDXqCmdjZ1_9f-iPJRHJISMefiJos9f_AtZns,8916
 fmot/nn/super_structures.py,sha256=bzPhxlmmHlgRixm7-BvhPcSnyBvty4UCr1qFLLXf1UI,1525
 fmot/nn/temporal_unfold.py,sha256=uE87sL-CEHcwMmsRcIB_epZyort3AoZG3nz1lt8tp98,1430
 fmot/qat/__init__.py,sha256=GWbiQ7uKgJxaWK4_62Ksx44I-sHWFxH-UKpmGlaAoU4,192
 fmot/qat/annotated_tensors.py,sha256=pKNpd0ujdz27eMy1vrGZDFwdTWC_RHxdw1pnCXVpTnc,4059
 fmot/qat/bitwidths.py,sha256=SONkjQsx0DAB9voBgewMFGbDbx58b5nQou6-8t3GErI,1659
 fmot/qat/control.py,sha256=zqirgQTZraD2xmiBTJh6rOiVDI-CuMy7n2Wuu0ji8lk,4381
 fmot/qat/fake_quantization.py,sha256=jzQJT-FKpg7DHMT0VfsfG2fJ3oOSbRVqxF95Lnafypc,2684
 fmot/qat/nn/__init__.py,sha256=MHRSPGRPFuHpdnt75uzJAnBx7NsFvvPDR3_cnnfKSR8,328
 fmot/qat/nn/_utils.py,sha256=ywQLkY7SpR7ES8r07XBHXFVzQOKMcSfMRqmD1t_9q6s,276
 fmot/qat/nn/act_density.py,sha256=zF7GCnKfopSfKEFIXI79W5z9C_X4uVTldgMXTdXJPho,3783
-fmot/qat/nn/atomics.py,sha256=2SSK6J_Eikqc5w2AUXSuxQ4bdm7hxXBGEEAWKBrEhTw,28153
-fmot/qat/nn/composites.py,sha256=ZJQoJXGdwMWSI63NJCrb013dV5yDkJvPzK0ZiwTZozE,9198
+fmot/qat/nn/atomics.py,sha256=jIDP-2juON1Ec29OkMosWy6nbMc57_v4zXK0vcfpa6g,28277
+fmot/qat/nn/composites.py,sha256=Gr44RCwrZ3EOjcXzvwaEnXsUdbJnOmaPCuH6QhQYHWU,9336
 fmot/qat/nn/density_matmul.py,sha256=hH7L4s2xg5ZBzV4IWdleW-PlBuJW29V8k04y44yjaGw,10828
 fmot/qat/nn/derived_param.py,sha256=0dnjLzUedGM4ib_1gLAmvHK6446OHq2qyKjKn86vqhA,2102
 fmot/qat/nn/linear.py,sha256=BeQh3-GAnrEVpuWuPjfxE_xe4IRuVhv_GIL02R-kRmU,8923
 fmot/qat/nn/lstm.py,sha256=KIZXGCVp9rcjWilX44KKnJvtRpxBHMbkGwa-mV6gkek,19303
 fmot/qat/nn/luts.py,sha256=FLAV9AdxqqfkAgi6iohuEJFumGiCQq1us4gs6lo_Nsk,14400
 fmot/qat/nn/norm.py,sha256=2BJHcjZO-q9S0kAxlV3uc6lGUj8ZaaSS1tdHLzNcxus,3922
 fmot/qat/nn/quant_wrap.py,sha256=QzS6bOdeOgk6RjG1gXVzSECdEEEQfOLfKAtnFOs_ib4,1350
@@ -99,27 +99,27 @@
 fmot/test/test_automp.py,sha256=ySoNtmNCuwJNhgi1vX1iF10ftmFePyo7-GE-rpLakws,936
 fmot/test/test_cqt.py,sha256=XviR6eHxQ9WC72RnhbYqW4GnGoXQWodEbo4Oez1QIR0,1373
 fmot/test/test_cuda.py,sha256=Oa8KXn7QFiJoW5eOyOxapBq87keAx3NH2UQWMwdC4rs,1688
 fmot/test/test_density_tracking.py,sha256=jYI2VZm6JNnxd8SWc4Se7GxDWT6hUPNIjDZfIAna4Bg,3878
 fmot/test/test_diagnosis.py,sha256=13dWgNzssiZZrolmPwvGq-DVobIxKgDun3sX135kCkU,1134
 fmot/test/test_dim_anno.py,sha256=ZKVJo9M2bHlIquxXEfPivXG1ieqzsodzjzwZ5ow6_qA,3433
 fmot/test/test_dropout.py,sha256=0XgNwJ609WHtLrXzqp8PSNZGYjZ9ntaRbATcorJK9NI,958
-fmot/test/test_features.py,sha256=nEunkALbXEqkWLiYJU-SHGV7CdNjNN53jDRffR2BhRY,14304
+fmot/test/test_features.py,sha256=LiqkjaZnuWj3IdkZjg2h3uKUKZLdy8OLPh0ePOS1GUs,14698
 fmot/test/test_fft.py,sha256=4TsK2dMlDEEjc9PS1mf-qQbXRRjbz6e_1fd233Hu3b8,722
 fmot/test/test_ifft.py,sha256=FWbgSLr2S-yymWMyn3Tjme7FsCP6DqwaRk00tvE4ymk,1664
 fmot/test/test_layernorm.py,sha256=XPCzKKfeOZW2wwyw7AWHbBFEwk-9cFBhJhGzjl5wi3A,378
 fmot/test/test_lut_grads.py,sha256=4vsB6UvrGSXLO_f4j0KRxvhckXeNarbQuhqSpWyEXVc,1412
 fmot/test/test_nn_stft.py,sha256=Mr1okpY15KvD9HHiZHde5rmd8XKbxf38_jA5UObJd3A,3581
 fmot/test/test_observers.py,sha256=brKTwKQTJUhJgcbxWZRz3QhtoEvmrNwI49XfgMlqikc,2347
 fmot/test/test_package.py,sha256=nGhN_t6QcGI0YYX_bAPPPLO5AmVId3mpJzYhw-jmS7g,310
 fmot/test/test_param_reuse.py,sha256=nq4rU4vQIccL8uYOC7APtOjAGPl1Q2jLUARxmi-LndA,1240
 fmot/test/test_precision_mod.py,sha256=FCKgaUfN6Yw9YN8eOMQQGPb9rpv0q2K0f0BqtdvKCgE,830
 fmot/test/test_pruning.py,sha256=by3oUaiX4fBOLFo2vyIP4wfVqkAwbr6newGx15JGmUA,6440
 fmot/test/test_sequencers.py,sha256=Emw6kM_9KqKByGMfaYcauglVS9IH5Sbgu0_5cicVs4k,25171
-fmot/test/test_serialization.py,sha256=WHKtFFXeuVcpk3thhlfFGZMUEqQ20afh2uFcFkAjnkU,2017
+fmot/test/test_serialization.py,sha256=QB1QH9nGFqzkrnloxyPDxpTkZh47-Hvj7QWLBlZF7ik,2381
 fmot/test/test_stft.py,sha256=VOGUKDZFj47GH0FZG-6NHdN7qD9b-GmStRPKMaCAIwk,4308
 fmot/test/test_temporal_unfold.py,sha256=ONAuazo-2BGjgfdVA-nJjrpmAUG38PbmXBh1kzW9VWA,1322
 fmot/test/test_fqir/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fmot/test/test_fqir/test_opchecks.py,sha256=iXzfg9BCv8NqFh5faSLnbj78LqXxZ3Hwl9Fb8SOdhG8,1657
 fmot/test/test_fqir/test_readme.py,sha256=IJIxJtkoUC3ApIeyyUQTzLv9iBLuU99VkpeYKCNBkgE,1138
 fmot/test/test_fqir/test_single_ops.py,sha256=v0_loWxEUsBWmc-mEa4ohq6-t9iiEJT_0vBfzZE6EeE,15396
 fmot/test/utm/__init__.py,sha256=pgT5r5mbG89As3Q1YGTDB2wI3qcVk8FyNwpGqdpDYjQ,147
@@ -147,12 +147,12 @@
 fmot/utils/quant_diagnostic.py,sha256=lafkYJV-51mHOE5lw7sE9x-4hExHIRpSqUrhSRugY9E,6744
 fmot/utils/quantizer_manager.py,sha256=bBicDeeeMrd8Hq4wXN92-53579j1-3feSqkbUTYzFoQ,1792
 fmot/utils/rich_attr.py,sha256=olbVQ0HCx41zcwwvAQUo9yt1PYMia3rUqd0j8XHDcMQ,357
 fmot/utils/saturation_opt.py,sha256=uiG3UPuAWq_ehA_lwRh-bVWPLIDam-cBifkpnxNKapA,8372
 fmot/utils/serialization.py,sha256=w_kYDaV_K84RYf9hGuLuhwiV-0s_OBaBIio8t2lNqU4,1986
 fmot/utils/quant_tools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 fmot/utils/quant_tools/diagnosis.py,sha256=Er7vE0XzNv2s8LkTSdeE1h4lzbLCOaax4XinIqm7pU0,4545
-fmot-1.5.6.dist-info/LICENSE,sha256=eN9ZI1xHjUmFvN3TEeop5kBGXRUBfbsl55KBNBYYFqI,36
-fmot-1.5.6.dist-info/METADATA,sha256=yDQSAPnrzBhCLt2HVtl9Ly-yHg42p2GFnT2CWr99wfM,3668
-fmot-1.5.6.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-fmot-1.5.6.dist-info/top_level.txt,sha256=y0zAp-_TLm8xLv-iKiAhM181teNK64s7Zvp2oshNxY8,5
-fmot-1.5.6.dist-info/RECORD,,
+fmot-1.5.8.dist-info/LICENSE,sha256=eN9ZI1xHjUmFvN3TEeop5kBGXRUBfbsl55KBNBYYFqI,36
+fmot-1.5.8.dist-info/METADATA,sha256=KtdBM-FT9UcHfcQKBwV-WCGIkffJWUPEfa0PU2IBPuQ,3668
+fmot-1.5.8.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+fmot-1.5.8.dist-info/top_level.txt,sha256=y0zAp-_TLm8xLv-iKiAhM181teNK64s7Zvp2oshNxY8,5
+fmot-1.5.8.dist-info/RECORD,,
```

